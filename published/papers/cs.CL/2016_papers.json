[
    {
        "url": "https://arxiv.org/abs/1601.00087",
        "title": "Sentiment/Subjectivity Analysis Survey for Languages other than English",
        "authors": [
            "Mohammed Korayem",
            "Khalifeh Aljadda",
            "David Crandall"
        ],
        "abstract": "Subjective and sentiment analysis have gained considerable attention recently. Most of the resources and systems built so far are done for English. The need for designing systems for other languages is increasing. This paper surveys different ways used for building systems for subjective and sentiment analysis for languages other than English. There are three different types of systems used for building these systems. The first (and the best) one is the language specific systems. The second type of systems involves reusing or transferring sentiment resources from English to the target language. The third type of methods is based on using language independent methods. The paper presents a separate section devoted to Arabic sentiment analysis.\n    ",
        "submission_date": "2016-01-01T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00248",
        "title": "Contrastive Entropy: A new evaluation metric for unnormalized language models",
        "authors": [
            "Kushal Arora",
            "Anand Rangarajan"
        ],
        "abstract": "Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric.\n    ",
        "submission_date": "2016-01-03T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00372",
        "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation",
        "authors": [
            "Jiwei Li",
            "Dan Jurafsky"
        ],
        "abstract": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.\n    ",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00620",
        "title": "Distant IE by Bootstrapping Using Lists and Document Structure",
        "authors": [
            "Lidong Bing",
            "Mingyang Ling",
            "Richard C. Wang",
            "William W. Cohen"
        ],
        "abstract": "Distant labeling for information extraction (IE) suffers from noisy training data. We describe a way of reducing the noise associated with distant IE by identifying coupling constraints between potential instance labels. As one example of coupling, items in a list are likely to have the same label. A second example of coupling comes from analysis of document structure: in some corpora, sections can be identified such that items in the same section are likely to have the same label. Such sections do not exist in all corpora, but we show that augmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task.\n    ",
        "submission_date": "2016-01-04T00:00:00",
        "last_modified_date": "2016-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00710",
        "title": "Multi-Source Neural Translation",
        "authors": [
            "Barret Zoph",
            "Kevin Knight"
        ],
        "abstract": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00770",
        "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
        "authors": [
            "Makoto Miwa",
            "Mohit Bansal"
        ],
        "abstract": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00893",
        "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings",
        "authors": [
            "Oren Melamud",
            "David McClosky",
            "Siddharth Patwardhan",
            "Mohit Bansal"
        ],
        "abstract": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2017-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01073",
        "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
        "authors": [
            "Orhan Firat",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01085",
        "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model",
        "authors": [
            "Trevor Cohn",
            "Cong Duy Vu Hoang",
            "Ekaterina Vymolova",
            "Kaisheng Yao",
            "Chris Dyer",
            "Gholamreza Haffari"
        ],
        "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01195",
        "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "This paper discusses the experiments carried out by us at Jadavpur University as part of the participation in ICON 2015 task: POS Tagging for Code-mixed Indian Social Media Text. The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information from dictionary as well as some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for Bengali-English, Hindi-English and Tamil-English Language pairs. Our system has been trained and tested on the datasets released for ICON 2015 shared task: POS Tagging For Code-mixed Indian Social Media Text. In constrained mode, our system obtains average overall accuracy (averaged over all three language pairs) of 75.60% which is very close to other participating two systems (76.79% for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In unconstrained mode, our system obtains average overall accuracy of 70.65% which is also close to the system (72.85% for AMRITA_CEN) which obtains the highest average overall accuracy.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01272",
        "title": "Recurrent Memory Networks for Language Modeling",
        "authors": [
            "Ke Tran",
            "Arianna Bisazza",
            "Christof Monz"
        ],
        "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01280",
        "title": "Language to Logical Form with Neural Attention",
        "authors": [
            "Li Dong",
            "Mirella Lapata"
        ],
        "abstract": "Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01343",
        "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
        "authors": [
            "Ikuya Yamada",
            "Hiroyuki Shindo",
            "Hideaki Takeda",
            "Yoshiyasu Takefuji"
        ],
        "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.\n    ",
        "submission_date": "2016-01-06T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01530",
        "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling",
        "authors": [
            "Gakuto Kurata",
            "Bing Xiang",
            "Bowen Zhou",
            "Mo Yu"
        ],
        "abstract": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01705",
        "title": "Learning to Compose Neural Networks for Question Answering",
        "authors": [
            "Jacob Andreas",
            "Marcus Rohrbach",
            "Trevor Darrell",
            "Dan Klein"
        ],
        "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01887",
        "title": "Research Project: Text Engineering Tool for Ontological Scientometry",
        "authors": [
            "Rustam Tagiew"
        ],
        "abstract": "The number of scientific papers grows exponentially in many disciplines. The share of online available papers grows as well. At the same time, the period of time for a paper to loose at chance to be cited anymore shortens. The decay of the citing rate shows similarity to ultradiffusional processes as for other online contents in social networks. The distribution of papers per author shows similarity to the distribution of posts per user in social networks. The rate of uncited papers for online available papers grows while some papers 'go viral' in terms of being cited. Summarized, the practice of scientific publishing moves towards the domain of social networks. The goal of this project is to create a text engineering tool, which can semi-automatically categorize a paper according to its type of contribution and extract relationships between them into an ontological database. Semi-automatic categorization means that the mistakes made by automatic pre-categorization and relationship-extraction will be corrected through a wikipedia-like front-end by volunteers from general public. This tool should not only help researchers and the general public to find relevant supplementary material and peers faster, but also provide more information for research funding agencies.\n    ",
        "submission_date": "2016-01-08T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02166",
        "title": "Empirical Gaussian priors for cross-lingual transfer learning",
        "authors": [
            "Anders S\u00f8gaard"
        ],
        "abstract": "Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-of-speech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $k$ languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the $k$ source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation.\n    ",
        "submission_date": "2016-01-09T00:00:00",
        "last_modified_date": "2016-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02403",
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "authors": [
            "Ivan Habernal",
            "Iryna Gurevych"
        ],
        "abstract": "The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people's argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02431",
        "title": "The Effects of Age, Gender and Region on Non-standard Linguistic Variation in Online Social Networks",
        "authors": [
            "Claudia Peersman",
            "Walter Daelemans",
            "Reinhild Vandekerckhove",
            "Bram Vandekerckhove",
            "Leona Van Vaerenbergh"
        ],
        "abstract": "We present a corpus-based analysis of the effects of age, gender and region of origin on the production of both \"netspeak\" or \"chatspeak\" features and regional speech features in Flemish Dutch posts that were collected from a Belgian online social network platform. The present study shows that combining quantitative and qualitative approaches is essential for understanding non-standard linguistic variation in a CMC corpus. It also presents a methodology that enables the systematic study of this variation by including all non-standard words in the corpus. The analyses resulted in a convincing illustration of the Adolescent Peak Principle. In addition, our approach revealed an intriguing correlation between the use of regional speech features and chatspeak features.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02502",
        "title": "Trans-gram, Fast Cross-lingual Word-embeddings",
        "authors": [
            "Jocelyn Coulmance",
            "Jean-Marc Marty",
            "Guillaume Wenzek",
            "Amine Benhalloum"
        ],
        "abstract": "We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02539",
        "title": "Investigating gated recurrent neural networks for speech synthesis",
        "authors": [
            "Zhizheng Wu",
            "Simon King"
        ],
        "abstract": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02543",
        "title": "Evaluating the Performance of a Speech Recognition based System",
        "authors": [
            "Vinod Kumar Pandey",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Speech based solutions have taken center stage with growth in the services industry where there is a need to cater to a very large number of people from all strata of the society. While natural language speech interfaces are the talk in the research community, yet in practice, menu based speech solutions thrive. Typically in a menu based speech solution the user is required to respond by speaking from a closed set of words when prompted by the system. A sequence of human speech response to the IVR prompts results in the completion of a transaction. A transaction is deemed successful if the speech solution can correctly recognize all the spoken utterances of the user whenever prompted by the system. The usual mechanism to evaluate the performance of a speech solution is to do an extensive test of the system by putting it to actual people use and then evaluating the performance by analyzing the logs for successful transactions. This kind of evaluation could lead to dissatisfied test users especially if the performance of the system were to result in a poor transaction completion rate. To negate this the Wizard of Oz approach is adopted during evaluation of a speech system. Overall this kind of evaluations is an expensive proposition both in terms of time and cost. In this paper, we propose a method to evaluate the performance of a speech solution without actually putting it to people use. We first describe the methodology and then show experimentally that this can be used to identify the performance bottlenecks of the speech solution even before the system is actually used thus saving evaluation time and expenses.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02553",
        "title": "Environmental Noise Embeddings for Robust Speech Recognition",
        "authors": [
            "Suyoun Kim",
            "Bhiksha Raj",
            "Ian Lane"
        ],
        "abstract": "We propose a novel deep neural network architecture for speech recognition that explicitly employs knowledge of the background environmental noise within a deep neural network acoustic model. A deep neural network is used to predict the acoustic environment in which the system in being used. The discriminative embedding generated at the bottleneck layer of this network is then concatenated with traditional acoustic features as input to a deep neural network acoustic model. Through a series of experiments on Resource Management, CHiME-3 task, and Aurora4, we show that the proposed approach significantly improves speech recognition accuracy in noisy and highly reverberant environments, outperforming multi-condition training, noise-aware training, i-vector framework, and multi-task learning on both in-domain noise and unseen noise.\n    ",
        "submission_date": "2016-01-11T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02789",
        "title": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Danijel Kor\u017einek"
        ],
        "abstract": "Re-speaking is a mechanism for obtaining high quality subtitles for use in live broadcast and other public events. Because it relies on humans performing the actual re-speaking, the task of estimating the quality of the results is non-trivial. Most organisations rely on humans to perform the actual quality assessment, but purely automatic methods have been developed for other similar problems, like Machine Translation. This paper will try to compare several of these methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will then be matched to the human-derived NER metric, commonly used in re-speaking.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.02828",
        "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation",
        "authors": [
            "Pawel Swietojanski",
            "Jinyu Li",
            "Steve Renals"
        ],
        "abstract": "This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to a more adaptable DNN acoustic model, working both in a speaker-dependent and a speaker-independent manner, without the requirements to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4) comprising 270 test speakers, we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition, we have investigated the effect of the amount of adaptation data per speaker, the quality of unsupervised adaptation targets, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.\n    ",
        "submission_date": "2016-01-12T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03210",
        "title": "The scarcity of crossing dependencies: a direct outcome of a specific constraint?",
        "authors": [
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "The structure of a sentence can be represented as a network where vertices are words and edges indicate syntactic dependencies. Interestingly, crossing syntactic dependencies have been observed to be infrequent in human languages. This leads to the question of whether the scarcity of crossings in languages arises from an independent and specific constraint on crossings. We provide statistical evidence suggesting that this is not the case, as the proportion of dependency crossings of sentences from a wide range of languages can be accurately estimated by a simple predictor based on a null hypothesis on the local probability that two dependencies cross given their lengths. The relative error of this predictor never exceeds 5% on average, whereas the error of a baseline predictor assuming a random ordering of the words of a sentence is at least 6 times greater. Our results suggest that the low frequency of crossings in natural languages is neither originated by hidden knowledge of language nor by the undesirability of crossings per se, but as a mere side effect of the principle of dependency length minimization.\n    ",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2017-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03288",
        "title": "Predicting the Effectiveness of Self-Training: Application to Sentiment Classification",
        "authors": [
            "Vincent Van Asch",
            "Walter Daelemans"
        ],
        "abstract": "The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.\n    ",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2016-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03313",
        "title": "Political Speech Generation",
        "authors": [
            "Valentin Kassarnig"
        ],
        "abstract": "In this report we present a system that can generate political speeches for a desired political party. Furthermore, the system allows to specify whether a speech should hold a supportive or opposing opinion. The system relies on a combination of several state-of-the-art NLP methods which are discussed in this report. These include n-grams, Justeson & Katz POS tag filter, recurrent neural networks, and latent Dirichlet allocation. Sequences of words are generated based on probabilities obtained from two underlying models: A language model takes care of the grammatical correctness while a topic model aims for textual consistency. Both models were trained on the Convote dataset which contains transcripts from US congressional floor debates. Furthermore, we present a manual and an automated approach to evaluate the quality of generated speeches. In an experimental evaluation generated speeches have shown very high quality in terms of grammatical correctness and sentence transitions.\n    ",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03317",
        "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model",
        "authors": [
            "Shi Feng",
            "Shujie Liu",
            "Mu Li",
            "Ming Zhou"
        ],
        "abstract": "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoder-decoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attention-based encoder-decoder.\n    ",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03348",
        "title": "EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations",
        "authors": [
            "Kayhan Moharreri",
            "Minsu Ha",
            "Ross H Nehm"
        ],
        "abstract": "EvoGrader is a free, online, on-demand formative assessment service designed for use in undergraduate biology classrooms. EvoGrader's web portal is powered by Amazon's Elastic Cloud and run with LightSIDE Lab's open-source machine-learning tools. The EvoGrader web portal allows biology instructors to upload a response file (.csv) containing unlimited numbers of evolutionary explanations written in response to 86 different ACORNS (Assessing COntextual Reasoning about Natural Selection) instrument items. The system automatically analyzes the responses and provides detailed information about the scientific and naive concepts contained within each student's response, as well as overall student (and sample) reasoning model types. Graphs and visual models provided by EvoGrader summarize class-level responses; downloadable files of raw scores (in .csv format) are also provided for more detailed analyses. Although the computational machinery that EvoGrader employs is complex, using the system is easy. Users only need to know how to use spreadsheets to organize student responses, upload files to the web, and use a web browser. A series of experiments using new samples of 2,200 written evolutionary explanations demonstrate that EvoGrader scores are comparable to those of trained human raters, although EvoGrader scoring takes 99% less time and is free. EvoGrader will be of interest to biology instructors teaching large classes who seek to emphasize scientific practices such as generating scientific explanations, and to teach crosscutting ideas such as evolution and natural selection. The software architecture of EvoGrader is described as it may serve as a template for developing machine-learning portals for other core concepts within biology and across other disciplines.\n    ",
        "submission_date": "2016-01-13T00:00:00",
        "last_modified_date": "2016-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03650",
        "title": "Smoothing parameter estimation framework for IBM word alignment models",
        "authors": [
            "Vuong Van Bui",
            "Cuong Anh Le"
        ],
        "abstract": "IBM models are very important word alignment models in Machine Translation. Following the Maximum Likelihood Estimation principle to estimate their parameters, the models will easily overfit the training data when the data are sparse. While smoothing is a very popular solution in Language Model, there still lacks studies on smoothing for word alignment. In this paper, we propose a framework which generalizes the notable work Moore [2004] of applying additive smoothing to word alignment models. The framework allows developers to customize the smoothing amount for each pair of word. The added amount will be scaled appropriately by a common factor which reflects how much the framework trusts the adding strategy according to the performance on data. We also carefully examine various performance criteria and propose a smoothened version of the error count, which generally gives the best result.\n    ",
        "submission_date": "2016-01-14T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03651",
        "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation",
        "authors": [
            "Yan Xu",
            "Ran Jia",
            "Lili Mou",
            "Ge Li",
            "Yunchuan Chen",
            "Yangyang Lu",
            "Zhi Jin"
        ],
        "abstract": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an F1-score of 86.1%, outperforming previous state-of-the-art recorded results.\n    ",
        "submission_date": "2016-01-14T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03764",
        "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
        "authors": [
            "Sanjeev Arora",
            "Yuanzhi Li",
            "Yingyu Liang",
            "Tengyu Ma",
            "Andrej Risteski"
        ],
        "abstract": "Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 \"discourse atoms\" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.\n    ",
        "submission_date": "2016-01-14T00:00:00",
        "last_modified_date": "2018-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03783",
        "title": "Towards Turkish ASR: Anatomy of a rule-based Turkish g2p",
        "authors": [
            "Duygu Altinok"
        ],
        "abstract": "This paper describes the architecture and implementation of a rule-based grapheme to phoneme converter for Turkish. The system accepts surface form as input, outputs SAMPA mapping of the all parallel pronounciations according to the morphological analysis together with stress positions. The system has been implemented in Python\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03896",
        "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures",
        "authors": [
            "Raffaella Bernardi",
            "Ruket Cakici",
            "Desmond Elliott",
            "Aykut Erdem",
            "Erkut Erdem",
            "Nazli Ikizler-Cinbis",
            "Frank Keller",
            "Adrian Muscat",
            "Barbara Plank"
        ],
        "abstract": "Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03916",
        "title": "Multimodal Pivots for Image Caption Translation",
        "authors": [
            "Julian Hitschler",
            "Shigehiko Schamoni",
            "Stefan Riezler"
        ],
        "abstract": "We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04012",
        "title": "Detecting and Extracting Events from Text Documents",
        "authors": [
            "Jugal Kalita"
        ],
        "abstract": "Events of various kinds are mentioned and discussed in text documents, whether they are books, news articles, blogs or microblog feeds. The paper starts by giving an overview of how events are treated in linguistics and philosophy. We follow this discussion by surveying how events and associated information are handled in computationally. In particular, we look at how textual documents can be mined to extract events and ancillary information. These days, it is mostly through the application of various machine learning techniques. We also discuss applications of event detection and extraction systems, particularly in summarization, in the medical domain and in the context of Twitter posts. We end the paper with a discussion of challenges and future directions.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04468",
        "title": "Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation",
        "authors": [
            "Artem Sokolov",
            "Stefan Riezler",
            "Tanguy Urvoy"
        ],
        "abstract": "We present an approach to structured prediction from bandit feedback, called Bandit Structured Prediction, where only the value of a task loss function at a single predicted point, instead of a correct structure, is observed in learning. We present an application to discriminative reranking in Statistical Machine Translation (SMT) where the learning algorithm only has access to a 1-BLEU loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. In our experiment bandit feedback is obtained by evaluating BLEU on reference translations without revealing them to the algorithm. This can be thought of as a simulation of interactive machine translation where an SMT system is personalized by a user who provides single point feedback to predicted translations. Our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning.\n    ",
        "submission_date": "2016-01-18T00:00:00",
        "last_modified_date": "2016-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04580",
        "title": "Nonparametric Bayesian Storyline Detection from Microtexts",
        "authors": [
            "Vinodh Krishnan",
            "Jacob Eisenstein"
        ],
        "abstract": "News events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying storylines requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach. We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP). To ensure efficient linear-time inference, we employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging results: despite using a weak baseline retrieval model, the dd-CRP story clustering method is competitive with the best entries in the 2014 TTG task.\n    ",
        "submission_date": "2016-01-18T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04811",
        "title": "Modeling Coverage for Neural Machine Translation",
        "authors": [
            "Zhaopeng Tu",
            "Zhengdong Lu",
            "Yang Liu",
            "Xiaohua Liu",
            "Hang Li"
        ],
        "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.\n    ",
        "submission_date": "2016-01-19T00:00:00",
        "last_modified_date": "2016-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04908",
        "title": "Graded Entailment for Compositional Distributional Semantics",
        "authors": [
            "Desislava Bankova",
            "Bob Coecke",
            "Martha Lewis",
            "Daniel Marsden"
        ],
        "abstract": "The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning.\n",
        "submission_date": "2016-01-19T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05194",
        "title": "Improved Spoken Document Summarization with Coverage Modeling Techniques",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Berlin Chen",
            "Hsin-Min Wang"
        ],
        "abstract": "Extractive summarization aims at selecting a set of indicative sentences from a source document as a summary that can express the major theme of the document. A general consensus on extractive summarization is that both relevance and coverage are critical issues to address. The existing methods designed to model coverage can be characterized by either reducing redundancy or increasing diversity in the summary. Maximal margin relevance (MMR) is a widely-cited method since it takes both relevance and redundancy into account when generating a summary for a given document. In addition to MMR, there is only a dearth of research concentrating on reducing redundancy or increasing diversity for the spoken document summarization task, as far as we are aware. Motivated by these observations, two major contributions are presented in this paper. First, in contrast to MMR, which considers coverage by reducing redundancy, we propose two novel coverage-based methods, which directly increase diversity. With the proposed methods, a set of representative sentences, which not only are relevant to the given document but also cover most of the important sub-themes of the document, can be selected automatically. Second, we make a step forward to plug in several document/sentence representation methods into the proposed framework to further enhance the summarization performance. A series of empirical evaluations demonstrate the effectiveness of our proposed methods.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05403",
        "title": "Semantic Word Clusters Using Signed Normalized Graph Cuts",
        "authors": [
            "Jo\u00e3o Sedoc",
            "Jean Gallier",
            "Lyle Ungar",
            "Dean Foster"
        ],
        "abstract": "Vector space representations of words capture many aspects of word similarity, but such methods tend to make vector spaces in which antonyms (as well as synonyms) are close to each other. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words which simultaneously capture distributional and synonym relations. We evaluate these clusters against the SimLex-999 dataset (Hill et al.,2014) of human judgments of word pair similarities, and also show the benefit of using our clusters to predict the sentiment of a given text.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05472",
        "title": "Hierarchical Latent Word Clustering",
        "authors": [
            "Halid Ziya Yerebakan",
            "Fitsum Reda",
            "Yiqiang Zhan",
            "Yoshihisa Shinagawa"
        ],
        "abstract": "This paper presents a new Bayesian non-parametric model by extending the usage of Hierarchical Dirichlet Allocation to extract tree structured word clusters from text data. The inference algorithm of the model collects words in a cluster if they share similar distribution over documents. In our experiments, we observed meaningful hierarchical structures on NIPS corpus and radiology reports collected from public repositories.\n    ",
        "submission_date": "2016-01-20T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05647",
        "title": "On Structured Sparsity of Phonological Posteriors for Linguistic Parsing",
        "authors": [
            "Milos Cernak",
            "Afsaneh Asaei",
            "Herv\u00e9 Bourlard"
        ],
        "abstract": "The speech signal conveys information on different time scales from short time scale or segmental, associated to phonological and phonetic information to long time scale or supra segmental, associated to syllabic and prosodic information. Linguistic and neurocognitive studies recognize the phonological classes at segmental level as the essential and invariant representations used in speech temporal organization. In the context of speech processing, a deep neural network (DNN) is an effective computational method to infer the probability of individual phonological classes from a short segment of speech signal. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector. Although the phonological posteriors are estimated at segmental level, we claim that they convey supra-segmental information. Specifically, we demonstrate that phonological posteriors are indicative of syllabic and prosodic events. Building on findings from converging linguistic evidence on the gestural model of Articulatory Phonology as well as the neural basis of speech perception, we hypothesize that phonological posteriors convey properties of linguistic classes at multiple time scales, and this information is embedded in their support (index) of active coefficients. To verify this hypothesis, we obtain a binary representation of phonological posteriors at the segmental level which is referred to as first-order sparsity structure; the high-order structures are obtained by the concatenation of first-order binary vectors. It is then confirmed that the classification of supra-segmental linguistic events, the problem known as linguistic parsing, can be achieved with high accuracy using asimple binary pattern matching of first-order or high-order structures.\n    ",
        "submission_date": "2016-01-21T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05768",
        "title": "Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA",
        "authors": [
            "Daniel Christen"
        ],
        "abstract": "This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky issues of syntactic ambiguity reduction and word sense disambiguation as well as providing innovative and original solutions for constituent generation and constraints management. To provide an insight into how it operates, the system's general architecture and components, as well as its lexical, syntactic and semantic resources are described. After that, the paper addresses the mechanism that performs selective parsing through an interaction between syntactic and semantic information, leading the parser to a coherent and accurate interpretation of the input text.\n    ",
        "submission_date": "2016-01-21T00:00:00",
        "last_modified_date": "2016-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05936",
        "title": "Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic Modeling in Speech Recognition",
        "authors": [
            "Pranay Dighe",
            "Gil Luyet",
            "Afsaneh Asaei",
            "Herve Bourlard"
        ],
        "abstract": "We propose to model the acoustic space of deep neural network (DNN) class-conditional posterior probabilities as a union of low-dimensional subspaces. To that end, the training posteriors are used for dictionary learning and sparse coding. Sparse representation of the test posteriors using this dictionary enables projection to the space of training data. Relying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank, we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions. The enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in both clean and noisy conditions, where upto 15.4% relative reduction in word error rate (WER) is achieved.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05991",
        "title": "Speech vocoding for laboratory phonology",
        "authors": [
            "Milos Cernak",
            "Stefan Benus",
            "Alexandros Lazaridis"
        ],
        "abstract": "Using phonological speech vocoding, we propose a platform for exploring relations between phonology and speech processing, and in broader terms, for exploring relations between the abstract and physical structures of a speech signal. Our goal is to make a step towards bridging phonology and speech processing and to contribute to the program of Laboratory Phonology. We show three application examples for laboratory phonology: compositional phonological speech modelling, a comparison of phonological systems and an experimental phonological parametric text-to-speech (TTS) system. The featural representations of the following three phonological systems are considered in this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English (SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded speech, we conclude that the latter achieves slightly better results than the former. However, GP - the most compact phonological speech representation - performs comparably to the systems with a higher number of phonological features. The parametric TTS based on phonological speech representation, and trained from an unlabelled audiobook in an unsupervised manner, achieves intelligibility of 85% of the state-of-the-art parametric speech synthesis. We envision that the presented approach paves the way for researchers in both fields to form meaningful hypotheses that are explicitly testable using the concepts developed and exemplified in this paper. On the one hand, laboratory phonologists might test the applied concepts of their theoretical models, and on the other hand, the speech processing community may utilize the concepts developed for the theoretical phonological models for improvements of the current state-of-the-art applications.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06068",
        "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
        "authors": [
            "Shashi Narayan",
            "Siva Reddy",
            "Shay B. Cohen"
        ],
        "abstract": "One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries -- there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser significantly improves over strong baselines.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06081",
        "title": "Why Do Urban Legends Go Viral?",
        "authors": [
            "Marco Guerini",
            "Carlo Strapparava"
        ],
        "abstract": "Urban legends are a genre of modern folklore, consisting of stories about rare and exceptional events, just plausible enough to be believed, which tend to propagate inexorably across communities. In our view, while urban legends represent a form of \"sticky\" deceptive text, they are marked by a tension between the credible and incredible. They should be credible like a news article and incredible like a fairy tale to go viral. In particular we will focus on the idea that urban legends should mimic the details of news (who, where, when) to be credible, while they should be emotional and readable like a fairy tale to be catchy and memorable. Using NLP tools we will provide a quantitative analysis of these prototypical characteristics. We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06579",
        "title": "A Kernel Independence Test for Geographical Language Variation",
        "authors": [
            "Dong Nguyen",
            "Jacob Eisenstein"
        ],
        "abstract": "Quantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation. However, existing approaches have important drawbacks. First, they are based on parametric models of dependence, which limits their power in cases where the underlying parametric assumptions are violated. Second, they are not applicable to all types of linguistic data: some approaches apply only to frequencies, others to boolean indicators of whether a linguistic variable is present. We present a new method for measuring geographical language variation, which solves both of these problems. Our approach builds on Reproducing Kernel Hilbert space (RKHS) representations for nonparametric statistics, and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real datasets: a corpus of Dutch tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in North American newspapers. Our proposed test is shown to support robust inferences across a broad range of scenarios and types of data.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06581",
        "title": "Character-Level Incremental Speech Recognition with Recurrent Neural Networks",
        "authors": [
            "Kyuyeon Hwang",
            "Wonyong Sung"
        ],
        "abstract": "In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90% on the Wall Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284 training set.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06733",
        "title": "Long Short-Term Memory-Networks for Machine Reading",
        "authors": [
            "Jianpeng Cheng",
            "Li Dong",
            "Mirella Lapata"
        ],
        "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06971",
        "title": "Sentiment Analysis of Twitter Data: A Survey of Techniques",
        "authors": [
            "Vishal.A.Kharde",
            "Prof. Sheetal.Sonawane"
        ],
        "abstract": "With the advancement of web technology and its growth, there is a huge volume of data present in the web for internet users and a lot of data is generated too. Internet has become a platform for online learning, exchanging ideas and sharing opinions. Social networking sites like Twitter, Facebook, Google+ are rapidly gaining popularity as they allow people to share and express their views about topics,have discussion with different communities, or post messages across the world. There has been lot of work in the field of sentiment analysis of twitter data. This survey focuses mainly on sentiment analysis of twitter data which is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, or neutral in some cases. In this paper, we provide a survey and a comparative analyses of existing techniques for opinion mining like machine learning and lexicon-based approaches, together with evaluation metrics. Using various machine learning algorithms like Naive Bayes, Max Entropy, and Support Vector Machine, we provide a research on twitter data ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07124",
        "title": "LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization",
        "authors": [
            "Elvys Linhares Pontes",
            "Juan-Manuel Torres-Moreno",
            "Andr\u00e9a Carneiro Linhares"
        ],
        "abstract": "This paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07215",
        "title": "Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis",
        "authors": [
            "Prasanna Kumar Muthukumar",
            "Alan W Black"
        ],
        "abstract": "In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.\n    ",
        "submission_date": "2016-01-26T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07435",
        "title": "Co-Occurrence Patterns in the Voynich Manuscript",
        "authors": [
            "Torsten Timm"
        ],
        "abstract": "The Voynich Manuscript is a medieval book written in an unknown script. This paper studies the distribution of similarly spelled words in the Voynich Manuscript. It shows that the distribution of words within the manuscript is not compatible with natural languages.\n    ",
        "submission_date": "2016-01-27T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.07969",
        "title": "Zipf's law is a consequence of coherent language production",
        "authors": [
            "Jake Ryland Williams",
            "James P. Bagrow",
            "Andrew J. Reagan",
            "Sharon E. Alajajian",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "The task of text segmentation may be undertaken at many levels in text analysis---paragraphs, sentences, words, or even letters. Here, we focus on a relatively fine scale of segmentation, hypothesizing it to be in accord with a stochastic model of language generation, as the smallest scale where independent units of meaning are produced. Our goals in this letter include the development of methods for the segmentation of these minimal independent units, which produce feature-representations of texts that align with the independence assumption of the bag-of-terms model, commonly used for prediction and classification in computational text analysis. We also propose the measurement of texts' association (with respect to realized segmentations) to the model of language generation. We find (1) that our segmentations of phrases exhibit much better associations to the generation model than words and (2), that texts which are well fit are generally topically homogeneous. Because our generative model produces Zipf's law, our study further suggests that Zipf's law may be a consequence of homogeneity in language production.\n    ",
        "submission_date": "2016-01-29T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00293",
        "title": "WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter",
        "authors": [
            "Suman Kalyan Maity",
            "Chaitanya Sarda",
            "Anshit Chaudhary",
            "Abhijeet Patil",
            "Shraman Kumar",
            "Akash Mondal",
            "Animesh Mukherjee"
        ],
        "abstract": "Language in social media is mostly driven by new words and spellings that are constantly entering the lexicon thereby polluting it and resulting in high deviation from the formal written version. The primary entities of such language are the out-of-vocabulary (OOV) words. In this paper, we study various sociolinguistic properties of the OOV words and propose a classification model to categorize them into at least six categories. We achieve 81.26% accuracy with high precision and recall. We observe that the content features are the most discriminative ones followed by lexical and context features.\n    ",
        "submission_date": "2016-01-31T00:00:00",
        "last_modified_date": "2016-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00367",
        "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers",
        "authors": [
            "Yijun Xiao",
            "Kyunghyun Cho"
        ],
        "abstract": "Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters.\n    ",
        "submission_date": "2016-02-01T00:00:00",
        "last_modified_date": "2016-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00426",
        "title": "An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection",
        "authors": [
            "Cheng-Tao Chung",
            "Cheng-Yu Tsai",
            "Hsiang-Hung Lu",
            "Chia-Hsiang Liu",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.\n    ",
        "submission_date": "2016-02-01T00:00:00",
        "last_modified_date": "2016-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00812",
        "title": "The Grail theorem prover: Type theory for syntax and semantics",
        "authors": [
            "Richard Moot"
        ],
        "abstract": "As the name suggests, type-logical grammars are a grammar formalism based on logic and type theory. From the prespective of grammar design, type-logical grammars develop the syntactic and semantic aspects of linguistic phenomena hand-in-hand, letting the desired semantics of an expression inform the syntactic type and vice versa. Prototypical examples of the successful application of type-logical grammars to the syntax-semantics interface include coordination, quantifier scope and ",
        "submission_date": "2016-02-02T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01428",
        "title": "\"Draw My Topics\": Find Desired Topics fast from large scale of Corpus",
        "authors": [
            "Jason Dou",
            "Ni Sun",
            "Xiaojun Zou"
        ],
        "abstract": "We develop the \"Draw My Topics\" toolkit, which provides a fast way to incorporate social scientists' interest into standard topic modelling. Instead of using raw corpus with primitive processing as input, an algorithm based on Vector Space Model and Conditional Entropy are used to connect social scientists' willingness and unsupervised topic models' output. Space for users' adjustment on specific corpus of their interest is also accommodated. We demonstrate the toolkit's use on the Diachronic People's Daily Corpus in Chinese.\n    ",
        "submission_date": "2016-02-03T00:00:00",
        "last_modified_date": "2016-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01576",
        "title": "A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling",
        "authors": [
            "Anantharaman Palacode Narayana Iyer"
        ],
        "abstract": "Statistical language models are central to many applications that use semantics. Recurrent Neural Networks (RNN) are known to produce state of the art results for language modelling, outperforming their traditional n-gram counterparts in many cases. To generate a probability distribution across a vocabulary, these models require a softmax output layer that linearly increases in size with the size of the vocabulary. Large vocabularies need a commensurately large softmax layer and training them on typical laptops/PCs requires significant time and machine resources. In this paper we present a new technique for implementing RNN based large vocabulary language models that substantially speeds up computation while optimally using the limited memory resources. Our technique, while building on the notion of factorizing the output layer by having multiple output layers, improves on the earlier work by substantially optimizing on the individual output layer size and also eliminating the need for a multistep prediction process.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01595",
        "title": "Many Languages, One Parser",
        "authors": [
            "Waleed Ammar",
            "George Mulcaire",
            "Miguel Ballesteros",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser's performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01635",
        "title": "A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras",
        "authors": [
            "Jules Hedges",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. We formalise in this model the generalised quantifier theory of natural language, due to Barwise and Cooper. The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. We prove the equivalence of the relational instantiation to the truth theoretic semantics of generalised quantifiers. The vector space instantiation formalises the statistical usages of words and enables us to, for the first time, reason about quantified phrases and sentences compositionally in distributional semantics.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2017-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01925",
        "title": "Massively Multilingual Word Embeddings",
        "authors": [
            "Waleed Ammar",
            "George Mulcaire",
            "Yulia Tsvetkov",
            "Guillaume Lample",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01929",
        "title": "Fantastic 4 system for NIST 2015 Language Recognition Evaluation",
        "authors": [
            "Kong Aik Lee",
            "Ville Hautam\u00e4ki",
            "Anthony Larcher",
            "Wei Rao",
            "Hanwu Sun",
            "Trung Hieu Nguyen",
            "Guangsen Wang",
            "Aleksandr Sizov",
            "Ivan Kukanov",
            "Amir Poorjam",
            "Trung Ngo Trong",
            "Xiong Xiao",
            "Cheng-Lin Xu",
            "Hai-Hua Xu",
            "Bin Ma",
            "Haizhou Li",
            "Sylvain Meignier"
        ],
        "abstract": "This article describes the systems jointly submitted by Institute for Infocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\u00e9 du Maine (LIUM), Nanyang Technology University (NTU) and the University of Eastern Finland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The submitted system is a fusion of nine sub-systems based on i-vectors extracted from different types of features. Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM), multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) and Deep Neural Networks (DNN).\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02047",
        "title": "Utiliza\u00e7\u00e3o de Grafos e Matriz de Similaridade na Sumariza\u00e7\u00e3o Autom\u00e1tica de Documentos Baseada em Extra\u00e7\u00e3o de Frases",
        "authors": [
            "Elvys Linhares Pontes"
        ],
        "abstract": "The internet increased the amount of information available. However, the reading and understanding of this information are costly tasks. In this scenario, the Natural Language Processing (NLP) applications enable very important solutions, highlighting the Automatic Text Summarization (ATS), which produce a summary from one or more source texts. Automatically summarizing one or more texts, however, is a complex task because of the difficulties inherent to the analysis and generation of this summary. This master's thesis describes the main techniques and methodologies (NLP and heuristics) to generate summaries. We have also addressed and proposed some heuristics based on graphs and similarity matrix to measure the relevance of judgments and to generate summaries by extracting sentences. We used the multiple languages (English, French and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA (French) corpus to evaluate the developped systems. The results obtained were quite interesting.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02068",
        "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification",
        "authors": [
            "Andr\u00e9 F. T. Martins",
            "Ram\u00f3n Fernandez Astudillo"
        ],
        "abstract": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02133",
        "title": "Mining Software Quality from Software Reviews: Research Trends and Open Issues",
        "authors": [
            "Issa Atoum",
            "Ahmed Otoom"
        ],
        "abstract": "Software review text fragments have considerably valuable information about users experience. It includes a huge set of properties including the software quality. Opinion mining or sentiment analysis is concerned with analyzing textual user judgments. The application of sentiment analysis on software reviews can find a quantitative value that represents software quality. Although many software quality methods are proposed they are considered difficult to customize and many of them are limited. This article investigates the application of opinion mining as an approach to extract software quality properties. We found that the major issues of software reviews mining using sentiment analysis are due to software lifecycle and the diverse users and teams.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02215",
        "title": "Swivel: Improving Embeddings by Noticing What's Missing",
        "authors": [
            "Noam Shazeer",
            "Ryan Doherty",
            "Colin Evans",
            "Chris Waterson"
        ],
        "abstract": "We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating low-dimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed co-occurrences, and can scale to much larger corpora than can be handled with sampling methods.\n    ",
        "submission_date": "2016-02-06T00:00:00",
        "last_modified_date": "2016-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02410",
        "title": "Exploring the Limits of Language Modeling",
        "authors": [
            "Rafal Jozefowicz",
            "Oriol Vinyals",
            "Mike Schuster",
            "Noam Shazeer",
            "Yonghui Wu"
        ],
        "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.\n    ",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03265",
        "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use",
        "authors": [
            "Aida Nematzadeh",
            "Filip Miscevic",
            "Suzanne Stevenson"
        ],
        "abstract": "Recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory. An interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it. It has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task. Here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) can replicate the human patterns. We suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03426",
        "title": "Automatic Sarcasm Detection: A Survey",
        "authors": [
            "Aditya Joshi",
            "Pushpak Bhattacharyya",
            "Mark James Carman"
        ],
        "abstract": "Automatic sarcasm detection is the task of predicting sarcasm in text. This is a crucial step to sentiment analysis, considering prevalence and challenges of sarcasm in sentiment-bearing text. Beginning with an approach that used speech-based features, sarcasm detection has witnessed great interest from the sentiment analysis community. This paper is the first known compilation of past work in automatic sarcasm detection. We observe three milestones in the research so far: semi-supervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and use of context beyond target text. In this paper, we describe datasets, approaches, trends and issues in sarcasm detection. We also discuss representative performance values, shared tasks and pointers to future work, as given in prior works. In terms of resources that could be useful for understanding state-of-the-art, the survey presents several useful illustrations - most prominently, a table that summarizes past papers along different dimensions such as features, annotation techniques, data forms, etc.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03483",
        "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
        "authors": [
            "Felix Hill",
            "Kyunghyun Cho",
            "Anna Korhonen"
        ],
        "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03551",
        "title": "Knowledge Transfer with Medical Language Embeddings",
        "authors": [
            "Stephanie L. Hyland",
            "Theofanis Karaletsos",
            "Gunnar R\u00e4tsch"
        ],
        "abstract": "Identifying relationships between concepts is a key aspect of scientific knowledge synthesis. Finding these links often requires a researcher to laboriously search through scien- tific papers and databases, as the size of these resources grows ever larger. In this paper we describe how distributional semantics can be used to unify structured knowledge graphs with unstructured text to predict new relationships between medical concepts, using a probabilistic generative model. Our approach is also designed to ameliorate data sparsity and scarcity issues in the medical domain, which make language modelling more challenging. Specifically, we integrate the medical relational database (SemMedDB) with text from electronic health records (EHRs) to perform knowledge graph completion. We further demonstrate the ability of our model to predict relationships between tokens not appearing in the relational database.\n    ",
        "submission_date": "2016-02-10T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03606",
        "title": "Variations of the Similarity Function of TextRank for Automated Summarization",
        "authors": [
            "Federico Barrios",
            "Federico L\u00f3pez",
            "Luis Argerich",
            "Rosa Wachenchauzer"
        ],
        "abstract": "This article presents new alternatives to the similarity function for the TextRank algorithm for automatic summarization of texts. We describe the generalities of the algorithm and the different functions we propose. Some of these variants achieve a significative improvement using the same metrics and dataset as the original publication.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03609",
        "title": "Attentive Pooling Networks",
        "authors": [
            "Cicero dos Santos",
            "Ming Tan",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03960",
        "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions",
        "authors": [
            "Sujay Kumar Jauhar",
            "Peter Turney",
            "Eduard Hovy"
        ],
        "abstract": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04278",
        "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation",
        "authors": [
            "Taehwan Kim",
            "Weiran Wang",
            "Hao Tang",
            "Karen Livescu"
        ],
        "abstract": "We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with frame-level adaptation labels and 69.7% with only word labels.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04341",
        "title": "Attention-Based Convolutional Neural Network for Machine Comprehension",
        "authors": [
            "Wenpeng Yin",
            "Sebastian Ebert",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04375",
        "title": "Science Question Answering using Instructional Materials",
        "authors": [
            "Mrinmaya Sachan",
            "Avinava Dubey",
            "Eric P. Xing"
        ],
        "abstract": "We provide a solution for elementary science test using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines.\n    ",
        "submission_date": "2016-02-13T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04427",
        "title": "Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents",
        "authors": [
            "Zheng Xu",
            "Douglas Burdick",
            "Louiqa Raschid"
        ],
        "abstract": "There is a wealth of information about financial systems that is embedded in document collections. In this paper, we focus on a specialized text extraction task for this domain. The objective is to extract mentions of names of financial institutions, or FI names, from financial prospectus documents, and to identify the corresponding real world entities, e.g., by matching against a corpus of such entities. The tasks are Named Entity Recognition (NER) and Entity Resolution (ER); both are well studied in the literature. Our contribution is to develop a rule-based approach that will exploit lists of FI names for both tasks; our solution is labeled Dict-based NER and Rank-based ER. Since the FI names are typically represented by a root, and a suffix that modifies the root, we use these lists of FI names to create specialized root and suffix dictionaries. To evaluate the effectiveness of our specialized solution for extracting FI names, we compare Dict-based NER with a general purpose rule-based NER solution, ORG NER. Our evaluation highlights the benefits and limitations of specialized versus general purpose approaches, and presents additional suggestions for tuning and customization for FI name extraction. To our knowledge, our proposed solutions, Dict-based NER and Rank-based ER, and the root and suffix dictionaries, are the first attempt to exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and ER.\n    ",
        "submission_date": "2016-02-14T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05292",
        "title": "Authorship Attribution Using a Neural Network Language Model",
        "authors": [
            "Zhenhao Ge",
            "Yufang Sun",
            "Mark J. T. Smith"
        ],
        "abstract": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05307",
        "title": "Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding",
        "authors": [
            "Xiang Ren",
            "Wenqi He",
            "Meng Qu",
            "Clare R. Voss",
            "Heng Ji",
            "Jiawei Han"
        ],
        "abstract": "Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy (i.e., incorrect for the entity mention's local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples, given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE, to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations. Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05388",
        "title": "Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages",
        "authors": [
            "Muhammad Imran",
            "Prasenjit Mitra",
            "Jaideep Srivastava"
        ],
        "abstract": "Rapid crisis response requires real-time analysis of messages. After a disaster happens, volunteers attempt to classify tweets to determine needs, e.g., supplies, infrastructure damage, etc. Given labeled data, supervised machine learning can help classify these messages. Scarcity of labeled data causes poor performance in machine training. Can we reuse old tweets to train classifiers? How can we choose labeled tweets for training? Specifically, we study the usefulness of labeled data of past events. Do labeled tweets in different language help? We observe the performance of our classifiers trained using different combinations of training sets obtained from past disasters. We perform extensive experimentation on real crisis datasets and show that the past labels are useful when both source and target events are of the same type (e.g. both earthquakes). For similar languages (e.g., Italian and Spanish), cross-language domain adaptation was useful, however, when for different languages (e.g., Italian and English), the performance decreased.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05753",
        "title": "Overview of Annotation Creation: Processes & Tools",
        "authors": [
            "Mark A. Finlayson",
            "Toma\u017e Erjavec"
        ],
        "abstract": "Creating linguistic annotations requires more than just a reliable annotation scheme. Annotation can be a complex endeavour potentially involving many people, stages, and tools. This chapter outlines the process of creating end-to-end linguistic annotations, identifying specific tasks that researchers often perform. Because tool support is so central to achieving high quality, reusable annotations with low cost, the focus is on identifying capabilities that are necessary or useful for annotation tools, as well as common problems these tools present that reduce their utility. Although examples of specific tools are provided in many cases, this chapter concentrates more on abstract capabilities and problems because new tools appear continuously, while old tools disappear into disuse or disrepair. The two core capabilities tools must have are support for the chosen annotation scheme and the ability to work on the language under study. Additional capabilities are organized into three categories: those that are widely provided; those that often useful but found in only a few tools; and those that have as yet little or no available tool support.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05772",
        "title": "Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure",
        "authors": [
            "Stefan Gerdjikov",
            "Klaus U. Schulz"
        ],
        "abstract": "When looking at the structure of natural language, \"phrases\" and \"words\" are central notions. We consider the problem of identifying such \"meaningful subparts\" of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying \"phrases\", mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a \"general dictionary and grammar of the corpus\". We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05944",
        "title": "The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation",
        "authors": [
            "Erin Grant",
            "Aida Nematzadeh",
            "Suzanne Stevenson"
        ],
        "abstract": "People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy -- a cognitively salient category such as \"dog\" -- with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06023",
        "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
        "authors": [
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Cicero Nogueira dos santos",
            "Caglar Gulcehre",
            "Bing Xiang"
        ],
        "abstract": "In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06064",
        "title": "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation",
        "authors": [
            "Tianxing He",
            "Yu Zhang",
            "Jasha Droppo",
            "Kai Yu"
        ],
        "abstract": "We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06289",
        "title": "Learning to SMILE(S)",
        "authors": [
            "Stanis\u0142aw Jastrz\u0119bski",
            "Damian Le\u015bniak",
            "Wojciech Marian Czarnecki"
        ],
        "abstract": "This paper shows how one can directly apply natural language processing (NLP) methods to classification problems in cheminformatics. Connection between these seemingly separate fields is shown by considering standard textual representation of compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of computer aided drug design process. Conducted experiments show that this way one can not only outrank state of the art results of hand crafted representations but also gets direct structural insights into the way decisions are made.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2018-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06291",
        "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks",
        "authors": [
            "Shalini Ghosh",
            "Oriol Vinyals",
            "Brian Strope",
            "Scott Roy",
            "Tom Dean",
            "Larry Heck"
        ],
        "abstract": "Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06359",
        "title": "Text Matching as Image Recognition",
        "authors": [
            "Liang Pang",
            "Yanyan Lan",
            "Jiafeng Guo",
            "Jun Xu",
            "Shengxian Wan",
            "Xueqi Cheng"
        ],
        "abstract": "Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.\n    ",
        "submission_date": "2016-02-20T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06797",
        "title": "Semi-supervised Clustering for Short Text via Deep Representation Learning",
        "authors": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Abraham Ittycheriah"
        ],
        "abstract": "In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the k-means clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed. Experimental results on four datasets show that our method works significantly better than several other text clustering methods.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06967",
        "title": "Blind score normalization method for PLDA based speaker recognition",
        "authors": [
            "Danila Doroshin",
            "Nikolay Lubimov",
            "Marina Nastasenko",
            "Mikhail Kotov"
        ],
        "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art method for modeling $i$-vector space in speaker recognition task. However the performance degradation is observed if enrollment data size differs from one speaker to another. This paper presents a solution to such problem by introducing new PLDA scoring normalization technique. Normalization parameters are derived in a blind way, so that, unlike traditional \\textit{ZT-norm}, no extra development data is required. Moreover, proposed method has shown to be optimal in terms of detection cost function. The experiments conducted on NIST SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number condition.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06979",
        "title": "Empath: Understanding Topic Signals in Large-Scale Text",
        "authors": [
            "Ethan Fast",
            "Binbin Chen",
            "Michael Bernstein"
        ],
        "abstract": "Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like \"bleed\" and \"punch\" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07019",
        "title": "Sentence Similarity Learning by Lexical Decomposition and Composition",
        "authors": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Abraham Ittycheriah"
        ],
        "abstract": "Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07236",
        "title": "Petrarch 2 : Petrarcher",
        "authors": [
            "Clayton Norris"
        ],
        "abstract": "PETRARCH 2 is the fourth generation of a series of Event-Data coders stemming from research by Phillip Schrodt. Each iteration has brought new functionality and usability, and this is no ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07393",
        "title": "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models",
        "authors": [
            "Zhenhao Ge",
            "Yufang Sun"
        ],
        "abstract": "Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07563",
        "title": "Multilingual Twitter Sentiment Classification: The Role of Human Annotators",
        "authors": [
            "Igor Mozetic",
            "Miha Grcar",
            "Jasmina Smailovic"
        ],
        "abstract": "What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07572",
        "title": "Ultradense Word Embeddings by Orthogonal Transformation",
        "authors": [
            "Sascha Rothe",
            "Sebastian Ebert",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07618",
        "title": "From quantum foundations via natural language meaning to a theory of everything",
        "authors": [
            "Bob Coecke"
        ],
        "abstract": "In this paper we argue for a paradigmatic shift from `reductionism' to `togetherness'. In particular, we show how interaction between systems in quantum theory naturally carries over to modelling how word meanings interact in natural language. Since meaning in natural language, depending on the subject domain, encompasses discussions within any scientific discipline, we obtain a template for theories such as social interaction, animal behaviour, and many others.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07749",
        "title": "Toward Mention Detection Robustness with Recurrent Neural Networks",
        "authors": [
            "Thien Huu Nguyen",
            "Avirup Sil",
            "Georgiana Dinu",
            "Radu Florian"
        ],
        "abstract": "One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9\\% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22\\% relative error reduction).\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07776",
        "title": "Recurrent Neural Network Grammars",
        "authors": [
            "Chris Dyer",
            "Adhiguna Kuncoro",
            "Miguel Ballesteros",
            "Noah A. Smith"
        ],
        "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07803",
        "title": "Automated Word Prediction in Bangla Language Using Stochastic Language Models",
        "authors": [
            "Md. Masudul Haque",
            "Md. Tarek Habib",
            "Md. Mokhlesur Rahman"
        ],
        "abstract": "Word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08657",
        "title": "QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source",
        "authors": [
            "Luc Herren"
        ],
        "abstract": "The software programs generally used with the TLG (Thesaurus Linguae Graecae) and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well suited for finding quotations and allusions. QuotationFinder uses more sophisticated criteria as it ranks search results based on how closely they match the source text, listing search results with literal quotations first and loose verbal parallels last.\n    ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2017-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08715",
        "title": "Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus",
        "authors": [
            "Avi Shmidman",
            "Moshe Koppel",
            "Ely Porat"
        ],
        "abstract": "We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods.\n    ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2018-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08741",
        "title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?",
        "authors": [
            "Nikolay N. Vasiliev"
        ],
        "abstract": "The most studied and most successful language models were developed and evaluated mainly for English and other close European languages, such as French, German, etc. It is important to study applicability of these models to other languages. The use of vector space models for Russian was recently studied for multiple corpora, such as Wikipedia, RuWac, ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2016-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08742",
        "title": "Optimizing the Learning Order of Chinese Characters Using a Novel Topological Sort Algorithm",
        "authors": [
            "James C. Loach",
            "Jinzhao Wang"
        ],
        "abstract": "We present a novel algorithm for optimizing the order in which Chinese characters are learned, one that incorporates the benefits of learning them in order of usage frequency and in order of their hierarchal structural relationships. We show that our work outperforms previously published orders and algorithms. Our algorithm is applicable to any scheduling task where nodes have intrinsic differences in importance and must be visited in topological order.\n    ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08844",
        "title": "Bioinformatics and Classical Literary Study",
        "authors": [
            "Pramit Chaudhuri",
            "Joseph P. Dexter"
        ],
        "abstract": "This paper describes the Quantitative Criticism Lab, a collaborative initiative between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate authorial style, intertextuality, and related phenomena of literary significance. As a case study in our approach, here we review the use of sequence alignment, a common technique in genomics and computational linguistics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal similarities, which makes it ideal for identifying phonetic echoes in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages.\n    ",
        "submission_date": "2016-02-29T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08952",
        "title": "Representation of linguistic form and function in recurrent neural networks",
        "authors": [
            "\u00c1kos K\u00e1d\u00e1r",
            "Grzegorz Chrupa\u0142a",
            "Afra Alishahi"
        ],
        "abstract": "We present novel methods for analyzing the activation patterns of RNNs from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. Based on our proposed method to estimate the amount of contribution of individual tokens in the input to the final prediction of the networks we show that the image prediction pathway: a) is sensitive to the information structure of the sentence b) pays selective attention to lexical categories and grammatical functions that carry semantic information c) learns to treat the same input token differently depending on its grammatical functions in the sentence. In contrast the language model is comparatively more sensitive to words with a syntactic function. Furthermore, we propose methods to ex- plore the function of individual hidden units in RNNs and show that the two pathways of the architecture in our case study contain specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.\n    ",
        "submission_date": "2016-02-29T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00223",
        "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition",
        "authors": [
            "Liang Lu",
            "Lingpeng Kong",
            "Chris Dyer",
            "Noah A. Smith",
            "Steve Renals"
        ],
        "abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00375",
        "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs",
        "authors": [
            "Eliyahu Kiperwasser",
            "Yoav Goldberg"
        ],
        "abstract": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00786",
        "title": "Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning",
        "authors": [
            "Nanyun Peng",
            "Mark Dredze"
        ],
        "abstract": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00810",
        "title": "Character-based Neural Machine Translation",
        "authors": [
            "Marta R. Costa-Juss\u00e0",
            "Jos\u00e9 A. R. Fonollosa"
        ],
        "abstract": "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00892",
        "title": "Counter-fitting Word Vectors to Linguistic Constraints",
        "authors": [
            "Nikola Mrk\u0161i\u0107",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Blaise Thomson",
            "Milica Ga\u0161i\u0107",
            "Lina Rojas-Barahona",
            "Pei-Hao Su",
            "David Vandyke",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.\n    ",
        "submission_date": "2016-03-02T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00957",
        "title": "Question Answering on Freebase via Relation Extraction and Textual Evidence",
        "authors": [
            "Kun Xu",
            "Siva Reddy",
            "Yansong Feng",
            "Songfang Huang",
            "Dongyan Zhao"
        ],
        "abstract": "Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00968",
        "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification",
        "authors": [
            "Ye Zhang",
            "Stephen Roller",
            "Byron Wallace"
        ],
        "abstract": "We introduce a novel, simple convolution neural network (CNN) architecture - multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of word embeddings for sentence classification. MGNC-CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01032",
        "title": "Right Ideals of a Ring and Sublanguages of Science",
        "authors": [
            "Javier Arias Navarro"
        ],
        "abstract": "Among Zellig Harris's numerous contributions to linguistics his theory of the sublanguages of science probably ranks among the most underrated. However, not only has this theory led to some exhaustive and meaningful applications in the study of the grammar of immunology language and its changes over time, but it also illustrates the nature of mathematical relations between chunks or subsets of a grammar and the language as a whole. This becomes most clear when dealing with the connection between metalanguage and language, as well as when reflecting on operators.\n",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01232",
        "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems",
        "authors": [
            "Tsung-Hsien Wen",
            "Milica Gasic",
            "Nikola Mrksic",
            "Lina M. Rojas-Barahona",
            "Pei-Hao Su",
            "David Vandyke",
            "Steve Young"
        ],
        "abstract": "Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.\n    ",
        "submission_date": "2016-03-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01333",
        "title": "Joint Learning Templates and Slots for Event Schema Induction",
        "authors": [
            "Lei Sha",
            "Sujian Li",
            "Baobao Chang",
            "Zhifang Sui"
        ],
        "abstract": "Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01360",
        "title": "Neural Architectures for Named Entity Recognition",
        "authors": [
            "Guillaume Lample",
            "Miguel Ballesteros",
            "Sandeep Subramanian",
            "Kazuya Kawakami",
            "Chris Dyer"
        ],
        "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01514",
        "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
        "authors": [
            "Nikhil Garg",
            "James Henderson"
        ],
        "abstract": "We propose a Bayesian model of unsupervised semantic role induction in multiple languages, and use it to explore the usefulness of parallel corpora for this task. Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly. We compare using only monolingual data, using a parallel corpus, using a parallel corpus with annotations in the other language, and using small amounts of annotation in the target language. We find that the biggest impact of adding a parallel corpus to training is actually the increase in mono-lingual data, with the alignments to another language resulting in small improvements, even with labeled data for the other language.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01541",
        "title": "Parallel Texts in the Hebrew Bible, New Methods and Visualizations",
        "authors": [
            "Martijn Naaijer",
            "Dirk Roorda"
        ],
        "abstract": "In this article we develop an algorithm to detect parallel texts in the Masoretic Text of the Hebrew Bible. The results are presented online and chapters in the Hebrew Bible containing parallel passages can be inspected synoptically. Differences between parallel passages are highlighted. In a similar way the MT of Isaiah is presented synoptically with 1QIsaa. We also investigate how one can investigate the degree of similarity between parallel passages with the help of a case study of 2 Kings 19-25 and its parallels in Isaiah, Jeremiah and 2 Chronicles.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01547",
        "title": "Text Understanding with the Attention Sum Reader Network",
        "authors": [
            "Rudolf Kadlec",
            "Martin Schmid",
            "Ondrej Bajgar",
            "Jan Kleindienst"
        ],
        "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01595",
        "title": "Sentiment Analysis in Scholarly Book Reviews",
        "authors": [
            "Hussam Hamdan",
            "Patrice Bellot",
            "Frederic Bechet"
        ],
        "abstract": "So far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews. But, this problem has not been studied in scholarly book reviews which is different in terms of review style and size. In this paper, we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews. We construct a labeled corpus for training and evaluating our methods in French book reviews. We also evaluate them on English restaurant reviews in order to measure their robustness across the domains and languages. The evaluation shows that our methods are enough robust for English restaurant reviews and French book reviews.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01597",
        "title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning",
        "authors": [
            "Mike Kestemont",
            "Jeroen De Gussem"
        ],
        "abstract": "In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01648",
        "title": "Getting More Out Of Syntax with PropS",
        "authors": [
            "Gabriel Stanovsky",
            "Jessica Ficler",
            "Ido Dagan",
            "Yoav Goldberg"
        ],
        "abstract": "Semantic NLP applications often rely on dependency trees to recognize major elements of the proposition structure of sentences. Yet, while much semantic structure is indeed expressed by syntax, many phenomena are not easily read out of dependency trees, often leading to further ad-hoc heuristic post-processing or to information loss. To directly address the needs of semantic applications, we present PropS -- an output representation designed to explicitly and uniformly express much of the proposition structure which is implied from syntax, and an associated tool for extracting it from dependency trees.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01833",
        "title": "Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive Disambiguation: the eXtended Revised AraMorph (XRAM)",
        "authors": [
            "Giuliano Lancioni",
            "Valeria Pettinari",
            "Laura Garofalo",
            "Marta Campanelli",
            "Ivana Pepe",
            "Simona Olivieri",
            "Ilaria Cicola"
        ],
        "abstract": "An extended, revised form of Tim Buckwalter's Arabic lexical and morphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM), is presented which addresses a number of weaknesses and inconsistencies of the original model by allowing a wider coverage of real-world Classical and contemporary (both formal and informal) Arabic texts. Building upon previous research, XRAM enhancements include (i) flag-selectable usage markers, (ii) probabilistic mildly context-sensitive POS tagging, filtering, disambiguation and ranking of alternative morphological analyses, (iii) semi-automatic increment of lexical coverage through extraction of lexical and morphological information from existing lexical resources. Testing of XRAM through a front-end Python module showed a remarkable success level.\n    ",
        "submission_date": "2016-03-06T00:00:00",
        "last_modified_date": "2016-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01913",
        "title": "A Latent Variable Recurrent Neural Network for Discourse Relation Language Models",
        "authors": [
            "Yangfeng Ji",
            "Gholamreza Haffari",
            "Jacob Eisenstein"
        ],
        "abstract": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02488",
        "title": "Extracting Arabic Relations from the Web",
        "authors": [
            "Shimaa M. Abd El-salam",
            "Enas M.F. El Houby",
            "A.K. Al Sammak",
            "T.A. El-Shishtawy"
        ],
        "abstract": "The goal of this research is to extract a large list or table from named entities and relations in a specific domain. A small set of a handful of instance relations is required as input from the user. The system exploits summaries from Google search engine as a source text. These instances are used to extract patterns. The output is a set of new entities and their relations. The results from four experiments show that precision and recall varies according to relation type. Precision ranges from 0.61 to 0.75 while recall ranges from 0.71 to 0.83. The best result is obtained for (player, club) relationship, 0.72 and 0.83 for precision and recall respectively.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02514",
        "title": "Variational Autoencoders for Semi-supervised Text Classification",
        "authors": [
            "Weidi Xu",
            "Haoze Sun",
            "Chao Deng",
            "Ying Tan"
        ],
        "abstract": "Although semi-supervised variational autoencoder (SemiVAE) works in image classification task, it fails in text classification task if using vanilla LSTM as its decoder. From a perspective of reinforcement learning, it is verified that the decoder's capability to distinguish between different categorical labels is essential. Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE) is proposed, which increases the capability by feeding label into its decoder RNN at each time-step. Two specific decoder structures are investigated and both of them are verified to be effective. Besides, in order to reduce the computational complexity in training, a novel optimization method is proposed, which estimates the gradient of the unlabeled objective function by sampling, along with two variance reduction techniques. Experimental results on Large Movie Review Dataset (IMDB) and AG's News corpus show that the proposed approach significantly improves the classification accuracy compared with pure-supervised classifiers, and achieves competitive performance against previous advanced methods. State-of-the-art results can be obtained by integrating other pretraining-based methods.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02604",
        "title": "Observing Trends in Automated Multilingual Media Analysis",
        "authors": [
            "Ralf Steinberger",
            "Aldo Podavini",
            "Alexandra Balahur",
            "Guillaume Jacquet",
            "Hristo Tanev",
            "Jens Linge",
            "Martin Atkinson",
            "Michele Chinosi",
            "Vanni Zavarella",
            "Yaniv Steiner",
            "Erik van der Goot"
        ],
        "abstract": "Any large organisation, be it public or private, monitors the media for information to keep abreast of developments in their field of interest, and usually also to become aware of positive or negative opinions expressed towards them. At least for the written media, computer programs have become very efficient at helping the human analysts significantly in their monitoring task by gathering media reports, analysing them, detecting trends and - in some cases - even to issue early warnings or to make predictions of likely future developments. We present here trend recognition-related functionality of the Europe Media Monitor (EMM) system, which was developed by the European Commission's Joint Research Centre (JRC) for public administrations in the European Union (EU) and beyond. EMM performs large-scale media analysis in up to seventy languages and recognises various types of trends, some of them combining information from news articles written in different languages and from social media posts. EMM also lets users explore the huge amount of multilingual media data through interactive maps and graphs, allowing them to examine the data from various view points and according to multiple criteria. A lot of EMM's functionality is accessibly freely over the internet or via apps for hand-held devices.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02618",
        "title": "The red one!: On learning to refer to things based on their discriminative properties",
        "authors": [
            "Angeliki Lazaridou",
            "Nghia The Pham",
            "Marco Baroni"
        ],
        "abstract": "As a first step towards agents learning to communicate about their visual environment, we propose a system that, given visual representations of a referent (cat) and a context (sofa), identifies their discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, despite the lack of direct supervision at the attribute level, the model learns to assign plausible attributes to objects (sofa-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes.\n    ",
        "submission_date": "2016-03-08T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02776",
        "title": "Implicit Discourse Relation Classification via Multi-Task Neural Networks",
        "authors": [
            "Yang Liu",
            "Sujian Li",
            "Xiaodong Zhang",
            "Zhifang Sui"
        ],
        "abstract": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.\n    ",
        "submission_date": "2016-03-09T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02845",
        "title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings",
        "authors": [
            "Herman Kamper",
            "Aren Jansen",
            "Sharon Goldwater"
        ],
        "abstract": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.\n    ",
        "submission_date": "2016-03-09T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.02905",
        "title": "Lexical bundles in computational linguistics academic literature",
        "authors": [
            "Adel Rahimi"
        ],
        "abstract": "In this study we analyzed a corpus of 8 million words academic literature from Computational lingustics' academic literature. the lexical bundles from this corpus are categorized based on structures and functions.\n    ",
        "submission_date": "2016-03-09T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03112",
        "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)",
        "authors": [
            "Lifu Huang",
            "Jonathan May",
            "Xiaoman Pan",
            "Heng Ji"
        ],
        "abstract": "Recent research has shown great progress on fine-grained entity typing. Most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features. They are thus limited to certain domains, genres and languages. In this paper, we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics. We start from learning general embeddings for each entity mention, compose the embeddings of specific contexts using linguistic structures, link the mention to knowledge bases and learn its related knowledge representations. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework doesn't rely on any annotated data, predefined typing schema, or hand-crafted features, therefore it can be quickly adapted to a new domain, genre and language. Furthermore, it has great flexibility at incorporating linguistic structures (e.g., Abstract Meaning Representation (AMR), dependency relations) to improve specific context representation. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03144",
        "title": "Part-of-Speech Tagging for Historical English",
        "authors": [
            "Yi Yang",
            "Jacob Eisenstein"
        ],
        "abstract": "As more historical texts are digitized, there is interest in applying natural language processing tools to these archives. However, the performance of these tools is often unsatisfactory, due to language change and genre differences. Spelling normalization heuristics are the dominant solution for dealing with historical texts, but this approach fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We evaluate several domain adaptation methods on the task of tagging Early Modern English and Modern British English texts in the Penn Corpora of Historical English. We demonstrate that the Feature Embedding method for unsupervised domain adaptation outperforms word embeddings and Brown clusters, showing the importance of embedding the entire feature space, rather than just individual words. Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03185",
        "title": "Personalized Speech recognition on mobile devices",
        "authors": [
            "Ian McGraw",
            "Rohit Prabhavalkar",
            "Raziel Alvarez",
            "Montse Gonzalez Arenas",
            "Kanishka Rao",
            "David Rybach",
            "Ouais Alsharif",
            "Hasim Sak",
            "Alexander Gruenstein",
            "Francoise Beaufays",
            "Carolina Parada"
        ],
        "abstract": "We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets, and further reduce its memory footprint using an SVD-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5% word error rate on an open-ended dictation task, running with a median speed that is seven times faster than real-time.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03758",
        "title": "Sieve-based Coreference Resolution in the Biomedical Domain",
        "authors": [
            "Dane Bell",
            "Gus Hahn-Powell",
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Mihai Surdeanu"
        ],
        "abstract": "We describe challenges and advantages unique to coreference resolution in the biomedical domain, and a sieve-based architecture that leverages domain knowledge for both entity and event coreference resolution. Domain-general coreference resolution algorithms perform poorly on biomedical documents, because the cues they rely on such as gender are largely absent in this domain, and because they do not encode domain-specific knowledge such as the number and type of participants required in chemical reactions. Moreover, it is difficult to directly encode this knowledge into most coreference resolution algorithms because they are not rule-based. Our rule-based architecture uses sequentially applied hand-designed \"sieves\", with the output of each sieve informing and constraining subsequent sieves. This architecture provides a 3.2% increase in throughput to our Reach event extraction system with precision parallel to that of the stricter system that relies solely on syntactic patterns for extraction.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03784",
        "title": "Towards using social media to identify individuals at risk for preventable chronic illness",
        "authors": [
            "Dane Bell",
            "Daniel Fried",
            "Luwen Huangfu",
            "Mihai Surdeanu",
            "Stephen Kobourov"
        ],
        "abstract": "We describe a strategy for the acquisition of training data necessary to build a social-media-driven early detection system for individuals at risk for (preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like quiz with data and questions acquired semi-automatically from Twitter. The questions are designed to inspire participant engagement and collect relevant data to train a public-health model applied to individuals. Prior systems designed to use social media such as Twitter to predict obesity (a risk factor for T2DM) operate on entire communities such as states, counties, or cities, based on statistics gathered by government agencies. Because there is considerable variation among individuals within these groups, training data on the individual level would be more effective, but this data is difficult to acquire. The approach proposed here aims to address this issue. Our strategy has two steps. First, we trained a random forest classifier on data gathered from (public) Twitter statuses and state-level statistics with state-of-the-art accuracy. We then converted this classifier into a 20-questions-style quiz and made it available online. In doing so, we achieved high engagement with individuals that took the quiz, while also building a training set of voluntarily supplied individual-level data for future classification.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03793",
        "title": "Training with Exploration Improves a Greedy Stack-LSTM Parser",
        "authors": [
            "Miguel Ballesteros",
            "Yoav Goldberg",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03827",
        "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
        "authors": [
            "Ji Young Lee",
            "Franck Dernoncourt"
        ],
        "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03873",
        "title": "Neural Discourse Relation Recognition with Semantic Memory",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su"
        ],
        "abstract": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\\% on average over current state-of-the-art baselines in terms of F1-score.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03876",
        "title": "Variational Neural Discourse Relation Recognizer",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su",
            "Qun Liu",
            "Rongrong Ji",
            "Hong Duan",
            "Min Zhang"
        ],
        "abstract": "Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof- the-art baselines without using any manual features.\n    ",
        "submission_date": "2016-03-12T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04236",
        "title": "Interactive Tools and Tasks for the Hebrew Bible",
        "authors": [
            "Nicolai Winther-Nielsen"
        ],
        "abstract": "This contribution to a special issue on \"Computer-aided processing of intertextuality\" in ancient texts will illustrate how using digital tools to interact with the Hebrew Bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Centre for Bible and Computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts, and how this can be envisioned as a new field of digital intertextuality. The article first describes how the corpus was used to develop the Bible Online Learner as a persuasive technology to enhance language learning with, in, and around a database that acts as the engine driving interactive tasks for learners. Intertextuality in this case is a matter of active exploration and ongoing practice. Furthermore, interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources. Commercial solutions developed by software companies like Logos and Accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions. It is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation. We have already seen the emergence of promising tools for text categorization, analysis of translation shifts, and interpretation. Broadly speaking, interactive tools and tasks within the three areas of language learning, textual criticism, and Biblical studies illustrate a new kind of intertextuality emerging within digital humanities.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2017-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04351",
        "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
        "authors": [
            "Eliyahu Kiperwasser",
            "Yoav Goldberg"
        ],
        "abstract": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.\n    ",
        "submission_date": "2016-03-14T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04513",
        "title": "Multichannel Variable-Size Convolution for Sentence Classification",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04553",
        "title": "Unsupervised Ranking Model for Entity Coreference Resolution",
        "authors": [
            "Xuezhe Ma",
            "Zhengzhong Liu",
            "Eduard Hovy"
        ],
        "abstract": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04747",
        "title": "Topic Modeling Using Distributed Word Embeddings",
        "authors": [
            "Ramandeep S Randhawa",
            "Parag Jain",
            "Gagan Madan"
        ],
        "abstract": "We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.04767",
        "title": "Evaluating the word-expert approach for Named-Entity Disambiguation",
        "authors": [
            "Angel X. Chang",
            "Valentin I. Spitkovsky",
            "Christopher D. Manning",
            "Eneko Agirre"
        ],
        "abstract": "Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia. This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed. In this work we present the results of the word-expert approach to NED, where one classifier is built for each target entity mention string. The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia. We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics.\n    ",
        "submission_date": "2016-03-15T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05118",
        "title": "Recurrent Dropout without Memory Loss",
        "authors": [
            "Stanislau Semeniuta",
            "Aliaksei Severyn",
            "Erhardt Barth"
        ],
        "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to \\textit{forward} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in \\textit{recurrent} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.\n    ",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05157",
        "title": "Comparing Convolutional Neural Networks to Traditional Models for Slot Filling",
        "authors": [
            "Heike Adel",
            "Benjamin Roth",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We address relation classification in the context of slot filling, the task of finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X founded Apple\". We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-of-the-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance.\n    ",
        "submission_date": "2016-03-16T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05350",
        "title": "Self-organization of vocabularies under different interaction orders",
        "authors": [
            "Javier Vera"
        ],
        "abstract": "Traditionally, the formation of vocabularies has been studied by agent-based models (specially, the Naming Game) in which random pairs of agents negotiate word-meaning associations at each discrete time step. This paper proposes a first approximation to a novel question: To what extent the negotiation of word-meaning associations is influenced by the order in which the individuals interact? Automata Networks provide the adequate mathematical framework to explore this question. Computer simulations suggest that on two-dimensional lattices the typical features of the formation of word-meaning associations are recovered under random schemes that update small fractions of the population at the same time.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05354",
        "title": "Modeling self-organization of vocabularies under phonological similarity effects",
        "authors": [
            "Javier Vera"
        ],
        "abstract": "This work develops a computational model (by Automata Networks) of phonological similarity effects involved in the formation of word-meaning associations on artificial populations of speakers. Classical studies show that in recalling experiments memory performance was impaired for phonologically similar words versus dissimilar ones. Here, the individuals confound phonologically similar words according to a predefined parameter. The main hypothesis is that there is a critical range of the parameter, and with this, of working-memory mechanisms, which implies drastic changes in the final consensus of the entire population. Theoretical results present proofs of convergence for a particular case of the model within a worst-case complexity framework. Computer simulations describe the evolution of an energy function that measures the amount of local agreement between individuals. The main finding is the appearance of sudden changes in the energy function at critical parameters.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05570",
        "title": "Predicate Gradual Logic and Linguistics",
        "authors": [
            "Ryuta Arisaka"
        ],
        "abstract": "There are several major proposals for treating donkey anaphora such as discourse representation theory and the likes, or E-Type theories and the likes. Every one of them works well for a set of specific examples that they use to demonstrate validity of their approaches. As I show in this paper, however, they are not very generalisable and do not account for essentially the same problem that they remedy when it manifests in other examples. I propose another logical approach. I develoop logic that extends a recent, propositional gradual logic, and show that it can treat donkey anaphora generally. I also identify and address a problem around the modern convention on existential import. Furthermore, I show that Aristotle's syllogisms and conversion are realisable in this logic.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05670",
        "title": "Bank distress in the news: Describing events through deep learning",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Peter Sarlin"
        ],
        "abstract": "While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05673",
        "title": "Predicting health inspection results from online restaurant reviews",
        "authors": [
            "Samantha Wong",
            "Hamidreza Chinaei",
            "Frank Rudzicz"
        ],
        "abstract": "Informatics around public health are increasingly shifting from the professional to the public spheres. In this work, we apply linguistic analytics to restaurant reviews, from Yelp, in order to automatically predict official health inspection reports. We consider two types of feature sets, i.e., keyword detection and topic model features, and use these in several classification methods. Our empirical analysis shows that these extracted features can predict public health inspection reports with over 90% accuracy using simple support vector machines.\n    ",
        "submission_date": "2016-03-17T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05739",
        "title": "A Readability Analysis of Campaign Speeches from the 2016 US Presidential Campaign",
        "authors": [
            "Elliot Schumacher",
            "Maxine Eskenazi"
        ],
        "abstract": "Readability is defined as the reading level of the speech from grade 1 to grade 12. It results from the use of the REAP readability analysis (vocabulary - Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), which use the lexical contents and grammatical structure of the sentences in a document to predict the reading level. After analysis, results were grouped into the average readability of each candidate, the evolution of the candidate's speeches' readability over time and the standard deviation, or how much each candidate varied their speech from one venue to another. For comparison, one speech from four past presidents and the Gettysburg Address were also analyzed.\n    ",
        "submission_date": "2016-03-18T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06009",
        "title": "Readability-based Sentence Ranking for Evaluating Text Simplification",
        "authors": [
            "Sowmya Vajjala",
            "Detmar Meurers"
        ],
        "abstract": "We propose a new method for evaluating the readability of simplified sentences through pair-wise ranking. The validity of the method is established through in-corpus and cross-corpus evaluation experiments. The approach correctly identifies the ranking of simplified and unsimplified sentences in terms of their reading level with an accuracy of over 80%, significantly outperforming previous results. To gain qualitative insights into the nature of simplification at the sentence level, we studied the impact of specific linguistic features. We empirically confirm that both word-level and syntactic features play a role in comparing the degree of simplification of authentic data. To carry out this research, we created a new sentence-aligned corpus from professionally simplified news articles. The new corpus resource enriches the empirical basis of sentence-level simplification research, which so far relied on a single resource. Most importantly, it facilitates cross-corpus evaluation for simplification, a key step towards generalizable results.\n    ",
        "submission_date": "2016-03-18T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06021",
        "title": "A Fast Unified Model for Parsing and Sentence Understanding",
        "authors": [
            "Samuel R. Bowman",
            "Jon Gauthier",
            "Abhinav Rastogi",
            "Raghav Gupta",
            "Christopher D. Manning",
            "Christopher Potts"
        ],
        "abstract": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25 times over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06042",
        "title": "Globally Normalized Transition-Based Neural Networks",
        "authors": [
            "Daniel Andor",
            "Chris Alberti",
            "David Weiss",
            "Aliaksei Severyn",
            "Alessandro Presta",
            "Kuzman Ganchev",
            "Slav Petrov",
            "Michael Collins"
        ],
        "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06059",
        "title": "Generating Natural Questions About an Image",
        "authors": [
            "Nasrin Mostafazadeh",
            "Ishan Misra",
            "Jacob Devlin",
            "Margaret Mitchell",
            "Xiaodong He",
            "Lucy Vanderwende"
        ],
        "abstract": "There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06067",
        "title": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings",
        "authors": [
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06075",
        "title": "Tree-to-Sequence Attentional Neural Machine Translation",
        "authors": [
            "Akiko Eriguchi",
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06076",
        "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method",
        "authors": [
            "Vered Shwartz",
            "Yoav Goldberg",
            "Ido Dagan"
        ],
        "abstract": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06111",
        "title": "How Transferable are Neural Networks in NLP Applications?",
        "authors": [
            "Lili Mou",
            "Zhao Meng",
            "Rui Yan",
            "Ge Li",
            "Yan Xu",
            "Lu Zhang",
            "Zhi Jin"
        ],
        "abstract": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06127",
        "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension",
        "authors": [
            "Petr Baudi\u0161",
            "Jan Pichl",
            "Tom\u00e1\u0161 Vysko\u010dil",
            "Jan \u0160ediv\u00fd"
        ],
        "abstract": "We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.\n",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06147",
        "title": "A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation",
        "authors": [
            "Junyoung Chung",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06155",
        "title": "A Persona-Based Neural Conversation Model",
        "authors": [
            "Jiwei Li",
            "Michel Galley",
            "Chris Brockett",
            "Georgios P. Spithourakis",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.\n    ",
        "submission_date": "2016-03-19T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06270",
        "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch",
        "authors": [
            "Zhilin Yang",
            "Ruslan Salakhutdinov",
            "William Cohen"
        ],
        "abstract": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.\n    ",
        "submission_date": "2016-03-20T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06393",
        "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
        "authors": [
            "Jiatao Gu",
            "Zhengdong Lu",
            "Hang Li",
            "Victor O.K. Li"
        ],
        "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06503",
        "title": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers",
        "authors": [
            "Bernd Bohnet",
            "Miguel Ballesteros",
            "Ryan McDonald",
            "Joakim Nivre"
        ],
        "abstract": "We study the use of greedy feature selection methods for morphosyntactic tagging under a number of different conditions. We compare a static ordering of features to a dynamic ordering based on mutual information statistics, and we apply the techniques to standalone taggers as well as joint systems for tagging and parsing. Experiments on five languages show that feature selection can result in more compact models as well as higher accuracy under all conditions, but also that a dynamic ordering works better than a static ordering and that joint systems benefit more than standalone taggers. We also show that the same techniques can be used to select which morphosyntactic categories to predict in order to maximize syntactic accuracy in a joint system. Our final results represent a substantial improvement of the state of the art for several languages, while at the same time reducing both the number of features and the running time by up to 80% in some cases.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06571",
        "title": "Bayesian Neural Word Embedding",
        "authors": [
            "Oren Barkan"
        ],
        "abstract": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-Gram with negative sampling, known also as word2vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm. The algorithm relies on a Variational Bayes solution for the Skip-Gram objective and a detailed step by step description is provided. We present experimental results that demonstrate the performance of the proposed algorithm for word analogy and similarity tasks on six different datasets and show it is competitive with the original Skip-Gram method.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06598",
        "title": "Stack-propagation: Improved Representation Learning for Syntax",
        "authors": [
            "Yuan Zhang",
            "David Weiss"
        ],
        "abstract": "Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call \"stack-propagation\". We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06677",
        "title": "Learning Executable Semantic Parsers for Natural Language Understanding",
        "authors": [
            "Percy Liang"
        ],
        "abstract": "For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06679",
        "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis",
        "authors": [
            "Wenya Wang",
            "Sinno Jialin Pan",
            "Daniel Dahlmeier",
            "Xiaokui Xiao"
        ],
        "abstract": "In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06744",
        "title": "Latent Predictor Networks for Code Generation",
        "authors": [
            "Wang Ling",
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Andrew Senior",
            "Fumin Wang",
            "Phil Blunsom"
        ],
        "abstract": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06785",
        "title": "Multi-domain machine translation enhancements by parallel data extraction from comparable corpora",
        "authors": [
            "Krzysztof Wo\u0142k",
            "Emilia Rejmund",
            "Krzysztof Marasek"
        ],
        "abstract": "Parallel texts are a relatively rare language resource, however, they constitute a very useful research material with a wide range of applications. This study presents and analyses new methodologies we developed for obtaining such data from previously built comparable corpora. The methodologies are automatic and unsupervised which makes them good for large scale research. The task is highly practical as non-parallel multilingual data occur much more frequently than parallel corpora and accessing them is easy, although parallel sentences are a considerably more useful resource. In this study, we propose a method of automatic web crawling in order to build topic-aligned comparable corpora, e.g. based on the Wikipedia or ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06807",
        "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus",
        "authors": [
            "Iulian Vlad Serban",
            "Alberto Garc\u00eda-Dur\u00e1n",
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Sarath Chandar",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07012",
        "title": "Semi-supervised Word Sense Disambiguation with Neural Models",
        "authors": [
            "Dayu Yuan",
            "Julian Richardson",
            "Ryan Doherty",
            "Colin Evans",
            "Eric Altendorf"
        ],
        "abstract": "Determining the intended sense of words in text - word sense disambiguation (WSD) - is a long standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs.\n    ",
        "submission_date": "2016-03-22T00:00:00",
        "last_modified_date": "2016-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07044",
        "title": "Recurrent Neural Network Encoder with Attention for Community Question Answering",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James Glass"
        ],
        "abstract": "We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07185",
        "title": "Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings",
        "authors": [
            "Rajesh Bordawekar",
            "Oded Shmueli"
        ],
        "abstract": "We apply distributed language embedding methods from Natural Language Processing to assign a vector to each database entity associated token (for example, a token may be a word occurring in a table row, or the name of a column). These vectors, of typical dimension 200, capture the meaning of tokens based on the contexts in which the tokens appear together. To form vectors, we apply a learning method to a token sequence derived from the database. We describe various techniques for extracting token sequences from a database. The techniques differ in complexity, in the token sequences they output and in the database information used (e.g., foreign keys). The vectors can be used to algebraically quantify semantic relationships between the tokens such as similarities and analogies. Vectors enable a dual view of the data: relational and (meaningful rather than purely syntactical) text. We introduce and explore a new class of queries called cognitive intelligence (CI) queries that extract information from the database based, in part, on the relationships encoded by vectors. We have implemented a prototype system on top of Spark to exhibit the power of CI queries. Here, CI queries are realized via SQL UDFs. This power goes far beyond text extensions to relational systems due to the information encoded in vectors. We also consider various extensions to the basic scheme, including using a collection of views derived from the database to focus on a domain of interest, utilizing vectors and/or text from external sources, maintaining vectors as the database evolves and exploring a database without utilizing its schema. For the latter, we consider minimal extensions to SQL to vastly improve query expressiveness.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07252",
        "title": "Neural Summarization by Extracting Sentences and Words",
        "authors": [
            "Jianpeng Cheng",
            "Mirella Lapata"
        ],
        "abstract": "Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07253",
        "title": "Evaluating semantic models with word-sentence relatedness",
        "authors": [
            "Kimberly Glasgow",
            "Matthew Roos",
            "Amy Haufler",
            "Mark Chevillet",
            "Michael Wolmetz"
        ],
        "abstract": "Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07603",
        "title": "Semantic Regularities in Document Representations",
        "authors": [
            "Fei Sun",
            "Jiafeng Guo",
            "Yanyan Lan",
            "Jun Xu",
            "Xueqi Cheng"
        ],
        "abstract": "Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07609",
        "title": "Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL",
        "authors": [
            "Yevgeni Berzak",
            "Roi Reichart",
            "Boris Katz"
        ],
        "abstract": "This work examines the impact of cross-linguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07624",
        "title": "Semantic Properties of Customer Sentiment in Tweets",
        "authors": [
            "Eun Hee Ko",
            "Diego Klabjan"
        ],
        "abstract": "An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form. Text mining is an emerging technique for mining useful information from the web. We aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07695",
        "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings",
        "authors": [
            "Quan Liu",
            "Zhen-Hua Ling",
            "Hui Jiang",
            "Yu Hu"
        ],
        "abstract": "This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07771",
        "title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
        "authors": [
            "Remi Lebret",
            "David Grangier",
            "Michael Auli"
        ],
        "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07954",
        "title": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning",
        "authors": [
            "Karthik Narasimhan",
            "Adam Yala",
            "Regina Barzilay"
        ],
        "abstract": "Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08016",
        "title": "Classifying Syntactic Regularities for Hundreds of Languages",
        "authors": [
            "Reed Coke",
            "Ben King",
            "Dragomir Radev"
        ],
        "abstract": "This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label pre- diction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08023",
        "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
        "authors": [
            "Chia-Wei Liu",
            "Ryan Lowe",
            "Iulian V. Serban",
            "Michael Noseworthy",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08042",
        "title": "On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition",
        "authors": [
            "Rohit Prabhavalkar",
            "Ouais Alsharif",
            "Antoine Bruguier",
            "Ian McGraw"
        ],
        "abstract": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.\n    ",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08048",
        "title": "\"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions",
        "authors": [
            "Michael Ruster"
        ],
        "abstract": "This thesis focuses on gaining linguistic insights into textual discussions on a word level. It was of special interest to distinguish messages that constructively contribute to a discussion from those that are detrimental to them. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are indicators for either of the two discussion styles. These messages are nowadays often used in guidelines for successful communication. Although their effects have been successfully evaluated multiple times, a large-scale analysis has never been conducted.\n",
        "submission_date": "2016-03-25T00:00:00",
        "last_modified_date": "2016-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08148",
        "title": "Pointing the Unknown Words",
        "authors": [
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Yoshua Bengio"
        ],
        "abstract": "The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context.~We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known.~We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.\n    ",
        "submission_date": "2016-03-26T00:00:00",
        "last_modified_date": "2016-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08458",
        "title": "Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks",
        "authors": [
            "Shaodian Zhang",
            "Edouard Grave",
            "Elizabeth Sklar",
            "Noemie Elhadad"
        ],
        "abstract": "Identifying topics of discussions in online health communities (OHC) is critical to various applications, but can be difficult because topics of OHC content are usually heterogeneous and domain-dependent. In this paper, we provide a multi-class schema, an annotated dataset, and supervised classifiers based on convolutional neural network (CNN) and other models for the task of classifying discussion topics. We apply the CNN classifier to the most popular breast cancer online community, and carry out a longitudinal analysis to show topic distributions and topic changes throughout members' participation. Our experimental results suggest that CNN outperforms other classifiers in the task of topic classification, and that certain trajectories can be detected with respect to topic changes.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08474",
        "title": "Deep Embedding for Spatial Role Labeling",
        "authors": [
            "Oswaldo Ludwig",
            "Xiao Liu",
            "Parisa Kordjamshidi",
            "Marie-Francine Moens"
        ],
        "abstract": "This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08594",
        "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
        "authors": [
            "Geetanjali Rakshit",
            "Sagar Sontakke",
            "Pushpak Bhattacharyya",
            "Gholamreza Haffari"
        ],
        "abstract": "In this paper, we attempt to solve the problem of Prepositional Phrase (PP) attachments in English. The motivation for the work comes from NLP applications like Machine Translation, for which, getting the correct attachment of prepositions is very crucial. The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08701",
        "title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets",
        "authors": [
            "Enrico Santus",
            "Tin-Shing Chiu",
            "Qin Lu",
            "Alessandro Lenci",
            "Chu-Ren Huang"
        ],
        "abstract": "In this paper, we claim that Vector Cosine, which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that, independently of the adopted parameters, outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08702",
        "title": "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations",
        "authors": [
            "Enrico Santus",
            "Alessandro Lenci",
            "Tin-Shing Chiu",
            "Qin Lu",
            "Chu-Ren Huang"
        ],
        "abstract": "ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08705",
        "title": "ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms",
        "authors": [
            "Enrico Santus",
            "Tin-Shing Chiu",
            "Qin Lu",
            "Alessandro Lenci",
            "Chu-Ren Huang"
        ],
        "abstract": "In this paper, we describe ROOT13, a supervised system for the classification of hypernyms, co-hyponyms and random words. The system relies on a Random Forest algorithm and 13 unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%, against a baseline of 57.6% (vector cosine). When the classification is binary, ROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%), hypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our results are competitive with stateof-the-art models.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08832",
        "title": "Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community",
        "authors": [
            "Ethan Fast",
            "Tina Vachovsky",
            "Michael S. Bernstein"
        ],
        "abstract": "Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors' use of gender stereotypes is associated with the community's ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08865",
        "title": "Compilation as a Typed EDSL-to-EDSL Transformation",
        "authors": [
            "Emil Axelsson"
        ],
        "abstract": "This article is about an implementation and compilation technique that is used in RAW-Feldspar which is a complete rewrite of the Feldspar embedded domain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level functional language that generates efficient C code to run on embedded targets. The gist of the technique presented in this post is the following: rather writing a back end that converts pure Feldspar expressions directly to C, we translate them to a low-level monadic EDSL. From the low-level EDSL, C code is then generated. This approach has several advantages:\n",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2018-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08868",
        "title": "A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity",
        "authors": [
            "Ildik\u00f3 Pil\u00e1n",
            "Sowmya Vajjala",
            "Elena Volodina"
        ],
        "abstract": "Corpora and web texts can become a rich language learning resource if we have a means of assessing whether they are linguistically appropriate for learners at a given proficiency level. In this paper, we aim at addressing this issue by presenting the first approach for predicting linguistic complexity for Swedish second language learning material on a 5-point scale. After showing that the traditional Swedish readability measure, L\u00e4sbarhetsindex (LIX), is not suitable for this task, we propose a supervised machine learning model, based on a range of linguistic features, that can reliably classify texts according to their difficulty level. Our model obtained an accuracy of 81.3% and an F-score of 0.8, which is comparable to the state of the art in English and is considerably higher than previously reported results for other languages. We further studied the utility of our features with single sentences instead of full texts since sentences are a common linguistic unit in language learning exercises. We trained a separate model on sentence-level data with five classes, which yielded 63.4% accuracy. Although this is lower than the document level performance, we achieved an adjacent accuracy of 92%. Furthermore, we found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. Our models are intended for use in a freely accessible web-based language learning platform for the automatic generation of exercises.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08884",
        "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
        "authors": [
            "Adam Trischler",
            "Zheng Ye",
            "Xingdi Yuan",
            "Jing He",
            "Phillip Bachman",
            "Kaheer Suleman"
        ],
        "abstract": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\\% absolute).\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08887",
        "title": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
        "authors": [
            "Greg Durrett",
            "Taylor Berg-Kirkpatrick",
            "Dan Klein"
        ],
        "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09054",
        "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs",
        "authors": [
            "Enrico Santus",
            "Tin-Shing Chiu",
            "Qin Lu",
            "Alessandro Lenci",
            "Chu-Ren Huang"
        ],
        "abstract": "In this paper, we claim that vector cosine, which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09128",
        "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders",
        "authors": [
            "Simon \u0160uster",
            "Ivan Titov",
            "Gertjan van Noord"
        ],
        "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09170",
        "title": "Model Interpolation with Trans-dimensional Random Field Language Models for Speech Recognition",
        "authors": [
            "Bin Wang",
            "Zhijian Ou",
            "Yong He",
            "Akinori Kawamura"
        ],
        "abstract": "The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we examine how the TRF models can be interpolated with the NN models, and obtain 12.1\\% and 17.9\\% relative error rate reductions over 6-gram LMs for English and Chinese speech recognition respectively through log-linear combination.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09188",
        "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings",
        "authors": [
            "Spandana Gella",
            "Mirella Lapata",
            "Frank Keller"
        ],
        "abstract": "We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09405",
        "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding",
        "authors": [
            "Peng Li",
            "Heng Huang"
        ],
        "abstract": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09457",
        "title": "LSTM based Conversation Models",
        "authors": [
            "Yi Luan",
            "Yangfeng Ji",
            "Mari Ostendorf"
        ],
        "abstract": "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09460",
        "title": "System Combination for Short Utterance Speaker Recognition",
        "authors": [
            "Lantian Li",
            "Dong Wang",
            "Xiaodong Zhang",
            "Thomas Fang Zheng",
            "Panshi Jin"
        ],
        "abstract": "For text-independent short-utterance speaker recognition (SUSR), the performance often degrades dramatically. This paper presents a combination approach to the SUSR tasks with two phonetic-aware systems: one is the DNN-based i-vector system and the other is our recently proposed subregion-based GMM-UBM system. The former employs phone posteriors to construct an i-vector model in which the shared statistics offers stronger robustness against limited test data, while the latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A score-level fusion is implemented to integrate the respective advantages from the two systems. Experimental results show that for the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the score-level system combination delivers performance improvement.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09509",
        "title": "Learning Multiscale Features Directly From Waveforms",
        "authors": [
            "Zhenyao Zhu",
            "Jesse H. Engel",
            "Awni Hannun"
        ],
        "abstract": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09630",
        "title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation",
        "authors": [
            "Pawel Swietojanski",
            "Steve Renals"
        ],
        "abstract": "We present a deep neural network (DNN) acoustic model that includes parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned $L_p$-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with relative word error rates reductions ranging from 5--20% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNN-based acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09631",
        "title": "Data Collection for Interactive Learning through the Dialog",
        "authors": [
            "Miroslav Vodol\u00e1n",
            "Filip Jur\u010d\u00ed\u010dek"
        ],
        "abstract": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09643",
        "title": "Multi-task Recurrent Model for Speech and Speaker Recognition",
        "authors": [
            "Zhiyuan Tang",
            "Lantian Li",
            "Dong Wang"
        ],
        "abstract": "Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09727",
        "title": "Neural Language Correction with Character-Based Attention",
        "authors": [
            "Ziang Xie",
            "Anand Avati",
            "Naveen Arivazhagan",
            "Dan Jurafsky",
            "Andrew Y. Ng"
        ],
        "abstract": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014 Shared Task. We further demonstrate that training the network on additional data with synthesized errors can improve performance.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00077",
        "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection",
        "authors": [
            "Sheng-syun Shen",
            "Hung-yi Lee"
        ],
        "abstract": "Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.\n    ",
        "submission_date": "2016-03-31T00:00:00",
        "last_modified_date": "2016-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00100",
        "title": "A Compositional Approach to Language Modeling",
        "authors": [
            "Kushal Arora",
            "Anand Rangarajan"
        ],
        "abstract": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00117",
        "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding",
        "authors": [
            "Aaron Jaech",
            "Larry Heck",
            "Mari Ostendorf"
        ],
        "abstract": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00119",
        "title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web Discussion Forums",
        "authors": [
            "Krish Perumal"
        ],
        "abstract": "Web discussion forums are used by millions of people worldwide to share information belonging to a variety of domains such as automotive vehicles, pets, sports, etc. They typically contain posts that fall into different categories such as problem, solution, feedback, spam, etc. Automatic identification of these categories can aid information retrieval that is tailored for specific user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying a single category or do not report category-specific performance. In contrast, this work proposes unsupervised and semi-supervised methods that require no or minimal training data to achieve this objective without compromising on performance. A fine-grained analysis is also carried out to discuss their limitations. The proposed methods are based on sequence models (specifically, Hidden Markov Models) that can model language for each category using word and part-of-speech probability distributions, and manually specified features. Empirical evaluations across domains demonstrate that the proposed methods are better suited for this task than existing ones.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00126",
        "title": "Nonparametric Spherical Topic Modeling with Word Embeddings",
        "authors": [
            "Kayhan Batmanghelich",
            "Ardavan Saeedi",
            "Karthik Narasimhan",
            "Sam Gershman"
        ],
        "abstract": "Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00317",
        "title": "A Semisupervised Approach for Language Identification based on Ladder Networks",
        "authors": [
            "Ehud Ben-Reuven",
            "Jacob Goldberger"
        ],
        "abstract": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00400",
        "title": "Revisiting Summarization Evaluation for Scientific Articles",
        "authors": [
            "Arman Cohan",
            "Nazli Goharian"
        ],
        "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00425",
        "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison",
        "authors": [
            "Shyam Upadhyay",
            "Manaal Faruqui",
            "Chris Dyer",
            "Dan Roth"
        ],
        "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00461",
        "title": "Embedding Lexical Features via Low-Rank Tensors",
        "authors": [
            "Mo Yu",
            "Mark Dredze",
            "Raman Arora",
            "Matthew Gormley"
        ],
        "abstract": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00466",
        "title": "Automatic Annotation of Structured Facts in Images",
        "authors": [
            "Mohamed Elhoseiny",
            "Scott Cohen",
            "Walter Chang",
            "Brian Price",
            "Ahmed Elgammal"
        ],
        "abstract": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00502",
        "title": "Online Updating of Word Representations for Part-of-Speech Tagging",
        "authors": [
            "Wenpeng Yin",
            "Tobias Schnabel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00503",
        "title": "Discriminative Phrase Embedding for Paraphrase Identification",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00562",
        "title": "Reasoning About Pragmatics with Neural Listeners and Speakers",
        "authors": [
            "Jacob Andreas",
            "Dan Klein"
        ],
        "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \"listener\" and \"speaker\" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.\n    ",
        "submission_date": "2016-04-02T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00727",
        "title": "Character-Level Question Answering with Attention",
        "authors": [
            "David Golub",
            "Xiaodong He"
        ],
        "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00734",
        "title": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks",
        "authors": [
            "Matthew Francis-Landau",
            "Greg Durrett",
            "Dan Klein"
        ],
        "abstract": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00788",
        "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
        "authors": [
            "Minh-Thang Luong",
            "Christopher D. Manning"
        ],
        "abstract": "Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00834",
        "title": "In narrative texts punctuation marks obey the same statistics as words",
        "authors": [
            "Andrzej Kulig",
            "Jaroslaw Kwapien",
            "Tomasz Stanisz",
            "Stanislaw Drozdz"
        ],
        "abstract": "From a grammar point of view, the role of punctuation marks in a sentence is formally defined and well understood. In semantic analysis punctuation plays also a crucial role as a method of avoiding ambiguity of the meaning. A different situation can be observed in the statistical analyses of language samples, where the decision on whether the punctuation marks should be considered or should be neglected is seen rather as arbitrary and at present it belongs to a researcher's preference. An objective of this work is to shed some light onto this problem by providing us with an answer to the question whether the punctuation marks may be treated as ordinary words and whether they should be included in any analysis of the word co-occurences. We already know from our previous study (S.~Dro\u017cd\u017c {\\it et al.}, Inf. Sci. 331 (2016) 32-44) that full stops that determine the length of sentences are the main carrier of long-range correlations. Now we extend that study and analyze statistical properties of the most common punctuation marks in a few Indo-European languages, investigate their frequencies, and locate them accordingly in the Zipf rank-frequency plots as well as study their role in the word-adjacency networks. We show that, from a statistical viewpoint, the punctuation marks reveal properties that are qualitatively similar to the properties of the most frequent words like articles, conjunctions, pronouns, and prepositions. This refers to both the Zipfian analysis and the network analysis. By adding the punctuation marks to the Zipf plots, we also show that these plots that are normally described by the Zipf-Mandelbrot distribution largely restore the power-law Zipfian behaviour for the most frequent items.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00933",
        "title": "Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding",
        "authors": [
            "Walid Shalaby",
            "Khalifeh Al Jadda",
            "Mohammed Korayem",
            "Trey Grainger"
        ],
        "abstract": "We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of 97% using the proposed distributional representations ensemble.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00938",
        "title": "Multi-Field Structural Decomposition for Question Answering",
        "authors": [
            "Tomasz Jurczyk",
            "Jinho D. Choi"
        ],
        "abstract": "This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an absolute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01178",
        "title": "Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks",
        "authors": [
            "Aliaksei Severyn",
            "Alessandro Moschitti"
        ],
        "abstract": "In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01221",
        "title": "Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project",
        "authors": [
            "Guntis Barzdins",
            "Steve Renals",
            "Didzis Gosko"
        ],
        "abstract": "The paper steps outside the comfort-zone of the traditional NLP tasks like automatic speech recognition (ASR) and machine translation (MT) to addresses two novel problems arising in the automated multilingual news monitoring: segmentation of the TV and radio program ASR transcripts into individual stories, and clustering of the individual stories coming from various sources and languages into storylines. Storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring. We address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models. To enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level. The story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process. The results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01235",
        "title": "A new TAG Formalism for Tamil and Parser Analytics",
        "authors": [
            "Vijay Krishna Menon",
            "S. Rajendran",
            "M. Anand Kumar",
            "K.P. Soman"
        ],
        "abstract": "Tree adjoining grammar (TAG) is specifically suited for morph rich and agglutinated languages like Tamil due to its psycho linguistic features and parse time dependency and morph resolution. Though TAG and LTAG formalisms have been known for about 3 decades, efforts on designing TAG Syntax for Tamil have not been entirely successful due to the complexity of its specification and the rich morphology of Tamil language. In this paper we present a minimalistic TAG for Tamil without much morphological considerations and also introduce a parser implementation with some obvious variations from the XTAG system\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01272",
        "title": "Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses",
        "authors": [
            "Despoina Christou"
        ],
        "abstract": "Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher level meaningful abstractions. This dissertation employs Neural Networks for distributed paragraph representations, and Latent Dirichlet Allocation to capture higher level features of paragraph vectors. Although Neural Networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors, we show that a quick topic analysis model such as Latent Dirichlet Allocation can provide meaningful features too. We evaluate the two methods on the CMU Movie Summary Corpus, a collection of 25,203 movie plot summaries extracted from Wikipedia. Finally, for both approaches, we use K-Nearest Neighbors to discover similar movies, and plot the projected representations using T-Distributed Stochastic Neighbor Embedding to depict the context similarities. These similarities, expressed as movie distances, can be used for movies recommendation. The recommended movies of this approach are compared with the recommended movies from IMDB, which use a collaborative filtering recommendation approach, to show that our two models could constitute either an alternative or a supplementary recommendation approach.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01278",
        "title": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy",
        "authors": [
            "Guntis Barzdins",
            "Didzis Gosko"
        ],
        "abstract": "Two extensions to the AMR smatch scoring script are presented. The first extension com-bines the smatch scoring script with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. This first extension results in 4% gain over the state-of-art CAMR baseline parser by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. The second extension combines a per-sentence smatch with an en-semble method for selecting the best AMR graph among the set of AMR graphs for the same sentence. This second modification au-tomatically yields further 0.4% gain when ap-plied to outputs of two nondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level neural translation AMR parser. For AMR parsing task the character-level neural translation attains surprising 7% gain over the carefully optimized word-level neural translation. Overall, we achieve smatch F1=62% on the SemEval-2016 official scor-ing set and F1=67% on the LDC2015E86 test set.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01537",
        "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder",
        "authors": [
            "Xiaoyuan Yi",
            "Ruoyu Li",
            "Maosong Sun"
        ],
        "abstract": "We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve performance.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01692",
        "title": "An Ensemble Method to Produce High-Quality Word Embeddings (2016)",
        "authors": [
            "Robyn Speer",
            "Joshua Chin"
        ],
        "abstract": "A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of $\\rho = .596$ on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2019-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01696",
        "title": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories",
        "authors": [
            "Nasrin Mostafazadeh",
            "Nathanael Chambers",
            "Xiaodong He",
            "Devi Parikh",
            "Dhruv Batra",
            "Lucy Vanderwende",
            "Pushmeet Kohli",
            "James Allen"
        ],
        "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01729",
        "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text",
        "authors": [
            "Subhashini Venugopalan",
            "Lisa Anne Hendricks",
            "Raymond Mooney",
            "Kate Saenko"
        ],
        "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01792",
        "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR",
        "authors": [
            "Tom Sercu",
            "Vaibhava Goel"
        ],
        "abstract": "Very deep CNNs with small 3x3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper we investigate how to efficiently scale these models to larger datasets. Specifically, we address the design choice of pooling and padding along the time dimension which renders convolutional evaluation of sequences highly inefficient. We propose a new CNN design without timepadding and without timepooling, which is slightly suboptimal for accuracy, but has two significant advantages: it enables sequence training and deployment by allowing efficient convolutional evaluation of full utterances, and, it allows for batch normalization to be straightforwardly adopted to CNNs on sequence data. Through batch normalization, we recover the lost peformance from removing the time-pooling, while keeping the benefit of efficient convolutional evaluation. We demonstrate the performance of our models both on larger scale data than before, and after sequence training. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of the 2015 IBM system combination, which was the previous best published result.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01904",
        "title": "Neural Headline Generation with Sentence-wise Optimization",
        "authors": [
            "Ayana",
            "Shiqi Shen",
            "Yu Zhao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Recently, neural models have been proposed for headline generation by learning to map documents to headlines with recurrent neural networks. Nevertheless, as traditional neural network utilizes maximum likelihood estimation for parameter optimization, it essentially constrains the expected training objective within word level rather than sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters in sentence level with respect to evaluation metrics and leads to significant improvements for headline generation. Experiment results show that our models outperforms state-of-the-art systems on both English and Chinese headline generation tasks.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02201",
        "title": "Transfer Learning for Low-Resource Neural Machine Translation",
        "authors": [
            "Barret Zoph",
            "Deniz Yuret",
            "Jonathan May",
            "Kevin Knight"
        ],
        "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.\n    ",
        "submission_date": "2016-04-08T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02506",
        "title": "Word embeddings and recurrent neural networks based on Long-Short Term Memory nodes in supervised biomedical word sense disambiguation",
        "authors": [
            "Antonio Jimeno Yepes"
        ],
        "abstract": "Word sense disambiguation helps identifying the proper sense of ambiguous words in text. With large terminologies such as the UMLS Metathesaurus ambiguities appear and highly effective disambiguation methods are required. Supervised learning algorithm methods are used as one of the approaches to perform disambiguation. Features extracted from the context of an ambiguous word are used to identify the proper sense of such a word. The type of features have an impact on machine learning methods, thus affect disambiguation performance. In this work, we have evaluated several types of features derived from the context of the ambiguous word and we have explored as well more global features derived from MEDLINE using word embeddings. Results show that word embeddings improve the performance of more traditional features and allow as well using recurrent neural network classifiers based on Long-Short Term Memory (LSTM) nodes. The combination of unigrams and word embeddings with an SVM sets a new state of the art performance with a macro accuracy of 95.97 in the MSH WSD data set.\n    ",
        "submission_date": "2016-04-09T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02612",
        "title": "Fusing Audio, Textual and Visual Features for Sentiment Analysis of News Videos",
        "authors": [
            "Mois\u00e9s H. R. Pereira",
            "Fl\u00e1vio L. C. P\u00e1dua",
            "Adriano C. M. Pereira",
            "Fabr\u00edcio Benevenuto",
            "Daniel H. Dalip"
        ],
        "abstract": "This paper presents a novel approach to perform sentiment analysis of news videos, based on the fusion of audio, textual and visual clues extracted from their contents. The proposed approach aims at contributing to the semiodiscoursive study regarding the construction of the ethos (identity) of this media universe, which has become a central part of the modern-day lives of millions of people. To achieve this goal, we apply state-of-the-art computational methods for (1) automatic emotion recognition from facial expressions, (2) extraction of modulations in the participants' speeches and (3) sentiment analysis from the closed caption associated to the videos of interest. More specifically, we compute features, such as, visual intensities of recognized emotions, field sizes of participants, voicing probability, sound loudness, speech fundamental frequencies and the sentiment scores (polarities) from text sentences in the closed caption. Experimental results with a dataset containing 520 annotated news videos from three Brazilian and one American popular TV newscasts show that our approach achieves an accuracy of up to 84% in the sentiments (tension levels) classification task, thus demonstrating its high potential to be used by media analysts in several applications, especially, in the journalistic domain.\n    ",
        "submission_date": "2016-04-09T00:00:00",
        "last_modified_date": "2016-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02843",
        "title": "Method of Tibetan Person Knowledge Extraction",
        "authors": [
            "Yuan Sun",
            "Zhen Zhu"
        ],
        "abstract": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02993",
        "title": "Using Sentence-Level LSTM Language Models for Script Inference",
        "authors": [
            "Karl Pichotta",
            "Raymond J. Mooney"
        ],
        "abstract": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03029",
        "title": "Mapping Out Narrative Structures and Dynamics Using Networks and Textual Information",
        "authors": [
            "Semi Min",
            "Juyong Park"
        ],
        "abstract": "Human communication is often executed in the form of a narrative, an account of connected events composed of characters, actions, and settings. A coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication, opening up the possibility of a deeper understanding of a narrative by studying its structural properties. In this paper we present a network-based framework for modeling and analyzing the structure of a narrative, which is further expanded by incorporating methods from computational linguistics to utilize the narrative text. Modeling a narrative as a dynamically unfolding system, we characterize its progression via the growth patterns of the character network, and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords. This is a network framework advanced beyond the simple occurrence-based one most often used until now, allowing one to utilize the unique characteristics of a given narrative to a high degree. Given the ubiquity and importance of narratives, such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions, expression of human sentiments, and communication.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03035",
        "title": "Learning Global Features for Coreference Resolution",
        "authors": [
            "Sam Wiseman",
            "Alexander M. Rush",
            "Stuart M. Shieber"
        ],
        "abstract": "There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03114",
        "title": "Conversational flow in Oxford-style debates",
        "authors": [
            "Justine Zhang",
            "Ravi Kumar",
            "Sujith Ravi",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03136",
        "title": "Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text",
        "authors": [
            "Arnav Sharma",
            "Sakshi Gupta",
            "Raveesh Motlani",
            "Piyush Bansal",
            "Manish Srivastava",
            "Radhika Mamidi",
            "Dipti M. Sharma"
        ],
        "abstract": "In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identifier, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the first to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03209",
        "title": "Disfluency Detection using a Bidirectional LSTM",
        "authors": [
            "Vicky Zayats",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "We introduce a new approach for disfluency detection using a Bidirectional Long-Short Term Memory neural network (BLSTM). In addition to the word sequence, the model takes as input pattern match features that were developed to reduce sensitivity to vocabulary size in training, which lead to improved performance over the word sequence alone. The BLSTM takes advantage of explicit repair states in addition to the standard reparandum states. The final output leverages integer linear programming to incorporate constraints of disfluency structure. In experiments on the Switchboard corpus, the model achieves state-of-the-art performance for both the standard disfluency detection task and the correction detection task. Analysis shows that the model has better detection of non-repetition disfluencies, which tend to be much harder to detect.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03357",
        "title": "Improving sentence compression by learning to predict gaze",
        "authors": [
            "Sigrid Klerke",
            "Yoav Goldberg",
            "Anders S\u00f8gaard"
        ],
        "abstract": "We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03968",
        "title": "Visual Storytelling",
        "authors": [
            "Ting-Hao",
            "Huang",
            "Francis Ferraro",
            "Nasrin Mostafazadeh",
            "Ishan Misra",
            "Aishwarya Agrawal",
            "Jacob Devlin",
            "Ross Girshick",
            "Xiaodong He",
            "Pushmeet Kohli",
            "Dhruv Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh",
            "Lucy Vanderwende",
            "Michel Galley",
            "Margaret Mitchell"
        ],
        "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04358",
        "title": "StalemateBreaker: A Proactive Content-Introducing Approach to Automatic Human-Computer Conversation",
        "authors": [
            "Xiang Li",
            "Lili Mou",
            "Rui Yan",
            "Ming Zhang"
        ],
        "abstract": "Existing open-domain human-computer conversation systems are typically passive: they either synthesize or retrieve a reply provided a human-issued utterance. It is generally presumed that humans should take the role to lead the conversation and introduce new content when a stalemate occurs, and that the computer only needs to \"respond.\" In this paper, we propose StalemateBreaker, a conversation system that can proactively introduce new content when appropriate. We design a pipeline to determine when, what, and how to introduce new content during human-computer conversation. We further propose a novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between conversation context and candidate replies. Experiments show that both the content-introducing approach and the reranking algorithm are effective. Our full StalemateBreaker model outperforms a state-of-the-practice conversation system by +14.4% p@1 when a stalemate occurs.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04378",
        "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN",
        "authors": [
            "Shengxian Wan",
            "Yanyan Lan",
            "Jun Xu",
            "Jiafeng Guo",
            "Liang Pang",
            "Xueqi Cheng"
        ],
        "abstract": "Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04562",
        "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
        "authors": [
            "Tsung-Hsien Wen",
            "David Vandyke",
            "Nikola Mrksic",
            "Milica Gasic",
            "Lina M. Rojas-Barahona",
            "Pei-Hao Su",
            "Stefan Ultes",
            "Steve Young"
        ],
        "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04677",
        "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction",
        "authors": [
            "Allen Schmaltz",
            "Yoon Kim",
            "Alexander M. Rush",
            "Stuart M. Shieber"
        ],
        "abstract": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04802",
        "title": "Supervised and Unsupervised Ensembling for Knowledge Base Population",
        "authors": [
            "Nazneen Fatema Rajani",
            "Raymond J. Mooney"
        ],
        "abstract": "We present results on combining supervised and unsupervised methods to ensemble multiple systems for two popular Knowledge Base Population (KBP) tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that our combined system along with auxiliary features outperforms the best performing system for both tasks in the 2015 competition, several ensembling baselines, as well as the state-of-the-art stacking approach to ensembling KBP systems. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling.\n    ",
        "submission_date": "2016-04-16T00:00:00",
        "last_modified_date": "2016-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04835",
        "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions",
        "authors": [
            "Han Xiao",
            "Minlie Huang",
            "Xiaoyan Zhu"
        ],
        "abstract": "Knowledge representation is an important, long-history topic in AI, and there have been a large amount of work for knowledge graph embedding which projects symbolic entities and relations into low-dimensional, real-valued vector space. However, most embedding methods merely concentrate on data fitting and ignore the explicit semantic expression, leading to uninterpretable representations. Thus, traditional embedding methods have limited potentials for many applications such as question answering, and entity classification. To this end, this paper proposes a semantic representation method for knowledge graph \\textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines substantially.\n    ",
        "submission_date": "2016-04-17T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04873",
        "title": "From Incremental Meaning to Semantic Unit (phrase by phrase)",
        "authors": [
            "Andreas Scherbakov",
            "Ekaterina Vylomova",
            "Fei Liu",
            "Timothy Baldwin"
        ],
        "abstract": "This paper describes an experimental approach to Detection of Minimal Semantic Units and their Meaning (DiMSUM), explored within the framework of SemEval 2016 Task 10. The approach is primarily based on a combination of word embeddings and parserbased features, and employs unidirectional incremental computation of compositional embeddings for multiword expressions.\n    ",
        "submission_date": "2016-04-17T00:00:00",
        "last_modified_date": "2016-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05073",
        "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization",
        "authors": [
            "Daniel Beck",
            "Adri\u00e0 de Gispert",
            "Gonzalo Iglesias",
            "Aurelien Waite",
            "Bill Byrne"
        ],
        "abstract": "We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05372",
        "title": "Clustering Comparable Corpora of Russian and Ukrainian Academic Texts: Word Embeddings and Semantic Fingerprints",
        "authors": [
            "Andrey Kutuzov",
            "Mikhail Kopotev",
            "Tatyana Sviridenko",
            "Lyubov Ivanova"
        ],
        "abstract": "We present our experience in applying distributional semantics (neural word embeddings) to the problem of representing and clustering documents in a bilingual comparable corpus. Our data is a collection of Russian and Ukrainian academic texts, for which topics are their academic fields. In order to build language-independent semantic representations of these documents, we train neural distributional models on monolingual corpora and learn the optimal linear transformation of vectors from one language to another. The resulting vectors are then used to produce `semantic fingerprints' of documents, serving as input to a clustering algorithm.\n",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05499",
        "title": "Exploring Segment Representations for Neural Segmentation Models",
        "authors": [
            "Yijia Liu",
            "Wanxiang Che",
            "Jiang Guo",
            "Bing Qin",
            "Ting Liu"
        ],
        "abstract": "Many natural language processing (NLP) tasks can be generalized into segmentation problem. In this paper, we combine semi-CRF with neural network to solve NLP segmentation tasks. Our model represents a segment both by composing the input units and embedding the entire segment. We thoroughly study different composition functions and different segment embeddings. We conduct extensive experiments on two typical segmentation tasks: named entity recognition (NER) and Chinese word segmentation (CWS). Experimental results show that our neural semi-CRF model benefits from representing the entire segment and achieves the state-of-the-art performance on CWS benchmark dataset and competitive results on the CoNLL03 dataset.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05519",
        "title": "M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep Convolutional Network for Answer Selection",
        "authors": [
            "Lingxun Meng",
            "Yan Li"
        ],
        "abstract": "Recent works using artificial neural networks based on distributed word representation greatly boost performance on various natural language processing tasks, especially the answer selection problem. Nevertheless, most of the previous works used deep learning methods (like LSTM-RNN, CNN, etc.) only to capture semantic representation of each sentence separately, without considering the interdependence between each other. In this paper, we propose a novel end-to-end learning framework which constitutes deep convolutional neural network based on multi-modal similarity metric learning (M$^2$S-Net) on pairwise tokens. The proposed model demonstrates its performance by surpassing previous state-of-the-art systems on the answer selection benchmark, i.e., TREC-QA dataset, in both MAP and MRR metrics.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2018-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05525",
        "title": "An Attentive Neural Architecture for Fine-grained Entity Type Classification",
        "authors": [
            "Sonse Shimaoka",
            "Pontus Stenetorp",
            "Kentaro Inui",
            "Sebastian Riedel"
        ],
        "abstract": "In this work we propose a novel attention-based neural network model for the task of fine-grained entity type classification that unlike previously proposed models recursively composes representations of entity mention contexts. Our model achieves state-of-the-art performance with 74.94% loose micro F1-score on the well-established FIGER dataset, a relative improvement of 2.59%. We also investigate the behavior of the attention mechanism of our model and observe that it can learn contextual linguistic expressions that indicate the fine-grained category memberships of an entity.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05529",
        "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
        "authors": [
            "Barbara Plank",
            "Anders S\u00f8gaard",
            "Yoav Goldberg"
        ],
        "abstract": "Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05559",
        "title": "Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts",
        "authors": [
            "Melvyn Drag",
            "Gauthaman Vasudevan"
        ],
        "abstract": "We show that an efficient and popular method for calculating bigram frequencies is unsuitable for bodies of short texts and offer a simple alternative. Our method has the same computational complexity as the old method and offers an exact count instead of an approximation.\n    ",
        "submission_date": "2016-04-18T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05747",
        "title": "Syntactic and semantic classification of verb arguments using dependency-based and rich semantic features",
        "authors": [
            "Francesco Elia"
        ],
        "abstract": "Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15, aimed at producing a system that can aid lexicographers in their efforts to build a dictionary of meanings for English verbs using the CPA annotation process. CPA parsing is one of the subtasks which this annotation process is made of and it is the focus of this report. A supervised machine-learning approach has been implemented, in which syntactic features derived from parse trees and semantic features derived from WordNet and word embeddings are used. It is shown that this approach performs well, even with the data sparsity issues that characterize the dataset, and can obtain better results than other system by a margin of about 4% f-score.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05800",
        "title": "A Deep Neural Network for Chinese Zero Pronoun Resolution",
        "authors": [
            "Qingyu Yin",
            "Weinan Zhang",
            "Yu Zhang",
            "Ting Liu"
        ],
        "abstract": "Existing approaches for Chinese zero pronoun resolution overlook semantic information. This is because zero pronouns have no descriptive information, which results in difficulty in explicitly capturing their semantic similarities with antecedents. Moreover, when dealing with candidate antecedents, traditional systems simply take advantage of the local information of a single candidate antecedent while failing to consider the underlying information provided by the other candidates from a global perspective. To address these weaknesses, we propose a novel zero pronoun-specific neural network, which is capable of representing zero pronouns by utilizing the contextual information at the semantic level. In addition, when dealing with candidate antecedents, a two-level candidate encoder is employed to explicitly capture both the local and global information of candidate antecedents. We conduct experiments on the Chinese portion of the OntoNotes 5.0 corpus. Experimental results show that our approach substantially outperforms the state-of-the-art method in various experimental settings.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2017-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05875",
        "title": "Distributed Entity Disambiguation with Per-Mention Learning",
        "authors": [
            "Tiep Mai",
            "Bichen Shi",
            "Patrick K. Nicholson",
            "Deepak Ajwani",
            "Alessandra Sala"
        ],
        "abstract": "Entity disambiguation, or mapping a phrase to its canonical representation in a knowledge base, is a fundamental step in many natural language processing applications. Existing techniques based on global ranking models fail to capture the individual peculiarities of the words and hence, either struggle to meet the accuracy requirements of many real-world applications or they are too complex to satisfy real-time constraints of applications.\n",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05878",
        "title": "A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion",
        "authors": [
            "Johannes Welbl",
            "Guillaume Bouchard",
            "Sebastian Riedel"
        ],
        "abstract": "Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06045",
        "title": "Dialog-based Language Learning",
        "authors": [
            "Jason Weston"
        ],
        "abstract": "A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06113",
        "title": "Speaker Cluster-Based Speaker Adaptive Training for Deep Neural Network Acoustic Modeling",
        "authors": [
            "Wei Chu",
            "Ruxin Chen"
        ],
        "abstract": "A speaker cluster-based speaker adaptive training (SAT) method under deep neural network-hidden Markov model (DNN-HMM) framework is presented in this paper. During training, speakers that are acoustically adjacent to each other are hierarchically clustered using an i-vector based distance metric. DNNs with speaker dependent layers are then adaptively trained for each cluster of speakers. Before decoding starts, an unseen speaker in test set is matched to the closest speaker cluster through comparing i-vector based distances. The previously trained DNN of the matched speaker cluster is used for decoding utterances of the test speaker. The performance of the proposed method on a large vocabulary spontaneous speech recognition task is evaluated on a training set of with 1500 hours of speech, and a test set of 24 speakers with 1774 utterances. Comparing to a speaker independent DNN with a baseline word error rate of 11.6%, a relative 6.8% reduction in word error rate is observed from the proposed method.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06274",
        "title": "Chinese Song Iambics Generation with Neural Attention-based Model",
        "authors": [
            "Qixin Wang",
            "Tianyi Luo",
            "Dong Wang",
            "Chao Xing"
        ],
        "abstract": "Learning and generating Chinese poems is a charming yet challenging task. Traditional approaches involve various language modeling and machine translation techniques, however, they perform not as well when generating poems with complex pattern constraints, for example Song iambics, a famous type of poems that involve variable-length sentences and strict rhythmic patterns. This paper applies the attention-based sequence-to-sequence model to generate Chinese Song iambics. Specifically, we encode the cue sentences by a bi-directional Long-Short Term Memory (LSTM) model and then predict the entire iambic with the information provided by the encoder, in the form of an attention-based LSTM that can regularize the generation process by the fine structure of the input cues. Several techniques are investigated to improve the model, including global context integration, hybrid style training, character vector initialization and adaptation. Both the automatic and subjective evaluation results show that our model indeed can learn the complex structural and rhythmic patterns of Song iambics, and the generation is rather successful.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06285",
        "title": "A Novel Approach to Dropped Pronoun Translation",
        "authors": [
            "Longyue Wang",
            "Zhaopeng Tu",
            "Xiaojun Zhang",
            "Hang Li",
            "Andy Way",
            "Qun Liu"
        ],
        "abstract": "Dropped Pronouns (DP) in which pronouns are frequently dropped in the source language but should be retained in the target language are challenge in machine translation. In response to this problem, we propose a semi-supervised approach to recall possibly missing pronouns in the translation. Firstly, we build training data for DP generation in which the DPs are automatically labelled according to the alignment information from a parallel corpus. Secondly, we build a deep learning-based DP generator for input sentences in decoding when no corresponding references exist. More specifically, the generation is two-phase: (1) DP position detection, which is modeled as a sequential labelling task with recurrent neural networks; and (2) DP prediction, which employs a multilayer perceptron with rich features. Finally, we integrate the above outputs into our translation system to recall missing pronouns by both extracting rules from the DP-labelled training data and translating the DP-generated input sentences. Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06361",
        "title": "Row-less Universal Schema",
        "authors": [
            "Patrick Verga",
            "Andrew McCallum"
        ],
        "abstract": "Universal schema jointly embeds knowledge bases and textual patterns to reason about entities and relations for automatic knowledge base construction and information extraction. In the past, entity pairs and relations were represented as learned vectors with compatibility determined by a scoring function, limiting generalization to unseen text patterns and entities. Recently, 'column-less' versions of Universal Schema have used compositional pattern encoders to generalize to all text patterns. In this work we take the next step and propose a 'row-less' model of universal schema, removing explicit entity pair representations. Instead of learning vector representations for each entity pair in our training set, we treat an entity pair as a function of its relation types. In experimental results on the FB15k-237 benchmark we demonstrate that we can match the performance of a comparable model with explicit entity pair representations using a model of attention over relation types. We further demonstrate that the model per- forms with nearly the same accuracy on entity pairs never seen during training.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06529",
        "title": "Dependency Parsing with LSTMs: An Empirical Evaluation",
        "authors": [
            "Adhiguna Kuncoro",
            "Yuichiro Sawai",
            "Kevin Duh",
            "Yuji Matsumoto"
        ],
        "abstract": "We propose a transition-based dependency parser using Recurrent Neural Networks with Long Short-Term Memory (LSTM) units. This extends the feedforward neural network parser of Chen and Manning (2014) and enables modelling of entire sequences of shift/reduce transition decisions. On the Google Web Treebank, our LSTM parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3% improvement for long-range dependencies, which has proved difficult for previous transition-based parsers due to error propagation and limited context information. Our findings additionally suggest that dropout regularisation on the embedding layer is crucial to improve the LSTM's generalisation.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06583",
        "title": "SweLL on the rise: Swedish Learner Language corpus for European Reference Level studies",
        "authors": [
            "Elena Volodina",
            "Ildik\u00f3 Pil\u00e1n",
            "Ingegerd Enstr\u00f6m",
            "Lorena Llozhi",
            "Peter Lundkvist",
            "Gunl\u00f6g Sundberg",
            "Monica Sandell"
        ],
        "abstract": "We present a new resource for Swedish, SweLL, a corpus of Swedish Learner essays linked to learners' performance according to the Common European Framework of Reference (CEFR). SweLL consists of three subcorpora - SpIn, SW1203 and Tisus, collected from three different educational establishments. The common metadata for all subcorpora includes age, gender, native languages, time of residence in Sweden, type of written task. Depending on the subcorpus, learner texts may contain additional information, such as text genres, topics, grades. Five of the six CEFR levels are represented in the corpus: A1, A2, B1, B2 and C1 comprising in total 339 essays. C2 level is not included since courses at C2 level are not offered. The work flow consists of collection of essays and permits, essay digitization and registration, meta-data annotation, automatic linguistic annotation. Inter-rater agreement is presented on the basis of SW1203 subcorpus. The work on SweLL is still ongoing with more than 100 essays waiting in the pipeline. This article both describes the resource and the \"how-to\" behind the compilation of SweLL.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06635",
        "title": "Bridging LSTM Architecture and the Neural Dynamics during Reading",
        "authors": [
            "Peng Qian",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06648",
        "title": "Automatic verbal aggression detection for Russian and American imageboards",
        "authors": [
            "Denis Gordeev"
        ],
        "abstract": "The problem of aggression for Internet communities is rampant. Anonymous forums usually called imageboards are notorious for their aggressive and deviant behaviour even in comparison with other Internet communities. This study is aimed at studying ways of automatic detection of verbal expression of aggression for the most popular American (",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06650",
        "title": "Detecting state of aggression in sentences using CNN",
        "authors": [
            "Rodmonga Potapova",
            "Denis Gordeev"
        ],
        "abstract": "In this article we study verbal expression of aggression and its detection using machine learning and neural networks methods. We test our results using our corpora of messages from anonymous imageboards. We also compare Random forest classifier with convolutional neural network for \"Movie reviews with one sentence per review\" corpus.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06896",
        "title": "Why and How to Pay Different Attention to Phrase Alignments of Different Intensities",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This work studies comparatively two typical sentence pair classification tasks: textual entailment (TE) and answer selection (AS), observing that phrase alignments of different intensities contribute differently in these tasks. We address the problems of identifying phrase alignments of flexible granularity and pooling alignments of different intensities for these tasks. Examples for flexible granularity are alignments between two single words, between a single word and a phrase and between a short phrase and a long phrase. By intensity we roughly mean the degree of match, it ranges from identity over surface-form co-occurrence, rephrasing and other semantic relatedness to unrelated words as in lots of parenthesis text. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single attention mechanism over alignment intensities without considering the characteristics of specific tasks, which limits the system's effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific focusing of phrase alignments between two sentences by attention pooling. Experimental results on TE and AS match our observation and are state-of-the-art.\n    ",
        "submission_date": "2016-04-23T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06952",
        "title": "Visualization of Jacques Lacan's Registers of the Psychoanalytic Field, and Discovery of Metaphor and of Metonymy. Analytical Case Study of Edgar Allan Poe's \"The Purloined Letter\"",
        "authors": [
            "Fionn Murtagh",
            "Giuseppe Iurato"
        ],
        "abstract": "We start with a description of Lacan's work that we then take into our analytics methodology. In a first investigation, a Lacan-motivated template of the Poe story is fitted to the data. A segmentation of the storyline is used in order to map out the diachrony. Based on this, it will be shown how synchronous aspects, potentially related to Lacanian registers, can be sought. This demonstrates the effectiveness of an approach based on a model template of the storyline narrative. In a second and more comprehensive investigation, we develop an approach for revealing, that is, uncovering, Lacanian register relationships. Objectives of this work include the wide and general application of our methodology. This methodology is strongly based on the \"letting the data speak\" Correspondence Analysis analytics platform of Jean-Paul Benz\u00e9cri, that is also the geometric data analysis, both qualitative and quantitative analytics, developed by Pierre Bourdieu.\n    ",
        "submission_date": "2016-04-23T00:00:00",
        "last_modified_date": "2017-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07370",
        "title": "Parsing Argumentation Structures in Persuasive Essays",
        "authors": [
            "Christian Stab",
            "Iryna Gurevych"
        ],
        "abstract": "In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using integer linear programming. We show that our model considerably improves the performance of base classifiers and significantly outperforms challenging heuristic baselines. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement. This corpus and the annotation guidelines are freely available for ensuring reproducibility and to encourage future research in computational argumentation.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07407",
        "title": "Conversational Markers of Constructive Discussions",
        "authors": [
            "Vlad Niculae",
            "Cristian Danescu-Niculescu-Mizil"
        ],
        "abstract": "Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)---through discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we find that over 30% of discussions are unproductive.\n",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07809",
        "title": "Entities as topic labels: Improving topic interpretability and evaluability combining Entity Linking and Labeled LDA",
        "authors": [
            "Federico Nanni",
            "Pablo Ruiz Fabo"
        ],
        "abstract": "In order to create a corpus exploration method providing topics that are easier to interpret than standard LDA topic models, here we propose combining two techniques called Entity linking and Labeled LDA. Our method identifies in an ontology a series of descriptive labels for each document in a corpus. Then it generates a specific topic for each label. Having a direct relation between topics and labels makes interpretation easier; using an ontology as background knowledge limits label ambiguity. As our topics are described with a limited number of clear-cut labels, they promote interpretability, and this may help quantitative evaluation. We illustrate the potential of the approach by applying it in order to define the most relevant topics addressed by each party in the European Parliament's fifth mandate (1999-2004).\n    ",
        "submission_date": "2016-04-26T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08120",
        "title": "Extracting Temporal and Causal Relations between Events",
        "authors": [
            "Paramita Mirza"
        ],
        "abstract": "Structured information resulting from temporal information processing is crucial for a variety of natural language processing tasks, for instance to generate timeline summarization of events from news documents, or to answer temporal/causal-related questions about some events. In this thesis we present a framework for an integrated temporal and causal relation extraction system. We first develop a robust extraction component for each type of relations, i.e. temporal order and causality. We then combine the two extraction components into an integrated relation extraction system, CATENA---CAusal and Temporal relation Extraction from NAtural language texts---, by utilizing the presumption about event precedence in causality, that causing events must happened BEFORE resulting events. Several resources and techniques to improve our relation extraction systems are also discussed, including word embeddings and training data expansion. Finally, we report our adaptation efforts of temporal information processing for languages other than English, namely Italian and Indonesian.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08242",
        "title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
        "authors": [
            "George Saon",
            "Tom Sercu",
            "Steven Rennie",
            "Hong-Kwang J. Kuo"
        ],
        "abstract": "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model \"M\" and hierarchical neural network LMs.\n    ",
        "submission_date": "2016-04-27T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08504",
        "title": "Detecting \"Smart\" Spammers On Social Network: A Topic Model Approach",
        "authors": [
            "Linqing Liu",
            "Yao Lu",
            "Ye Luo",
            "Renxian Zhang",
            "Laurent Itti",
            "Jianwei Lu"
        ],
        "abstract": "Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of \"smart\" spammers. They resemble legitimate users who are difficult to identify. In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation(LDA), a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other state-of-the-art methods in terms of averaged F1-score.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08561",
        "title": "Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance",
        "authors": [
            "Ehsaneddin Asgari",
            "Mohammad R.K. Mofrad"
        ],
        "abstract": "We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is defined as divergence between unified similarity distribution of words between languages. Using such a measure, we perform language comparison for fifty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward defining a quantitative measure of similarity between languages, with applications in languages classification, genre identification, dialect identification, and evaluation of translations.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08633",
        "title": "Word Ordering Without Syntax",
        "authors": [
            "Allen Schmaltz",
            "Alexander M. Rush",
            "Stuart M. Shieber"
        ],
        "abstract": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.\n    ",
        "submission_date": "2016-04-28T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08672",
        "title": "Distance Metric Learning for Aspect Phrase Grouping",
        "authors": [
            "Shufeng Xiong",
            "Yue Zhang",
            "Donghong Ji",
            "Yinxia Lou"
        ],
        "abstract": "Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning (ADDML) method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08781",
        "title": "Teaching natural language to computers",
        "authors": [
            "Joseph Corneli",
            "Miriam Corneli"
        ],
        "abstract": "\"Natural Language,\" whether spoken and attended to by humans, or processed and generated by computers, requires networked structures that reflect creative processes in semantic, syntactic, phonetic, linguistic, social, emotional, and cultural modules. Being able to produce novel and useful behavior following repeated practice gets to the root of both artificial intelligence and human language. This paper investigates the modalities involved in language-like applications that computers -- and programmers -- engage with, and aims to fine tune the questions we ask to better account for context, self-awareness, and embodiment.\n    ",
        "submission_date": "2016-04-29T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00090",
        "title": "Response Selection with Topic Clues for Retrieval-based Chatbots",
        "authors": [
            "Yu Wu",
            "Wei Wu",
            "Zhoujun Li",
            "Ming Zhou"
        ],
        "abstract": "We consider incorporating topic information into message-response matching to boost responses with rich content in retrieval-based chatbots. To this end, we propose a topic-aware convolutional neural tensor network (TACNTN). In TACNTN, matching between a message and a response is not only conducted between a message vector and a response vector generated by convolutional neural networks, but also leverages extra topic information encoded in two topic vectors. The two topic vectors are linear combinations of topic words of the message and the response respectively, where the topic words are obtained from a pre-trained LDA model and their weights are determined by themselves as well as the message vector and the response vector. The message vector, the response vector, and the two topic vectors are fed to neural tensors to calculate a matching score. Empirical study on a public data set and a human annotated data set shows that TACNTN can significantly outperform state-of-the-art methods for message-response matching.\n    ",
        "submission_date": "2016-04-30T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00459",
        "title": "Multi30K: Multilingual English-German Image Descriptions",
        "authors": [
            "Desmond Elliott",
            "Stella Frank",
            "Khalil Sima'an",
            "Lucia Specia"
        ],
        "abstract": "We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) descriptions crowdsourced independently of the original English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00482",
        "title": "Compositional Sentence Representation from Character within Large Context Text",
        "authors": [
            "Geonmin Kim",
            "Hwaran Lee",
            "Jisu Choi",
            "Soo-young Lee"
        ],
        "abstract": "This paper describes a Hierarchical Composition Recurrent Network (HCRN) consisting of a 3-level hierarchy of compositional models: character, word and sentence. This model is designed to overcome two problems of representing a sentence on the basis of a constituent word sequence. The first is a data-sparsity problem in word embedding, and the other is a no usage of inter-sentence dependency. In the HCRN, word representations are built from characters, thus resolving the data-sparsity problem, and inter-sentence dependency is embedded into sentence representation at the level of sentence composition. We adopt a hierarchy-wise learning scheme in order to alleviate the optimization difficulties of learning deep hierarchical recurrent network in end-to-end fashion. The HCRN was quantitatively and qualitatively evaluated on a dialogue act classification task. Especially, sentence representations with an inter-sentence dependency are able to capture both implicit and explicit semantics of sentence, significantly improving performance. In the end, the HCRN achieved state-of-the-art performance with a test error rate of 22.7% for dialogue act classification on the SWBD-DAMSL database.\n    ",
        "submission_date": "2016-05-02T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00942",
        "title": "TheanoLM - An Extensible Toolkit for Neural Network Language Modeling",
        "authors": [
            "Seppo Enarvi",
            "Mikko Kurimo"
        ],
        "abstract": "We present a new tool for training neural network language models (NNLMs), scoring sentences, and generating text. The tool has been written using Python library Theano, which allows researcher to easily extend it and tune any aspect of the training process. Regardless of the flexibility, Theano is able to generate extremely fast native code that can utilize a GPU or multiple CPU cores in order to parallelize the heavy numerical computations. The tool has been evaluated in difficult Finnish and English conversational speech recognition tasks, and significant improvement was obtained over our best back-off n-gram models. The results that we obtained in the Finnish task were compared to those from existing RNNLM and RWTHLM toolkits, and found to be as good or better, while training times were an order of magnitude shorter.\n    ",
        "submission_date": "2016-05-03T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01194",
        "title": "IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner",
        "authors": [
            "Lavanya Sita Tekumalla",
            "Sharmistha"
        ],
        "abstract": "Interpretable semantic textual similarity (iSTS) task adds a crucial explanatory layer to pairwise sentence similarity. We address various components of this task: chunk level semantic alignment along with assignment of similarity type and score for aligned chunks with a novel system presented in this paper. We propose an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forrest Classifier. Results show that our algorithm iMATCH has low execution time and outperforms most other participating systems in terms of alignment score. Of the three datasets, we are top ranked for answer- students dataset in terms of overall score and have top alignment score for headlines dataset in the gold chunks track.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01326",
        "title": "Compression and the origins of Zipf's law for word frequencies",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Here we sketch a new derivation of Zipf's law for word frequencies based on optimal coding. The structure of the derivation is reminiscent of Mandelbrot's random typing model but it has multiple advantages over random typing: (1) it starts from realistic cognitive pressures (2) it does not require fine tuning of parameters and (3) it sheds light on the origins of other statistical laws of language and thus can lead to a compact theory of linguistic laws. Our findings suggest that the recurrence of Zipf's law in human languages could originate from pressure for easy and fast communication.\n    ",
        "submission_date": "2016-05-04T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01478",
        "title": "Modeling Rich Contexts for Sentiment Classification with LSTM",
        "authors": [
            "Minlie Huang",
            "Yujie Cao",
            "Chao Dong"
        ],
        "abstract": "Sentiment analysis on social media data such as tweets and weibo has become a very important and challenging task. Due to the intrinsic properties of such data, tweets are short, noisy, and of divergent topics, and sentiment classification on these data requires to modeling various contexts such as the retweet/reply history of a tweet, and the social context about authors and relationships. While few prior study has approached the issue of modeling contexts in tweet, this paper proposes to use a hierarchical LSTM to model rich contexts in tweet, particularly long-range context. Experimental results show that contexts can help us to perform sentiment classification remarkably better.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01635",
        "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis",
        "authors": [
            "Seyed Omid Sadjadi",
            "Jason Pelecanos",
            "Sriram Ganapathy"
        ],
        "abstract": "We present the recent advances along with an error analysis of the IBM speaker recognition system for conversational speech. Some of the key advancements that contribute to our system include: a nearest-neighbor discriminant analysis (NDA) approach (as opposed to LDA) for intersession variability compensation in the i-vector space, the application of speaker and channel-adapted features derived from an automatic speech recognition (ASR) system for speaker recognition, and the use of a DNN acoustic model with a very large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as the 10sec-10sec condition. To our knowledge, results achieved by our system represent the best performances published to date on these conditions. For example, on the extended tel-tel condition (C5) the system achieves an EER of 0.59%. To garner further understanding of the remaining errors (on C5), we examine the recordings associated with the low scoring target trials, where various issues are identified for the problematic recordings/trials. Interestingly, it is observed that correcting the pathological recordings not only improves the scores for the target trials but also for the nontarget trials.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01655",
        "title": "Stance and Sentiment in Tweets",
        "authors": [
            "Saif M. Mohammad",
            "Parinaz Sobhani",
            "Svetlana Kiritchenko"
        ],
        "abstract": "We can often detect from a person's utterances whether he/she is in favor of or against a given target entity -- their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet--target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01744",
        "title": "Improving Automated Patent Claim Parsing: Dataset, System, and Experiments",
        "authors": [
            "Mengke Hu",
            "David Cinciruk",
            "John MacLaren Walsh"
        ],
        "abstract": "Off-the-shelf natural language processing software performs poorly when parsing patent claims owing to their use of irregular language relative to the corpora built from news articles and the web typically utilized to train this software. Stopping short of the extensive and expensive process of accumulating a large enough dataset to completely retrain parsers for patent claims, a method of adapting existing natural language processing software towards patent claims via forced part of speech tag correction is proposed. An Amazon Mechanical Turk collection campaign organized to generate a public corpus to train such an improved claim parsing system is discussed, identifying lessons learned during the campaign that can be of use in future NLP dataset collection campaigns with AMT. Experiments utilizing this corpus and other patent claim sets measure the parsing performance improvement garnered via the claim parsing system. Finally, the utility of the improved claim parsing system within other patent processing applications is demonstrated via experiments showing improved automated patent subject classification when the new claim parsing system is utilized to generate the features.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01845",
        "title": "Detecting Context Dependence in Exercise Item Candidates Selected from Corpora",
        "authors": [
            "Ildik\u00f3 Pil\u00e1n"
        ],
        "abstract": "We explore the factors influencing the dependence of single sentences on their larger textual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation. An in-depth investigation of this question has not been previously carried out. Understanding this aspect can contribute to a more efficient selection of candidate sentences which, besides reducing the time required for item writing, can also ensure a higher degree of variability and authenticity. We present a set of relevant aspects collected based on the qualitative analysis of a smaller set of context-dependent corpus example sentences. Furthermore, we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identification of different issues related to context dependence. The method has also been evaluated empirically where 80% of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02019",
        "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec",
        "authors": [
            "Christopher E Moody"
        ],
        "abstract": "Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02129",
        "title": "Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Trung H. Bui",
            "Hung H. Bui"
        ],
        "abstract": "The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks. In this paper, we focus on the spoken language understanding pilot task, which consists of tagging a given utterance with speech acts and semantic slots. We compare different classifiers: the best system obtains 0.52 and 0.67 F1-scores on the test set for speech act recognition for the tourist and the guide respectively, and 0.52 F1-score for semantic tagging for both the guide and the tourist.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02130",
        "title": "Robust Dialog State Tracking for Large Ontologies",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Trung H. Bui",
            "Hung H. Bui"
        ],
        "abstract": "The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the previous three editions as follows: the number of slot-value pairs present in the ontology is much larger, no spoken language understanding output is given, and utterances are labeled at the subdialog level. This paper describes a novel dialog state tracking method designed to work robustly under these conditions, using elaborate string matching, coreference resolution tailored for dialogs and a few other improvements. The method can correctly identify many values that are not explicitly present in the utterance. On the final evaluation, our method came in first among 7 competing teams and 24 entries. The F1-score achieved by our method was 9 and 7 percentage points higher than that of the runner-up for the utterance-level evaluation and for the subdialog-level evaluation, respectively.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02134",
        "title": "Neural Recovery Machine for Chinese Dropped Pronoun",
        "authors": [
            "Wei-Nan Zhang",
            "Ting Liu",
            "Qingyu Yin",
            "Yu Zhang"
        ],
        "abstract": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2019-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02150",
        "title": "On Improving Informativity and Grammaticality for Multi-Sentence Compression",
        "authors": [
            "Elaheh ShafieiBavani",
            "Mohammad Ebrahimi",
            "Raymond Wong",
            "Fang Chen"
        ],
        "abstract": "Multi Sentence Compression (MSC) is of great value to many real world applications, such as guided microblog summarization, opinion summarization and newswire summarization. Recently, word graph-based approaches have been proposed and become popular in MSC. Their key assumption is that redundancy among a set of related sentences provides a reliable way to generate informative and grammatical sentences. In this paper, we propose an effective approach to enhance the word graph-based MSC and tackle the issue that most of the state-of-the-art MSC approaches are confronted with: i.e., improving both informativity and grammaticality at the same time. Our approach consists of three main components: (1) a merging method based on Multiword Expressions (MWE); (2) a mapping strategy based on synonymy between words; (3) a re-ranking step to identify the best compression candidates generated using a POS-based language model (POS-LM). We demonstrate the effectiveness of this novel approach using a dataset made of clusters of English newswire sentences. The observed improvements on informativity and grammaticality of the generated compressions show that our approach is superior to state-of-the-art MSC methods.\n    ",
        "submission_date": "2016-05-07T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02257",
        "title": "A corpus of preposition supersenses in English web reviews",
        "authors": [
            "Nathan Schneider",
            "Jena D. Hwang",
            "Vivek Srikumar",
            "Meredith Green",
            "Kathryn Conger",
            "Tim O'Gorman",
            "Martha Palmer"
        ],
        "abstract": "We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). That scheme improves upon its predecessors to better facilitate comprehensive manual annotation. Moreover, unlike the previous schemes, the preposition supersenses are organized hierarchically. Our data will be publicly released on the web upon publication.\n    ",
        "submission_date": "2016-05-08T00:00:00",
        "last_modified_date": "2016-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02276",
        "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
        "authors": [
            "Manaal Faruqui",
            "Yulia Tsvetkov",
            "Pushpendre Rastogi",
            "Chris Dyer"
        ],
        "abstract": "Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.\n    ",
        "submission_date": "2016-05-08T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02457",
        "title": "The Controlled Natural Language of Randall Munroe's Thing Explainer",
        "authors": [
            "Tobias Kuhn"
        ],
        "abstract": "It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers. This restricted language is a very interesting new case for the CNL community. I describe here its place in the context of existing approaches on Controlled Natural Languages, and I provide a first analysis from a scientific perspective, covering the word production rules and word distributions.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02592",
        "title": "GLEU Without Tuning",
        "authors": [
            "Courtney Napoles",
            "Keisuke Sakaguchi",
            "Matt Post",
            "Joel Tetreault"
        ],
        "abstract": "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02916",
        "title": "Grammatical Case Based IS-A Relation Extraction with Boosting for Polish",
        "authors": [
            "Pawe\u0142 \u0141ozi\u0144ski",
            "Dariusz Czerski",
            "Mieczys\u0142aw A. K\u0142opotek"
        ],
        "abstract": "Pattern-based methods of IS-A relation extraction rely heavily on so called Hearst patterns. These are ways of expressing instance enumerations of a class in natural language. While these lexico-syntactic patterns prove quite useful, they may not capture all taxonomical relations expressed in text. Therefore in this paper we describe a novel method of IS-A relation extraction from patterns, which uses morpho-syntactical annotations along with grammatical case of noun phrases that constitute entities participating in IS-A relation. We also describe a method for increasing the number of extracted relations that we call pseudo-subclass boosting which has potential application in any pattern-based relation extraction method. Experiments were conducted on a corpus of about 0.5 billion web documents in Polish language.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02945",
        "title": "The Yahoo Query Treebank, V. 1.0",
        "authors": [
            "Yuval Pinter",
            "Roi Reichart",
            "Idan Szpektor"
        ],
        "abstract": "A description and annotation guidelines for the Yahoo Webscope release of Query Treebank, Version 1.0, May 2016.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02948",
        "title": "Different approaches for identifying important concepts in probabilistic biomedical text summarization",
        "authors": [
            "Milad Moradi",
            "Nasser Ghadiri"
        ],
        "abstract": "Automatic text summarization tools help users in biomedical domain to acquire their intended information from various textual resources more efficiently. Some of the biomedical text summarization systems put the basis of their sentence selection approach on the frequency of concepts extracted from the input text. However, it seems that exploring other measures rather than the frequency for identifying the valuable content of the input document, and considering the correlations existing between concepts may be more useful for this type of summarization. In this paper, we describe a Bayesian summarizer for biomedical text documents. The Bayesian summarizer initially maps the input text to the Unified Medical Language System (UMLS) concepts, then it selects the important ones to be used as classification features. We introduce different feature selection approaches to identify the most important concepts of the text and to select the most informative content according to the distribution of these concepts. We show that with the use of an appropriate feature selection approach, the Bayesian biomedical summarizer can improve the performance of summarization. We perform extensive evaluations on a corpus of scientific papers in biomedical domain. The results show that the Bayesian summarizer outperforms the biomedical summarizers that rely on the frequency of concepts, the domain-independent and baseline methods based on the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover, the results suggest that using the meaningfulness measure and considering the correlations of concepts in the feature selection step lead to a significant increase in the performance of summarization.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03148",
        "title": "Coverage Embedding Models for Neural Machine Translation",
        "authors": [
            "Haitao Mi",
            "Baskaran Sankaran",
            "Zhiguo Wang",
            "Abe Ittycheriah"
        ],
        "abstract": "In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03209",
        "title": "Vocabulary Manipulation for Neural Machine Translation",
        "authors": [
            "Haitao Mi",
            "Zhiguo Wang",
            "Abe Ittycheriah"
        ],
        "abstract": "In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).\n    ",
        "submission_date": "2016-05-10T00:00:00",
        "last_modified_date": "2016-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03284",
        "title": "Machine Comprehension Based on Learning to Rank",
        "authors": [
            "Tian Tian",
            "Yuezhang Li"
        ],
        "abstract": "Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest. However, this dataset is too simple and too small for learning true reasoning abilities. \\cite{hermann2015teaching} therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03664",
        "title": "Real-Time Web Scale Event Summarization Using Sequential Decision Making",
        "authors": [
            "Chris Kedzie",
            "Fernando Diaz",
            "Kathleen McKeown"
        ],
        "abstract": "We present a system based on sequential decision making for the online summarization of massive document streams, such as those found on the web. Given an event of interest (e.g. \"Boston marathon bombing\"), our system is able to filter the stream for relevance and produce a series of short text updates describing the event as it unfolds over time. Unlike previous work, our approach is able to jointly model the relevance, comprehensiveness, novelty, and timeliness required by time-sensitive queries. We demonstrate a 28.3% improvement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03832",
        "title": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning",
        "authors": [
            "Yulia Tsvetkov",
            "Sunayana Sitaram",
            "Manaal Faruqui",
            "Guillaume Lample",
            "Patrick Littell",
            "David Mortensen",
            "Alan W Black",
            "Lori Levin",
            "Chris Dyer"
        ],
        "abstract": "We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03835",
        "title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model",
        "authors": [
            "Kyunghyun Cho"
        ],
        "abstract": "Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En->Cz translation.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03852",
        "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning",
        "authors": [
            "Yulia Tsvetkov",
            "Manaal Faruqui",
            "Wang Ling",
            "Brian MacWhinney",
            "Chris Dyer"
        ],
        "abstract": "We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03924",
        "title": "Joint Embeddings of Hierarchical Categories and Entities",
        "authors": [
            "Yuezhang Li",
            "Ronghuo Zheng",
            "Tian Tian",
            "Zhiting Hu",
            "Rahul Iyer",
            "Katia Sycara"
        ],
        "abstract": "Due to the lack of structured knowledge applied in learning distributed representation of categories, existing work cannot incorporate category hierarchies into entity information.~We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to compute meaningful semantic relatedness between entities and categories.~Compared with the previous state of the art, our framework can handle both single-word concepts and multiple-word concepts with superior performance in concept categorization and semantic relatedness.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03956",
        "title": "On the Convergent Properties of Word Embedding Methods",
        "authors": [
            "Yingtao Tian",
            "Vivek Kulkarni",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04002",
        "title": "Which Learning Algorithms Can Generalize Identity-Based Rules to Novel Inputs?",
        "authors": [
            "Paul Tupper",
            "Bobak Shahriari"
        ],
        "abstract": "We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data. In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical. We call such a basis for discrimination an identity-based rule. Identity-based rules have proven to be difficult or impossible for certain types of learning algorithms to acquire from limited datasets. This is in contrast to human behaviour on similar tasks. Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli. We use this framework to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs. We demonstrate these results computationally with a multilayer feedforward neural network.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04013",
        "title": "A Corpus-based Toy Model for DisCoCat",
        "authors": [
            "Stefano Gogioso"
        ],
        "abstract": "The categorical compositional distributional (DisCoCat) model of meaning rigorously connects distributional semantics and pregroup grammars, and has found a variety of applications in computational linguistics. From a more abstract standpoint, the DisCoCat paradigm predicates the construction of a mapping from syntax to categorical semantics. In this work we present a concrete construction of one such mapping, from a toy model of syntax for corpora annotated with constituent structure trees, to categorical semantics taking place in a category of free R-semimodules over an involutive commutative semiring R.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04072",
        "title": "Towards Empathetic Human-Robot Interactions",
        "authors": [
            "Pascale Fung",
            "Dario Bertero",
            "Yan Wan",
            "Anik Dey",
            "Ricky Ho Yin Chan",
            "Farhad Bin Siddique",
            "Yang Yang",
            "Chien-Sheng Wu",
            "Ruixi Lin"
        ],
        "abstract": "Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them which can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the early stage, we described our approach using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can \"understand\" human emotion. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software based virtual android, with an animated cartoon character to present itself on the screen. She will get \"smarter\" and more empathetic through its deep learning algorithms, and by gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04122",
        "title": "Natural Language Semantics and Computability",
        "authors": [
            "Richard Moot",
            "Christian Retor\u00e9"
        ],
        "abstract": "This paper is a reflexion on the computability of natural language semantics. It does not contain a new model or new results in the formal semantics of natural language: it is rather a computational analysis of the logical models and algorithms currently used in natural language semantics, defined as the mapping of a statement to logical formulas - formulas, because a statement can be ambiguous. We argue that as long as possible world semantics is left out, one can compute the semantic representation(s) of a given statement, including aspects of lexical meaning. We also discuss the algorithmic complexity of this process.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04238",
        "title": "Semantic Spaces",
        "authors": [
            "Yuri Manin",
            "Matilde Marcolli"
        ],
        "abstract": "Any natural language can be considered as a tool for producing large databases (consisting of texts, written, or discursive). This tool for its description in turn requires other large databases (dictionaries, grammars etc.). Nowadays, the notion of database is associated with computer processing and computer memory. However, a natural language resides also in human brains and functions in human communication, from interpersonal to intergenerational one. We discuss in this survey/research paper mathematical, in particular geometric, constructions, which help to bridge these two worlds. In particular, in this paper we consider the Vector Space Model of semantics based on frequency matrices, as used in Natural Language Processing. We investigate underlying geometries, formulated in terms of Grassmannians, projective spaces, and flag varieties. We formulate the relation between vector space models and semantic spaces based on semic axes in terms of projectability of subvarieties in Grassmannians and projective spaces. We interpret Latent Semantics as a geometric flow on Grassmannians. We also discuss how to formulate G\u00e4rdenfors' notion of \"meeting of minds\" in our geometric setting.\n    ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04278",
        "title": "Universal Dependencies for Learner English",
        "authors": [
            "Yevgeni Berzak",
            "Jessica Kenney",
            "Carolyn Spadine",
            "Jing Xian Wang",
            "Lucia Lam",
            "Keiko Sophie Mori",
            "Sebastian Garza",
            "Boris Katz"
        ],
        "abstract": "We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language. The treebank is available at ",
        "submission_date": "2016-05-13T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04359",
        "title": "Occurrence Statistics of Entities, Relations and Types on the Web",
        "authors": [
            "Aman Madaan",
            "Sunita Sarawagi"
        ],
        "abstract": "The problem of collecting reliable estimates of occurrence of entities on the open web forms the premise for this report. The models learned for tagging entities cannot be expected to perform well when deployed on the web. This is owing to the severe mismatch in the distributions of such entities on the web and in the relatively diminutive training data. In this report, we build up the case for maximum mean discrepancy for estimation of occurrence statistics of entities on the web, taking a review of named entity disambiguation techniques and related concepts along the way.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04462",
        "title": "Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health",
        "authors": [
            "Tim Althoff",
            "Kevin Clark",
            "Jure Leskovec"
        ],
        "abstract": "Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04469",
        "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification",
        "authors": [
            "Ye Zhang",
            "Iain Marshall",
            "Byron C. Wallace"
        ],
        "abstract": "We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04475",
        "title": "Capturing divergence in dependency trees to improve syntactic projection",
        "authors": [
            "Ryan Georgi",
            "Fei Xia",
            "William D. Lewis"
        ],
        "abstract": "Obtaining syntactic parses is a crucial part of many NLP pipelines. However, most of the world's languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when the two languages are divergent. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of projection algorithms.\n    ",
        "submission_date": "2016-05-14T00:00:00",
        "last_modified_date": "2016-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04481",
        "title": "Anchoring and Agreement in Syntactic Annotations",
        "authors": [
            "Yevgeni Berzak",
            "Yan Huang",
            "Andrei Barbu",
            "Anna Korhonen",
            "Boris Katz"
        ],
        "abstract": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.\n    ",
        "submission_date": "2016-05-15T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04515",
        "title": "Meta-Evaluation of Translation Evaluation Methods: a systematic up-to-date overview",
        "authors": [
            "Lifeng Han",
            "Serge Gladkoff"
        ],
        "abstract": "Starting from the 1950s, Machine Translation (MT) was challenged by different scientific solutions, which included rule-based methods, example-based and statistical models (SMT), to hybrid models, and very recent years the neural models (NMT). While NMT has achieved a huge quality improvement in comparison to conventional methodologies, by taking advantage of a huge amount of parallel corpora available from the internet and the recently developed super computational power support with an acceptable cost, it struggles to achieve real human parity in many domains and most language pairs, if not all of them. Alongside the long road of MT research and development, quality evaluation metrics played very important roles in MT advancement and evolution. In this tutorial, we overview the traditional human judgement criteria, automatic evaluation metrics, unsupervised quality estimation models, as well as the meta-evaluation of the evaluation methods. Among these, we will also cover the very recent work in the MT evaluation (MTE) fields, taking advantage of the large size of pre-trained language models for automatic metric customisation towards exactly deployed language pairs and domains. In addition, we also introduce the statistical confidence estimation regarding the sample size needed for human evaluation in real practice simulation. Full tutorial material is \\textbf{available} to download at ",
        "submission_date": "2016-05-15T00:00:00",
        "last_modified_date": "2025-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04553",
        "title": "A Proposal for Linguistic Similarity Datasets Based on Commonality Lists",
        "authors": [
            "Dmitrijs Milajevs",
            "Sascha Griffiths"
        ],
        "abstract": "Similarity is a core notion that is used in psychology and two branches of linguistics: theoretical and computational. The similarity datasets that come from the two fields differ in design: psychological datasets are focused around a certain topic such as fruit names, while linguistic datasets contain words from various categories. The later makes humans assign low similarity scores to the words that have nothing in common and to the words that have contrast in meaning, making similarity scores ambiguous. In this work we discuss the similarity collection procedure for a multi-category dataset that avoids score ambiguity and suggest changes to the evaluation procedure to reflect the insights of psychological literature for word, phrase and sentence similarity. We suggest to ask humans to provide a list of commonalities and differences instead of numerical similarity scores and employ the structure of human judgements beyond pairwise similarity for model evaluation. We believe that the proposed approach will give rise to datasets that test meaning representation models more thoroughly with respect to the human treatment of similarity.\n    ",
        "submission_date": "2016-05-15T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04569",
        "title": "Syntactically Guided Neural Machine Translation",
        "authors": [
            "Felix Stahlberg",
            "Eva Hasler",
            "Aurelien Waite",
            "Bill Byrne"
        ],
        "abstract": "We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.\n    ",
        "submission_date": "2016-05-15T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04655",
        "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment",
        "authors": [
            "Petr Baudis",
            "Silvestr Stanko",
            "Jan Sedivy"
        ],
        "abstract": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question.\n",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04800",
        "title": "Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Roman Grundkiewicz"
        ],
        "abstract": "This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2\\% TER and +5.5\\% BLEU and outperforms any other system submitted to the shared-task by a large margin.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04809",
        "title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Tomasz Dwojak",
            "Rico Sennrich"
        ],
        "abstract": "This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, our system stays behind the state-of-the-art pure neural models in terms of BLEU. Among restricted systems, manual evaluation places it in the first cluster tied with the pure neural model. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the best restricted system in its own cluster. In follow-up experiments we improve results by additional 0.8 BLEU.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05101",
        "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning",
        "authors": [
            "Pengfei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05110",
        "title": "Incorporating Loose-Structured Knowledge into Conversation Modeling via Recall-Gate LSTM",
        "authors": [
            "Zhen Xu",
            "Bingquan Liu",
            "Baoxun Wang",
            "Chengjie Sun",
            "Xiaolong Wang"
        ],
        "abstract": "Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05150",
        "title": "Automatic Detection and Categorization of Election-Related Tweets",
        "authors": [
            "Prashanth Vijayaraghavan",
            "Soroush Vosoughi",
            "Deb Roy"
        ],
        "abstract": "With the rise in popularity of public social media and micro-blogging services, most notably Twitter, the people have found a venue to hear and be heard by their peers without an intermediary. As a consequence, and aided by the public nature of Twitter, political scientists now potentially have the means to analyse and understand the narratives that organically form, spread and decline among the public in a political campaign. However, the volume and diversity of the conversation on Twitter, combined with its noisy and idiosyncratic nature, make this a hard task. Thus, advanced data mining and language processing techniques are required to process and analyse the data. In this paper, we present and evaluate a technical framework, based on recent advances in deep neural networks, for identifying and analysing election-related conversation on Twitter on a continuous, longitudinal basis. Our models can detect election-related tweets with an F-score of 0.92 and can categorize these tweets into 22 topics with an F-score of 0.90.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05156",
        "title": "Tweet Acts: A Speech Act Classifier for Twitter",
        "authors": [
            "Soroush Vosoughi",
            "Deb Roy"
        ],
        "abstract": "Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. In this paper, we explored speech act recognition on Twitter by treating it as a multi-class classification problem. We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. We trained and tested a logistic regression classifier using a data set of manually labelled tweets. Our method achieved a state-of-the-art performance with an average F1 score of more than $0.70$. We also explored classifiers with three different granularities (Twitter-wide, type-specific and topic-specific) in order to find the right balance between generalization and overfitting for our task.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05172",
        "title": "Siamese convolutional networks based on phonetic features for cognate identification",
        "authors": [
            "Taraka Rama"
        ],
        "abstract": "In this paper, we explore the use of convolutional networks (ConvNets) for the purpose of cognate identification. We compare our architecture with binary classifiers based on string similarity measures on different language families. Our experiments show that convolutional networks achieve competitive results across concepts and across language families at the task of cognate identification.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05362",
        "title": "Yelp Dataset Challenge: Review Rating Prediction",
        "authors": [
            "Nabiha Asghar"
        ],
        "abstract": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05414",
        "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification",
        "authors": [
            "Ryan Lowe",
            "Iulian V. Serban",
            "Mike Noseworthy",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05416",
        "title": "Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data",
        "authors": [
            "Teng Long",
            "Ryan Lowe",
            "Jackie Chi Kit Cheung",
            "Doina Precup"
        ],
        "abstract": "Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new state-of-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a trade-off between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05433",
        "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment",
        "authors": [
            "Stephen Roller",
            "Katrin Erk"
        ],
        "abstract": "We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05573",
        "title": "Modelling Interaction of Sentence Pair with coupled-LSTMs",
        "authors": [
            "Pengfei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods.\n    ",
        "submission_date": "2016-05-18T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05894",
        "title": "Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages",
        "authors": [
            "Muhammad Imran",
            "Prasenjit Mitra",
            "Carlos Castillo"
        ],
        "abstract": "Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05906",
        "title": "Automatic TM Cleaning through MT and POS Tagging: Autodesk's Submission to the NLP4TM 2016 Shared Task",
        "authors": [
            "Alena Zwahlen",
            "Olivier Carnal",
            "Samuel L\u00e4ubli"
        ],
        "abstract": "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06069",
        "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues",
        "authors": [
            "Iulian Vlad Serban",
            "Alessandro Sordoni",
            "Ryan Lowe",
            "Laurent Charlin",
            "Joelle Pineau",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06083",
        "title": "Stereotyping and Bias in the Flickr30K Dataset",
        "authors": [
            "Emiel van Miltenburg"
        ],
        "abstract": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \"focus only on the information that can be obtained from the image alone\" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.\n    ",
        "submission_date": "2016-05-19T00:00:00",
        "last_modified_date": "2016-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06319",
        "title": "As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian",
        "authors": [
            "Nikola Milosevic",
            "Goran Nenadic"
        ],
        "abstract": "Similes are natural language expressions used to compare unlikely things, where the comparison is not taken literally. They are often used in everyday communication and are an important part of cultural heritage. Having an up-to-date corpus of similes is challenging, as they are constantly coined and/or adapted to the contemporary times. In this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques. We expanded an existing corpus of traditional similes (containing 333 similes) by collecting 446 additional expressions. We, also, explore how crowdsourcing can be used to extract and curate new similes.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06353",
        "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Roman Grundkiewicz"
        ],
        "abstract": "In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06650",
        "title": "Latent Tree Models for Hierarchical Topic Detection",
        "authors": [
            "Peixian Chen",
            "Nevin L. Zhang",
            "Tengfei Liu",
            "Leonard K.M. Poon",
            "Zhourong Chen",
            "Farhan Khawar"
        ],
        "abstract": "We present a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways. Specifically, we model document collections using a class of graphical models called hierarchical latent tree models (HLTMs). The variables at the bottom level of an HLTM are observed binary variables that represent the presence/absence of words in a document. The variables at other levels are binary latent variables, with those at the lowest latent level representing word co-occurrence patterns and those at higher levels representing co-occurrence of patterns at the level below. Each latent variable gives a soft partition of the documents, and document clusters in the partitions are interpreted as topics. Latent variables at high levels of the hierarchy capture long-range word co-occurrence patterns and hence give thematically more general topics, while those at low levels of the hierarchy capture short-range word co-occurrence patterns and give thematically more specific topics. Unlike LDA-based topic models, HLTMs do not refer to a document generation process and use word variables instead of token variables. They use a tree structure to model the relationships between topics and words, which is conducive to the discovery of meaningful topics and topic hierarchies.\n    ",
        "submission_date": "2016-05-21T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06770",
        "title": "Automatic Construction of Discourse Corpora for Dialogue Translation",
        "authors": [
            "Longyue Wang",
            "Xiaojun Zhang",
            "Zhaopeng Tu",
            "Andy Way",
            "Qun Liu"
        ],
        "abstract": "In this paper, a novel approach is proposed to automatically construct parallel discourse corpus for dialogue machine translation. Firstly, the parallel subtitle data and its corresponding monolingual movie script data are crawled and collected from Internet. Then tags such as speaker and discourse boundary from the script data are projected to its subtitle data via an information retrieval approach in order to map monolingual discourse to bilingual texts. We not only evaluate the mapping results, but also integrate speaker information into the translation. Experiments show our proposed method can achieve 81.79% and 98.64% accuracy on speaker and dialogue boundary annotation, and speaker-based language model adaptation can obtain around 0.5 BLEU points improvement in translation qualities. Finally, we publicly release around 100K parallel discourse data with manual speaker and dialogue boundary annotation.\n    ",
        "submission_date": "2016-05-22T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06799",
        "title": "Textual Paralanguage and its Implications for Marketing Communications",
        "authors": [
            "Andrea Webb Luangrath",
            "Joann Peck",
            "Victor A. Barger"
        ],
        "abstract": "Both face-to-face communication and communication in online environments convey information beyond the actual verbal message. In a traditional face-to-face conversation, paralanguage, or the ancillary meaning- and emotion-laden aspects of speech that are not actual verbal prose, gives contextual information that allows interactors to more appropriately understand the message being conveyed. In this paper, we conceptualize textual paralanguage (TPL), which we define as written manifestations of nonverbal audible, tactile, and visual elements that supplement or replace written language and that can be expressed through words, symbols, images, punctuation, demarcations, or any combination of these elements. We develop a typology of textual paralanguage using data from Twitter, Facebook, and Instagram. We present a conceptual framework of antecedents and consequences of brands' use of textual paralanguage. Implications for theory and practice are discussed.\n    ",
        "submission_date": "2016-05-22T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07133",
        "title": "Towards Multi-Agent Communication-Based Language Learning",
        "authors": [
            "Angeliki Lazaridou",
            "Nghia The Pham",
            "Marco Baroni"
        ],
        "abstract": "We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing\n    ",
        "submission_date": "2016-05-23T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07333",
        "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification",
        "authors": [
            "Ngoc Thang Vu",
            "Heike Adel",
            "Pankaj Gupta",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07346",
        "title": "Multi-Level Analysis and Annotation of Arabic Corpora for Text-to-Sign Language MT",
        "authors": [
            "Abdelaziz Lakhfif",
            "Mohammed T. Laskri",
            "Eric Atwell"
        ],
        "abstract": "In this paper, we present an ongoing effort in lexical semantic analysis and annotation of Modern Standard Arabic (MSA) text, a semi automatic annotation tool concerned with the morphologic, syntactic, and semantic levels of description.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07366",
        "title": "Experiments in Linear Template Combination using Genetic Algorithms",
        "authors": [
            "Nikhilesh Bhatnagar",
            "Radhika Mamidi"
        ],
        "abstract": "Natural Language Generation systems typically have two parts - strategic ('what to say') and tactical ('how to say'). We present our experiments in building an unsupervised corpus-driven template based tactical NLG system. We consider templates as a sequence of words containing gaps. Our idea is based on the observation that templates are grammatical locally (within their textual span). We posit the construction of a sentence as a highly restricted sequence of such templates. This work is an attempt to explore the resulting search space using Genetic Algorithms to arrive at acceptable solutions. We present a baseline implementation of this approach which outputs gapped text.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07515",
        "title": "Neural Semantic Role Labeling with Dependency Path Embeddings",
        "authors": [
            "Michael Roth",
            "Mirella Lapata"
        ],
        "abstract": "This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07669",
        "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems",
        "authors": [
            "Pei-Hao Su",
            "Milica Gasic",
            "Nikola Mrksic",
            "Lina Rojas-Barahona",
            "Stefan Ultes",
            "David Vandyke",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07683",
        "title": "Learning End-to-End Goal-Oriented Dialog",
        "authors": [
            "Antoine Bordes",
            "Y-Lan Boureau",
            "Jason Weston"
        ],
        "abstract": "Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07735",
        "title": "Design and development a children's speech database",
        "authors": [
            "Radoslava Kraleva"
        ],
        "abstract": "The report presents the process of planning, designing and the development of a database of spoken children's speech whose native language is Bulgarian. The proposed model is designed for children between the age of 4 and 6 without speech disorders, and reflects their specific capabilities. At this age most children cannot read, there is no sustained concentration, they are emotional, etc. The aim is to unite all the media information accompanying the recording and processing of spoken speech, thereby to facilitate the work of researchers in the field of speech recognition. This database will be used for the development of systems for children's speech recognition, children's speech synthesis systems, games which allow voice control, etc. As a result of the proposed model a prototype system for speech recognition is presented.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07766",
        "title": "Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction",
        "authors": [
            "Kim Anh Nguyen",
            "Sabine Schulte im Walde",
            "Ngoc Thang Vu"
        ],
        "abstract": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07843",
        "title": "Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction",
        "authors": [
            "Yichun Yin",
            "Furu Wei",
            "Li Dong",
            "Kaimeng Xu",
            "Ming Zhang",
            "Ming Zhou"
        ],
        "abstract": "In this paper, we develop a novel approach to aspect term extraction based on unsupervised learning of distributed representations of words and dependency paths. The basic idea is to connect two words (w1 and w2) with the dependency path (r) between them in the embedding space. Specifically, our method optimizes the objective w1 + r = w2 in the low-dimensional space, where the multi-hop dependency paths are treated as a sequence of grammatical relations and modeled by a recurrent neural network. Then, we design the embedding features that consider linear context and dependency context information, for the conditional random field (CRF) based aspect term extraction. Experimental results on the SemEval datasets show that, (1) with only embedding features, we can achieve state-of-the-art results; (2) our embedding method which incorporates the syntactic information among words yields better performance than other representative ones in aspect term extraction.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07869",
        "title": "Variational Neural Machine Translation",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su",
            "Hong Duan",
            "Min Zhang"
        ],
        "abstract": "Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07874",
        "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su"
        ],
        "abstract": "In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate clues and sourcetarget interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrases with embeddings at different levels of granularity (e.g., words, sub-phrases and phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. These weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. To evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.63 BLEU points on average over the baseline.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07918",
        "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling",
        "authors": [
            "Byungsoo Kim",
            "Hwanjo Yu",
            "Gary Geunbae Lee"
        ],
        "abstract": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08675",
        "title": "Boosting Question Answering by Deep Entity Recognition",
        "authors": [
            "Piotr Przyby\u0142a"
        ],
        "abstract": "In this paper an open-domain factoid question answering system for Polish, RAFAEL, is presented. The system goes beyond finding an answering sentence; it also extracts a single string, corresponding to the required entity. Herein the focus is placed on different approaches to entity recognition, essential for retrieving information matching question constraints. Apart from traditional approach, including named entity recognition (NER) solutions, a novel technique, called Deep Entity Recognition (DeepER), is introduced and implemented. It allows a comprehensive search of all forms of entity references matching a given WordNet synset (e.g. an impressionist), based on a previously assembled entity library. It has been created by analysing the first sentences of encyclopaedia entries and disambiguation and redirect pages. DeepER also provides automatic evaluation, which makes possible numerous experiments, including over a thousand questions from a quiz TV show answered on the grounds of Polish Wikipedia. The final results of a manual evaluation on a separate question set show that the strength of DeepER approach lies in its ability to answer questions that demand answers beyond the traditional categories of named entities.\n    ",
        "submission_date": "2016-05-27T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08764",
        "title": "Stacking With Auxiliary Features",
        "authors": [
            "Nazneen Fatema Rajani",
            "Raymond J. Mooney"
        ],
        "abstract": "Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.\n    ",
        "submission_date": "2016-05-27T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08889",
        "title": "Building an Evaluation Scale using Item Response Theory",
        "authors": [
            "John P. Lalor",
            "Hao Wu",
            "Hong Yu"
        ],
        "abstract": "Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items - their difficulty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.\n    ",
        "submission_date": "2016-05-28T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08900",
        "title": "Aspect Level Sentiment Classification with Deep Memory Network",
        "authors": [
            "Duyu Tang",
            "Bing Qin",
            "Ting Liu"
        ],
        "abstract": "We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.\n    ",
        "submission_date": "2016-05-28T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09090",
        "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention",
        "authors": [
            "Yang Liu",
            "Chengjie Sun",
            "Lei Lin",
            "Xiaolong Wang"
        ],
        "abstract": "In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09096",
        "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
        "authors": [
            "William L. Hamilton",
            "Jure Leskovec",
            "Dan Jurafsky"
        ],
        "abstract": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2018-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09186",
        "title": "Does Multimodality Help Human and Machine for Translation and Image Captioning?",
        "authors": [
            "Ozan Caglayan",
            "Walid Aransa",
            "Yaxing Wang",
            "Marc Masana",
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Fethi Bougares",
            "Lo\u00efc Barrault",
            "Joost van de Weijer"
        ],
        "abstract": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.\n    ",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09564",
        "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler",
        "authors": [
            "Gregory Grefenstette",
            "Lawrence Muchemi"
        ],
        "abstract": "Specialized dictionaries are used to understand concepts in specific domains, especially where those concepts are not part of the general vocabulary, or having meanings that differ from ordinary languages. The first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question. Classical methods for detecting this vocabulary involve gathering a domain corpus, calculating statistics on the terms found there, and then comparing these statistics to a background or general language corpus. Terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain. Here we present two tools, a directed crawler, and a distributional semantics package, that can be used together, circumventing the need of a background corpus. Both tools are available on the web.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00025",
        "title": "Implementing a Reverse Dictionary, based on word definitions, using a Node-Graph Architecture",
        "authors": [
            "Sushrut Thorat",
            "Varad Choudhari"
        ],
        "abstract": "In this paper, we outline an approach to build graph-based reverse dictionaries using word definitions. A reverse dictionary takes a phrase as an input and outputs a list of words semantically similar to that phrase. It is a solution to the Tip-of-the-Tongue problem. We use a distance-based similarity measure, computed on a graph, to assess the similarity between a word and the input phrase. We compare the performance of our approach with the Onelook Reverse Dictionary and a distributional semantics method based on word2vec, and show that our approach is much better than the distributional semantics method, and as good as Onelook, on a 3k lexicon. This simple approach sets a new performance baseline for reverse dictionaries.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00189",
        "title": "Neural Network Translation Models for Grammatical Error Correction",
        "authors": [
            "Shamil Chollampatt",
            "Kaveh Taghipour",
            "Hwee Tou Ng"
        ],
        "abstract": "Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, without explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00210",
        "title": "Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical Error Correction",
        "authors": [
            "Duc Tam Hoang",
            "Shamil Chollampatt",
            "Hwee Tou Ng"
        ],
        "abstract": "Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in texts written by second language learners. The statistical machine translation (SMT) approach to GEC, in which sentences written by second language learners are translated to grammatically correct sentences, has achieved state-of-the-art accuracy. However, the SMT approach is unable to utilize global context. In this paper, we propose a novel approach to improve the accuracy of GEC, by exploiting the n-best hypotheses generated by an SMT approach. Specifically, we build a classifier to score the edits in the n-best hypotheses. The classifier can be used to select appropriate edits or re-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC system that uses the SMT approach. Our experiments show that our methods achieve statistically significant improvements in accuracy over the best published results on a benchmark test dataset on GEC.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00253",
        "title": "On a Topic Model for Sentences",
        "authors": [
            "Georgios Balikas",
            "Massih-Reza Amini",
            "Marianne Clausel"
        ],
        "abstract": "Probabilistic topic models are generative models that describe the content of documents by discovering the latent topics underlying them. However, the structure of the textual input, and for instance the grouping of words in coherent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA, an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00294",
        "title": "Improved Parsing for Argument-Clusters Coordination",
        "authors": [
            "Jessica Ficler",
            "Yoav Goldberg"
        ],
        "abstract": "Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modified trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive x2.7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00372",
        "title": "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking",
        "authors": [
            "Rami Al-Rfou",
            "Marc Pickett",
            "Javier Snaider",
            "Yun-hsuan Sung",
            "Brian Strope",
            "Ray Kurzweil"
        ],
        "abstract": "We investigate the task of modeling open-domain, multi-turn, unstructured, multi-participant, conversational dialogue. We specifically study the effect of incorporating different elements of the conversation. Unlike previous efforts, which focused on modeling messages and responses, we extend the modeling to long context and participant's history. Our system does not rely on handwritten rules or engineered features; instead, we train deep neural networks on a large conversational dataset. In particular, we exploit the structure of Reddit comments and posts to extract 2.1 billion messages and 133 million conversations. We evaluate our models on the task of predicting the next response in a conversation, and we find that modeling both context and participants improves prediction accuracy.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00414",
        "title": "On a Possible Similarity between Gene and Semantic Networks",
        "authors": [
            "Nicolas Turenne"
        ],
        "abstract": "In several domains such as linguistics, molecular biology or social sciences, holistic effects are hardly well-defined by modeling with single units, but more and more studies tend to understand macro structures with the help of meaningful and useful associations in fields such as social networks, systems biology or semantic web. A stochastic multi-agent system offers both accurate theoretical framework and operational computing implementations to model large-scale associations, their dynamics and patterns extraction. We show that clustering around a target object in a set of associations of object prove some similarity in specific data and two case studies about gene-gene and term-term relationships leading to an idea of a common organizing principle of cognition with random and deterministic effects.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00499",
        "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
        "authors": [
            "Graham Neubig",
            "Chris Dyer"
        ],
        "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00577",
        "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge sources",
        "authors": [
            "Justin Wood",
            "Patrick Tan",
            "Wei Wang",
            "Corey Arnold"
        ],
        "abstract": "A popular approach to topic modeling involves extracting co-occurring n-grams of a corpus into semantic themes. The set of n-grams in a theme represents an underlying topic, but most topic modeling approaches are not able to label these sets of words with a single n-gram. Such labels are useful for topic identification in summarization systems. This paper introduces a novel approach to labeling a group of n-grams comprising an individual topic. The approach taken is to complement the existing topic distributions over words with a known distribution based on a predefined set of topics. This is done by integrating existing labeled knowledge sources representing known potential topics into the probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. In the inference these modified distributions guide the convergence of the latent topics to conform with the complementary distributions. This approach ensures that the topic inference process is consistent with existing knowledge. The label assignment from the complementary knowledge sources are then transferred to the latent topics of the corpus. The results show both accurate label assignment to topics as well as improved topic generation than those obtained using various labeling approaches based off Latent Dirichlet allocation (LDA).\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00589",
        "title": "Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection",
        "authors": [
            "Katharina Kann",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Morphological reinflection is the task of generating a target form given a source form, a source tag and a target tag. We propose a new way of modeling this task with neural encoder-decoder models. Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinflection even for low-resource languages. We further present a new automatic correction method for the outputs based on edit trees.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00739",
        "title": "Stochastic Structured Prediction under Bandit Feedback",
        "authors": [
            "Artem Sokolov",
            "Julia Kreutzer",
            "Christopher Lo",
            "Stefan Riezler"
        ],
        "abstract": "Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00776",
        "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation",
        "authors": [
            "Iulian Vlad Serban",
            "Tim Klinger",
            "Gerald Tesauro",
            "Kartik Talamadupula",
            "Bowen Zhou",
            "Yoshua Bengio",
            "Aaron Courville"
        ],
        "abstract": "We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00819",
        "title": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations",
        "authors": [
            "Alexandre Salle",
            "Marco Idiart",
            "Aline Villavicencio"
        ],
        "abstract": "In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.\n    ",
        "submission_date": "2016-06-02T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01151",
        "title": "Using Neural Generative Models to Release Synthetic Twitter Corpora with Reduced Stylometric Identifiability of Users",
        "authors": [
            "Alexander G. Ororbia II",
            "Fridolin Linder",
            "Joshua Snoke"
        ],
        "abstract": "We present a method for generating synthetic versions of Twitter data using neural generative models. The goal is protecting individuals in the source data from stylometric re-identification attacks while still releasing data that carries research value. Specifically, we generate tweet corpora that maintain user-level word distributions by augmenting the neural language models with user-specific components. We compare our approach to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on measures of risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general word distribution measures and two common text analysis research tasks. We find that neural models are able to significantly lower risk over previous methods with little cost to utility. We also demonstrate that the neural models allow data providers to actively control the risk-utility trade-off through model tuning parameters. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2018-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01161",
        "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning",
        "authors": [
            "Jiang Guo",
            "Wanxiang Che",
            "Haifeng Wang",
            "Ting Liu"
        ],
        "abstract": "Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01219",
        "title": "Learning Stylometric Representations for Authorship Analysis",
        "authors": [
            "Steven H. H. Ding",
            "Benjamin C. M. Fung",
            "Farkhund Iqbal",
            "William K. Cheung"
        ],
        "abstract": "Authorship analysis (AA) is the study of unveiling the hidden properties of authors from a body of exponentially exploding textual data. It extracts an author's identity and sociolinguistic characteristics based on the reflected writing styles in the text. It is an essential process for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenario- or dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for authorship analysis. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization and authorship verification with the Twitter, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the bag-of-lexical-n-grams, Latent Dirichlet Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec representations.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01269",
        "title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning",
        "authors": [
            "Jason D. Williams",
            "Geoffrey Zweig"
        ],
        "abstract": "This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01280",
        "title": "Dependency Parsing as Head Selection",
        "authors": [
            "Xingxing Zhang",
            "Jianpeng Cheng",
            "Mirella Lapata"
        ],
        "abstract": "Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf N}eural {\\bf Se}lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, \\textsc{DeNSe} generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01283",
        "title": "Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory",
        "authors": [
            "Alexandre Salle",
            "Marco Idiart",
            "Aline Villavicencio"
        ],
        "abstract": "In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01292",
        "title": "An Attentional Neural Conversation Model with Improved Specificity",
        "authors": [
            "Kaisheng Yao",
            "Baolin Peng",
            "Geoffrey Zweig",
            "Kam-Fai Wong"
        ],
        "abstract": "In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention. Secondly, it avoids generating non-specific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01323",
        "title": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations",
        "authors": [
            "Kevin Clark",
            "Christopher D. Manning"
        ],
        "abstract": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01341",
        "title": "Neural Architectures for Fine-grained Entity Type Classification",
        "authors": [
            "Sonse Shimaoka",
            "Pontus Stenetorp",
            "Kentaro Inui",
            "Sebastian Riedel"
        ],
        "abstract": "In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01404",
        "title": "Generating Natural Language Inference Chains",
        "authors": [
            "Vladyslav Kolesnyk",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel"
        ],
        "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.\n    ",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01433",
        "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction",
        "authors": [
            "Jason Alan Fries"
        ],
        "abstract": "We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time.\n",
        "submission_date": "2016-06-04T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01515",
        "title": "Coordination in Categorical Compositional Distributional Semantics",
        "authors": [
            "Dimitri Kartsaklis"
        ],
        "abstract": "An open problem with categorical compositional distributional semantics is the representation of words that are considered semantically vacuous from a distributional perspective, such as determiners, prepositions, relative pronouns or coordinators. This paper deals with the topic of coordination between identical syntactic types, which accounts for the majority of coordination cases in language. By exploiting the compact closed structure of the underlying category and Frobenius operators canonically induced over the fixed basis of finite-dimensional vector spaces, we provide a morphism as representation of a coordinator tensor, and we show how it lifts from atomic types to compound types. Linguistic intuitions are provided, and the importance of the Frobenius operators as an addition to the compact closed setting with regard to language is discussed.\n    ",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01541",
        "title": "Deep Reinforcement Learning for Dialogue Generation",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Alan Ritter",
            "Michel Galley",
            "Jianfeng Gao",
            "Dan Jurafsky"
        ],
        "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.\n    ",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01545",
        "title": "Neural Net Models for Open-Domain Discourse Coherence",
        "authors": [
            "Jiwei Li",
            "Dan Jurafsky"
        ],
        "abstract": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains.\n",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2017-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01549",
        "title": "Gated-Attention Readers for Text Comprehension",
        "authors": [
            "Bhuwan Dhingra",
            "Hanxiao Liu",
            "Zhilin Yang",
            "William W. Cohen",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at ",
        "submission_date": "2016-06-05T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01603",
        "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution",
        "authors": [
            "Ting Liu",
            "Yiming Cui",
            "Qingyu Yin",
            "Weinan Zhang",
            "Shijin Wang",
            "Guoping Hu"
        ],
        "abstract": "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01614",
        "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification",
        "authors": [
            "Xilun Chen",
            "Yu Sun",
            "Ben Athiwaratkun",
            "Claire Cardie",
            "Kilian Weinberger"
        ],
        "abstract": "In recent years great success has been achieved in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of labeled data. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exists. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2018-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01700",
        "title": "Gated Word-Character Recurrent Language Model",
        "authors": [
            "Yasumasa Miyamoto",
            "Kyunghyun Cho"
        ],
        "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01781",
        "title": "Very Deep Convolutional Networks for Text Classification",
        "authors": [
            "Alexis Conneau",
            "Holger Schwenk",
            "Lo\u00efc Barrault",
            "Yann Lecun"
        ],
        "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01792",
        "title": "Neural Machine Translation with External Phrase Memory",
        "authors": [
            "Yaohua Tang",
            "Fandong Meng",
            "Zhengdong Lu",
            "Hang Li",
            "Philip L.H. Yu"
        ],
        "abstract": "In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01933",
        "title": "A Decomposable Attention Model for Natural Language Inference",
        "authors": [
            "Ankur P. Parikh",
            "Oscar T\u00e4ckstr\u00f6m",
            "Dipanjan Das",
            "Jakob Uszkoreit"
        ],
        "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01990",
        "title": "Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features",
        "authors": [
            "Attapol T. Rutherford",
            "Vera Demberg",
            "Nianwen Xue"
        ],
        "abstract": "Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01994",
        "title": "CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases",
        "authors": [
            "Zihang Dai",
            "Lei Li",
            "Wei Xu"
        ],
        "abstract": "How can we enable computers to automatically answer questions like \"Who created the character Harry Potter\"? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions --- ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neural-network-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions - the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02003",
        "title": "Memory-enhanced Decoder for Neural Machine Translation",
        "authors": [
            "Mingxuan Wang",
            "Zhengdong Lu",
            "Hang Li",
            "Qun Liu"
        ],
        "abstract": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At each time during decoding, \\textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\\cite{RNNsearch} to store the representation of source sentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02006",
        "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation",
        "authors": [
            "Philip Arthur",
            "Graham Neubig",
            "Satoshi Nakamura"
        ],
        "abstract": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02012",
        "title": "Can neural machine translation do simultaneous translation?",
        "authors": [
            "Kyunghyun Cho",
            "Masha Esipova"
        ],
        "abstract": "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02126",
        "title": "Supervised Syntax-based Alignment between English Sentences and Abstract Meaning Representation Graphs",
        "authors": [
            "Chenhui Chu",
            "Sadao Kurohashi"
        ],
        "abstract": "As alignment links are not given between English sentences and Abstract Meaning Representation (AMR) graphs in the AMR annotation, automatic alignment becomes indispensable for training an AMR parser. Previous studies formalize it as a string-to-string problem and solve it in an unsupervised way, which suffers from data sparseness due to the small size of training data for English-AMR alignment. In this paper, we formalize it as a syntax-based alignment problem and solve it in a supervised manner based on syntax trees, which can address the data sparseness problem by generalizing English-AMR tokens to syntax tags. Experiments verify the effectiveness of the proposed method not only for English-AMR alignment, but also for AMR parsing.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2017-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02245",
        "title": "Iterative Alternating Neural Attention for Machine Reading",
        "authors": [
            "Alessandro Sordoni",
            "Philip Bachman",
            "Adam Trischler",
            "Yoshua Bengio"
        ],
        "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02270",
        "title": "Natural Language Comprehension with the EpiReader",
        "authors": [
            "Adam Trischler",
            "Zheng Ye",
            "Xingdi Yuan",
            "Kaheer Suleman"
        ],
        "abstract": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02276",
        "title": "Multilingual Visual Sentiment Concept Matching",
        "authors": [
            "Nikolaos Pappas",
            "Miriam Redi",
            "Mercan Topkara",
            "Brendan Jou",
            "Hongyi Liu",
            "Tao Chen",
            "Shih-Fu Chang"
        ],
        "abstract": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we pro- vide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to repre- sent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural in- sights of portrait-related affective visual concepts.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02342",
        "title": "Optimizing Spectral Learning for Parsing",
        "authors": [
            "Shashi Narayan",
            "Shay B. Cohen"
        ],
        "abstract": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages.\n    ",
        "submission_date": "2016-06-07T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02440",
        "title": "On the Place of Text Data in Lifelogs, and Text Analysis via Semantic Facets",
        "authors": [
            "Gregory Grefenstette",
            "Lawrence Muchemi"
        ],
        "abstract": "Current research in lifelog data has not paid enough attention to analysis of cognitive activities in comparison to physical activities. We argue that as we look into the future, wearable devices are going to be cheaper and more prevalent and textual data will play a more significant role. Data captured by lifelogging devices will increasingly include speech and text, potentially useful in analysis of intellectual activities. Analyzing what a person hears, reads, and sees, we should be able to measure the extent of cognitive activity devoted to a certain topic or subject by a learner. Test-based lifelog records can benefit from semantic analysis tools developed for natural language processing. We show how semantic analysis of such text data can be achieved through the use of taxonomic subject facets and how these facets might be useful in quantifying cognitive activity devoted to various topics in a person's day. We are currently developing a method to automatically create taxonomic topic vocabularies that can be applied to this detection of intellectual activity.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02447",
        "title": "Learning Language Games through Interaction",
        "authors": [
            "Sida I. Wang",
            "Percy Liang",
            "Christopher D. Manning"
        ],
        "abstract": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02461",
        "title": "Learning Semantically and Additively Compositional Distributional Representations",
        "authors": [
            "Ran Tian",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "abstract": "This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02514",
        "title": "DefExt: A Semi Supervised Definition Extraction Tool",
        "authors": [
            "Luis Espinosa-Anke",
            "Roberto Carlini",
            "Horacio Saggion",
            "Francesco Ronzano"
        ],
        "abstract": "We present DefExt, an easy to use semi supervised Definition Extraction Tool. DefExt is designed to extract from a target corpus those textual fragments where a term is explicitly mentioned together with its core features, i.e. its definition. It works on the back of a Conditional Random Fields based sequential labeling algorithm and a bootstrapping approach. Bootstrapping enables the model to gradually become more aware of the idiosyncrasies of the target corpus. In this paper we describe the main components of the toolkit as well as experimental results stemming from both automatic and manual evaluation. We release DefExt as open source along with the necessary files to run it in any Unix machine. We also provide access to training and test data for immediate use.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02529",
        "title": "Coordination Annotation Extension in the Penn Tree Bank",
        "authors": [
            "Jessica Ficler",
            "Yoav Goldberg"
        ],
        "abstract": "Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02555",
        "title": "Improving Recurrent Neural Networks For Sequence Labelling",
        "authors": [
            "Marco Dinarelli",
            "Isabelle Tellier"
        ],
        "abstract": "In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02601",
        "title": "A Joint Model for Word Embedding and Word Morphology",
        "authors": [
            "Kris Cao",
            "Marek Rei"
        ],
        "abstract": "This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02638",
        "title": "Addressing Limited Data for Textual Entailment Across Domains",
        "authors": [
            "Chaitanya Shivade",
            "Preethi Raghavan",
            "Siddharth Patwardhan"
        ],
        "abstract": "We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02680",
        "title": "First Result on Arabic Neural Machine Translation",
        "authors": [
            "Amjad Almahairi",
            "Kyunghyun Cho",
            "Nizar Habash",
            "Aaron Courville"
        ],
        "abstract": "Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar<->En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02689",
        "title": "Continuously Learning Neural Dialogue Management",
        "authors": [
            "Pei-Hao Su",
            "Milica Gasic",
            "Nikola Mrksic",
            "Lina Rojas-Barahona",
            "Stefan Ultes",
            "David Vandyke",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model. The experiments demonstrate the supervised model's effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model's performance in both interactive settings, especially under higher-noise conditions.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02785",
        "title": "Neural Network-Based Abstract Generation for Opinions and Arguments",
        "authors": [
            "Lu Wang",
            "Wang Ling"
        ],
        "abstract": "We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02820",
        "title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora",
        "authors": [
            "William L. Hamilton",
            "Kevin Clark",
            "Jure Leskovec",
            "Dan Jurafsky"
        ],
        "abstract": "A word's sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are specific to the domains being studied. We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words, achieving state-of-the-art performance competitive with approaches that rely on hand-curated resources. Using our framework we perform two large-scale empirical studies to quantify the extent to which sentiment varies across time and between communities. We induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02821",
        "title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change",
        "authors": [
            "William L. Hamilton",
            "Jure Leskovec",
            "Dan Jurafsky"
        ],
        "abstract": "Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics, it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (\"I promise.\" -> \"It promised to be exciting.\"). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (\"prison cell\" -> \"cell phone\"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02858",
        "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
        "authors": [
            "Danqi Chen",
            "Jason Bolton",
            "Christopher D. Manning"
        ],
        "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02891",
        "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
        "authors": [
            "Rico Sennrich",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English<->Czech, English<->German, English<->Romanian and English<->Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02892",
        "title": "Linguistic Input Features Improve Neural Machine Translation",
        "authors": [
            "Rico Sennrich",
            "Barry Haddow"
        ],
        "abstract": "Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English<->German, and English->Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An open-source implementation of our neural MT system is available, as are sample files and configurations.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02960",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
        "authors": [
            "Sam Wiseman",
            "Alexander M. Rush"
        ],
        "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02979",
        "title": "Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs)",
        "authors": [
            "Shaohua Li",
            "Tat-Seng Chua",
            "Jun Zhu",
            "Chunyan Miao"
        ],
        "abstract": "Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03126",
        "title": "Key-Value Memory Networks for Directly Reading Documents",
        "authors": [
            "Alexander Miller",
            "Adam Fisch",
            "Jesse Dodge",
            "Amir-Hossein Karimi",
            "Antoine Bordes",
            "Jason Weston"
        ],
        "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03143",
        "title": "PerSum: Novel Systems for Document Summarization in Persian",
        "authors": [
            "Saeid Parvandeh",
            "Shibamouli Lahiri",
            "Fahimeh Boroumand"
        ],
        "abstract": "In this paper we explore the problem of document summarization in Persian language from two distinct angles. In our first approach, we modify a popular and widely cited Persian document summarization framework to see how it works on a realistic corpus of news articles. Human evaluation on generated summaries shows that graph-based methods perform better than the modified systems. We carry this intuition forward in our second approach, and probe deeper into the nature of graph-based systems by designing several summarizers based on centrality measures. Ad hoc evaluation using ROUGE score on these summarizers suggests that there is a small class of centrality measures that perform better than three strong unsupervised baselines.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03144",
        "title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical Relevance in Learner Essays",
        "authors": [
            "Marek Rei",
            "Ronan Cummins"
        ],
        "abstract": "We investigate the task of assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03152",
        "title": "Policy Networks with Two-Stage Training for Dialogue Systems",
        "authors": [
            "Mehdi Fatemi",
            "Layla El Asri",
            "Hannes Schulz",
            "Jing He",
            "Kaheer Suleman"
        ],
        "abstract": "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03153",
        "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition",
        "authors": [
            "Furong Huang",
            "Animashree Anandkumar"
        ],
        "abstract": "Unsupervised text embeddings extraction is crucial for text understanding in machine learning. Word2Vec and its variants have received substantial success in mapping words with similar syntactic or semantic meaning to vectors close to each other. However, extracting context-aware word-sequence embedding remains a challenging task. Training over large corpus is difficult as labels are difficult to get. More importantly, it is challenging for pre-trained models to obtain word-sequence embeddings that are universally good for all downstream tasks or for any new datasets. We propose a two-phased ConvDic+DeconvDec framework to solve the problem by combining a word-sequence dictionary learning model with a word-sequence embedding decode model. We propose a convolutional tensor decomposition mechanism to learn good word-sequence phrase dictionary in the learning phase. It is proved to be more accurate and much more efficient than the popular alternating minimization method. In the decode phase, we introduce a deconvolution framework that is immune to the problem of varying sentence lengths. The word-sequence embeddings we extracted using ConvDic+DeconvDec are universally good for a few downstream tasks we test on. The framework requires neither pre-training nor prior/outside information.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2018-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03192",
        "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding",
        "authors": [
            "Shaohua Li",
            "Jun Zhu",
            "Chunyan Miao"
        ],
        "abstract": "PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03207",
        "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations",
        "authors": [
            "Hwaran Lee",
            "Geonmin Kim",
            "Ho-Gyeong Kim",
            "Sang-Hoon Oh",
            "Soo-Young Lee"
        ],
        "abstract": "Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7% on the SWB part of the Hub5'2000 evaluation test set, which is competitive with other state-of-the-art methods.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03254",
        "title": "Natural Language Generation enhances human decision-making with uncertain information",
        "authors": [
            "Dimitra Gkatzia",
            "Oliver Lemon",
            "Verena Rieser"
        ],
        "abstract": "Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores or probabilities. We present a comparison of different information presentations for uncertain data and, for the first time, measure their effects on human decision-making. We show that the use of Natural Language Generation (NLG) improves decision-making under uncertainty, compared to state-of-the-art graphical-based representation methods. In a task-based study with 442 adults, we found that presentations using NLG lead to 24% better decision-making on average than the graphical presentations, and to 44% better decision-making when NLG is combined with graphics. We also show that women achieve significantly better results when presented with NLG output (an 87% increase on average compared to graphical presentations).\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03335",
        "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method",
        "authors": [
            "Roman Bartusiak",
            "\u0141ukasz Augustyniak",
            "Tomasz Kajdanowicz",
            "Przemys\u0142aw Kazienko",
            "Maciej Piasecki"
        ],
        "abstract": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position - role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03352",
        "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems",
        "authors": [
            "Tsung-Hsien Wen",
            "Milica Gasic",
            "Nikola Mrksic",
            "Lina M. Rojas-Barahona",
            "Pei-Hao Su",
            "Stefan Ultes",
            "David Vandyke",
            "Steve Young"
        ],
        "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03391",
        "title": "Simple Question Answering by Attentive Convolutional Neural Network",
        "authors": [
            "Wenpeng Yin",
            "Mo Yu",
            "Bing Xiang",
            "Bowen Zhou",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This work focuses on answering single-relation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a two-step pipeline: entity linking and fact selection. In fact selection, we match the subject entity in a fact candidate with the entity mention in the question by a character-level convolutional neural network (char-CNN), and match the predicate in that fact with the question by a word-level CNN (word-CNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03398",
        "title": "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured Corpora",
        "authors": [
            "Lidong Bing",
            "Bhuwan Dhingra",
            "Kathryn Mazaitis",
            "Jong Hyuk Park",
            "William W. Cohen"
        ],
        "abstract": "We propose a framework to improve performance of distantly-supervised relation extraction, by jointly learning to solve two related tasks: concept-instance extraction and relation extraction. We combine this with a novel use of document structure: in some small, well-structured corpora, sections can be identified that correspond to relation arguments, and distantly-labeled examples from such sections tend to have good precision. Using these as seeds we extract additional relation examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second argument of the relation, we get significant improvements over several state-of-the-art approaches to distantly-supervised relation extraction.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03475",
        "title": "De-identification of Patient Notes with Recurrent Neural Networks",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Ozlem Uzuner",
            "Peter Szolovits"
        ],
        "abstract": "Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\n",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03568",
        "title": "Word Sense Disambiguation using a Bidirectional LSTM",
        "authors": [
            "Mikael K\u00e5geb\u00e4ck",
            "Hans Salomonsson"
        ],
        "abstract": "In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03622",
        "title": "Data Recombination for Neural Semantic Parsing",
        "authors": [
            "Robin Jia",
            "Percy Liang"
        ],
        "abstract": "Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2016-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03632",
        "title": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data",
        "authors": [
            "Shikhar Sharma",
            "Jing He",
            "Kaheer Suleman",
            "Hannes Schulz",
            "Philip Bachman"
        ],
        "abstract": "Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03667",
        "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads",
        "authors": [
            "Ji He",
            "Mari Ostendorf",
            "Xiaodong He",
            "Jianshu Chen",
            "Jianfeng Gao",
            "Lihong Li",
            "Li Deng"
        ],
        "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2016-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03676",
        "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
        "authors": [
            "Beno\u00eet Sagot"
        ],
        "abstract": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results.  Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03777",
        "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
        "authors": [
            "Nikola Mrk\u0161i\u0107",
            "Diarmuid \u00d3 S\u00e9aghdha",
            "Tsung-Hsien Wen",
            "Blaise Thomson",
            "Steve Young"
        ],
        "abstract": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03821",
        "title": "Learning to Generate Compositional Color Descriptions",
        "authors": [
            "Will Monroe",
            "Noah D. Goodman",
            "Christopher Potts"
        ],
        "abstract": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\", \"dull\"), and compositional phrases (\"faded teal\") not seen in training.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04052",
        "title": "Dialog state tracking, a machine reading approach using Memory Network",
        "authors": [
            "Julien Perez",
            "Fei Liu"
        ],
        "abstract": "In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04081",
        "title": "Graph-Community Detection for Cross-Document Topic Segment Relationship Identification",
        "authors": [
            "Pedro Mota",
            "Maxine Eskenazi",
            "Luisa Coheur"
        ],
        "abstract": "In this paper we propose a graph-community detection approach to identify cross-document relationships at the topic segment level. Given a set of related documents, we automatically find these relationships by clustering segments with similar content (topics). In this context, we study how different weighting mechanisms influence the discovery of word communities that relate to the different topics found in the documents. Finally, we test different mapping functions to assign topic segments to word communities, determining which topic segments are considered equivalent.\n",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04155",
        "title": "Rationalizing Neural Predictions",
        "authors": [
            "Tao Lei",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ],
        "abstract": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04164",
        "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation",
        "authors": [
            "Orhan Firat",
            "Baskaran Sankaran",
            "Yaser Al-Onaizan",
            "Fatos T. Yarman Vural",
            "Kyunghyun Cho"
        ],
        "abstract": "In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04199",
        "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
        "authors": [
            "Jie Zhou",
            "Ying Cao",
            "Xuguang Wang",
            "Peng Li",
            "Wei Xu"
        ],
        "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04212",
        "title": "Active Discriminative Text Representation Learning",
        "authors": [
            "Ye Zhang",
            "Matthew Lease",
            "Byron C. Wallace"
        ],
        "abstract": "We propose a new active learning (AL) method for text classification with convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as representations (features), tuning these to the task at hand. We argue that AL strategies for multi-layered neural models should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., entropy-based uncertainty sampling), which specify higher level objectives. We propose a simple approach for sentence classification that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. We extend this approach to document classification by jointly considering: (1) the expected changes to the constituent word representations; and (2) the model's current overall uncertainty regarding the instance. The relative emphasis placed on these criteria is governed by a stochastic process that favors selecting instances likely to improve representations at the outset of learning, and then shifts toward general uncertainty sampling as AL progresses. Empirical results show that our method outperforms baseline AL approaches on both sentence and document classification tasks. We also show that, as expected, the method quickly learns discriminative word embeddings. To the best of our knowledge, this is the first work on AL addressing neural models for text classification.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04279",
        "title": "Cross-Lingual Morphological Tagging for Low-Resource Languages",
        "authors": [
            "Jan Buys",
            "Jan A. Botha"
        ],
        "abstract": "Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embedding-based model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04289",
        "title": "Automatic Text Scoring Using Neural Networks",
        "authors": [
            "Dimitrios Alikaniotis",
            "Helen Yannakoudakis",
            "Marek Rei"
        ],
        "abstract": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04300",
        "title": "Neural Word Segmentation Learning for Chinese",
        "authors": [
            "Deng Cai",
            "Hai Zhao"
        ],
        "abstract": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task where only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long short-term memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous state-of-the-art methods.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04351",
        "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification",
        "authors": [
            "Georgios Balikas",
            "Massih-Reza Amini"
        ],
        "abstract": "This paper describes the participation of the team \"TwiSE\" in the SemEval 2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment Analysis in Twitter\" for which we implemented sentiment classification systems for subtasks A, B, C and D. Our approach consists of two steps. In the first step, we generate and validate diverse feature sets for twitter sentiment evaluation, inspired by the work of participants of previous editions of such challenges. In the second step, we focus on the optimization of the evaluation measures of the different subtasks. To this end, we examine different learning strategies by validating them on the data provided by the task organisers. For our final submissions we used an ensemble learning approach (stacked generalization) for Subtask A and single linear models for the rest of the subtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively.\\footnote{We make the code available for research purposes at \\url{",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04503",
        "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization",
        "authors": [
            "Akanksha",
            "Jacob Eisenstein"
        ],
        "abstract": "This paper describes the Georgia Tech team's approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04582",
        "title": "Query-Reduction Networks for Question Answering",
        "authors": [
            "Minjoon Seo",
            "Sewon Min",
            "Ali Farhadi",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04596",
        "title": "Semi-Supervised Learning for Neural Machine Translation",
        "authors": [
            "Yong Cheng",
            "Wei Xu",
            "Zhongjun He",
            "Wei He",
            "Hua Wu",
            "Maosong Sun",
            "Yang Liu"
        ],
        "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04597",
        "title": "Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora",
        "authors": [
            "Chunyang Liu",
            "Yang Liu",
            "Huanbo Luan",
            "Maosong Sun",
            "Heng Yu"
        ],
        "abstract": "We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asymmetric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is defined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently. Experiments on the Chinese-English dataset show that agreement-based learning significantly improves both alignment and translation performance.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04640",
        "title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations",
        "authors": [
            "Tom Kenter",
            "Alexey Borisov",
            "Maarten de Rijke"
        ],
        "abstract": "We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high-quality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, word embeddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged. The underlying neural network learns word embeddings by predicting, from a sentence representation, its surrounding sentences. We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04672",
        "title": "Constitutional Precedent of Amicus Briefs",
        "authors": [
            "Allen Huang",
            "Lars Roemheld"
        ],
        "abstract": "We investigate shared language between U.S. Supreme Court majority opinions and interest groups' corresponding amicus briefs. Specifically, we evaluate whether language that originated in an amicus brief acquired legal precedent status by being cited in the Court's opinion. Using plagiarism detection software, automated querying of a large legal database, and manual analysis, we establish seven instances where interest group amici were able to formulate constitutional case law, setting binding legal precedent. We discuss several such instances for their implications in the Supreme Court's creation of case law.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04686",
        "title": "Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning",
        "authors": [
            "Verena Rieser",
            "Oliver Lemon"
        ],
        "abstract": "We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex trade- offs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG pol- icy using Reinforcement Learning (RL), which adapts its behaviour to noisy feed- back from the current generation context. This policy is compared to several base- lines derived from previous work in this area. The learned policy significantly out- performs all the prior approaches.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04754",
        "title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation",
        "authors": [
            "Amrita Saha",
            "Mitesh M. Khapra",
            "Sarath Chandar",
            "Janarthanan Rajendran",
            "Kyunghyun Cho"
        ],
        "abstract": "Interlingua based Machine Translation (MT) aims to encode multiple languages into a common linguistic representation and then decode sentences in multiple target languages from this representation. In this work we explore this idea in the context of neural encoder decoder architectures, albeit on a smaller scale and without MT as the end goal. Specifically, we consider the case of three languages or modalities X, Z and Y wherein we are interested in generating sequences in Y starting from information available in X. However, there is no parallel training data available between X and Y but, training data is available between X & Z and Z & Y (as is often the case in many real world applications). Z thus acts as a pivot/bridge. An obvious solution, which is perhaps less elegant but works very well in practice is to train a two stage model which first converts from X to Z and then from Z to Y. Instead we explore an interlingua inspired solution which jointly learns to do the following (i) encode X and Z to a common representation and (ii) decode Y from this common representation. We evaluate our model on two tasks: (i) bridge transliteration and (ii) bridge captioning. We report promising results in both these applications and believe that this is a right step towards truly interlingua inspired encoder decoder architectures.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04835",
        "title": "Learning Word Sense Embeddings from Word Sense Definitions",
        "authors": [
            "Qi Li",
            "Tianshi Li",
            "Baobao Chang"
        ],
        "abstract": "Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04870",
        "title": "Smart Reply: Automated Response Suggestion for Email",
        "authors": [
            "Anjuli Kannan",
            "Karol Kurach",
            "Sujith Ravi",
            "Tobias Kaufmann",
            "Andrew Tomkins",
            "Balint Miklos",
            "Greg Corrado",
            "Laszlo Lukacs",
            "Marina Ganea",
            "Peter Young",
            "Vivek Ramavajjala"
        ],
        "abstract": "In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning.\n",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04963",
        "title": "The Edit Distance Transducer in Action: The University of Cambridge English-German System at WMT16",
        "authors": [
            "Felix Stahlberg",
            "Eva Hasler",
            "Bill Byrne"
        ],
        "abstract": "This paper presents the University of Cambridge submission to WMT16. Motivated by the complementary nature of syntactical machine translation and neural machine translation (NMT), we exploit the synergies of Hiero and NMT in different combination schemes. Starting out with a simple neural lattice rescoring approach, we show that the Hiero lattices are often too narrow for NMT ensembles. Therefore, instead of a hard restriction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modified version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05007",
        "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks",
        "authors": [
            "Naoya Takahashi",
            "Tofigh Naghibi",
            "Beat Pfister"
        ],
        "abstract": "Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs. However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages. Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05029",
        "title": "No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering \"Simple\" Questions)",
        "authors": [
            "Ferhan Ture",
            "Oliver Jojic"
        ],
        "abstract": "First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results --- even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users every day.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2017-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05250",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "authors": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\n",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05286",
        "title": "Spectral decomposition method of dialog state tracking via collective matrix factorization",
        "authors": [
            "Julien Perez"
        ],
        "abstract": "The task of dialog management is commonly decomposed into two sequential subtasks: dialog state tracking and dialog policy learning. In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate the true dialog state from noisy observations produced by the speech recognition and the natural language understanding modules. The state tracking task is primarily meant to support a dialog policy. From a probabilistic perspective, this is achieved by maintaining a posterior distribution over hidden dialog states composed of a set of context dependent variables. Once a dialog policy is learned, it strives to select an optimal dialog act given the estimated dialog state and a defined reward function. This paper introduces a novel method of dialog state tracking based on a bilinear algebric decomposition model that provides an efficient inference schema through collective matrix factorization. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives encouraging results compared to the state-of-the-art trackers that participated in this standard benchmark. Finally, we show that the prediction schema is computationally efficient in comparison to the previous approaches.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05378",
        "title": "Simpler Context-Dependent Logical Forms via Model Projections",
        "authors": [
            "Reginald Long",
            "Panupong Pasupat",
            "Percy Liang"
        ],
        "abstract": "We consider the task of learning a context-dependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context-dependent semantic parsing datasets, and develop a new left-to-right parser.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05409",
        "title": "Sense Embedding Learning for Word Sense Induction",
        "authors": [
            "Linfeng Song",
            "Zhiguo Wang",
            "Haitao Mi",
            "Daniel Gildea"
        ],
        "abstract": "Conventional word sense induction (WSI) methods usually represent each instance with discrete linguistic features or cooccurrence features, and train a model for each polysemous word individually. In this work, we propose to learn sense embeddings for the WSI task. In the training stage, our method induces several sense centroids (embedding) for each polysemous word. In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space. The advantages of our method are (1) distributed sense vectors are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models, and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. We further verify the two advantages by comparing with carefully designed baselines.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05464",
        "title": "Stance Detection with Bidirectional Conditional Encoding",
        "authors": [
            "Isabelle Augenstein",
            "Tim Rockt\u00e4schel",
            "Andreas Vlachos",
            "Kalina Bontcheva"
        ],
        "abstract": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05467",
        "title": "Gender Inference using Statistical Name Characteristics in Twitter",
        "authors": [
            "Juergen Mueller",
            "Gerd Stumme"
        ],
        "abstract": "Much attention has been given to the task of gender inference of Twitter users. Although names are strong gender indicators, the names of Twitter users are rarely used as a feature; probably due to the high number of ill-formed names, which cannot be found in any name dictionary. Instead of relying solely on a name database, we propose a novel name classifier. Our approach extracts characteristics from the user names and uses those in order to assign the names to a gender. This enables us to classify international first names as well as ill-formed names.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05491",
        "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings",
        "authors": [
            "Ond\u0159ej Du\u0161ek",
            "Filip Jur\u010d\u00ed\u010dek"
        ],
        "abstract": "We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach. We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05545",
        "title": "Universal, Unsupervised (Rule-Based), Uncovered Sentiment Analysis",
        "authors": [
            "David Vilares",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Miguel A. Alonso"
        ],
        "abstract": "We present a novel unsupervised approach for multilingual sentiment analysis driven by compositional syntax-based rules. On the one hand, we exploit some of the main advantages of unsupervised algorithms: (1) the interpretability of their output, in contrast with most supervised models, which behave as a black box and (2) their robustness across different corpora and domains. On the other hand, by introducing the concept of compositional operations and exploiting syntactic information in the form of universal dependencies, we tackle one of their main drawbacks: their rigidity on data that are structured differently depending on the language concerned. Experiments show an improvement both over existing unsupervised methods, and over state-of-the-art supervised models when evaluating outside their corpus of origin. Experiments also show how the same compositional operations can be shared across languages. The system is available at ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2017-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05554",
        "title": "SMS Spam Filtering using Probabilistic Topic Modelling and Stacked Denoising Autoencoder",
        "authors": [
            "Noura Al Moubayed",
            "Toby Breckon",
            "Peter Matthews",
            "A. Stephen McGough"
        ],
        "abstract": "In This paper we present a novel approach to spam filtering and demonstrate its applicability with respect to SMS messages. Our approach requires minimum features engineering and a small set of la- belled data samples. Features are extracted using topic modelling based on latent Dirichlet allocation, and then a comprehensive data model is created using a Stacked Denoising Autoencoder (SDA). Topic modelling summarises the data providing ease of use and high interpretability by visualising the topics using word clouds. Given that the SMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA is able to model the messages and accurately discriminate between the two classes without the need for a pre-labelled training set. The results are compared against the state-of-the-art spam detection algorithms with our proposed approach achieving over 97% accuracy which compares favourably to the best reported algorithms presented in the literature.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05611",
        "title": "Data-driven HR - R\u00e9sum\u00e9 Analysis Based on Natural Language Processing and Machine Learning",
        "authors": [
            "Tim Zimmermann",
            "Leo Kotschenreuther",
            "Karsten Schmidt"
        ],
        "abstract": "Recruiters usually spend less than a minute looking at each r\u00e9sum\u00e9 when deciding whether it's worth continuing the recruitment process with the candidate. Recruiters focus on keywords, and it's almost impossible to guarantee a fair process of candidate selection. The main scope of this paper is to tackle this issue by introducing a data-driven approach that shows how to process r\u00e9sum\u00e9s automatically and give recruiters more time to only examine promising candidates. Furthermore, we show how to leverage Machine Learning and Natural Language Processing in order to extract all required information from the r\u00e9sum\u00e9s. Once the information is extracted, a ranking score is calculated. The score describes how well the candidates fit based on their education, work experience and skills. Later this paper illustrates a prototype application that shows how this novel approach can increase the productivity of recruiters. The application enables them to filter and rank candidates based on predefined job descriptions. Guided by the ranking, recruiters can get deeper insights from candidate profiles and validate why and how the application ranked them. This application shows how to improve the hiring process by giving an unbiased hiring decision support.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05679",
        "title": "Two Discourse Driven Language Models for Semantics",
        "authors": [
            "Haoruo Peng",
            "Dan Roth"
        ],
        "abstract": "Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a \"standard\" N-gram language model and three discriminatively trained \"neural\" language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05694",
        "title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs",
        "authors": [
            "Prashanth Vijayaraghavan",
            "Ivan Sysoev",
            "Soroush Vosoughi",
            "Deb Roy"
        ],
        "abstract": "This paper describes our approach for the Detecting Stance in Tweets task (SemEval-2016 Task 6). We utilized recent advances in short text categorization using deep learning to create word-level and character-level models. The choice between word-level and character-level models in each particular case was informed through validation performance. Our final system is a combination of classifiers using word-level or character-level models. We also employed novel data augmentation techniques to expand and diversify our training dataset, thus making our system more robust. Our system achieved a macro-average precision, recall and F1-scores of 0.67, 0.61 and 0.635 respectively.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05699",
        "title": "Socially-Informed Timeline Generation for Complex Events",
        "authors": [
            "Lu Wang",
            "Claire Cardie",
            "Galen Marchetti"
        ],
        "abstract": "Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions. We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments. We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event. Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-the-art systems. In human evaluation, the associated comment summaries are furthermore rated more insightful than editor's picks and comments ranked highly by users.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05702",
        "title": "Query-Focused Opinion Summarization for User-Generated Content",
        "authors": [
            "Lu Wang",
            "Hema Raghavan",
            "Claire Cardie",
            "Vittorio Castelli"
        ],
        "abstract": "We present a submodular function-based framework for query-focused opinion summarization. Within our framework, relevance ordering produced by a statistical ranker, and information coverage with respect to topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for submodularity-based summarization methods. By experimenting on community QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05704",
        "title": "A Piece of My Mind: A Sentiment Analysis Approach for Online Dispute Detection",
        "authors": [
            "Lu Wang",
            "Claire Cardie"
        ],
        "abstract": "We investigate the novel task of online dispute detection and propose a sentiment analysis solution to the problem: we aim to identify the sequence of sentence-level sentiments expressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05706",
        "title": "Improving Agreement and Disagreement Identification in Online Discussions with A Socially-Tuned Sentiment Lexicon",
        "authors": [
            "Lu Wang",
            "Claire Cardie"
        ],
        "abstract": "We study the problem of agreement and disagreement detection in online discussions. An isotonic Conditional Random Fields (isotonic CRF) based sequential model is proposed to make predictions on sentence- or segment-level. We automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance. We evaluate our agreement and disagreement tagging model on two disparate online discussion corpora -- Wikipedia Talk pages and online debates. Our model is shown to outperform the state-of-the-art approaches in both datasets. For example, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages.\n    ",
        "submission_date": "2016-06-17T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05759",
        "title": "Egyptian Arabic to English Statistical Machine Translation System for NIST OpenMT'2015",
        "authors": [
            "Hassan Sajjad",
            "Nadir Durrani",
            "Francisco Guzman",
            "Preslav Nakov",
            "Ahmed Abdelali",
            "Stephan Vogel",
            "Wael Salloum",
            "Ahmed El Kholy",
            "Nizar Habash"
        ],
        "abstract": "The paper describes the Egyptian Arabic-to-English statistical machine translation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to the NIST OpenMT'2015 competition. The competition focused on informal dialectal Arabic, as used in SMS, chat, and speech. Thus, our efforts focused on processing and standardizing Arabic, e.g., using tools such as 3arrib and MADAMIRA. We further trained a phrase-based SMT system using state-of-the-art features and components such as operation sequence model, class-based language model, sparse features, neural network joint model, genre-based hierarchically-interpolated language model, unsupervised transliteration mining, phrase-table merging, and hypothesis combination. Our system ranked second on all three genres.\n    ",
        "submission_date": "2016-06-18T00:00:00",
        "last_modified_date": "2016-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05804",
        "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema",
        "authors": [
            "Patrick Verga",
            "Arvind Neelakantan",
            "Andrew McCallum"
        ],
        "abstract": "Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types---not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time.\n    ",
        "submission_date": "2016-06-18T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05829",
        "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test",
        "authors": [
            "Qixin Wang",
            "Tianyi Luo",
            "Dong Wang"
        ],
        "abstract": "Recent progress in neural learning demonstrated that machines can do well in regularized tasks, e.g., the game of Go. However, artistic activities such as poem generation are still widely regarded as human's special capability. In this paper, we demonstrate that a simple neural model can imitate human in some tasks of art generation. We particularly focus on traditional Chinese poetry, and show that machines can do as well as many contemporary poets and weakly pass the Feigenbaum Test, a variant of Turing test in professional domains. Our method is based on an attention-based recurrent neural network, which accepts a set of keywords as the theme and generates poems by looking at each keyword during the generation. A number of techniques are proposed to improve the model, including character vector initialization, attention to input and hybrid-style training. Compared to existing poetry generation methods, our model can generate much more theme-consistent and semantic-rich poems.\n    ",
        "submission_date": "2016-06-19T00:00:00",
        "last_modified_date": "2016-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05854",
        "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question Answering",
        "authors": [
            "Dong Xu",
            "Wu-Jun Li"
        ],
        "abstract": "Recently, bidirectional recurrent neural network (BRNN) has been widely used for question answering (QA) tasks with promising performance. However, most existing BRNN models extract the information of questions and answers by directly using a pooling operation to generate the representation for loss or similarity calculation. Hence, these existing models don't put supervision (loss or similarity calculation) at every time step, which will lose some useful information. In this paper, we propose a novel BRNN model called full-time supervision based BRNN (FTS-BRNN), which can put supervision at every time step. Experiments on the factoid QA task show that our FTS-BRNN can outperform other baselines to achieve the state-of-the-art accuracy.\n    ",
        "submission_date": "2016-06-19T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05967",
        "title": "A Nonparametric Bayesian Approach for Spoken Term detection by Example Query",
        "authors": [
            "Amir Hossein Harati Nejad Torbati",
            "Joseph Picone"
        ],
        "abstract": "State of the art speech recognition systems use data-intensive context-dependent phonemes as acoustic units. However, these approaches do not translate well to low resourced languages where large amounts of training data is not available. For such languages, automatic discovery of acoustic units is critical. In this paper, we demonstrate the application of nonparametric Bayesian models to acoustic unit discovery. We show that the discovered units are correlated with phonemes and therefore are linguistically meaningful. We also present a spoken term detection (STD) by example query algorithm based on these automatically learned units. We show that our proposed system produces a P@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the EER is 5% while P@N is only slightly lower than the best reported system in the literature.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05994",
        "title": "The Role of CNL and AMR in Scalable Abstractive Summarization for Multilingual Media Monitoring",
        "authors": [
            "Normunds Gruzitis",
            "Guntis Barzdins"
        ],
        "abstract": "In the era of Big Data and Deep Learning, there is a common view that machine learning approaches are the only way to cope with the robust and scalable information extraction and summarization. It has been recently proposed that the CNL approach could be scaled up, building on the concept of embedded CNL and, thus, allowing for CNL-based information extraction from e.g. normative or medical texts that are rather controlled by nature but still infringe the boundaries of CNL. Although it is arguable if CNL can be exploited to approach the robust wide-coverage semantic parsing for use cases like media monitoring, its potential becomes much more obvious in the opposite direction: generation of story highlights from the summarized AMR graphs, which is in the focus of this position paper.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06031",
        "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context",
        "authors": [
            "Denis Paperno",
            "Germ\u00e1n Kruszewski",
            "Angeliki Lazaridou",
            "Quan Ngoc Pham",
            "Raffaella Bernardi",
            "Sandro Pezzelle",
            "Marco Baroni",
            "Gemma Boleda",
            "Raquel Fern\u00e1ndez"
        ],
        "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06086",
        "title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold for Similarity",
        "authors": [
            "Navid Rekabsaz",
            "Mihai Lupu",
            "Allan Hanbury"
        ],
        "abstract": "Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to or statistically indistinguishable from the optimal results.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2018-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06121",
        "title": "Quantifying and Reducing Stereotypes in Word Embeddings",
        "authors": [
            "Tolga Bolukbasi",
            "Kai-Wei Chang",
            "James Zou",
            "Venkatesh Saligrama",
            "Adam Kalai"
        ],
        "abstract": "Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in {\\em word embedding}, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06125",
        "title": "Introducing a Calculus of Effects and Handlers for Natural Language Semantics",
        "authors": [
            "Jirka Mar\u0161\u00edk",
            "Maxime Amblard"
        ],
        "abstract": "In compositional model-theoretic semantics, researchers assemble truth-conditions or other kinds of denotations using the lambda calculus. It was previously observed that the lambda terms and/or the denotations studied tend to follow the same pattern: they are instances of a monad. In this paper, we present an extension of the simply-typed lambda calculus that exploits this uniformity using the recently discovered technique of effect handlers. We prove that our calculus exhibits some of the key formal properties of the lambda calculus and we use it to construct a modular semantics for a small fragment that involves multiple distinct semantic phenomena.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06164",
        "title": "Pragmatic factors in image description: the case of negations",
        "authors": [
            "Emiel van Miltenburg",
            "Roser Morante",
            "Desmond Elliott"
        ],
        "abstract": "We provide a qualitative analysis of the descriptions containing negations (no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of negation uses. Based on this analysis, we provide a set of requirements that an image description system should have in order to generate negation sentences. As a pilot experiment, we used our categorization to manually annotate sentences containing negations in the Flickr30K corpus, with an agreement score of K=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06259",
        "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
        "authors": [
            "Amir Zadeh",
            "Rowan Zellers",
            "Eli Pincus",
            "Louis-Philippe Morency"
        ],
        "abstract": "People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality sources relate to each other. This paper introduces to the scientific community the first opinion-level annotated corpus of sentiment and subjectivity analysis in online videos called Multimodal Opinion-level Sentiment Intensity dataset (MOSI). The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features. Furthermore, we present baselines for future studies in this direction as well as a new multimodal fusion approach that jointly models spoken words and visual gestures.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06274",
        "title": "A Data-Driven Approach for Semantic Role Labeling from Induced Grammar Structures in Language",
        "authors": [
            "Vivek Datla",
            "David Lin",
            "Max Louwerse",
            "Abhinav Vishnu"
        ],
        "abstract": "Semantic roles play an important role in extracting knowledge from text. Current unsupervised approaches utilize features from grammar structures, to induce semantic roles. The dependence on these grammars, however, makes it difficult to adapt to noisy and new languages. In this paper we develop a data-driven approach to identifying semantic roles, the approach is entirely unsupervised up to the point where rules need to be learned to identify the position the semantic role occurs. Specifically we develop a modified-ADIOS algorithm based on ADIOS Solan et al. (2005) to learn grammar structures, and use these grammar structures to learn the rules for identifying the semantic roles based on the context in which the grammar structures appeared. The results obtained are comparable with the current state-of-art models that are inherently dependent on human annotated data.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06361",
        "title": "A Probabilistic Generative Grammar for Semantic Parsing",
        "authors": [
            "Abulhair Saparov"
        ],
        "abstract": "Domain-general semantic parsing is a long-standing goal in natural language processing, where the semantic parser is capable of robustly parsing sentences from domains outside of which it was trained. Current approaches largely rely on additional supervision from new domains in order to generalize to those domains. We present a generative model of natural language utterances and logical forms and demonstrate its application to semantic parsing. Our approach relies on domain-independent supervision to generalize to new domains. We derive and implement efficient algorithms for training, parsing, and sentence generation. The work relies on a novel application of hierarchical Dirichlet processes (HDPs) for structured prediction, which we also present in this manuscript.\n",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2022-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06406",
        "title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM",
        "authors": [
            "James Cross",
            "Liang Huang"
        ],
        "abstract": "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06461",
        "title": "Neighborhood Mixture Model for Knowledge Base Completion",
        "authors": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson"
        ],
        "abstract": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE model, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06623",
        "title": "An empirical study on large scale text classification with skip-gram embeddings",
        "authors": [
            "Georgios Balikas",
            "Massih-Reza Amini"
        ],
        "abstract": "We investigate the integration of word embeddings as classification features in the setting of large scale text classification. Such representations have been used in a plethora of tasks, however their application in classification scenarios with thousands of classes has not been extensively researched, partially due to hardware limitations. In this work, we examine efficient composition functions to obtain document-level from word-level embeddings and we subsequently investigate their combination with the traditional one-hot-encoding representations. By presenting empirical evidence on large, multi-class, multi-label classification problems, we demonstrate the efficiency and the performance benefits of this combination.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06640",
        "title": "Neural Morphological Tagging from Characters for Morphologically Rich Languages",
        "authors": [
            "Georg Heigold",
            "Guenter Neumann",
            "Josef van Genabith"
        ],
        "abstract": "This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. We systematically explore a variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors combined with bidirectional LSTMs to model across-word context in an end-to-end setting. We explore supplementary use of word-based vectors trained on large amounts of unlabeled data. Our experiments for morphological tagging suggest that for \"simple\" model configurations, the choice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy. Increasing the model capacity by adding depth, for example, and carefully optimizing the neural networks can lead to substantial improvements, and the differences in accuracy (but not training time) become much smaller or even negligible. Overall, our best morphological taggers for German and Czech outperform the best results reported in the literature by a large margin.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06710",
        "title": "Correlation-based Intrinsic Evaluation of Word Vector Representations",
        "authors": [
            "Yulia Tsvetkov",
            "Manaal Faruqui",
            "Chris Dyer"
        ],
        "abstract": "We introduce QVEC-CCA--an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources. We show that QVEC-CCA scores are an effective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06820",
        "title": "Divergent discourse between protests and counter-protests: #BlackLivesMatter and #AllLivesMatter",
        "authors": [
            "Ryan J. Gallagher",
            "Andrew J. Reagan",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "Since the shooting of Black teenager Michael Brown by White police officer Darren Wilson in Ferguson, Missouri, the protest hashtag #BlackLivesMatter has amplified critiques of extrajudicial killings of Black Americans. In response to #BlackLivesMatter, other Twitter users have adopted #AllLivesMatter, a counter-protest hashtag whose content argues that equal attention should be given to all lives regardless of race. Through a multi-level analysis of over 860,000 tweets, we study how these protests and counter-protests diverge by quantifying aspects of their discourse. We find that #AllLivesMatter facilitates opposition between #BlackLivesMatter and hashtags such as #PoliceLivesMatter and #BlueLivesMatter in such a way that historically echoes the tension between Black protesters and law enforcement. In addition, we show that a significant portion of #AllLivesMatter use stems from hijacking by #BlackLivesMatter advocates. Beyond simply injecting #AllLivesMatter with #BlackLivesMatter content, these hijackers use the hashtag to directly confront the counter-protest notion of \"All lives matter.\" Our findings suggest that Black Lives Matter movement was able to grow, exhibit diverse conversations, and avoid derailment on social media by making discussion of counter-protest opinions a central topic of #AllLivesMatter, rather than the movement itself.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2017-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06864",
        "title": "A Curriculum Learning Method for Improved Noise Robustness in Automatic Speech Recognition",
        "authors": [
            "Stefan Braun",
            "Daniel Neil",
            "Shih-Chii Liu"
        ],
        "abstract": "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a conventional multi-condition training method.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06900",
        "title": "Inferring Logical Forms From Denotations",
        "authors": [
            "Panupong Pasupat",
            "Percy Liang"
        ],
        "abstract": "A core problem in learning semantic parsers from denotations is picking out consistent logical forms--those that yield the correct denotation--from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WikiTableQuestions dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06905",
        "title": "Learning text representation using recurrent convolutional neural network with highway layers",
        "authors": [
            "Ying Wen",
            "Weinan Zhang",
            "Rui Luo",
            "Jun Wang"
        ],
        "abstract": "Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06950",
        "title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition",
        "authors": [
            "Herman Kamper",
            "Aren Jansen",
            "Sharon Goldwater"
        ],
        "abstract": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units---effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported---in the order of 70--80% for speaker-dependent and 80--95% for speaker-independent systems---highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system's discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2017-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06996",
        "title": "The word entropy of natural languages",
        "authors": [
            "Christian Bentz",
            "Dimitrios Alikaniotis"
        ],
        "abstract": "The average uncertainty associated with words is an information-theoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty - also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07046",
        "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering",
        "authors": [
            "Jayant Krishnamurthy",
            "Oyvind Tafjord",
            "Aniruddha Kembhavi"
        ],
        "abstract": "Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07189",
        "title": "Gender and Interest Targeting for Sponsored Post Advertising at Tumblr",
        "authors": [
            "Mihajlo Grbovic",
            "Vladan Radosavljevic",
            "Nemanja Djuric",
            "Narayan Bhamidipati",
            "Ananth Nagarajan"
        ],
        "abstract": "As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and promote that content by sponsoring it to appear as an advertisement in the streams of Tumblr users. In this paper we present a framework that enabled one of the key targeted advertising components for Tumblr, specifically gender and interest targeting. We describe the main challenges involved in development of the framework, which include creating the ground truth for training gender prediction models, as well as mapping Tumblr content to an interest taxonomy. For purposes of inferring user interests we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6.8 billion user posts, with very limited amount of categorized keywords, and was shown to have superior performance over the bag-of-words model. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that cover more than 90% of daily activities at Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% lift in user engagement with sponsored posts as compared to untargeted campaigns.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07298",
        "title": "Explaining Predictions of Non-Linear Classifiers in NLP",
        "authors": [
            "Leila Arras",
            "Franziska Horn",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classifiers in terms of input variables. In this paper, we apply LRP for the first time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a specific prediction of the CNN. We compare our technique to standard sensitivity analysis, both qualitatively and quantitatively, using a \"word deleting\" perturbation experiment, a PCA analysis, and various visualizations. All experiments validate the suitability of LRP for explaining the CNN predictions, which is also in line with results reported in recent image classification studies.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07356",
        "title": "Analyzing the Behavior of Visual Question Answering Models",
        "authors": [
            "Aishwarya Agrawal",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.\n",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07461",
        "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
        "authors": [
            "Hendrik Strobelt",
            "Sebastian Gehrmann",
            "Hanspeter Pfister",
            "Alexander M. Rush"
        ],
        "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2017-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07470",
        "title": "NN-grams: Unifying neural network and n-gram language models for Speech Recognition",
        "authors": [
            "Babak Damavandi",
            "Shankar Kumar",
            "Noam Shazeer",
            "Antoine Bruguier"
        ],
        "abstract": "We present NN-grams, a novel, hybrid language model integrating n-grams and neural networks (NN) for speech recognition. The model takes as input both word histories as well as n-gram counts. Thus, it combines the memorization capacity and scalability of an n-gram model with the generalization ability of neural networks. We report experiments where the model is trained on 26B words. NN-grams are efficient at run-time since they do not include an output soft-max layer. The model is trained using noise contrastive estimation (NCE), an approach that transforms the estimation problem of neural networks into one of binary classification between data samples and noise samples. We present results with noise samples derived from either an n-gram distribution or from speech recognition lattices. NN-grams outperforms an n-gram model on an Italian speech recognition dictation task.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07481",
        "title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks",
        "authors": [
            "Jind\u0159ich Libovick\u00fd",
            "Jind\u0159ich Helcl",
            "Marek Tlust\u00fd",
            "Pavel Pecina",
            "Ond\u0159ej Bojar"
        ],
        "abstract": "Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems. In this system description paper, we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07493",
        "title": "Sort Story: Sorting Jumbled Images and Captions into Stories",
        "authors": [
            "Harsh Agrawal",
            "Arjun Chandrasekaran",
            "Dhruv Batra",
            "Devi Parikh",
            "Mohit Bansal"
        ],
        "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07545",
        "title": "Interactive Semantic Featuring for Text Classification",
        "authors": [
            "Camille Jandot",
            "Patrice Simard",
            "Max Chickering",
            "David Grangier",
            "Jina Suh"
        ],
        "abstract": "In text classification, dictionaries can be used to define human-comprehensible features. We propose an improvement to dictionary features called smoothed dictionary features. These features recognize document contexts instead of n-grams. We describe a principled methodology to solicit dictionary features from a teacher, and present results showing that models built using these human-comprehensible features are competitive with models trained with Bag of Words features.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07548",
        "title": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization",
        "authors": [
            "Lu Wang",
            "Hema Raghavan",
            "Vittorio Castelli",
            "Radu Florian",
            "Claire Cardie"
        ],
        "abstract": "We consider the problem of using sentence compression techniques to facilitate query-focused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07601",
        "title": "Evaluation method of word embedding by roots and affixes",
        "authors": [
            "KeBin Peng"
        ],
        "abstract": "Word embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach---roots and affixes model(RAAM)---to interpret it from the intrinsic structures of natural language. Also it can be used as an evaluation measure of the quality of word embedding. We introduce the information entropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then considering each category as a whole rather than individually. We experimented with English Wikipedia corpus. Our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07736",
        "title": "Issues in evaluating semantic spaces using word analogies",
        "authors": [
            "Tal Linzen"
        ],
        "abstract": "The offset method for solving word analogies has become a standard evaluation tool for vector-space semantic models: it is considered desirable for a space to represent semantic relations as consistent vector offsets. We show that the method's reliance on cosine similarity conflates offset consistency with largely irrelevant neighborhood structure, and propose simple baselines that should be used to improve the utility of the method in vector space evaluation.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07772",
        "title": "The emotional arcs of stories are dominated by six basic shapes",
        "authors": [
            "Andrew J. Reagan",
            "Lewis Mitchell",
            "Dilan Kiley",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "abstract": "Advances in computing power, natural language processing, and digitization of text now make it possible to study a culture's evolution through its texts using a \"big data\" lens. Our ability to communicate relies in part upon a shared emotional experience, with stories often following distinct emotional trajectories and forming patterns that are meaningful to us. Here, by classifying the emotional arcs for a filtered subset of 1,327 stories from Project Gutenberg's fiction collection, we find a set of six core emotional arcs which form the essential building blocks of complex emotional trajectories. We strengthen our findings by separately applying Matrix decomposition, supervised learning, and unsupervised learning. For each of these six core emotional arcs, we examine the closest characteristic stories in publication today and find that particular emotional arcs enjoy greater success, as measured by downloads.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07783",
        "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding",
        "authors": [
            "Ngoc Thang Vu"
        ],
        "abstract": "We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07822",
        "title": "Efficient Parallel Learning of Word2Vec",
        "authors": [
            "Jeroen B.P. Vuurens",
            "Carsten Eickhoff",
            "Arjen P. de Vries"
        ],
        "abstract": "Since its introduction, Word2Vec and its variants are widely used to learn semantics-preserving representations of words or entities in an embedding space, which can be used to produce state-of-art results for various Natural Language Processing tasks. Existing implementations aim to learn efficiently by running multiple threads in parallel while operating on a single model in shared memory, ignoring incidental memory update collisions. We show that these collisions can degrade the efficiency of parallel learning, and propose a straightforward caching strategy that improves the efficiency by a factor of 4.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07829",
        "title": "Unsupervised Topic Modeling Approaches to Decision Summarization in Spoken Meetings",
        "authors": [
            "Lu Wang",
            "Claire Cardie"
        ],
        "abstract": "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify \"summary-worthy\" words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decision-making process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07849",
        "title": "Focused Meeting Summarization via Unsupervised Relation Extraction",
        "authors": [
            "Lu Wang",
            "Claire Cardie"
        ],
        "abstract": "We present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction. We adapt an existing in-domain relation learner (Chen et al., 2011) by exploiting a set of task-specific constraints and features. We evaluate the approach on a decision summarization task and show that it outperforms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07901",
        "title": "Corpus-level Fine-grained Entity Typing Using Contextual Information",
        "authors": [
            "Yadollah Yaghoobzadeh",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as \"food\" or \"artist\". The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07902",
        "title": "Intrinsic Subspace Evaluation of Word Embedding Representations",
        "authors": [
            "Yadollah Yaghoobzadeh",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We introduce a new methodology for intrinsic evaluation of word representations. Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07947",
        "title": "Sequence-Level Knowledge Distillation",
        "authors": [
            "Yoon Kim",
            "Alexander M. Rush"
        ],
        "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07950",
        "title": "Word sense disambiguation: a complex network approach",
        "authors": [
            "Edilson A. Correa Jr.",
            "Alneu de Andrade Lopes",
            "Diego R. Amancio"
        ],
        "abstract": "In recent years, concepts and methods of complex networks have been employed to tackle the word sense disambiguation (WSD) task by representing words as nodes, which are connected if they are semantically similar. Despite the increasingly number of studies carried out with such models, most of them use networks just to represent the data, while the pattern recognition performed on the attribute space is performed using traditional learning techniques. In other words, the structural relationship between words have not been explicitly used in the pattern recognition process. In addition, only a few investigations have probed the suitability of representations based on bipartite networks and graphs (bigraphs) for the problem, as many approaches consider all possible links between words. In this context, we assess the relevance of a bipartite network model representing both feature words (i.e. the words characterizing the context) and target (ambiguous) words to solve ambiguities in written texts. Here, we focus on the semantical relationships between these two type of words, disregarding the relationships between feature words. In special, the proposed method not only serves to represent texts as graphs, but also constructs a structure on which the discrimination of senses is accomplished. Our results revealed that the proposed learning algorithm in such bipartite networks provides excellent results mostly when topical features are employed to characterize the context. Surprisingly, our method even outperformed the support vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. Taken together, the results obtained here show that the proposed representation/classification method might be useful to improve the semantical characterization of written texts.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2018-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07953",
        "title": "Bidirectional Recurrent Neural Networks for Medical Event Detection in Electronic Health Records",
        "authors": [
            "Abhyuday Jagannatha",
            "Hong Yu"
        ],
        "abstract": "Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored various recurrent neural network frameworks and show that they significantly outperformed the CRF models.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07965",
        "title": "Summarizing Decisions in Spoken Meetings",
        "authors": [
            "Lu Wang",
            "Claire Cardie"
        ],
        "abstract": "This paper addresses the problem of summarizing decisions in spoken meetings: our goal is to produce a concise {\\it decision abstract} for each meeting decision. We explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks. In the supervised summarization setting, and given true clusterings of decision-related utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs).\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07967",
        "title": "Leveraging Semantic Web Search and Browse Sessions for Multi-Turn Spoken Dialog Systems",
        "authors": [
            "Lu Wang",
            "Larry Heck",
            "Dilek Hakkani-Tur"
        ],
        "abstract": "Training statistical dialog models in spoken dialog systems (SDS) requires large amounts of annotated data. The lack of scalable methods for data mining and annotation poses a significant hurdle for state-of-the-art statistical dialog managers. This paper presents an approach that directly leverage billions of web search and browse sessions to overcome this hurdle. The key insight is that task completion through web search and browse sessions is (a) predictable and (b) generalizes to spoken dialog task completion. The new method automatically mines behavioral search and browse patterns from web logs and translates them into spoken dialog models. We experiment with naturally occurring spoken dialogs and large scale web logs. Our session-based models outperform the state-of-the-art method for entity extraction task in SDS. We also achieve better performance for both entity and relation extraction on web search queries when compared with nontrivial baselines.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07993",
        "title": "Learning for Biomedical Information Extraction: Methodological Review of Recent Advances",
        "authors": [
            "Feifan Liu",
            "Jinying Chen",
            "Abhyuday Jagannatha",
            "Hong Yu"
        ],
        "abstract": "Biomedical information extraction (BioIE) is important to many applications, including clinical decision support, integrative biology, and pharmacovigilance, and therefore it has been an active research. Unlike existing reviews covering a holistic view on BioIE, this review focuses on mainly recent advances in learning based approaches, by systematically summarizing them into different aspects of methodological development. In addition, we dive into open information extraction and deep learning, two emerging and influential techniques and envision next generation of BioIE.\n    ",
        "submission_date": "2016-06-26T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08003",
        "title": "Functional Distributional Semantics",
        "authors": [
            "Guy Emerson",
            "Ann Copestake"
        ],
        "abstract": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.\n    ",
        "submission_date": "2016-06-26T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08089",
        "title": "This before That: Causal Precedence in the Biomedical Domain",
        "authors": [
            "Gus Hahn-Powell",
            "Dane Bell",
            "Marco A. Valenzuela-Esc\u00e1rcega",
            "Mihai Surdeanu"
        ],
        "abstract": "Causal precedence between biochemical interactions is crucial in the biomedical domain, because it transforms collections of individual interactions, e.g., bindings and phosphorylations, into the causal mechanisms needed to inform meaningful search and inference. Here, we analyze causal precedence in the biomedical domain as distinct from open-domain, temporal precedence. First, we describe a novel, hand-annotated text corpus of causal precedence in the biomedical domain. Second, we use this corpus to investigate a battery of models of precedence, covering rule-based, feature-based, and latent representation models. The highest-performing individual model achieved a micro F1 of 43 points, approaching the best performers on the simpler temporal-only precedence tasks. Feature-based and latent representation models each outperform the rule-based models, but their performance is complementary to one another. We apply a sieve-based architecture to capitalize on this lack of overlap, achieving a micro F1 score of 46 points.\n    ",
        "submission_date": "2016-06-26T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08140",
        "title": "STransE: a novel embedding model of entities and relationships in knowledge bases",
        "authors": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson"
        ],
        "abstract": "Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform link prediction or knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This paper combines insights from several previous link prediction models into a new embedding model STransE that represents each entity as a low-dimensional vector, and each relation by two matrices and a translation vector. STransE is a simple combination of the SE and TransE models, but it obtains better link prediction performance on two benchmark datasets than previous embedding models. Thus, STransE can serve as a new baseline for the more complex models in the link prediction task.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08270",
        "title": "Evaluating Informal-Domain Word Representations With UrbanDictionary",
        "authors": [
            "Naomi Saphra",
            "Adam Lopez"
        ],
        "abstract": "Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08340",
        "title": "Topic Aware Neural Response Generation",
        "authors": [
            "Chen Xing",
            "Wei Wu",
            "Yu Wu",
            "Jie Liu",
            "Yalou Huang",
            "Ming Zhou",
            "Wei-Ying Ma"
        ],
        "abstract": "We consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. To this end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The model utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability. The joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention, synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained LDA model, and let these vectors jointly affect the generation of words in decoding. To increase the possibility of topic words appearing in responses, the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution. Empirical study on both automatic evaluation metrics and human annotations shows that TA-Seq2Seq can generate more informative and interesting responses, and significantly outperform the-state-of-the-art response generation models.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08425",
        "title": "Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context",
        "authors": [
            "Elliot Schumacher",
            "Maxine Eskenazi",
            "Gwen Frishkoff",
            "Kevyn Collins-Thompson"
        ],
        "abstract": "The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored document- and passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08495",
        "title": "Network-Efficient Distributed Word2vec Training System for Large Vocabularies",
        "authors": [
            "Erik Ordentlich",
            "Lee Yang",
            "Andy Feng",
            "Peter Cnudde",
            "Mihajlo Grbovic",
            "Nemanja Djuric",
            "Vladan Radosavljevic",
            "Gavin Owens"
        ],
        "abstract": "Word2vec is a popular family of algorithms for unsupervised training of dense vector representations of words on large text corpuses. The resulting vectors have been shown to capture semantic relationships among their corresponding words, and have shown promise in reducing a number of natural language processing (NLP) tasks to mathematical operations on these vectors. While heretofore applications of word2vec have centered around vocabularies with a few million words, wherein the vocabulary is the set of words for which vectors are simultaneously trained, novel applications are emerging in areas outside of NLP with vocabularies comprising several 100 million words. Existing word2vec training systems are impractical for training such large vocabularies as they either require that the vectors of all vocabulary words be stored in the memory of a single server or suffer unacceptable training latency due to massive network data transfer. In this paper, we present a novel distributed, parallel training system that enables unprecedented practical training of vectors for vocabularies with several 100 million words on a shared cluster of commodity servers, using far less network traffic than the existing solutions. We evaluate the proposed system on a benchmark dataset, showing that the quality of vectors does not degrade relative to non-distributed training. Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08513",
        "title": "SelQA: A New Benchmark for Selection-based Question Answering",
        "authors": [
            "Tomasz Jurczyk",
            "Michael Zhai",
            "Jinho D. Choi"
        ],
        "abstract": "This paper presents a new selection-based question answering dataset, SelQA. The dataset consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. We introduce a corpus annotation scheme that enhances the generation of large, diverse, and challenging datasets by explicitly aiming to reduce word co-occurrences between the question and answers. Our annotation scheme is composed of a series of crowdsourcing tasks with a view to more effectively utilize crowdsourcing in the creation of question answering datasets in various domains. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08689",
        "title": "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content",
        "authors": [
            "Nemanja Djuric",
            "Hao Wu",
            "Vladan Radosavljevic",
            "Mihajlo Grbovic",
            "Narayan Bhamidipati"
        ],
        "abstract": "We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08733",
        "title": "Recurrent Neural Networks for Dialogue State Tracking",
        "authors": [
            "Ond\u0159ej Pl\u00e1tek",
            "Petr B\u011blohl\u00e1vek",
            "Vojt\u011bch Hude\u010dek",
            "Filip Jur\u010d\u00ed\u010dek"
        ],
        "abstract": "This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve such results. We implemented two architectures which can be used in incremental settings and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of- the-art results.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08777",
        "title": "\"Show me the cup\": Reference with Continuous Representations",
        "authors": [
            "Gemma Boleda",
            "Sebastian Pad\u00f3",
            "Marco Baroni"
        ],
        "abstract": "One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08821",
        "title": "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy",
        "authors": [
            "Zhenhao Ge",
            "Aravind Ganapathiraju",
            "Ananth N. Iyer",
            "Scott A. Randal",
            "Felix I. Wyss"
        ],
        "abstract": "Speech recognition, especially name recognition, is widely used in phone services such as company directory dialers, stock quote providers or location finders. It is usually challenging due to pronunciation variations. This paper proposes an efficient and robust data-driven technique which automatically learns acceptable word pronunciations and updates the pronunciation dictionary to build a better lexicon without affecting recognition of other words similar to the target word. It generalizes well on datasets with various sizes, and reduces the error rate on a database with 13000+ human names by 42%, compared to a baseline with regular dictionaries already covering canonical pronunciations of 97%+ words in names, plus a well-trained spelling-to-pronunciation (STP) engine.\n    ",
        "submission_date": "2016-06-28T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08954",
        "title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",
        "authors": [
            "Swabha Swayamdipta",
            "Miguel Ballesteros",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2018-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09058",
        "title": "A Distributional Semantics Approach to Implicit Language Learning",
        "authors": [
            "Dimitrios Alikaniotis",
            "John N. Williams"
        ],
        "abstract": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a Vector-Space model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09163",
        "title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme Recognition for Low Latency Processing",
        "authors": [
            "Akash Kumar Dhaka",
            "Giampiero Salvi"
        ],
        "abstract": "We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09239",
        "title": "Learning Concept Taxonomies from Multi-modal Data",
        "authors": [
            "Hao Zhang",
            "Zhiting Hu",
            "Yuntian Deng",
            "Mrinmaya Sachan",
            "Zhicheng Yan",
            "Eric P. Xing"
        ],
        "abstract": "We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09370",
        "title": "Relation extraction from clinical texts using domain invariant convolutional neural network",
        "authors": [
            "Sunil Kumar Sahu",
            "Ashish Anand",
            "Krishnadev Oruganty",
            "Mahanandeeshwar Gattu"
        ],
        "abstract": "In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert's knowledge on defining quality features.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09371",
        "title": "Recurrent neural network models for disease name recognition using domain invariant features",
        "authors": [
            "Sunil Kumar Sahu",
            "Ashish Anand"
        ],
        "abstract": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09403",
        "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora",
        "authors": [
            "Long Duong",
            "Hiroshi Kanayama",
            "Tengfei Ma",
            "Steven Bird",
            "Trevor Cohn"
        ],
        "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09560",
        "title": "Neural Network-based Word Alignment through Score Aggregation",
        "authors": [
            "Joel Legrand",
            "Michael Auli",
            "Ronan Collobert"
        ],
        "abstract": "We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09600",
        "title": "Exploring Prediction Uncertainty in Machine Translation Quality Estimation",
        "authors": [
            "Daniel Beck",
            "Lucia Specia",
            "Trevor Cohn"
        ],
        "abstract": "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09604",
        "title": "SnapToGrid: From Statistical to Interpretable Models for Biomedical Information Extraction",
        "authors": [
            "Marco A. Valenzuela-Escarcega",
            "Gus Hahn-Powell",
            "Dane Bell",
            "Mihai Surdeanu"
        ],
        "abstract": "We propose an approach for biomedical information extraction that marries the advantages of machine learning models, e.g., learning directly from data, with the benefits of rule-based approaches, e.g., interpretability. Our approach starts by training a feature-based statistical model, then converts this model to a rule-based variant by converting its features to rules, and \"snapping to grid\" the feature weights to discrete votes. In doing so, our proposal takes advantage of the large body of work in machine learning, but it produces an interpretable model, which can be directly edited by experts. We evaluate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09636",
        "title": "Representation of texts as complex networks: a mesoscopic approach",
        "authors": [
            "Henrique F. de Arruda",
            "Filipi N. Silva",
            "Vanessa Q. Marinho",
            "Diego R. Amancio",
            "Luciano da F. Costa"
        ],
        "abstract": "Statistical techniques that analyze texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticated set of methods, including the representations in terms of complex networks. While well-established word-adjacency (co-occurrence) methods successfully grasp syntactical features of written texts, they are unable to represent important aspects of textual data, such as its topical structure, i.e. the sequence of subjects developing at a mesoscopic level along the text. Such aspects are often overlooked by current methodologies. In order to grasp the mesoscopic characteristics of semantical content in written texts, we devised a network model which is able to analyze documents in a multi-scale fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a case example, a qualitative analysis of \"Alice's Adventures in Wonderland\". We show that the mesoscopic structure of a document, modeled as a network, reveals many semantic traits of texts. Such an approach paves the way to a myriad of semantic-based applications. In addition, our approach is illustrated in a machine learning context, in which texts are classified among real texts and randomized instances.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2017-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00030",
        "title": "HUME: Human UCCA-Based Evaluation of Machine Translation",
        "authors": [
            "Alexandra Birch",
            "Omri Abend",
            "Ondrej Bojar",
            "Barry Haddow"
        ],
        "abstract": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00070",
        "title": "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems",
        "authors": [
            "Layla El Asri",
            "Jing He",
            "Kaheer Suleman"
        ],
        "abstract": "User simulation is essential for generating enough data to train a statistical spoken dialogue system. Previous models for user simulation suffer from several drawbacks, such as the inability to take dialogue history into account, the need of rigid structure to ensure coherent user behaviour, heavy dependence on a specific domain, the inability to output several user intentions during one dialogue turn, or the requirement of a summarized action space for tractability. This paper introduces a data-driven user simulator based on an encoder-decoder recurrent neural network. The model takes as input a sequence of dialogue contexts and outputs a sequence of dialogue acts corresponding to user intentions. The dialogue contexts include information about the machine acts and the status of the user goal. We show on the Dialogue State Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model outperforms an agenda-based simulator and an n-gram simulator, according to F-score. Furthermore, we show how this model can be used on the original action space and thereby models user behaviour with finer granularity.\n    ",
        "submission_date": "2016-06-30T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00139",
        "title": "TensiStrength: Stress and relaxation magnitude detection for social media texts",
        "authors": [
            "Mike Thelwall"
        ],
        "abstract": "Computer systems need to be able to react to stress in order to perform optimally on some tasks. This article describes TensiStrength, a system to detect the strength of stress and relaxation expressed in social media text messages. TensiStrength uses a lexical approach and a set of rules to detect direct and indirect expressions of stress or relaxation, particularly in the context of transportation. It is slightly more effective than a comparable sentiment analysis program, although their similar performances occur despite differences on almost half of the tweets gathered. The effectiveness of TensiStrength depends on the nature of the tweets classified, with tweets that are rich in stress-related terms being particularly problematic. Although generic machine learning methods can give better performance than TensiStrength overall, they exploit topic-related terms in a way that may be undesirable in practical applications and that may not work as well in more focused contexts. In conclusion, TensiStrength and generic machine learning approaches work well enough to be practical choices for intelligent applications that need to take advantage of stress information, and the decision about which to use depends on the nature of the texts analysed and the purpose of the task.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00186",
        "title": "Throwing fuel on the embers: Probability or Dichotomy, Cognitive or Linguistic?",
        "authors": [
            "David M. W. Powers"
        ],
        "abstract": "Prof. Robert Berwick's abstract for his forthcoming invited talk at the ACL2016 workshop on Cognitive Aspects of Computational Language Learning revives an ancient debate. Entitled \"Why take a chance?\", Berwick seems to refer implicitly to Chomsky's critique of the statistical approach of Harris as well as the currently dominant paradigms in CoNLL.\n",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00198",
        "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition",
        "authors": [
            "Rudra Murthy V",
            "Mitesh Khapra",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language ($L_1$) and abundant annotated corpora is available in another language ($L_2$). Sharing the network architecture and parameters between $L_1$ and $L_2$ leads to improved performance in $L_1$. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup (lesser annotated corpora), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00225",
        "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource",
        "authors": [
            "St\u00e9phan Tulkens",
            "Chris Emmery",
            "Walter Daelemans"
        ],
        "abstract": "Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, hand-crafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00325",
        "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation",
        "authors": [
            "Dong Yu",
            "Morten Kolb\u00e6k",
            "Zheng-Hua Tan",
            "Jesper Jensen"
        ],
        "abstract": "We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00359",
        "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models",
        "authors": [
            "S\u00e9bastien Gagnon",
            "Jean Rouat"
        ],
        "abstract": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-the-art speech recognition systems since the 70's. However, from Gaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The \"left-to-right\" topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical \"left-to-right\" models.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00410",
        "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
        "authors": [
            "Yusuke Watanabe",
            "Kazuma Hashimoto",
            "Yoshimasa Tsuruoka"
        ],
        "abstract": "We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00534",
        "title": "Text comparison using word vector representations and dimensionality reduction",
        "authors": [
            "Hendrik Heuer"
        ],
        "abstract": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.\n    ",
        "submission_date": "2016-07-02T00:00:00",
        "last_modified_date": "2016-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00578",
        "title": "Context-Dependent Word Representation for Neural Machine Translation",
        "authors": [
            "Heeyoul Choi",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ],
        "abstract": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.\n    ",
        "submission_date": "2016-07-03T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00623",
        "title": "Visualizing Natural Language Descriptions: A Survey",
        "authors": [
            "Kaveh Hassani",
            "Won-Sook Lee"
        ],
        "abstract": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field.\n    ",
        "submission_date": "2016-07-03T00:00:00",
        "last_modified_date": "2016-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00718",
        "title": "Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization",
        "authors": [
            "Minsoo Kim",
            "Moirangthem Dennis Singh",
            "Minho Lee"
        ],
        "abstract": "In this work, we introduce temporal hierarchies to the sequence to sequence (seq2seq) model to tackle the problem of abstractive summarization of scientific articles. The proposed Multiple Timescale model of the Gated Recurrent Unit (MTGRU) is implemented in the encoder-decoder setting to better deal with the presence of multiple compositionalities in larger texts. The proposed model is compared to the conventional RNN encoder-decoder, and the results demonstrate that our model trains faster and shows significant performance gains. The results also show that the temporal hierarchies help improve the ability of seq2seq models to capture compositionalities better without the presence of highly complex architectural hierarchies.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00970",
        "title": "Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation",
        "authors": [
            "Lili Mou",
            "Yiping Song",
            "Rui Yan",
            "Ge Li",
            "Lu Zhang",
            "Zhi Jin"
        ],
        "abstract": "Using neural networks to generate replies in human-computer dialogue systems is attracting increasing attention over the past few years. However, the performance is not satisfactory: the neural network tends to generate safe, universally relevant replies which carry little meaning. In this paper, we propose a content-introducing approach to neural network-based generative dialogue systems. We first use pointwise mutual information (PMI) to predict a noun as a keyword, reflecting the main gist of the reply. We then propose seq2BF, a \"sequence to backward and forward sequences\" model, which generates a reply containing the given keyword. Experimental results show that our approach significantly outperforms traditional sequence-to-sequence models in terms of human evaluation and the entropy measure, and that the predicted keyword can appear at an appropriate position in the reply.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00976",
        "title": "Modelling Context with User Embeddings for Sarcasm Detection in Social Media",
        "authors": [
            "Silvio Amir",
            "Byron C. Wallace",
            "Hao Lyu",
            "Paula Carvalho M\u00e1rio J. Silva"
        ],
        "abstract": "We introduce a deep neural network for automated sarcasm detection. Recent work has emphasized the need for models to capitalize on contextual features, beyond lexical and syntactic cues present in utterances. For example, different speakers will tend to employ sarcasm regarding different subjects and, thus, sarcasm detection models ought to encode such speaker information. Current methods have achieved this by way of laborious feature engineering. By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm. Our approach does not require elaborate feature engineering (and concomitant data scraping); fitting user embeddings requires only the text from their previous posts. The experimental results show that our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01133",
        "title": "Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection",
        "authors": [
            "Meng Fang",
            "Trevor Cohn"
        ],
        "abstract": "Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01149",
        "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation",
        "authors": [
            "Ale\u0161 Tamchyna",
            "Alexander Fraser",
            "Ond\u0159ej Bojar",
            "Marcin Junczys-Dowmunt"
        ],
        "abstract": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01274",
        "title": "Temporal Topic Analysis with Endogenous and Exogenous Processes",
        "authors": [
            "Baiyang Wang",
            "Diego Klabjan"
        ],
        "abstract": "We consider the problem of modeling temporal textual data taking endogenous and exogenous processes into account. Such text documents arise in real world applications, including job advertisements and economic news articles, which are influenced by the fluctuations of the general economy. We propose a hierarchical Bayesian topic model which imposes a \"group-correlated\" hierarchical structure on the evolution of topics over time incorporating both processes, and show that this model can be estimated from Markov chain Monte Carlo sampling methods. We further demonstrate that this model captures the intrinsic relationships between the topic distribution and the time-dependent factors, and compare its performance with latent Dirichlet allocation (LDA) and two other related models. The model is applied to two collections of documents to illustrate its empirical performance: online job advertisements from DirectEmployers Association and journalists' postings on ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01426",
        "title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",
        "authors": [
            "Rajarshi Das",
            "Arvind Neelakantan",
            "David Belanger",
            "Andrew McCallum"
        ],
        "abstract": "Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art. The code and data are available at ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01432",
        "title": "Global Neural CCG Parsing with Optimality Guarantees",
        "authors": [
            "Kenton Lee",
            "Mike Lewis",
            "Luke Zettlemoyer"
        ],
        "abstract": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.\n    ",
        "submission_date": "2016-07-05T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01485",
        "title": "Extracting Formal Models from Normative Texts",
        "authors": [
            "John J. Camilleri",
            "Normunds Gruzitis",
            "Gerardo Schneider"
        ],
        "abstract": "Normative texts are documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to model such texts using the C-O Diagram formalism, making them amenable to formal analysis, in particular verifying that a text satisfies properties concerning causality of actions and timing constraints. We present an experimental, semi-automatic aid to bridge the gap between a normative text and its formal representation. Our approach uses dependency trees combined with our own rules and heuristics for extracting the relevant components. The resulting tabular data can then be converted into a C-O Diagram.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01628",
        "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation",
        "authors": [
            "Wenhu Chen",
            "Evgeny Matusov",
            "Shahram Khadivi",
            "Jan-Thorsten Peter"
        ],
        "abstract": "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01759",
        "title": "Bag of Tricks for Efficient Text Classification",
        "authors": [
            "Armand Joulin",
            "Edouard Grave",
            "Piotr Bojanowski",
            "Tomas Mikolov"
        ],
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01856",
        "title": "Neural Name Translation Improves Neural Machine Translation",
        "authors": [
            "Xiaoqing Li",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "In order to control computational complexity, neural machine translation (NMT) systems convert all rare words outside the vocabulary into a single unk symbol. Previous solution (Luong et al., 2015) resorts to use multiple numbered unks to learn the correspondence between source and target rare words. However, testing words unseen in the training corpus cannot be handled by this method. And it also suffers from the noisy word alignment. In this paper, we focus on a major type of rare words -- named entity (NE), and propose to translate them with character level sequence to sequence model. The NE translation model is further used to derive high quality NE alignment in the bilingual training corpus. With the integration of NE translation and alignment modules, our NMT system is able to surpass the baseline system by 2.9 BLEU points on the Chinese to English task.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01958",
        "title": "Stock trend prediction using news sentiment analysis",
        "authors": [
            "Joshi Kalyani",
            "Prof. H. N. Bharathi",
            "Prof. Rao Jyothi"
        ],
        "abstract": "Efficient Market Hypothesis is the popular theory about stock prediction. With its failure much research has been carried in the area of prediction of stocks. This project is about taking non quantifiable data such as financial news articles about a company and predicting its future stock trend with news sentiment classification. Assuming that news articles have impact on stock market, this is an attempt to study relationship between news and stock trend. To show this, we created three different classification models which depict polarity of news articles being positive or negative. Observations show that RF and SVM perform well in all types of testing. Na\u00efve Bayes gives good result but not compared to the other two. Experiments are conducted to evaluate various aspects of the proposed model and encouraging results are obtained in all of the experiments. The accuracy of the prediction model is more than 80% and in comparison with news random labeling with 50% of accuracy; the model has increased the accuracy by 30%.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01963",
        "title": "Sequence Training and Adaptation of Highway Deep Neural Networks",
        "authors": [
            "Liang Lu"
        ],
        "abstract": "Highway deep neural network (HDNN) is a type of depth-gated feedforward neural network, which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks (DNNs). Previously, we investigated a structured HDNN architecture for speech recognition, in which the two gate functions were tied across all the hidden layers, and we were able to train a much smaller model without sacrificing the recognition accuracy. In this paper, we carry on the study of this architecture with sequence-discriminative training criterion and speaker adaptation techniques on the AMI meeting speech recognition corpus. We show that these two techniques improve speech recognition accuracy on top of the model trained with the cross entropy criterion. Furthermore, we demonstrate that the two gate functions that are tied across all the hidden layers are able to control the information flow over the whole network, and we can achieve considerable improvements by only updating these gate functions in both sequence training and adaptation experiments.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2017-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02061",
        "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity",
        "authors": [
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Alessandro Lenci",
            "Philippe Blache",
            "Chu-Ren Huang"
        ],
        "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02109",
        "title": "Predicting and Understanding Law-Making with Word Vectors and an Ensemble Model",
        "authors": [
            "John J. Nay"
        ],
        "abstract": "Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015, only 2,513 were enacted. We developed a machine learning approach to forecasting the probability that any bill will become law. Starting in 2001 with the 107th Congress, we trained models on data from previous Congresses, predicted all bills in the current Congress, and repeated until the 113th Congress served as the test. For prediction we scored each sentence of a bill with a language model that embeds legislative vocabulary into a high-dimensional, semantic-laden vector space. This language representation enables our investigation into which words increase the probability of enactment for any topic. To test the relative importance of text and context, we compared the text model to a context-only model that uses variables such as whether the bill's sponsor is in the majority party. To test the effect of changes to bills after their introduction on our ability to predict their final outcome, we compared using the bill text and meta-data available at the time of introduction with using the most recent data. At the time of introduction context-only predictions outperform text-only, and with the newest data text-only outperforms context-only. Combining text and context always performs best. We conducted a global sensitivity analysis on the combined model to determine important variables predicting enactment.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2017-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02250",
        "title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension",
        "authors": [
            "Yiming Cui",
            "Ting Liu",
            "Zhipeng Chen",
            "Shijin Wang",
            "Guoping Hu"
        ],
        "abstract": "Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2018-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02310",
        "title": "Collaborative Training of Tensors for Compositional Distributional Semantics",
        "authors": [
            "Tamara Polajnar"
        ],
        "abstract": "Type-based compositional distributional semantic models present an interesting line of research into functional representations of linguistic meaning. One of the drawbacks of such models, however, is the lack of training data required to train each word-type combination. In this paper we address this by introducing training methods that share parameters between similar words. We show that these methods enable zero-shot learning for words that have no training data at all, as well as enabling construction of high-quality tensors from very few training examples per word.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2017-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02355",
        "title": "Lexical Based Semantic Orientation of Online Customer Reviews and Blogs",
        "authors": [
            "Aurangzeb khan",
            "Khairullah khan",
            "Shakeel Ahmad",
            "Fazal Masood Kundi",
            "Irum Tareen",
            "Muhammad Zubair Asghar"
        ],
        "abstract": "Rapid increase in internet users along with growing power of online review sites and social media has given birth to sentiment analysis or opinion mining, which aims at determining what other people think and comment. Sentiments or Opinions contain public generated content about products, services, policies and politics. People are usually interested to seek positive and negative opinions containing likes and dislikes, shared by users for features of particular product or service. This paper proposed sentence-level lexical based domain independent sentiment classification method for different types of data such as reviews and blogs. The proposed method is based on general lexicons i.e. WordNet, SentiWordNet and user defined lexical dictionaries for semantic orientation. The relations and glosses of these dictionaries provide solution to the domain portability problem. The method performs better than word and text level corpus based machine learning methods for semantic orientation. The results show the proposed method performs better as it shows precision of 87% and83% at document and sentence levels respectively for online comments.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02501",
        "title": "Actionable and Political Text Classification using Word Embeddings and LSTM",
        "authors": [
            "Adithya Rao",
            "Nemanja Spasojevic"
        ],
        "abstract": "In this work, we apply word embeddings and neural networks with Long Short-Term Memory (LSTM) to text classification problems, where the classification criteria are decided by the context of the application. We examine two applications in particular. The first is that of Actionability, where we build models to classify social media messages from customers of service providers as Actionable or Non-Actionable. We build models for over 30 different languages for actionability, and most of the models achieve accuracy around 85%, with some reaching over 90% accuracy. We also show that using LSTM neural networks with word embeddings vastly outperform traditional techniques. Second, we explore classification of messages with respect to political leaning, where social media messages are classified as Democratic or Republican. The model is able to classify messages with a high accuracy of 87.57%. As part of our experiments, we vary different hyperparameters of the neural networks, and report the effect of such variation on the accuracy. These actionability models have been deployed to production and help company agents provide customer support by prioritizing which messages to respond to. The model for political leaning has been opened and made available for wider use.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02576",
        "title": "Analysis of opinionated text for opinion mining",
        "authors": [
            "K Paramesha",
            "K C Ravishankar"
        ],
        "abstract": "In sentiment analysis, the polarities of the opinions expressed on an object/feature are determined to assess the sentiment of a sentence or document whether it is positive/negative/neutral. Naturally, the object/feature is a noun representation which refers to a product or a component of a product, let us say, the \"lens\" in a camera and opinions emanating on it are captured in adjectives, verbs, adverbs and noun words themselves. Apart from such words, other meta-information and diverse effective features are also going to play an important role in influencing the sentiment polarity and contribute significantly to the performance of the system. In this paper, some of the associated information/meta-data are explored and investigated in the sentiment text. Based on the analysis results presented here, there is scope for further assessment and utilization of the meta-information as features in text categorization, ranking text document, identification of spam documents and polarity classification problems.\n    ",
        "submission_date": "2016-07-09T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02784",
        "title": "Open Information Extraction",
        "authors": [
            "Duc-Thuan Vo",
            "Ebrahim Bagheri"
        ],
        "abstract": "Open Information Extraction (Open IE) systems aim to obtain relation tuples with highly scalable extraction in portable across domain by identifying a variety of relation phrases and their arguments in arbitrary sentences. The first generation of Open IE learns linear chain models based on unlexicalized features such as Part-of-Speech (POS) or shallow tags to label the intermediate words between pair of potential arguments for identifying extractable relations. Open IE currently is developed in the second generation that is able to extract instances of the most frequently observed relation types such as Verb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic analysis. They expose simple yet principled ways in which verbs express relationships in linguistics such as verb phrase-based extraction or clause-based extraction. They obtain a significantly higher performance over previous systems in the first generation. In this paper, we describe an overview of two Open IE generations including strengths, weaknesses and application areas.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02789",
        "title": "Charagram: Embedding Words and Sentences via Character n-grams",
        "authors": [
            "John Wieting",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02791",
        "title": "Syntactic Phylogenetic Trees",
        "authors": [
            "Kevin Shu",
            "Sharjeel Aziz",
            "Vy-Luan Huynh",
            "David Warrick",
            "Matilde Marcolli"
        ],
        "abstract": "In this paper we identify several serious problems that arise in the use of syntactic data from the SSWL database for the purpose of computational phylogenetic reconstruction. We show that the most naive approach fails to produce reliable linguistic phylogenetic trees. We identify some of the sources of the observed problems and we discuss how they may be, at least partly, corrected by using additional information, such as prior subdivision into language families and subfamilies, and a better use of the information about ancient languages. We also describe how the use of phylogenetic algebraic geometry can help in estimating to what extent the probability distribution at the leaves of the phylogenetic tree obtained from the SSWL data can be considered reliable, by testing it on phylogenetic trees established by other forms of linguistic analysis. In simple examples, we find that, after restricting to smaller language subfamilies and considering only those SSWL parameters that are fully mapped for the whole subfamily, the SSWL data match extremely well reliable phylogenetic trees, according to the evaluation of phylogenetic invariants. This is a promising sign for the use of SSWL data for linguistic phylogenetics.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02802",
        "title": "Mapping distributional to model-theoretic semantic spaces: a baseline",
        "authors": [
            "Franck Dernoncourt"
        ],
        "abstract": "Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset.\n    ",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02810",
        "title": "The Benefits of Word Embeddings Features for Active Learning in Clinical Information Extraction",
        "authors": [
            "Mahnoosh Kholghi",
            "Lance De Vine",
            "Laurianne Sitbon",
            "Guido Zuccon",
            "Anthony Nguyen"
        ],
        "abstract": "This study investigates the use of unsupervised word embeddings and sequence features for sample representation in an active learning framework built to extract clinical concepts from clinical free text. The objective is to further reduce the manual annotation effort while achieving higher effectiveness compared to a set of baseline features. Unsupervised features are derived from skip-gram word embeddings and a sequence representation approach. The comparative performance of unsupervised features and baseline hand-crafted features in an active learning framework are investigated using a wide range of selection criteria including least confidence, information diversity, information density and diversity, and domain knowledge informativeness. Two clinical datasets are used for evaluation: the i2b2/VA 2010 NLP challenge and the ShARe/CLEF 2013 eHealth Evaluation Lab. Our results demonstrate significant improvements in terms of effectiveness as well as annotation effort savings across both datasets. Using unsupervised features along with baseline features for sample representation lead to further savings of up to 9% and 10% of the token and concept annotation rates, respectively.\n    ",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03055",
        "title": "Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach",
        "authors": [
            "Derek Greene",
            "James P. Cross"
        ],
        "abstract": "This study analyzes the political agenda of the European Parliament (EP) plenary, how it has evolved over time, and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making plenary speeches. To unveil the plenary agenda and detect latent themes in legislative speeches over time, MEP speech content is analyzed using a new dynamic topic modeling method based on two layers of Non-negative Matrix Factorization (NMF). This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999-2014. Our findings suggest that two-layer NMF is a valuable alternative to existing dynamic topic modeling approaches found in the literature, and can unveil niche topics and associated vocabularies not captured by existing methods. Substantively, our findings suggest that the political agenda of the EP evolves significantly over time and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro-crisis. MEP contributions to the plenary agenda are also found to be impacted upon by voting behaviour and the committee structure of the Parliament.\n    ",
        "submission_date": "2016-07-11T00:00:00",
        "last_modified_date": "2016-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03316",
        "title": "Separating Answers from Queries for Neural Reading Comprehension",
        "authors": [
            "Dirk Weissenborn"
        ],
        "abstract": "We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways (modules) for updating the query and accumulating evidence. This separation of responsibilities effectively decouples the search for query related support and the prediction of the answer. On recent benchmark datasets for reading comprehension, our model achieves state-of-the-art results. A qualitative analysis reveals that the model effectively accumulates weighted evidence from the query and over multiple support retrieval cycles which results in a robust answer prediction.\n    ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03542",
        "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
        "authors": [
            "Matt Gardner",
            "Jayant Krishnamurthy"
        ],
        "abstract": "Traditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting---these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information, making no use of the KB in their models. We show how to combine the disparate representations used by these two approaches, presenting for the first time a semantic parser that (1) produces compositional, executable representations of language, (2) can successfully leverage the information contained in both a formal KB and a large corpus, and (3) is not limited to the schema of the underlying KB. We demonstrate significantly improved performance over state-of-the-art baselines on an open-domain natural language question answering task.\n    ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03707",
        "title": "Re-presenting a Story by Emotional Factors using Sentimental Analysis Method",
        "authors": [
            "Hwiyeol Jo",
            "Yohan Moon",
            "Jong In Kim",
            "Jeong Ryu"
        ],
        "abstract": "Remembering an event is affected by personal emotional status. We examined the psychological status and personal factors; depression (Center for Epidemiological Studies - Depression, Radloff, 1977), present affective (Positive Affective and Negative Affective Schedule, Watson et al., 1988), life orient (Life Orient Test, Scheier & Carver, 1985), self-awareness (Core Self Evaluation Scale, Judge et al., 2003), and social factor (Social Support, Sarason et al., 1983) of undergraduate students (N=64) and got summaries of a story, Chronicle of a Death Foretold (Gabriel Garcia Marquez, 1981) from them. We implement a sentimental analysis model based on convolutional neural network (LeCun & Bengio, 1995) to evaluate each summary. From the same vein used for transfer learning (Pan & Yang, 2010), we collected 38,265 movie review data to train the model and then use them to score summaries of each student. The results of CES-D and PANAS show the relationship between emotion and memory retrieval as follows: depressed people have shown a tendency of representing a story more negatively, and they seemed less expressive. People with full of emotion - high in PANAS - have retrieved their memory more expressively than others, using more negative words then others. The contributions of this study can be summarized as follows: First, lightening the relationship between emotion and its effect during times of storing or retrieving a memory. Second, suggesting objective methods to evaluate the intensity of emotion in natural language format, using a sentimental analysis model.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03780",
        "title": "A Vector Space for Distributional Semantics for Entailment",
        "authors": [
            "James Henderson",
            "Diana Nicoleta Popa"
        ],
        "abstract": "Distributional semantics creates vector-space representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributional-semantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03895",
        "title": "Tie-breaker: Using language models to quantify gender bias in sports journalism",
        "authors": [
            "Liye Fu",
            "Cristian Danescu-Niculescu-Mizil",
            "Lillian Lee"
        ],
        "abstract": "Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04110",
        "title": "Using Recurrent Neural Network for Learning Expressive Ontologies",
        "authors": [
            "Giulio Petrucci",
            "Chiara Ghidini",
            "Marco Rospocher"
        ],
        "abstract": "Recently, Neural Networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis, question answering, or machine translation. Aiming to exploit such advantages in the Ontology Learning process, in this technical report we present a detailed description of a Recurrent Neural Network based system to be used to pursue such goal.\n    ",
        "submission_date": "2016-07-14T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04423",
        "title": "Attention-over-Attention Neural Networks for Reading Comprehension",
        "authors": [
            "Yiming Cui",
            "Zhipeng Chen",
            "Si Wei",
            "Shijin Wang",
            "Ting Liu",
            "Guoping Hu"
        ],
        "abstract": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2017-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04492",
        "title": "Neural Tree Indexers for Text Understanding",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Hong Yu"
        ],
        "abstract": "Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and node function. We implemented and evaluated a binarytree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04576",
        "title": "Neural Discourse Modeling of Conversations",
        "authors": [
            "John M. Pierre",
            "Mark Butler",
            "Jacob Portnoff",
            "Luis Aguilar"
        ],
        "abstract": "Deep neural networks have shown recent promise in many language-related tasks such as the modeling of conversations. We extend RNN-based sequence to sequence models to capture the long range discourse across many turns of conversation. We perform a sensitivity analysis on how much additional context affects performance, and provide quantitative and qualitative evidence that these models are able to capture discourse relationships across multiple utterances. Our results quantifies how adding an additional RNN layer for modeling discourse improves the quality of output utterances and providing more of the previous conversation as input also improves performance. By searching the generated outputs for specific discourse markers we show how neural discourse models can exhibit increased coherence and cohesion in conversations.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04606",
        "title": "Enriching Word Vectors with Subword Information",
        "authors": [
            "Piotr Bojanowski",
            "Edouard Grave",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04660",
        "title": "Identification of promising research directions using machine learning aided medical literature analysis",
        "authors": [
            "Victor Andrei",
            "Ognjen Arandjelovic"
        ],
        "abstract": "The rapidly expanding corpus of medical research literature presents major challenges in the understanding of previous work, the extraction of maximum information from collected data, and the identification of promising research directions. We present a case for the use of advanced machine learning techniques as an aide in this task and introduce a novel methodology that is shown to be capable of extracting meaningful information from large longitudinal corpora, and of tracking complex temporal changes within it.\n    ",
        "submission_date": "2016-05-16T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04853",
        "title": "An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks",
        "authors": [
            "Anirban Laha",
            "Vikas Raykar"
        ],
        "abstract": "Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification). For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures (specifically on different ways of handling context) for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks.\n    ",
        "submission_date": "2016-07-17T00:00:00",
        "last_modified_date": "2016-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04982",
        "title": "Dependency Language Models for Transition-based Dependency Parsing",
        "authors": [
            "Juntao Yu",
            "Bernd Bohnet"
        ],
        "abstract": "In this paper, we present an approach to improve the accuracy of a strong transition-based dependency parser by exploiting dependency language models that are extracted from a large parsed corpus. We integrated a small number of features based on the dependency language models into the parser. To demonstrate the effectiveness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2017-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05014",
        "title": "Language classification from bilingual word embedding graphs",
        "authors": [
            "Steffen Eger",
            "Armin Hoenen",
            "Alexander Mehler"
        ],
        "abstract": "We study the role of the second language in bilingual word embeddings in monolingual semantic evaluation tasks. We find strongly and weakly positive correlations between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05142",
        "title": "Joint Event Detection and Entity Resolution: a Virtuous Cycle",
        "authors": [
            "Matthias Galle",
            "Jean-Michel Renders",
            "Guillaume Jacquet"
        ],
        "abstract": "Clustering web documents has numerous applications, such as aggregating news articles into meaningful events, detecting trends and hot topics on the Web, preserving diversity in search results, etc. At the same time, the importance of named entities and, in particular, the ability to recognize them and to solve the associated co-reference resolution problem are widely recognized as key enabling factors when mining, aggregating and comparing content on the Web.\n",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05241",
        "title": "Imitation Learning with Recurrent Neural Networks",
        "authors": [
            "Khanh Nguyen"
        ],
        "abstract": "We present a novel view that unifies two frameworks that aim to solve sequential prediction problems: learning to search (L2S) and recurrent neural networks (RNN). We point out equivalences between elements of the two frameworks. By complementing what is missing from one framework comparing to the other, we introduce a more advanced imitation learning framework that, on one hand, augments L2S s notion of search space and, on the other hand, enhances RNNs training procedure to be more robust to compounding errors arising from training on highly correlated examples.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05368",
        "title": "An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation",
        "authors": [
            "Jey Han Lau",
            "Timothy Baldwin"
        ],
        "abstract": "Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05408",
        "title": "Discriminating between similar languages in Twitter using label propagation",
        "authors": [
            "Will Radford",
            "Matthias Galle"
        ],
        "abstract": "Identifying the language of social media messages is an important first step in linguistic processing. Existing models for Twitter focus on content analysis, which is successful for dissimilar language pairs. We propose a label propagation approach that takes the social graph of tweet authors into account as well as content to better tease apart similar languages. This results in state-of-the-art shared task performance of $76.63\\%$, $1.4\\%$ higher than the top system.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05650",
        "title": "A Supervised Authorship Attribution Framework for Bengali Language",
        "authors": [
            "Shanta Phani",
            "Shibamouli Lahiri",
            "Arindam Biswas"
        ],
        "abstract": "Authorship Attribution is a long-standing problem in Natural Language Processing. Several statistical and computational methods have been used to find a solution to this problem. In this paper, we have proposed methods to deal with the authorship attribution problem in Bengali.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05666",
        "title": "Trainable Frontend For Robust and Far-Field Keyword Spotting",
        "authors": [
            "Yuxuan Wang",
            "Pascal Getreuer",
            "Thad Hughes",
            "Richard F. Lyon",
            "Rif A. Saurous"
        ],
        "abstract": "Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05755",
        "title": "A New Bengali Readability Score",
        "authors": [
            "Shanta Phani",
            "Shibamouli Lahiri",
            "Arindam Biswas"
        ],
        "abstract": "In this paper we have proposed methods to analyze the readability of Bengali language texts. We have got some exceptionally good results out of the experiments.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05809",
        "title": "Neural Contextual Conversation Learning with Labeled Question-Answering Pairs",
        "authors": [
            "Kun Xiong",
            "Anqi Cui",
            "Zefeng Zhang",
            "Ming Li"
        ],
        "abstract": "Neural conversational models tend to produce generic or safe responses in different contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or \\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end approach to avoid such problem in neural generative models. Additional memory mechanisms have been introduced to standard sequence-to-sequence (seq2seq) models, so that context can be considered while generating sentences. Three seq2seq models, which memorize a fix-sized contextual vector from hidden input, hidden input/output and a gated contextual attention structure respectively, have been trained and tested on a dataset of labeled question-answering pairs in Chinese. The model with contextual attention outperforms others including the state-of-the-art seq2seq models on perplexity test. The novel contextual model generates diverse and robust responses, and is able to carry out conversations on a wide range of topics appropriately.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05818",
        "title": "An Adaptation of Topic Modeling to Sentences",
        "authors": [
            "Ruey-Cheng Chen",
            "Reid Swanson",
            "Andrew S. Gordon"
        ],
        "abstract": "Advances in topic modeling have yielded effective methods for characterizing the latent semantics of textual data. However, applying standard topic modeling approaches to sentence-level tasks introduces a number of challenges. In this paper, we adapt the approach of latent-Dirichlet allocation to include an additional layer for incorporating information about the sentence boundaries in documents. We show that the addition of this minimal information of document structure improves the perplexity results of a trained model.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05822",
        "title": "Incremental Learning for Fully Unsupervised Word Segmentation Using Penalized Likelihood and Model Selection",
        "authors": [
            "Ruey-Cheng Chen"
        ],
        "abstract": "We present a novel incremental learning approach for unsupervised word segmentation that combines features from probabilistic modeling and model selection. This includes super-additive penalties for addressing the cognitive burden imposed by long word formation, and new model selection criteria based on higher-order generative assumptions. Our approach is fully unsupervised; it relies on a small number of parameters that permits flexible modeling and a mechanism that automatically learns parameters from the data. Through experimentation, we show that this intricate design has led to top-tier performance in both phonemic and orthographic word segmentation.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06153",
        "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing",
        "authors": [
            "Marek Rei",
            "Helen Yannakoudakis"
        ],
        "abstract": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06208",
        "title": "Exploring phrase-compositionality in skip-gram models",
        "authors": [
            "Xiaochang Peng",
            "Daniel Gildea"
        ],
        "abstract": "In this paper, we introduce a variation of the skip-gram model which jointly learns distributed word vector representations and their way of composing to form phrase embeddings. In particular, we propose a learning procedure that incorporates a phrase-compositionality function which can capture how we want to compose phrases vectors from their component word vectors. Our experiments show improvement in word and phrase similarity tasks as well as syntactic tasks like dependency parsing using the proposed joint models.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06221",
        "title": "A Perspective on Sentiment Analysis",
        "authors": [
            "K Paramesha",
            "K C Ravishankar"
        ],
        "abstract": "Sentiment Analysis (SA) is indeed a fascinating area of research which has stolen the attention of researchers as it has many facets and more importantly it promises economic stakes in the corporate and governance sector. SA has been stemmed out of text analytics and established itself as a separate identity and a domain of research. The wide ranging results of SA have proved to influence the way some critical decisions are taken. Hence, it has become relevant in thorough understanding of the different dimensions of the input, output and the processes and approaches of SA.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06275",
        "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering",
        "authors": [
            "Peng Li",
            "Wei Li",
            "Zhengyan He",
            "Xuguang Wang",
            "Ying Cao",
            "Jie Zhou",
            "Wei Xu"
        ],
        "abstract": "While question answering (QA) with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. As existing neural QA methods resolve QA either as sequence generation or classification/ranking problem, they face challenges of expensive softmax computation, unseen answers handling or separate candidate answer generation component. In this work, we cast neural QA as a sequence labeling problem and propose an end-to-end sequence labeling model, which overcomes all the above challenges. Experimental results on WebQA show that our model outperforms the baselines significantly with an F1 score of 74.69% with word-based input, and the performance drops only 3.72 F1 points with more challenging character-based input.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06299",
        "title": "Opinion Mining in Online Reviews About Distance Education Programs",
        "authors": [
            "Janik Jaskolski",
            "Fabian Siegberg",
            "Thomas Tibroni",
            "Philipp Cimiano",
            "Roman Klinger"
        ],
        "abstract": "The popularity of distance education programs is increasing at a fast pace. En par with this development, online communication in fora, social media and reviewing platforms between students is increasing as well. Exploiting this information to support fellow students or institutions requires to extract the relevant opinions in order to automatically generate reports providing an overview of pros and cons of different distance education programs. We report on an experiment involving distance education experts with the goal to develop a dataset of reviews annotated with relevant categories and aspects in each category discussed in the specific review together with an indication of the sentiment.\n",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06330",
        "title": "La representaci\u00f3n de la variaci\u00f3n contextual mediante definiciones terminol\u00f3gicas flexibles",
        "authors": [
            "Antonio San Mart\u00edn"
        ],
        "abstract": "In this doctoral thesis, we apply premises of cognitive linguistics to terminological definitions and present a proposal called the flexible terminological definition. This consists of a set of definitions of the same concept made up of a general definition (in this case, one encompassing the entire environmental domain) along with additional definitions describing the concept from the perspective of the subdomains in which it is relevant. Since context is a determining factor in the construction of the meaning of lexical units (including terms), we assume that terminological definitions can, and should, reflect the effects of context, even though definitions have traditionally been treated as the expression of meaning void of any contextual effect. The main objective of this thesis is to analyze the effects of contextual variation on specialized environmental concepts with a view to their representation in terminological definitions. Specifically, we focused on contextual variation based on thematic restrictions. To accomplish the objectives of this doctoral thesis, we conducted an empirical study consisting of the analysis of a set of contextually variable concepts and the creation of a flexible definition for two of them. As a result of the first part of our empirical study, we divided our notion of domain-dependent contextual variation into three different phenomena: modulation, perspectivization and subconceptualization. These phenomena are additive in that all concepts experience modulation, some concepts also undergo perspectivization, and finally, a small number of concepts are additionally subjected to subconceptualization. In the second part, we applied these notions to terminological definitions and we presented we presented guidelines on how to build flexible definitions, from the extraction of knowledge to the actual writing of the definition.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06520",
        "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
        "authors": [
            "Tolga Bolukbasi",
            "Kai-Wei Chang",
            "James Zou",
            "Venkatesh Saligrama",
            "Adam Kalai"
        ],
        "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06532",
        "title": "Novel Word Embedding and Translation-based Language Modeling for Extractive Speech Summarization",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Berlin Chen",
            "Hsin-Min Wang",
            "Hsin-Hsi Chen"
        ],
        "abstract": "Word embedding methods revolve around learning continuous distributed vector representations of words with neural networks, which can capture semantic and/or syntactic cues, and in turn be used to induce similarity measures among words, sentences and documents in context. Celebrated methods can be categorized as prediction-based and count-based methods according to the training objectives and model architectures. Their pros and cons have been extensively analyzed and evaluated in recent studies, but there is relatively less work continuing the line of research to develop an enhanced learning method that brings together the advantages of the two model families. In addition, the interpretation of the learned word representations still remains somewhat opaque. Motivated by the observations and considering the pressing need, this paper presents a novel method for learning the word representations, which not only inherits the advantages of classic word embedding methods but also offers a clearer and more rigorous interpretation of the learned word representations. Built upon the proposed word embedding method, we further formulate a translation-based language modeling framework for the extractive speech summarization task. A series of empirical evaluations demonstrate the effectiveness of the proposed word representation learning and language modeling techniques in extractive speech summarization.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06556",
        "title": "Syntax-based Attention Model for Natural Language Inference",
        "authors": [
            "PengFei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Introducing attentional mechanism in neural network is a powerful concept, and has achieved impressive results in many natural language processing tasks. However, most of the existing models impose attentional distribution on a flat topology, namely the entire input representation sequence. Clearly, any well-formed sentence has its accompanying syntactic tree structure, which is a much rich topology. Applying attention to such topology not only exploits the underlying syntax, but also makes attention more interpretable. In this paper, we explore this direction in the context of natural language inference. The results demonstrate its efficacy. We also perform extensive qualitative analysis, deriving insights and intuitions of why and how our model works.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06560",
        "title": "Automated Prediction of Temporal Relations",
        "authors": [
            "Amol S Patwardhan",
            "Jacob Badeaux",
            "Siavash",
            "Gerald M Knapp"
        ],
        "abstract": "Background: There has been growing research interest in automated answering of questions or generation of summary of free form text such as news article. In order to implement this task, the computer should be able to identify the sequence of events, duration of events, time at which event occurred and the relationship type between event pairs, time pairs or event-time pairs. Specific Problem: It is important to accurately identify the relationship type between combinations of event and time before the temporal ordering of events can be defined. The machine learning approach taken in Mani et. al (2006) provides an accuracy of only 62.5 on the baseline data from TimeBank. The researchers used maximum entropy classifier in their methodology. TimeML uses the TLINK annotation to tag a relationship type between events and time. The time complexity is quadratic when it comes to tagging documents with TLINK using human annotation. This research proposes using decision tree and parsing to improve the relationship type tagging. This research attempts to solve the gaps in human annotation by automating the task of relationship type tagging in an attempt to improve the accuracy of event and time relationship in annotated documents. Scope information: The documents from the domain of news will be used. The tagging will be performed within the same document and not across documents. The relationship types will be identified only for a pair of event and time and not a chain of events. The research focuses on documents tagged using the TimeML specification which contains tags such as EVENT, TLINK, and TIMEX. Each tag has attributes such as identifier, relation, POS, time etc.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06852",
        "title": "CFGs-2-NLU: Sequence-to-Sequence Learning for Mapping Utterances to Semantics and Pragmatics",
        "authors": [
            "Adam James Summerville",
            "James Ryan",
            "Michael Mateas",
            "Noah Wardrip-Fruin"
        ],
        "abstract": "In this paper, we present a novel approach to natural language understanding that utilizes context-free grammars (CFGs) in conjunction with sequence-to-sequence (seq2seq) deep learning. Specifically, we take a CFG authored to generate dialogue for our target application for NLU, a videogame, and train a long short-term memory (LSTM) recurrent neural network (RNN) to map the surface utterances that it produces to traces of the grammatical expansions that yielded them. Critically, this CFG was authored using a tool we have developed that supports arbitrary annotation of the nonterminal symbols in the grammar. Because we already annotated the symbols in this grammar for the semantic and pragmatic considerations that our game's dialogue manager operates over, we can use the grammatical trace associated with any surface utterance to infer such information. During gameplay, we translate player utterances into grammatical traces (using our RNN), collect the mark-up attributed to the symbols included in that trace, and pass this information to the dialogue manager, which updates the conversation state accordingly. From an offline evaluation task, we demonstrate that our trained RNN translates surface utterances to grammatical traces with great accuracy. To our knowledge, this is the first usage of seq2seq learning for conversational agents (our game's characters) who explicitly reason over semantic and pragmatic considerations.\n    ",
        "submission_date": "2016-07-22T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06952",
        "title": "Neural Sentence Ordering",
        "authors": [
            "Xinchi Chen",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Sentence ordering is a general and critical task for natural language generation applications. Previous works have focused on improving its performance in an external, downstream task, such as multi-document summarization. Given its importance, we propose to study it as an isolated task. We collect a large corpus of academic texts, and derive a data driven approach to learn pairwise ordering of sentences, and validate the efficacy with extensive experiments. Source codes and dataset of this paper will be made publicly available.\n    ",
        "submission_date": "2016-07-23T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06961",
        "title": "Authorship attribution via network motifs identification",
        "authors": [
            "Vanessa Queiroz Marinho",
            "Graeme Hirst",
            "Diego Raphael Amancio"
        ],
        "abstract": "Concepts and methods of complex networks can be used to analyse texts at their different complexity levels. Examples of natural language processing (NLP) tasks studied via topological analysis of networks are keyword identification, automatic extractive summarization and authorship attribution. Even though a myriad of network measurements have been applied to study the authorship attribution problem, the use of motifs for text analysis has been restricted to a few works. The goal of this paper is to apply the concept of motifs, recurrent interconnection patterns, in the authorship attribution task. The absolute frequencies of all thirteen directed motifs with three nodes were extracted from the co-occurrence networks and used as classification features. The effectiveness of these features was verified with four machine learning methods. The results show that motifs are able to distinguish the writing style of different authors. In our best scenario, 57.5% of the books were correctly classified. The chance baseline for this problem is 12.5%. In addition, we have found that function words play an important role in these recurrent patterns. Taken together, our findings suggest that motifs should be further explored in other related linguistic tasks.\n    ",
        "submission_date": "2016-07-23T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07057",
        "title": "Latent Tree Language Model",
        "authors": [
            "Tomas Brychcin"
        ],
        "abstract": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles.\n",
        "submission_date": "2016-07-24T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07514",
        "title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder",
        "authors": [
            "Soroush Vosoughi",
            "Prashanth Vijayaraghavan",
            "Deb Roy"
        ],
        "abstract": "We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07565",
        "title": "Grounding Dynamic Spatial Relations for Embodied (Robot) Interaction",
        "authors": [
            "Michael Spranger",
            "Jakob Suchan",
            "Mehul Bhatt",
            "Manfred Eppe"
        ],
        "abstract": "This paper presents a computational model of the processing of dynamic spatial relations occurring in an embodied robotic interaction setup. A complete system is introduced that allows autonomous robots to produce and interpret dynamic spatial phrases (in English) given an environment of moving objects. The model unites two separate research strands: computational cognitive semantics and on commonsense spatial representation and reasoning. The model for the first time demonstrates an integration of these different strands.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07630",
        "title": "Grounded Lexicon Acquisition - Case Studies in Spatial Language",
        "authors": [
            "Michael Spranger"
        ],
        "abstract": "This paper discusses grounded acquisition experiments of increasing complexity. Humanoid robots acquire English spatial lexicons from robot tutors. We identify how various spatial language systems, such as projective, absolute and proximal can be learned. The proposed learning mechanisms do not rely on direct meaning transfer or direct access to world models of interlocutors. Finally, we show how multiple systems can be acquired at the same time.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07657",
        "title": "Machine Learned Resume-Job Matching Solution",
        "authors": [
            "Yiou Lin",
            "Hang Lei",
            "Prince Clement Addo",
            "Xiaoyu Li"
        ],
        "abstract": "Job search through online matching engines nowadays are very prominent and beneficial to both job seekers and employers. But the solutions of traditional engines without understanding the semantic meanings of different resumes have not kept pace with the incredible changes in machine learning techniques and computing capability. These solutions are usually driven by manual rules and predefined weights of keywords which lead to an inefficient and frustrating search experience. To this end, we present a machine learned solution with rich features and deep learning methods. Our solution includes three configurable modules that can be plugged with little restrictions. Namely, unsupervised feature extraction, base classifiers training and ensemble method learning. In our solution, rather than using manual rules, machine learned methods to automatically detect the semantic similarity of positions are proposed. Then four competitive \"shallow\" estimators and \"deep\" estimators are selected. Finally, ensemble methods to bag these estimators and aggregate their individual predictions to form a final prediction are verified. Experimental results of over 47 thousand resumes show that our solution can significantly improve the predication precision current position, salary, educational background and company scale.\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07788",
        "title": "How scientific literature has been evolving over the time? A novel statistical approach using tracking verbal-based methods",
        "authors": [
            "Daria Micaela Hernandez",
            "Monica Becue-Bertaut",
            "Igor Barahona"
        ],
        "abstract": "This paper provides a global vision of the scientific publications related with the Systemic Lupus Erythematosus (SLE), taking as starting point abstracts of articles. Through the time, abstracts have been evolving towards higher complexity on used terminology, which makes necessary the use of sophisticated statistical methods and answering questions including: how vocabulary is evolving through the time? Which ones are most influential articles? And which one are the articles that introduced new terms and vocabulary? To answer these, we analyze a dataset composed by 506 abstracts and downloaded from 115 different journals and cover a 18 year-period.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07931",
        "title": "Synthetic Language Generation and Model Validation in BEAST2",
        "authors": [
            "Stuart Bradley"
        ],
        "abstract": "Generating synthetic languages aids in the testing and validation of future computational linguistic models and methods. This thesis extends the BEAST2 phylogenetic framework to add linguistic sequence generation under multiple models. The new plugin is then used to test the effects of the phenomena of word borrowing on the inference process under two widely used phylolinguistic models.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07956",
        "title": "Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification",
        "authors": [
            "Yuezhang Li",
            "Ronghuo Zheng",
            "Tian Tian",
            "Zhiting Hu",
            "Rahul Iyer",
            "Katia Sycara"
        ],
        "abstract": "Due to the lack of structured knowledge applied in learning distributed representation of cate- gories, existing work cannot incorporate category hierarchies into entity information. We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to com- pute meaningful semantic relatedness between entities and categories. Our framework can han- dle both single-word concepts and multiple-word concepts with superior performance on concept categorization and yield state of the art results on dataless hierarchical classification.\n    ",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08592",
        "title": "Modeling selectional restrictions in a relational type system",
        "authors": [
            "Erkki Luuk"
        ],
        "abstract": "Selectional restrictions are semantic constraints on forming certain complex types in natural language. The paper gives an overview of modeling selectional restrictions in a relational type system with morphological and syntactic types. We discuss some foundations of the system and ways of formalizing selectional restrictions.\n",
        "submission_date": "2016-07-28T00:00:00",
        "last_modified_date": "2016-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08692",
        "title": "A Novel Bilingual Word Embedding Method for Lexical Translation Using Bilingual Sense Clique",
        "authors": [
            "Rui Wang",
            "Hai Zhao",
            "Sabine Ploux",
            "Bao-Liang Lu",
            "Masao Utiyama",
            "Eiichiro Sumita"
        ],
        "abstract": "Most of the existing methods for bilingual word embedding only consider shallow context or simple co-occurrence information. In this paper, we propose a latent bilingual sense unit (Bilingual Sense Clique, BSC), which is derived from a maximum complete sub-graph of pointwise mutual information based graph over bilingual corpus. In this way, we treat source and target words equally and a separated bilingual projection processing that have to be used in most existing works is not necessary any more. Several dimension reduction methods are evaluated to summarize the BSC-word relationship. The proposed method is evaluated on bilingual lexicon translation tasks and empirical results show that bilingual sense embedding methods outperform existing bilingual word embedding methods.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08693",
        "title": "Connecting Phrase based Statistical Machine Translation Adaptation",
        "authors": [
            "Rui Wang",
            "Hai Zhao",
            "Bao-Liang Lu",
            "Masao Utiyama",
            "Eiichro Sumita"
        ],
        "abstract": "Although more additional corpora are now available for Statistical Machine Translation (SMT), only the ones which belong to the same or similar domains with the original corpus can indeed enhance SMT performance directly. Most of the existing adaptation methods focus on sentence selection. In comparison, phrase is a smaller and more fine grained unit for data selection, therefore we propose a straightforward and efficient connecting phrase based adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram adaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performance are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods).\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08723",
        "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner",
        "authors": [
            "Emmanuel Dupoux"
        ],
        "abstract": "During their first years of life, infants learn the language(s) of their environment at an amazing speed despite large cross cultural variations in amount and complexity of the available language input. Understanding this simple fact still escapes current cognitive and linguistic theories. Recently, spectacular progress in the engineering science, notably, machine learning and wearable technology, offer the promise of revolutionizing the study of cognitive development. Machine learning offers powerful learning algorithms that can achieve human-like performance on many linguistic tasks. Wearable sensors can capture vast amounts of data, which enable the reconstruction of the sensory experience of infants in their natural environment. The project of 'reverse engineering' language development, i.e., of building an effective system that mimics infant's achievements appears therefore to be within reach. Here, we analyze the conditions under which such a project can contribute to our scientific understanding of early language development. We argue that instead of defining a sub-problem or simplifying the data, computational models should address the full complexity of the learning situation, and take as input the raw sensory signals available to infants. This implies that (1) accessible but privacy-preserving repositories of home data be setup and widely shared, and (2) models be evaluated at different linguistic levels through a benchmark of psycholinguist tests that can be passed by machines and humans alike, (3) linguistically and psychologically plausible learning architectures be scaled up to real data using probabilistic/optimization principles from machine learning. We discuss the feasibility of this approach and present preliminary results.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2018-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08725",
        "title": "Cseq2seq: Cyclic Sequence-to-Sequence Learning",
        "authors": [
            "Biao Zhang",
            "Deyi Xiong",
            "Jinsong Su"
        ],
        "abstract": "The vanilla sequence-to-sequence learning (seq2seq) reads and encodes a source sequence into a fixed-length vector only once, suffering from its insufficiency in modeling structural correspondence between the source and target sequence. Instead of handling this insufficiency with a linearly weighted attention mechanism, in this paper, we propose to use a recurrent neural network (RNN) as an alternative (Cseq2seq-I). During decoding, Cseq2seq-I cyclically feeds the previous decoding state back to the encoder as the initial state of the RNN, and reencodes source representations to produce context vectors. We surprisingly find that the introduced RNN succeeds in dynamically detecting translationrelated source tokens according to the partial target sequence. Based on this finding, we further hypothesize that the partial target sequence can act as a feedback to improve the understanding of the source sequence. To test this hypothesis, we propose cyclic sequence-to-sequence learning (Cseq2seq-II) which differs from the seq2seq only in the reintroduction of previous decoding state into the same encoder. We further perform parameter sharing on Cseq2seq-II to reduce parameter redundancy and enhance regularization. In particular, we share the weights of the encoder and decoder, and two targetside word embeddings, making Cseq2seq-II equivalent to a single conditional RNN model, with 31% parameters pruned but even better performance. Cseq2seq-II not only preserves the simplicity of seq2seq but also yields comparable and promising results on machine translation tasks. Experiments on Chinese- English and English-German translation show that Cseq2seq achieves significant and consistent improvements over seq2seq and is as competitive as the attention-based seq2seq model.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2018-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08864",
        "title": "The DLVHEX System for Knowledge Representation: Recent Advances (System Description)",
        "authors": [
            "Christoph Redl"
        ],
        "abstract": "The DLVHEX system implements the HEX-semantics, which integrates answer set programming (ASP) with arbitrary external sources. Since its first release ten years ago, significant advancements were achieved. Most importantly, the exploitation of properties of external sources led to efficiency improvements and flexibility enhancements of the language, and technical improvements on the system side increased user's convenience. In this paper, we present the current status of the system and point out the most important recent enhancements over early versions. While existing literature focuses on theoretical aspects and specific components, a bird's eye view of the overall system is missing. In order to promote the system for real-world applications, we further present applications which were already successfully realized on top of DLVHEX. This paper is under consideration for acceptance in Theory and Practice of Logic Programming.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08885",
        "title": "Authorship Verification - An Approach based on Random Forest",
        "authors": [
            "Promita Maitra",
            "Souvick Ghosh",
            "Dipankar Das"
        ],
        "abstract": "Authorship attribution, being an important problem in many areas in-cluding information retrieval, computational linguistics, law and journalism etc., has been identified as a subject of increasingly research interest in the re-cent years. In case of Author Identification task in PAN at CLEF 2015, the main focus was given on cross-genre and cross-topic author verification tasks. We have used several word-based and style-based features to identify the dif-ferences between the known and unknown problems of one given set and label the unknown ones accordingly using a Random Forest based classifier.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00112",
        "title": "Supervised Attentions for Neural Machine Translation",
        "authors": [
            "Haitao Mi",
            "Zhiguo Wang",
            "Abe Ittycheriah"
        ],
        "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \"true\" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.\n    ",
        "submission_date": "2016-07-30T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00293",
        "title": "Left-corner Methods for Syntactic Modeling with Universal Structural Constraints",
        "authors": [
            "Hiroshi Noji"
        ],
        "abstract": "The primary goal in this thesis is to identify better syntactic constraint or bias, that is language independent but also efficiently exploitable during sentence processing. We focus on a particular syntactic construction called center-embedding, which is well studied in psycholinguistics and noted to cause particular difficulty for comprehension. Since people use language as a tool for communication, one expects such complex constructions to be avoided for communication efficiency. From a computational perspective, center-embedding is closely relevant to a left-corner parsing algorithm, which can capture the degree of center-embedding of a parse tree being constructed. This connection suggests left-corner methods can be a tool to exploit the universal syntactic constraint that people avoid generating center-embedded structures. We explore such utilities of center-embedding as well as left-corner methods extensively through several theoretical and empirical examinations.\n",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00318",
        "title": "A Neural Knowledge Language Model",
        "authors": [
            "Sungjin Ahn",
            "Heeyoul Choi",
            "Tanel P\u00e4rnamaa",
            "Yoshua Bengio"
        ],
        "abstract": "Current language models have a significant limitation in the ability to encode and decode factual knowledge. This is mainly because they acquire such knowledge from statistical co-occurrences although most of the knowledge words are rarely observed. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by the knowledge graph with the RNN language model. By predicting whether the word to generate has an underlying fact or not, the model can generate such knowledge-related words by copying from the description of the predicted fact. In experiments, we show that the NKLM significantly improves the performance while generating a much smaller number of unknown words.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00329",
        "title": "Keyphrase Extraction using Sequential Labeling",
        "authors": [
            "Sujatha Das Gollapalli",
            "Xiao-li Li"
        ],
        "abstract": "Keyphrases efficiently summarize a document's content and are used in various document processing and retrieval tasks. Several unsupervised techniques and classifiers exist for extracting keyphrases from text documents. Most of these methods operate at a phrase-level and rely on part-of-speech (POS) filters for candidate phrase generation. In addition, they do not directly handle keyphrases of varying lengths. We overcome these modeling shortcomings by addressing keyphrase extraction as a sequential labeling task in this paper. We explore a basic set of features commonly used in NLP tasks as well as predictions from various unsupervised methods to train our taggers. In addition to a more natural modeling for the keyphrase extraction problem, we show that tagging models yield significant performance benefits over existing state-of-the-art extraction methods.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00339",
        "title": "Crowd-sourcing NLG Data: Pictures Elicit Better Data",
        "authors": [
            "Jekaterina Novikova",
            "Oliver Lemon",
            "Verena Rieser"
        ],
        "abstract": "Recent advances in corpus-based Natural Language Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances. In this work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating different MRs with which to elicit data. We show that pictorial MRs result in better NL data being collected than logic-based MRs: utterances elicited by pictorial MRs are judged as significantly more natural, more informative, and better phrased, with a significant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs. As the MR becomes more complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00466",
        "title": "Learning Semantically Coherent and Reusable Kernels in Convolution Neural Nets for Sentence Classification",
        "authors": [
            "Madhusudan Lakshmana",
            "Sundararajan Sellamanickam",
            "Shirish Shevade",
            "Keerthi Selvaraj"
        ],
        "abstract": "The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the learned kernels do not have semantic coherence. Motivated by this observation, we propose to learn kernels with semantic coherence using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We suggest a technique to visualize attention mechanism of CNNs for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific filters may have to be learned. We demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00470",
        "title": "Labeling Topics with Images using Neural Networks",
        "authors": [
            "Nikolaos Aletras",
            "Arpit Mittal"
        ],
        "abstract": "Topics generated by topic models are usually represented by lists of $t$ terms or alternatively using short phrases and images. The current state-of-the-art work on labeling topics using images selects images by re-ranking a small set of candidates for a given topic. In this paper, we present a more generic method that can estimate the degree of association between any arbitrary pair of an unseen topic and image using a deep neural network. Our method has better runtime performance $O(n)$ compared to $O(n^2)$ for the current state-of-the-art method, and is also significantly more accurate.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00508",
        "title": "Blind phoneme segmentation with temporal prediction errors",
        "authors": [
            "Paul Michel",
            "Okko R\u00e4s\u00e4nen",
            "Roland Thiolli\u00e8re",
            "Emmanuel Dupoux"
        ],
        "abstract": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2017-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00612",
        "title": "Structured prediction models for RNN based sequence labeling in clinical text",
        "authors": [
            "Abhyuday Jagannatha",
            "Hong Yu"
        ],
        "abstract": "Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities.\n    ",
        "submission_date": "2016-08-01T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00789",
        "title": "New word analogy corpus for exploring embeddings of Czech words",
        "authors": [
            "Luk\u00e1\u0161 Svoboda",
            "Tom\u00e1\u0161 Brychc\u00edn"
        ],
        "abstract": "The word embedding methods have been proven to be very useful in many tasks of NLP (Natural Language Processing). Much has been investigated about word embeddings of English words and phrases, but only little attention has been dedicated to other languages.\n",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00841",
        "title": "Semantic Representations of Word Senses and Concepts",
        "authors": [
            "Jos\u00e9 Camacho-Collados",
            "Ignacio Iacobacci",
            "Roberto Navigli",
            "Mohammad Taher Pilehvar"
        ],
        "abstract": "Representing the semantics of linguistic items in a machine-interpretable form has been a major goal of Natural Language Processing since its earliest days. Among the range of different linguistic items, words have attracted the most research attention. However, word representations have an important limitation: they conflate different meanings of a word into a single vector. Representations of word senses have the potential to overcome this inherent limitation. Indeed, the representation of individual word senses and concepts has recently gained in popularity with several experimental results showing that a considerable performance improvement can be achieved across different NLP applications upon moving from word level to the deeper sense and concept levels. Another interesting point regarding the representation of concepts and word senses is that these models can be seamlessly applied to other linguistic items, such as words, phrases and sentences.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00869",
        "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity",
        "authors": [
            "Daniela Gerz",
            "Ivan Vuli\u0107",
            "Felix Hill",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00892",
        "title": "Knowledge Distillation for Small-footprint Highway Networks",
        "authors": [
            "Liang Lu",
            "Michelle Guo",
            "Steve Renals"
        ],
        "abstract": "Deep learning has significantly advanced state-of-the-art of speech recognition in the past few years. However, compared to conventional Gaussian mixture acoustic models, neural network models are usually much larger, and are therefore not very deployable in embedded devices. Previously, we investigated a compact highway deep neural network (HDNN) for acoustic modelling, which is a type of depth-gated feedforward neural network. We have shown that HDNN-based acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to plain deep neural network (DNN) acoustic models. In this paper, we push the boundary further by leveraging on the knowledge distillation technique that is also known as {\\it teacher-student} training, i.e., we train the compact HDNN model with the supervision of a high accuracy cumbersome model. Furthermore, we also investigate sequence training and adaptation in the context of teacher-student training. Our experiments were performed on the AMI meeting speech recognition corpus. With this technique, we significantly improved the recognition accuracy of the HDNN acoustic model with less than 0.8 million parameters, and narrowed the gap between this model and the plain DNN with 30 million parameters.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00929",
        "title": "Efficient Segmental Cascades for Speech Recognition",
        "authors": [
            "Hao Tang",
            "Weiran Wang",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "Discriminative segmental models offer a way to incorporate flexible feature functions into speech recognition. However, their appeal has been limited by their computational requirements, due to the large number of possible segments to consider. Multi-pass cascades of segmental models introduce features of increasing complexity in different passes, where in each pass a segmental model rescores lattices produced by a previous (simpler) segmental model. In this paper, we explore several ways of making segmental cascades efficient and practical: reducing the feature set in the first pass, frame subsampling, and various pruning approaches. In experiments on phonetic recognition, we find that with a combination of such techniques, it is possible to maintain competitive performance while greatly reducing decoding, pruning, and training time.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01018",
        "title": "Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science",
        "authors": [
            "Dimitrios Kartsaklis",
            "Martha Lewis",
            "Laura Rimell"
        ],
        "abstract": "This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which was held on the 11th of June at the University of Strathclyde, Glasgow, and was co-located with Quantum Physics and Logic (QPL 2016). Exploiting the common ground provided by the concept of a vector space, the workshop brought together researchers working at the intersection of Natural Language Processing (NLP), cognitive science, and physics, offering them an appropriate forum for presenting their uniquely motivated work and ideas. The interplay between these three disciplines inspired theoretically motivated approaches to the understanding of how word meanings interact with each other in sentences and discourse, how diagrammatic reasoning depicts and simplifies this interaction, how language models are determined by input from the world, and how word and sentence meanings interact logically. This first edition of the workshop consisted of three invited talks from distinguished speakers (Hans Briegel, Peter G\u00e4rdenfors, Dominic Widdows) and eight presentations of selected contributed papers. Each submission was refereed by at least three members of the Programme Committee, who delivered detailed and insightful comments and suggestions.\n\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01056",
        "title": "Morphological Priors for Probabilistic Neural Word Embeddings",
        "authors": [
            "Parminder Bhatia",
            "Robert Guthrie",
            "Jacob Eisenstein"
        ],
        "abstract": "Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01084",
        "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation",
        "authors": [
            "Christian Hadiwinoto",
            "Yang Liu",
            "Hwee Tou Ng"
        ],
        "abstract": "Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01238",
        "title": "Improving Quality of Hierarchical Clustering for Large Data Series",
        "authors": [
            "Manuel R. Ciosici"
        ],
        "abstract": "Brown clustering is a hard, hierarchical, bottom-up clustering of words in a vocabulary. Words are assigned to clusters based on their usage pattern in a given corpus. The resulting clusters and hierarchical structure can be used in constructing class-based language models and for generating features to be used in NLP tasks. Because of its high computational cost, the most-used version of Brown clustering is a greedy algorithm that uses a window to restrict its search space. Like other clustering algorithms, Brown clustering finds a sub-optimal, but nonetheless effective, mapping of words to clusters. Because of its ability to produce high-quality, human-understandable cluster, Brown clustering has seen high uptake the NLP research community where it is used in the preprocessing and feature generation steps.\n",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01298",
        "title": "A Physical Metaphor to Study Semantic Drift",
        "authors": [
            "S\u00e1ndor Dar\u00e1nyi",
            "Peter Wittek",
            "Konstantinos Konstantinidis",
            "Symeon Papadopoulos",
            "Efstratios Kontopoulos"
        ],
        "abstract": "In accessibility tests for digital preservation, over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field. This articulates the need to detect, measure, interpret and model outcomes of knowledge dynamics. To this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis. The working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of Newtonian mechanics called social mechanics. By using term distances as a measure of semantic relatedness vs. their PageRank values indicating social importance and applied as variable `term mass', gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation. From `term gravitation' over time, one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance, thereby updating Osgood's semantic differential. The dataset examined is the public catalog metadata of Tate Galleries, London.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01401",
        "title": "Dual Density Operators and Natural Language Meaning",
        "authors": [
            "Daniela Ashoush",
            "Bob Coecke"
        ],
        "abstract": "Density operators allow for representing ambiguity about a vector representation, both in quantum theory and in  distributional natural language meaning.  Formally equivalently, they allow for discarding part of the description of a composite system, where we consider the discarded part to be the context.  We introduce dual density operators, which allow for two independent notions of context. We demonstrate the use of dual density operators within a grammatical-compositional distributional framework for natural language meaning. We show that dual density operators can be used to simultaneously represent: (i) ambiguity about word meanings (e.g. queen as a person vs. queen as a band), and (ii) lexical entailment (e.g. tiger -> mammal).  We provide a proof-of-concept example. \n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01403",
        "title": "Words, Concepts, and the Geometry of Analogy",
        "authors": [
            "Stephen McGregor",
            "Matthew Purver",
            "Geraint Wiggins"
        ],
        "abstract": "This paper presents a geometric approach to the problem of modelling the relationship between words and concepts, focusing in particular on analogical phenomena in language and cognition.  Grounded in recent theories regarding geometric conceptual spaces, we begin with an analysis of existing static distributional semantic models and move on to an exploration of a dynamic approach to using high dimensional spaces of word meaning to project subspaces where analogies can potentially be solved in an online, contextualised way.  The crucial element of this analysis is the positioning of statistics in a geometric environment replete with opportunities for interpretation.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01404",
        "title": "Quantifier Scope in Categorical Compositional Distributional Semantics",
        "authors": [
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "In  previous  work with J. Hedges, we formalised a generalised quantifiers theory of natural language in  categorical compositional distributional semantics  with the help of bialgebras. In this paper, we show how  quantifier scope ambiguity can be represented in that setting and how this representation can be generalised to branching  quantifiers. \n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01405",
        "title": "Entailment Relations on Distributions",
        "authors": [
            "John van de Wetering"
        ],
        "abstract": "In this paper we give an overview of partial orders on the space of probability distributions that carry a notion of information content and serve as a generalisation of the Bayesian order given in (Coecke and Martin, 2011). We investigate what constraints are necessary in order to get a unique notion of information content. These partial orders can be used to give an ordering on words in vector space models of natural language meaning relating to the contexts in which words are used, which is useful for a notion of entailment and word disambiguation. The construction used also points towards a way to create orderings on the space of density operators which allow a more fine-grained study of entailment. The partial orders in this paper are directed complete and form domains in the sense of domain theory.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01406",
        "title": "Quantum Algorithms for Compositional Natural Language Processing",
        "authors": [
            "William Zeng",
            "Bob Coecke"
        ],
        "abstract": "We propose a new application of quantum computing to the field of natural language processing.  Ongoing work in this field attempts to incorporate grammatical structure into algorithms that compute meaning.  In (Coecke, Sadrzadeh and Clark, 2010), the authors introduce such a model (the CSC model) based on tensor product composition. While this algorithm has many advantages, its implementation is hampered by the large classical computational resources that it requires. In this work we show how computational shortcomings of the  CSC approach could be resolved using quantum computation (possibly in addition to existing techniques for dimension reduction). We address the value of quantum RAM (Giovannetti,2008) for this model and extend an algorithm from Wiebe, Braun and Lloyd (2012) into a quantum algorithm to categorize  sentences in CSC. Our new algorithm demonstrates a quadratic speedup over classical methods under certain conditions.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01413",
        "title": "Solving General Arithmetic Word Problems",
        "authors": [
            "Subhro Roy",
            "Dan Roth"
        ],
        "abstract": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of {\\em quantity schemas} that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01448",
        "title": "Word Segmentation on Micro-blog Texts with External Lexicon and Heterogeneous Data",
        "authors": [
            "Qingrong Xia",
            "Zhenghua Li",
            "Jiayuan Chao",
            "Min Zhang"
        ],
        "abstract": "This paper describes our system designed for the NLPCC 2016 shared task on word segmentation on micro-blog texts.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01561",
        "title": "UsingWord Embeddings for Query Translation for Hindi to English Cross Language Information Retrieval",
        "authors": [
            "Paheli Bhattacharya",
            "Pawan Goyal",
            "Sudeshna Sarkar"
        ],
        "abstract": "Cross-Language Information Retrieval (CLIR) has become an important problem to solve in the recent years due to the growth of content in multiple languages in the Web. One of the standard methods is to use query translation from source to target language. In this paper, we propose an approach based on word embeddings, a method that captures contextual clues for a particular word in the source language and gives those words as translations that occur in a similar context in the target language. Once we obtain the word embeddings of the source and target language pairs, we learn a projection from source to target word embeddings, making use of a dictionary with word translation ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01910",
        "title": "Resolving Out-of-Vocabulary Words with Bilingual Embeddings in Machine Translation",
        "authors": [
            "Pranava Swaroop Madhyastha",
            "Cristina Espa\u00f1a-Bonet"
        ],
        "abstract": "Out-of-vocabulary words account for a large proportion of errors in machine translation systems, especially when the system is used on a different domain than the one where it was trained. In order to alleviate the problem, we propose to use a log-bilinear softmax-based model for vocabulary expansion, such that given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language. Our model uses only word embeddings trained on significantly large unlabelled monolingual corpora and trains over a fairly small, word-to-word bilingual dictionary. We input this probabilistic list into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English-Spanish language pair. Especially, we get an improvement of 3.9 BLEU points when tested over an out-of-domain test set.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01961",
        "title": "De-Conflated Semantic Representations",
        "authors": [
            "Mohammad Taher Pilehvar",
            "Nigel Collier"
        ],
        "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01965",
        "title": "Text authorship identified using the dynamics of word co-occurrence networks",
        "authors": [
            "Camilo Akimushkin",
            "Diego R. Amancio",
            "Osvaldo N. Oliveira Jr"
        ],
        "abstract": "The identification of authorship in disputed documents still requires human expertise, which is now unfeasible for many tasks owing to the large volumes of text and authors in practical applications. In this study, we introduce a methodology based on the dynamics of word co-occurrence networks representing written texts to classify a corpus of 80 texts by 8 authors. The texts were divided into sections with equal number of linguistic tokens, from which time series were created for 12 topological metrics. The series were proven to be stationary (p-value>0.05), which permits to use distribution moments as learning attributes. With an optimized supervised learning procedure using a Radial Basis Function Network, 68 out of 80 texts were correctly classified, i.e. a remarkable 85% author matching success rate. Therefore, fluctuations in purely dynamic network metrics were found to characterize authorship, thus opening the way for the description of texts in terms of small evolving networks. Moreover, the approach introduced allows for comparison of texts with diverse characteristics in a simple, fast fashion.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01972",
        "title": "Bridging the Gap: Incorporating a Semantic Similarity Measure for Effectively Mapping PubMed Queries to Documents",
        "authors": [
            "Sun Kim",
            "Nicolas Fiorini",
            "W. John Wilbur",
            "Zhiyong Lu"
        ],
        "abstract": "The main approach of traditional information retrieval (IR) is to examine how many words from a query appear in a document. A drawback of this approach, however, is that it may fail to detect relevant documents where no or only few words from a query are found. The semantic analysis methods such as LSA (latent semantic analysis) and LDA (latent Dirichlet allocation) have been proposed to address the issue, but their performance is not superior compared to common IR approaches. Here we present a query-document similarity measure motivated by the Word Mover's Distance. Unlike other similarity measures, the proposed method relies on neural word embeddings to compute the distance between words. This process helps identify related words when no direct matches are found between a query and a document. Our method is efficient and straightforward to implement. The experimental results on TREC Genomics data show that our approach outperforms the BM25 ranking function by an average of 12% in mean average precision. Furthermore, for a real-world dataset collected from the PubMed search logs, we combine the semantic measure with BM25 using a learning to rank method, which leads to improved ranking scores by up to 25%. This experiment demonstrates that the proposed approach and BM25 nicely complement each other and together produce superior performance.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2017-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02025",
        "title": "Boundary-based MWE segmentation with text partitioning",
        "authors": [
            "Jake Ryland Williams"
        ],
        "abstract": "This work presents a fine-grained, text-chunking algorithm designed for the task of multiword expressions (MWEs) segmentation. As a lexical class, MWEs include a wide variety of idioms, whose automatic identification are a necessity for the handling of colloquial language. This algorithm's core novelty is its use of non-word tokens, i.e., boundaries, in a bottom-up strategy. Leveraging boundaries refines token-level information, forging high-level performance from relatively basic data. The generality of this model's feature space allows for its application across languages and domains. Experiments spanning 19 different languages exhibit a broadly-applicable, state-of-the-art model. Evaluation against recent shared-task data places text partitioning as the overall, best performing MWE segmentation algorithm, covering all MWE classes and multiple English domains (including user-generated text). This performance, coupled with a non-combinatorial, fast-running design, produces an ideal combination for implementations at scale, which are facilitated through the release of open-source software.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02076",
        "title": "Bi-directional Attention with Agreement for Dependency Parsing",
        "authors": [
            "Hao Cheng",
            "Hao Fang",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02094",
        "title": "Desiderata for Vector-Space Word Representations",
        "authors": [
            "Leon Derczynski"
        ],
        "abstract": "A plethora of vector-space representations for words is currently available, which is growing. These consist of fixed-length vectors containing real values, which represent a word. The result is a representation upon which the power of many conventional information processing and data mining techniques can be brought to bear, as long as the representations are designed with some forethought and fit certain constraints. This paper details desiderata for the design of vector space representations of words.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02097",
        "title": "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding",
        "authors": [
            "Su Zhu",
            "Kai Yu"
        ],
        "abstract": "This paper investigates the framework of encoder-decoder with attention for sequence labelling based spoken language understanding. We introduce Bidirectional Long Short Term Memory - Long Short Term Memory networks (BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep learning. In the sequence labelling task, the input and output sequences are aligned word by word, while the attention mechanism cannot provide the exact alignment. To address this limitation, we propose a novel focus mechanism for encoder-decoder framework. Experiments on the standard ATIS dataset showed that BLSTM-LSTM with focus mechanism defined the new state-of-the-art by outperforming standard BLSTM and attention based encoder-decoder. Further experiments also show that the proposed model is more robust to speech recognition errors.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02117",
        "title": "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment",
        "authors": [
            "Ivan Vuli\u0107",
            "Daniela Gerz",
            "Douwe Kiela",
            "Felix Hill",
            "Anna Korhonen"
        ],
        "abstract": "We introduce HyperLex - a dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category/class membership are computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most NLP research, and existing large-scale invetories of concept category membership (WordNet, DBPedia, etc.) treat category membership and LE as binary. To address this, we asked hundreds of native English speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and LE are indeed more gradual than binary. We then compare these human judgements with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02153",
        "title": "OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus",
        "authors": [
            "U. Springmann",
            "A. L\u00fcdeling"
        ],
        "abstract": "This article describes the results of a case study that applies Neural Network-based Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus [@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted]. Training specific OCR models was possible because the necessary *ground truth* is available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Character and word accuracies (percentage of correctly recognized items) for the resulting machine-readable texts of individual documents range from 94% to more than 99% (character level) and from 76% to 97% (word level). This includes the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Furthermore, OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets *(mixed models)* have been tested for their predictive power on the books from the other part containing yet other fonts, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books.\n    ",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02214",
        "title": "Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network",
        "authors": [
            "Keisuke Sakaguchi",
            "Kevin Duh",
            "Matt Post",
            "Benjamin Van Durme"
        ],
        "abstract": "Language processing mechanism by humans is generally more robust than computers. The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated such a robust word processing mechanism, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. On the other hand, computational models for word recognition (e.g. spelling checkers) perform poorly on data with such noise. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recurrent neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers and character-based convolutional neural network. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02689",
        "title": "Multi-task Domain Adaptation for Sequence Tagging",
        "authors": [
            "Nanyun Peng",
            "Mark Dredze"
        ],
        "abstract": "Many domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains. Traditional domain adaptation only considers adapting for one task. In this paper, we explore multi-task representation learning under the domain adaptation scenario. We propose a neural network framework that supports domain adaptation for multiple tasks simultaneously, and learns shared representations that better generalize for domain adaptation. We apply the proposed framework to domain adaptation for sequence tagging problems considering two tasks: Chinese word segmentation and named entity recognition. Experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task, and achieves the state-of-the-art results for both tasks in the social media domain.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2017-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02784",
        "title": "Canonical Correlation Inference for Mapping Abstract Scenes to Text",
        "authors": [
            "Nikos Papasarantopoulos",
            "Helen Jiang",
            "Shay B. Cohen"
        ],
        "abstract": "We describe a technique for structured prediction, based on canonical correlation analysis. Our learning algorithm finds two projections for the input and the output spaces that aim at projecting a given input and its correct output into points close to each other. We demonstrate our technique on a language-vision problem, namely the problem of giving a textual description to an \"abstract scene\".\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02926",
        "title": "The Language of Generalization",
        "authors": [
            "Michael Henry Tessler",
            "Noah D. Goodman"
        ],
        "abstract": "Language provides simple ways of communicating generalizable knowledge to each other (e.g., \"Birds fly\", \"John hikes\", \"Fire makes smoke\"). Though found in every language and emerging early in development, the language of generalization is philosophically puzzling and has resisted precise formalization. Here, we propose the first formal account of generalizations conveyed with language that makes quantitative predictions about human understanding. We test our model in three diverse domains: generalizations about categories (generic language), events (habitual language), and causes (causal language). The model explains the gradience in human endorsement through the interplay between a simple truth-conditional semantic theory and diverse beliefs about properties, formalized in a probabilistic model of language understanding. This work opens the door to understanding precisely how abstract knowledge is learned from language.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2018-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02927",
        "title": "Temporal Attention Model for Neural Machine Translation",
        "authors": [
            "Baskaran Sankaran",
            "Haitao Mi",
            "Yaser Al-Onaizan",
            "Abe Ittycheriah"
        ],
        "abstract": "Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02996",
        "title": "Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders",
        "authors": [
            "Antonio Valerio Miceli Barone"
        ],
        "abstract": "Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level. We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle, to learn compatible vector representations just by analyzing the monolingual distribution of words.\n",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03000",
        "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge",
        "authors": [
            "Nicholas Locascio",
            "Karthik Narasimhan",
            "Eduardo DeLeon",
            "Nate Kushman",
            "Regina Barzilay"
        ],
        "abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03030",
        "title": "Hierarchical Character-Word Models for Language Identification",
        "authors": [
            "Aaron Jaech",
            "George Mulcaire",
            "Shobhit Hathi",
            "Mari Ostendorf",
            "Noah A. Smith"
        ],
        "abstract": "Social media messages' brevity and unconventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong base- lines, and can also reveal code-switching.\n    ",
        "submission_date": "2016-08-10T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03065",
        "title": "An assessment of orthographic similarity measures for several African languages",
        "authors": [
            "C. Maria Keet"
        ],
        "abstract": "Natural Language Interfaces and tools such as spellcheckers and Web search in one's own language are known to be useful in ICT-mediated communication. Most languages in Southern Africa are under-resourced, however. Therefore, it would be very useful if both the generic and the few language-specific NLP tools could be reused or easily adapted across languages. This depends on the notion, and extent, of similarity between the languages. We assess this from the angle of orthography and corpora. Twelve versions of the Universal Declaration of Human Rights (UDHR) are examined, showing clusters of languages, and which are thus more or less amenable to cross-language adaptation of NLP tools, which do not match with Guthrie zones. To examine the generalisability of these results, we zoom in on isiZulu both quantitatively and qualitatively with four other corpora and texts in different genres. The results show that the UDHR is a typical text document orthographically. The results also provide insight into usability of typical measures such as lexical diversity and genre, and that the same statistic may mean different things in different documents. While NLTK for Python could be used for basic analyses of text, it, and similar NLP tools, will need considerable customization.\n    ",
        "submission_date": "2016-08-10T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03448",
        "title": "Sex, drugs, and violence",
        "authors": [
            "Stefania Raimondo",
            "Frank Rudzicz"
        ],
        "abstract": "Automatically detecting inappropriate content can be a difficult NLP task, requiring understanding context and innuendo, not just identifying specific keywords. Due to the large quantity of online user-generated content, automatic detection is becoming increasingly necessary. We take a largely unsupervised approach using a large corpus of narratives from a community-based self-publishing website and a small segment of crowd-sourced annotations. We explore topic modelling using latent Dirichlet allocation (and a variation), and use these to regress appropriateness ratings, effectively automating rating for suitability. The results suggest that certain topics inferred may be useful in detecting latent inappropriateness -- yielding recall up to 96% and low regression errors.\n    ",
        "submission_date": "2016-08-11T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03542",
        "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia",
        "authors": [
            "Daniel Hewlett",
            "Alexandre Lacoste",
            "Llion Jones",
            "Illia Polosukhin",
            "Andrew Fandrianto",
            "Jay Han",
            "Matthew Kelcey",
            "David Berthelot"
        ],
        "abstract": "We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.\n    ",
        "submission_date": "2016-08-11T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03587",
        "title": "The statistical trade-off between word order and word structure - large-scale evidence for the principle of least effort",
        "authors": [
            "Alexander Koplenig",
            "Peter Meyer",
            "Sascha Wolfer",
            "Carolin Mueller-Spitzer"
        ],
        "abstract": "Languages employ different strategies to transmit structural and grammatical information. While, for example, grammatical dependency relationships in sentences are mainly conveyed by the ordering of the words for languages like Mandarin Chinese, or Vietnamese, the word ordering is much less restricted for languages such as Inupiatun or Quechua, as those languages (also) use the internal structure of words (e.g. inflectional morphology) to mark grammatical relationships in a sentence. Based on a quantitative analysis of more than 1,500 unique translations of different books of the Bible in more than 1,100 different languages that are spoken as a native language by approximately 6 billion people (more than 80% of the world population), we present large-scale evidence for a statistical trade-off between the amount of information conveyed by the ordering of words and the amount of information conveyed by internal word structure: languages that rely more strongly on word order information tend to rely less on word structure information and vice versa. In addition, we find that - despite differences in the way information is expressed - there is also evidence for a trade-off between different books of the biblical canon that recurs with little variation across languages: the more informative the word order of the book, the less informative its word structure and vice versa. We argue that this might suggest that, on the one hand, languages encode information in very different (but efficient) ways. On the other hand, content-related and stylistic features are statistically encoded in very similar ways.\n    ",
        "submission_date": "2016-08-11T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03764",
        "title": "Extracting Biological Pathway Models From NLP Event Representations",
        "authors": [
            "Michael Spranger",
            "Sucheendra K. Palaniappan",
            "Samik Ghosh"
        ],
        "abstract": "This paper describes an an open-source software system for the automatic conversion of NLP event representations to system biology structured data interchange formats such as SBML and BioPAX. It is part of a larger effort to make results of the NLP community available for system biology pathway modelers.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03767",
        "title": "Measuring the State of the Art of Automated Pathway Curation Using Graph Algorithms - A Case Study of the mTOR Pathway",
        "authors": [
            "Michael Spranger",
            "Sucheendra K. Palaniappan",
            "Samik Ghosh"
        ],
        "abstract": "This paper evaluates the difference between human pathway curation and current NLP systems. We propose graph analysis methods for quantifying the gap between human curated pathway maps and the output of state-of-the-art automatic NLP systems. Evaluation is performed on the popular mTOR pathway. Based on analyzing where current systems perform well and where they fail, we identify possible avenues for progress.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03803",
        "title": "Redefining part-of-speech classes with distributional semantic models",
        "authors": [
            "Andrey Kutuzov",
            "Erik Velldal",
            "Lilja \u00d8vrelid"
        ],
        "abstract": "This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS affiliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech.\n",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03902",
        "title": "Rapid Classification of Crisis-Related Data on Social Networks using Convolutional Neural Networks",
        "authors": [
            "Dat Tien Nguyen",
            "Kamela Ali Al Mannai",
            "Shafiq Joty",
            "Hassan Sajjad",
            "Muhammad Imran",
            "Prasenjit Mitra"
        ],
        "abstract": "The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The Scarcity of labeled data, particularly in the early hours of a crisis, delays the machine learning process. The current state-of-the-art classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for binary and multi-class tweet classification task. We show that neural network based models do not require any feature engineering and perform better than state-of-the-art methods. In the early hours of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03938",
        "title": "Determining Health Utilities through Data Mining of Social Media",
        "authors": [
            "Christopher Thompson",
            "Josh Introne",
            "Clint Young"
        ],
        "abstract": "'Health utilities' measure patient preferences for perfect health compared to specific unhealthy states, such as asthma, a fractured hip, or colon cancer. When integrated over time, these estimations are called quality adjusted life years (QALYs). Until now, characterizing health utilities (HUs) required detailed patient interviews or written surveys. While reliable and specific, this data remained costly due to efforts to locate, enlist and coordinate participants. Thus the scope, context and temporality of diseases examined has remained limited.\n",
        "submission_date": "2016-08-13T00:00:00",
        "last_modified_date": "2016-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03995",
        "title": "An Analysis of Lemmatization on Topic Models of Morphologically Rich Language",
        "authors": [
            "Chandler May",
            "Ryan Cotterell",
            "Benjamin Van Durme"
        ],
        "abstract": "Topic models are typically represented by top-$m$ word lists for human interpretation. The corpus is often pre-processed with lemmatization (or stemming) so that those representations are not undermined by a proliferation of words with similar meanings, but there is little public work on the effects of that pre-processing. Recent work studied the effect of stemming on topic models of English texts and found no supporting evidence for the practice. We study the effect of lemmatization on topic models of Russian Wikipedia articles, finding in one configuration that it significantly improves interpretability according to a word intrusion metric. We conclude that lemmatization may benefit topic models on morphologically rich languages, but that further investigation is needed.\n    ",
        "submission_date": "2016-08-13T00:00:00",
        "last_modified_date": "2019-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04089",
        "title": "Viewpoint and Topic Modeling of Current Events",
        "authors": [
            "Kerry Zhang",
            "Jussi Karlgren",
            "Cheng Zhang",
            "Jens Lagergren"
        ],
        "abstract": "There are multiple sides to every story, and while statistical topic models have been highly successful at topically summarizing the stories in corpora of text documents, they do not explicitly address the issue of learning the different sides, the viewpoints, expressed in the documents. In this paper, we show how these viewpoints can be learned completely unsupervised and represented in a human interpretable form. We use a novel approach of applying CorrLDA2 for this purpose, which learns topic-viewpoint relations that can be used to form groups of topics, where each group represents a viewpoint. A corpus of documents about the Israeli-Palestinian conflict is then used to demonstrate how a Palestinian and an Israeli viewpoint can be learned. By leveraging the magnitudes and signs of the feature weights of a linear SVM, we introduce a principled method to evaluate associations between topics and viewpoints. With this, we demonstrate, both quantitatively and qualitatively, that the learned topic groups are contextually coherent, and form consistently correct topic-viewpoint associations.\n    ",
        "submission_date": "2016-08-14T00:00:00",
        "last_modified_date": "2016-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04147",
        "title": "Numerically Grounded Language Models for Semantic Error Correction",
        "authors": [
            "Georgios P. Spithourakis",
            "Isabelle Augenstein",
            "Sebastian Riedel"
        ],
        "abstract": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.\n    ",
        "submission_date": "2016-08-14T00:00:00",
        "last_modified_date": "2016-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04207",
        "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
        "authors": [
            "Yossi Adi",
            "Einat Kermany",
            "Yonatan Belinkov",
            "Ofer Lavi",
            "Yoav Goldberg"
        ],
        "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04434",
        "title": "Natural Language Processing using Hadoop and KOSHIK",
        "authors": [
            "Emre Erturk",
            "Hong Shi"
        ],
        "abstract": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04465",
        "title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees",
        "authors": [
            "Ehsan Shareghi",
            "Matthias Petri",
            "Gholamreza Haffari",
            "Trevor Cohn"
        ],
        "abstract": "Efficient methods for storing and querying are critical for scaling high-order n-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500x, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying).\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04485",
        "title": "Authorship clustering using multi-headed recurrent neural networks",
        "authors": [
            "Douglas Bagnall"
        ],
        "abstract": "A recurrent neural network that has been trained to separately model the language of several documents by unknown authors is used to measure similarity between the documents. It is able to find clues of common authorship even when the documents are very short and about disparate topics. While it is easy to make statistically significant predictions regarding authorship, it is difficult to group documents into definite clusters with high accuracy.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04631",
        "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study",
        "authors": [
            "Luisa Bentivogli",
            "Arianna Bisazza",
            "Mauro Cettolo",
            "Marcello Federico"
        ],
        "abstract": "Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04670",
        "title": "Attribute Extraction from Product Titles in eCommerce",
        "authors": [
            "Ajinkya More"
        ],
        "abstract": "This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute - brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes.\n    ",
        "submission_date": "2016-08-15T00:00:00",
        "last_modified_date": "2016-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04738",
        "title": "An Efficient Character-Level Neural Machine Translation",
        "authors": [
            "Shenjian Zhao",
            "Zhihua Zhang"
        ],
        "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems on the task of English-to-French translation. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose an efficient architecture to train a deep character-level neural machine translation by introducing a decimator and an interpolator. The decimator is used to sample the source sequence before encoding while the interpolator is used to resample after decoding. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is much faster and more memory-efficient in training than conventional character-based models. More interestingly, our model is able to translate the misspelled word like human beings.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04767",
        "title": "Proceedings of the LexSem+Logics Workshop 2016",
        "authors": [
            "Steven Neale",
            "Valeria de Paiva",
            "Arantxa Otegi",
            "Alexandre Rademaker"
        ],
        "abstract": "Lexical semantics continues to play an important role in driving research directions in NLP, with the recognition and understanding of context becoming increasingly important in delivering successful outcomes in NLP tasks. Besides traditional processing areas such as word sense and named entity disambiguation, the creation and maintenance of dictionaries, annotated corpora and resources have become cornerstones of lexical semantics research and produced a wealth of contextual information that NLP processes can exploit. New efforts both to link and construct from scratch such information - as Linked Open Data or by way of formal tools coming from logic, ontologies and automated reasoning - have increased the interoperability and accessibility of resources for lexical and computational semantics, even in those languages for which they have previously been limited.\n",
        "submission_date": "2016-08-14T00:00:00",
        "last_modified_date": "2016-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04917",
        "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
        "authors": [
            "Darko Cherepnalkoski",
            "Andreas Karpf",
            "Igor Mozetic",
            "Miha Grcar"
        ],
        "abstract": "We study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes. On one hand, we analyze their co-voting patterns and, on the other, their retweeting behavior. We make use of two diverse datasets in the analysis. The first one is the roll-call vote dataset, where cohesion is regarded as the tendency to co-vote within a group, and a coalition is formed when the members of several groups exhibit a high degree of co-voting agreement on a subject. The second dataset comes from Twitter; it captures the retweeting (i.e., endorsing) behavior of the MEPs and implies cohesion (retweets within the same group) and coalitions (retweets between groups) from a completely different perspective.\n",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04983",
        "title": "Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models for Reverberant Speech Recognition",
        "authors": [
            "Jeehye Lee",
            "Myungin Lee",
            "Joon-Hyuk Chang"
        ],
        "abstract": "Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks (DNNs) and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60 (RT60) in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori (MAP) probabilities, computed in an online fashion by using maximum likelihood (ML)-based blind RT60 estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05014",
        "title": "Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations",
        "authors": [
            "Vered Shwartz",
            "Ido Dagan"
        ],
        "abstract": "Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former's contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05129",
        "title": "SlangSD: Building and Using a Sentiment Dictionary of Slang Words for Short-Text Sentiment Classification",
        "authors": [
            "Liang Wu",
            "Fred Morstatter",
            "Huan Liu"
        ],
        "abstract": "Sentiment in social media is increasingly considered as an important resource for customer segmentation, market understanding, and tackling other socio-economic issues. However, sentiment in social media is difficult to measure since user-generated content is usually short and informal. Although many traditional sentiment analysis methods have been proposed, identifying slang sentiment words remains untackled. One of the reasons is that slang sentiment words are not available in existing dictionaries or sentiment lexicons. To this end, we propose to build the first sentiment dictionary of slang words to aid sentiment analysis of social media content. It is laborious and time-consuming to collect and label the sentiment polarity of a comprehensive list of slang words. We present an approach to leverage web resources to construct an extensive Slang Sentiment word Dictionary (SlangSD) that is easy to maintain and extend. SlangSD is publicly available for research purposes. We empirically show the advantages of using SlangSD, the newly-built slang sentiment word dictionary for sentiment classification, and provide examples demonstrating its ease of use with an existing sentiment system.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05243",
        "title": "Multilingual Modal Sense Classification using a Convolutional Neural Network",
        "authors": [
            "Ana Marasovi\u0107",
            "Anette Frank"
        ],
        "abstract": "Modal sense classification (MSC) is a special WSD task that depends on the meaning of the proposition in the modal's scope. We explore a CNN architecture for classifying modal sense in English and German. We show that CNNs are superior to manually designed feature-based classifiers and a standard NN classifier. We analyze the feature maps learned by the CNN and identify known and previously unattested linguistic features. We benchmark the CNN on a standard WSD task, where it compares favorably to models using sense-disambiguated target vectors.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2016-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05374",
        "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text",
        "authors": [
            "Srikanth Ronanki",
            "Siva Reddy",
            "Bajibabu Bollepalli",
            "Simon King"
        ],
        "abstract": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text. Such text is generally noisy with many variations in spelling for the same word. In this paper we evaluate three approaches to synthesize speech from such noisy ASCII text: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the ASCII text to a phonetic script, and then learn a Deep Neural Network to synthesize speech from that. We train and test our models on Blizzard Challenge datasets that were transliterated to ASCII using crowdsourcing. Our experiments on Hindi, Tamil and Telugu demonstrate that our models generate speech of competetive quality from ASCII text compared to the speech synthesized from the native scripts. All the accompanying transliterated datasets are released for public access.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2016-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05426",
        "title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments",
        "authors": [
            "Omer Levy",
            "Anders S\u00f8gaard",
            "Yoav Goldberg"
        ],
        "abstract": "While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.\n    ",
        "submission_date": "2016-08-18T00:00:00",
        "last_modified_date": "2017-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05457",
        "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset",
        "authors": [
            "Takeshi Onishi",
            "Hai Wang",
            "Mohit Bansal",
            "Kevin Gimpel",
            "David McAllester"
        ],
        "abstract": "We have constructed a new \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05528",
        "title": "Automatic Selection of Context Configurations for Improved Class-Specific Word Representations",
        "authors": [
            "Ivan Vuli\u0107",
            "Roy Schwartz",
            "Ari Rappoport",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "abstract": "This paper is concerned with identifying contexts useful for training word representation models for different word classes such as adjectives (A), verbs (V), and nouns (N). We introduce a simple yet effective framework for an automatic selection of class-specific context configurations. We construct a context configuration space based on universal dependency relations between words, and efficiently search this space with an adapted beam search algorithm. In word similarity tasks for each word class, we show that our framework is both effective and efficient. Particularly, it improves the Spearman's rho correlation with human scores on SimLex-999 over the best previously proposed class-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected context configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in a reduced training time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05554",
        "title": "Learning to Start for Sequence to Sequence Architecture",
        "authors": [
            "Qingfu Zhu",
            "Weinan Zhang",
            "Lianqiang Zhou",
            "Ting Liu"
        ],
        "abstract": "The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder's output and the last generated word. However, it faces to the cold start problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol </s>to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05604",
        "title": "Modeling Human Reading with Neural Attention",
        "authors": [
            "Michael Hahn",
            "Frank Keller"
        ],
        "abstract": "When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05605",
        "title": "Using Distributed Representations to Disambiguate Biomedical and Clinical Concepts",
        "authors": [
            "St\u00e9phan Tulkens",
            "Simon \u0160uster",
            "Walter Daelemans"
        ],
        "abstract": "In this paper, we report a knowledge-based method for Word Sense Disambiguation in the domains of biomedical and clinical text. We combine word representations created on large corpora with a small number of definitions from the UMLS to create concept representations, which we then compare to representations of the context of ambiguous terms. Using no relational information, we obtain comparable performance to previous approaches on the MSH-WSD dataset, which is a well-known dataset in the biomedical domain. Additionally, our method is fast and easy to set up and extend to other domains. Supplementary materials, including source code, can be found at https: //github.com/clips/yarn\n    ",
        "submission_date": "2016-08-19T00:00:00",
        "last_modified_date": "2016-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05777",
        "title": "Topic Sensitive Neural Headline Generation",
        "authors": [
            "Lei Xu",
            "Ziyun Wang",
            "Ayana",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Neural models have recently been used in text summarization including headline generation. The model can be trained using a set of document-headline pairs. However, the model does not explicitly consider topical similarities and differences of documents. We suggest to categorizing documents into various topics so that documents within the same topic are similar in content and share similar summarization patterns. Taking advantage of topic information of documents, we propose topic sensitive neural headline generation model. Our model can generate more accurate summaries guided by document topics. We test our model on LCSTS dataset, and experiments show that our method outperforms other baselines on each topic and achieves the state-of-art performance.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05813",
        "title": "phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning",
        "authors": [
            "Ying Hua Tan",
            "Chee Seng Chan"
        ],
        "abstract": "A picture is worth a thousand words. Not until recently, however, we noticed some success stories in understanding of visual scenes: a model that is able to detect/name objects, describe their attributes, and recognize their relationships/interactions. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. The proposed model encodes sentence as a sequence of combination of phrases and words, instead of a sequence of words alone as in those conventional solutions. The two levels of this model are dedicated to i) learn to generate image relevant noun phrases, and ii) produce appropriate image description from the phrases and other words in the corpus. Adopting a convolutional neural network to learn image features and the LSTM to learn the word sequence in a sentence, the proposed model has shown better or competitive results in comparison to the state-of-the-art models on Flickr8k and Flickr30k datasets.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2017-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05852",
        "title": "Learning Word Embeddings from Intrinsic and Extrinsic Views",
        "authors": [
            "Jifan Chen",
            "Kan Chen",
            "Xipeng Qiu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Zheng Zhang"
        ],
        "abstract": "While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link prediction, and document classification. Experiment results show that our model is powerful in both word and document modeling.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.05859",
        "title": "Using the Output Embedding to Improve Language Models",
        "authors": [
            "Ofir Press",
            "Lior Wolf"
        ],
        "abstract": "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.\n    ",
        "submission_date": "2016-08-20T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06043",
        "title": "Context Gates for Neural Machine Translation",
        "authors": [
            "Zhaopeng Tu",
            "Yang Liu",
            "Zhengdong Lu",
            "Xiaohua Liu",
            "Hang Li"
        ],
        "abstract": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.\n    ",
        "submission_date": "2016-08-22T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06111",
        "title": "An Incremental Parser for Abstract Meaning Representation",
        "authors": [
            "Marco Damonte",
            "Shay B. Cohen",
            "Giorgio Satta"
        ],
        "abstract": "Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.\n    ",
        "submission_date": "2016-08-22T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06134",
        "title": "Median-Based Generation of Synthetic Speech Durations using a Non-Parametric Approach",
        "authors": [
            "Srikanth Ronanki",
            "Oliver Watts",
            "Simon King",
            "Gustav Eje Henter"
        ],
        "abstract": "This paper proposes a new approach to duration modelling for statistical parametric speech synthesis in which a recurrent statistical model is trained to output a phone transition probability at each timestep (acoustic frame). Unlike conventional approaches to duration modelling -- which assume that duration distributions have a particular form (e.g., a Gaussian) and use the mean of that distribution for synthesis -- our approach can in principle model any distribution supported on the non-negative integers. Generation from this model can be performed in many ways; here we consider output generation based on the median predicted duration. The median is more typical (more probable) than the conventional mean duration, is robust to training-data irregularities, and enables incremental generation. Furthermore, a frame-level approach to duration prediction is consistent with a longer-term goal of modelling durations and acoustic features together. Results indicate that the proposed method is competitive with baseline approaches in approximating the median duration of held-out natural speech.\n    ",
        "submission_date": "2016-08-22T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06378",
        "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine",
        "authors": [
            "Bo-Hsiang Tseng",
            "Sheng-Syun Shen",
            "Hung-Yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06386",
        "title": "Which techniques does your application use?: An information extraction framework for scientific articles",
        "authors": [
            "Soham Dan",
            "Sanyam Agarwal",
            "Mayank Singh",
            "Pawan Goyal",
            "Animesh Mukherjee"
        ],
        "abstract": "Every field of research consists of multiple application areas with various techniques routinely used to solve problems in these wide range of application areas. With the exponential growth in research volumes, it has become difficult to keep track of the ever-growing number of application areas as well as the corresponding problem solving techniques. In this paper, we consider the computational linguistics domain and present a novel information extraction system that automatically constructs a pool of all application areas in this domain and appropriately links them with corresponding problem solving techniques. Further, we categorize individual research articles based on their application area and the techniques proposed/used in the article. k-gram based discounting method along with handwritten rules and bootstrapped pattern learning is employed to extract application areas. Subsequently, a language modeling approach is proposed to characterize each article based on its application area. Similarly, regular expressions and high-scoring noun phrases are used for the extraction of the problem solving techniques. We propose a greedy approach to characterize each article based on the techniques. Towards the end, we present a table representing the most frequent techniques adopted for a particular application area. Finally, we propose three use cases presenting an extensive temporal analysis of the usage of techniques and application areas.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06459",
        "title": "Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads",
        "authors": [
            "Henrik Hermansson",
            "James P. Cross"
        ],
        "abstract": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the prob- lem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and trans- positions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing stud- ies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06697",
        "title": "Semantic descriptions of 24 evaluational adjectives, for application in sentiment analysis",
        "authors": [
            "Cliff Goddard",
            "Maite Taboada",
            "Radoslava Trnavac"
        ],
        "abstract": "We apply the Natural Semantic Metalanguage (NSM) approach (Goddard and Wierzbicka 2014) to the lexical-semantic analysis of English evaluational adjectives and compare the results with the picture developed in the Appraisal Framework (Martin and White 2005). The analysis is corpus-assisted, with examples mainly drawn from film and book reviews, and supported by collocational and statistical information from WordBanks Online. We propose NSM explications for 24 evaluational adjectives, arguing that they fall into five groups, each of which corresponds to a distinct semantic template. The groups can be sketched as follows: \"First-person thought-plus-affect\", e.g. wonderful; \"Experiential\", e.g. entertaining; \"Experiential with bodily reaction\", e.g. gripping; \"Lasting impact\", e.g. memorable; \"Cognitive evaluation\", e.g. complex, excellent. These groupings and semantic templates are compared with the classifications in the Appraisal Framework's system of Appreciation. In addition, we are particularly interested in sentiment analysis, the automatic identification of evaluation and subjectivity in text. We discuss the relevance of the two frameworks for sentiment analysis and other language technology applications.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06718",
        "title": "A Large-Scale Multilingual Disambiguation of Glosses",
        "authors": [
            "Jos\u00e9 Camacho Collados",
            "Claudio Delli Bovi",
            "Alessandro Raganato",
            "Roberto Navigli"
        ],
        "abstract": "Linking concepts and named entities to knowledge bases has become a crucial Natural Language Understanding task. In this respect, recent works have shown the key advantage of exploiting textual definitions in various Natural Language Processing applications. However, to date there are no reliable large-scale corpora of sense-annotated textual definitions available to the research community. In this paper we present a large-scale high-quality corpus of disambiguated glosses in multiple languages, comprising sense annotations of both concepts and named entities from a unified sense inventory. Our approach for the construction and disambiguation of the corpus builds upon the structure of a large multilingual semantic network and a state-of-the-art disambiguation system; first, we gather complementary information of equivalent definitions across different languages to provide context for disambiguation, and then we combine it with a semantic similarity-based refinement. As a result we obtain a multilingual corpus of textual definitions featuring over 38 million definitions in 263 languages, and we make it freely available at ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06757",
        "title": "Robust Named Entity Recognition in Idiosyncratic Domains",
        "authors": [
            "Sebastian Arnold",
            "Felix A. Gers",
            "Torsten Kilias",
            "Alexander L\u00f6ser"
        ],
        "abstract": "Named entity recognition often fails in idiosyncratic domains. That causes a problem for depending tasks, such as entity linking and relation extraction. We propose a generic and robust approach for high-recall named entity recognition. Our approach is easy to train and offers strong generalization over diverse domain-specific language, such as news documents (e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM networks. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We report from our results F1 scores in the range of 84-94% on standard datasets.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06794",
        "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition",
        "authors": [
            "Thomas Kober",
            "Julie Weeds",
            "Jeremy Reffin",
            "David Weir"
        ],
        "abstract": "Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable.\n    ",
        "submission_date": "2016-08-24T00:00:00",
        "last_modified_date": "2016-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07076",
        "title": "A Context-aware Natural Language Generator for Dialogue Systems",
        "authors": [
            "Ond\u0159ej Du\u0161ek",
            "Filip Jur\u010d\u00ed\u010dek"
        ],
        "abstract": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07115",
        "title": "Aligning Packed Dependency Trees: a theory of composition for distributional semantics",
        "authors": [
            "David Weir",
            "Julie Weeds",
            "Jeremy Reffin",
            "Thomas Kober"
        ],
        "abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07720",
        "title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features",
        "authors": [
            "Fei Li",
            "Meishan Zhang",
            "Guohong Fu",
            "Tao Qian",
            "Donghong Ji"
        ],
        "abstract": "Relation classification is associated with many potential applications in the artificial intelligence area. Recent approaches usually leverage neural networks based on structure features such as syntactic or dependency features to solve this problem. However, high-cost structure features make such approaches inconvenient to be directly used. In addition, structure features are probably domain-dependent. Therefore, this paper proposes a bi-directional long-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on low-cost sequence features to address relation classification. This model divides a sentence or text segment into five parts, namely two target entities and their three contexts. It learns the representations of entities and their contexts, and uses them to classify relations. We evaluate our model on two standard benchmark datasets in different domains, namely SemEval-2010 Task 8 and BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves comparable performance compared with other models using sequence features. In the latter dataset, our model obtains the third best results compared with other models in the official evaluation. Moreover, we find that the context between two target entities plays the most important role in relation classification. Furthermore, statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest dependency path when dependency parsing is not used.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2016-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07738",
        "title": "Testing APSyn against Vector Cosine on Similarity Estimation",
        "authors": [
            "Enrico Santus",
            "Emmanuele Chersoni",
            "Alessandro Lenci",
            "Chu-Ren Huang",
            "Philippe Blache"
        ],
        "abstract": "In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07775",
        "title": "Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content",
        "authors": [
            "Wei Fang",
            "Jui-Yang Hsu",
            "Hung-yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.\n    ",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2017-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07836",
        "title": "What to do about non-standard (or non-canonical) language in NLP",
        "authors": [
            "Barbara Plank"
        ],
        "abstract": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language.\n",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07852",
        "title": "Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties: Using Changing Colors and Innovative Terms as Examples",
        "authors": [
            "Chao-Lin Liu"
        ],
        "abstract": "Tang (618-907 AD) and Song (960-1279) dynasties are two very important periods in the development of Chinese literary. The most influential forms of the poetry in Tang and Song were Shi and Ci, respectively. Tang Shi and Song Ci established crucial foundations of the Chinese literature, and their influences in both literary works and daily lives of the Chinese communities last until today.\n",
        "submission_date": "2016-08-28T00:00:00",
        "last_modified_date": "2016-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07905",
        "title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08339",
        "title": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence",
        "authors": [
            "Taehwan Kim"
        ],
        "abstract": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08515",
        "title": "Language Detection For Short Text Messages In Social Media",
        "authors": [
            "Ivana Balazevic",
            "Mikio Braun",
            "Klaus-Robert M\u00fcller"
        ],
        "abstract": "With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given.\n    ",
        "submission_date": "2016-08-30T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08738",
        "title": "A Dictionary-based Approach to Racism Detection in Dutch Social Media",
        "authors": [
            "St\u00e9phan Tulkens",
            "Lisa Hilte",
            "Elise Lodewyckx",
            "Ben Verhoeven",
            "Walter Daelemans"
        ],
        "abstract": "We present a dictionary-based approach to racism detection in Dutch social media comments, which were retrieved from two public Belgian social media sites likely to attract racist reactions. These comments were labeled as racist or non-racist by multiple annotators. For our approach, three discourse dictionaries were created: first, we created a dictionary by retrieving possibly racist and more neutral terms from the training data, and then augmenting these with more general words to remove some bias. A second dictionary was created through automatic expansion using a \\texttt{word2vec} model trained on a large corpus of general Dutch text. Finally, a third dictionary was created by manually filtering out incorrect expansions. We trained multiple Support Vector Machines, using the distribution of words over the different categories in the dictionaries as features. The best-performing model used the manually cleaned dictionary and obtained an F-score of 0.46 for the racist class on a test set consisting of unseen Dutch comments, retrieved from the same sites used for the training set. The automated expansion of the dictionary only slightly boosted the model's performance, and this increase in performance was not statistically significant. The fact that the coverage of the expanded dictionaries did increase indicates that the words that were automatically added did occur in the corpus, but were not able to meaningfully impact performance. The dictionaries, code, and the procedure for requesting the corpus are available at: ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08868",
        "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
        "authors": [
            "Su Lin Blodgett",
            "Lisa Green",
            "Brendan O'Connor"
        ],
        "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08927",
        "title": "The Generalized Smallest Grammar Problem",
        "authors": [
            "Payam Siyari",
            "Matthias Gall\u00e9"
        ],
        "abstract": "The Smallest Grammar Problem -- the problem of finding the smallest context-free grammar that generates exactly one given sequence -- has never been successfully applied to grammatical inference. We investigate the reasons and propose an extended formulation that seeks to minimize non-recursive grammars, instead of straight-line programs. In addition, we provide very efficient algorithms that approximate the minimization problem of this class of grammars. Our empirical evaluation shows that we are able to find smaller models than the current best approximations to the Smallest Grammar Problem on standard benchmarks, and that the inferred rules capture much better the syntactic structure of natural language.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08940",
        "title": "Hash2Vec, Feature Hashing for Word Embeddings",
        "authors": [
            "Luis Argerich",
            "Joaqu\u00edn Torr\u00e9 Zaffaroni",
            "Mat\u00edas J Cano"
        ],
        "abstract": "In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm, that does not need training, is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00070",
        "title": "How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions",
        "authors": [
            "Arun Tejasvi Chaganty",
            "Percy Liang"
        ],
        "abstract": "How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. \"$131 million is about the cost to employ everyone in Texas over a lunch period\". First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00081",
        "title": "All Fingers are not Equal: Intensity of References in Scientific Articles",
        "authors": [
            "Tanmoy Chakraborty",
            "Ramasuri Narayanam"
        ],
        "abstract": "Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semi-supervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00425",
        "title": "Identifying Dogmatism in Social Media: Signals and Models",
        "authors": [
            "Ethan Fast",
            "Eric Horvitz"
        ],
        "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.\n    ",
        "submission_date": "2016-09-01T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00435",
        "title": "Citation Classification for Behavioral Analysis of a Scientific Field",
        "authors": [
            "David Jurgens",
            "Srijan Kumar",
            "Raine Hoover",
            "Dan McFarland",
            "Dan Jurafsky"
        ],
        "abstract": "  Citations are an important indicator of the state of a scientific field, reflecting how authors frame their work, and influencing uptake by future scholars. However, our understanding of citation behavior has been limited to small-scale manual citation analysis. We perform the largest behavioral study of citations to date, analyzing how citations are both framed and taken up by scholars in one entire field: natural language processing. We introduce a new dataset of nearly 2,000 citations annotated for function and centrality, and use it to develop a state-of-the-art classifier and label the entire ACL Reference Corpus. We then study how citations are framed by authors and use both papers and online traces to track how citations are followed by readers. We demonstrate that authors are sensitive to discourse structure and publication venue when citing, that online readers follow temporal links to previous and future work rather than methodological links, and that how a paper cites related work is predictive of its citation count. Finally, we use changes in citation roles to show that the field of NLP is undergoing a significant increase in consensus.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00559",
        "title": "Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors",
        "authors": [
            "Bridget T. McInnes",
            "Ted Pedersen"
        ],
        "abstract": "Vector space methods that measure semantic similarity and relatedness often rely on distributional information such as co--occurrence frequencies or statistical measures of association to weight the importance of particular co--occurrences. In this paper, we extend these methods by incorporating a measure of semantic similarity based on a human curated taxonomy into a second--order vector representation. This results in a measure of semantic relatedness that combines both the contextual information available in a corpus--based vector space representation with the semantic knowledge found in a biomedical ontology. Our results show that incorporating semantic similarity into a second order co--occurrence matrices improves correlation with human judgments for both similarity and relatedness, and that our method compares favorably to various different word embedding methods that have recently been evaluated on the same reference standards we have used.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2017-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00565",
        "title": "Skipping Word: A Character-Sequential Representation based Framework for Question Answering",
        "authors": [
            "Lingxun Meng",
            "Yan Li",
            "Mengyi Liu",
            "Peng Shu"
        ],
        "abstract": "Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00626",
        "title": "SynsetRank: Degree-adjusted Random Walk for Relation Identification",
        "authors": [
            "Shinichi Nakajima",
            "Sebastian Krause",
            "Dirk Weissenborn",
            "Sven Schmeier",
            "Nico Goernitz",
            "Feiyu Xu"
        ],
        "abstract": "In relation extraction, a key process is to obtain good detectors that find relevant sentences describing the target relation. To minimize the necessity of labeled data for refining detectors, previous work successfully made use of BabelNet, a semantic graph structure expressing relationships between synsets, as side information or prior knowledge. The goal of this paper is to enhance the use of graph structure in the framework of random walk with a few adjustable parameters. Actually, a straightforward application of random walk degrades the performance even after parameter optimization. With the insight from this unsuccessful trial, we propose SynsetRank, which adjusts the initial probability so that high degree nodes influence the neighbors as strong as low degree nodes. In our experiment on 13 relations in the FB15K-237 dataset, SynsetRank significantly outperforms baselines and the plain random walk approach.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00718",
        "title": "Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level",
        "authors": [
            "Rie Johnson",
            "Tong Zhang"
        ],
        "abstract": "This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015), on the eight datasets with relatively large training data that were used for testing the very deep character-level CNN in Conneau et al. (2016). Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in Conneau et al., though the results should be interpreted with some consideration due to the unique pre-processing of Conneau et al. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00777",
        "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access",
        "authors": [
            "Bhuwan Dhingra",
            "Lihong Li",
            "Xiujun Li",
            "Jianfeng Gao",
            "Yun-Nung Chen",
            "Faisal Ahmed",
            "Li Deng"
        ],
        "abstract": "This paper proposes KB-InfoBot -- a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced \"soft\" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. The source code is available at ",
        "submission_date": "2016-09-03T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01188",
        "title": "Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic Statistical Machine Translation",
        "authors": [
            "Fahad Al-Obaidli",
            "Stephen Cox",
            "Preslav Nakov"
        ],
        "abstract": "We describe efforts towards getting better resources for English-Arabic machine translation of spoken text. In particular, we look at movie subtitles as a unique, rich resource, as subtitles in one language often get translated into other languages. Movie subtitles are not new as a resource and have been explored in previous research; however, here we create a much larger bi-text (the biggest to date), and we further generate better quality alignment for it. Given the subtitles for the same movie in different languages, a key problem is how to align them at the fragment level. Typically, this is done using length-based alignment, but for movie subtitles, there is also time information. Here we exploit this information to develop an original algorithm that outperforms the current best subtitle alignment tool, subalign. The evaluation results show that adding our bi-text to the IWSLT training bi-text yields an improvement of over two BLEU points absolute.\n    ",
        "submission_date": "2016-09-05T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01235",
        "title": "PMI Matrix Approximations with Applications to Neural Language Modeling",
        "authors": [
            "Oren Melamud",
            "Ido Dagan",
            "Jacob Goldberger"
        ],
        "abstract": "The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE.\n    ",
        "submission_date": "2016-09-05T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01454",
        "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01462",
        "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks",
        "authors": [
            "Bing Liu",
            "Ian Lane"
        ],
        "abstract": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01574",
        "title": "Automatically extracting, ranking and visually summarizing the treatments for a disease",
        "authors": [
            "Prakash Reddy Putta",
            "John J. Dzak III",
            "Siddhartha R. Jonnalagadda"
        ],
        "abstract": "Clinicians are expected to have up-to-date and broad knowledge of disease treatment options for a patient. Online health knowledge resources contain a wealth of information. However, because of the time investment needed to disseminate and rank pertinent information, there is a need to summarize the information in a more concise format. Our aim of the study is to provide clinicians with a concise overview of popular treatments for a given disease using information automatically computed from Medline abstracts. We analyzed the treatments of two disorders - Atrial Fibrillation and Congestive Heart Failure. We calculated the precision, recall, and f-scores of our two ranking methods to measure the accuracy of the results. For Atrial Fibrillation disorder, maximum f-score for the New Treatments weighing method is 0.611, which occurs at 60 treatments. For Congestive Heart Failure disorder, maximum f-score for the New Treatments weighing method is 0.503, which occurs at 80 treatments.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01580",
        "title": "Using Natural Language Processing to Screen Patients with Active Heart Failure: An Exploration for Hospital-wide Surveillance",
        "authors": [
            "Shu Dong",
            "R Kannan Mutharasan",
            "Siddhartha Jonnalagadda"
        ],
        "abstract": "In this paper, we proposed two different approaches, a rule-based approach and a machine-learning based approach, to identify active heart failure cases automatically by analyzing electronic health records (EHR). For the rule-based approach, we extracted cardiovascular data elements from clinical notes and matched patients to different colors according their heart failure condition by using rules provided by experts in heart failure. It achieved 69.4% accuracy and 0.729 F1-Score. For the machine learning approach, with bigram of clinical notes as features, we tried four different models while SVM with linear kernel achieved the best performance with 87.5% accuracy and 0.86 F1-Score. Also, from the classification comparison between the four different models, we believe that linear models fit better for this problem. Once we combine the machine-learning and rule-based algorithms, we will enable hospital-wide surveillance of active heart failure through increased accuracy and interpretability of the outputs.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01592",
        "title": "CRTS: A type system for representing clinical recommendations",
        "authors": [
            "Ravi P Garg",
            "Kalpana Raja",
            "Siddhartha R Jonnalagadda"
        ],
        "abstract": "Background: Clinical guidelines and recommendations are the driving wheels of the evidence-based medicine (EBM) paradigm, but these are available primarily as unstructured text and are generally highly heterogeneous in nature. This significantly reduces the dissemination and automatic application of these recommendations at the point of care. A comprehensive structured representation of these recommendations is highly beneficial in this regard. Objective: The objective of this paper to present Clinical Recommendation Type System (CRTS), a common type system that can effectively represent a clinical recommendation in a structured form. Methods: CRTS is built by analyzing 125 recommendations and 195 research articles corresponding to 6 different diseases available from UpToDate, a publicly available clinical knowledge system, and from the National Guideline Clearinghouse, a public resource for evidence-based clinical practice guidelines. Results: We show that CRTS not only covers the recommendations but also is flexible to be extended to represent information from primary literature. We also describe how our developed type system can be applied for clinical decision support, medical knowledge summarization, and citation retrieval. Conclusion: We showed that our proposed type system is precise and comprehensive in representing a large sample of recommendations available for various disorders. CRTS can now be used to build interoperable information extraction systems that automatically extract clinical recommendations and related data elements from clinical evidence resources, guidelines, systematic reviews and primary publications.\n",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01594",
        "title": "An Information Extraction Approach to Prescreen Heart Failure Patients for Clinical Trials",
        "authors": [
            "Abhishek Kalyan Adupa",
            "Ravi Prakash Garg",
            "Jessica Corona-Cox",
            "Sanjiv. J. Shah",
            "Siddhartha R. Jonnalagadda"
        ],
        "abstract": "To reduce the large amount of time spent screening, identifying, and recruiting patients into clinical trials, we need prescreening systems that are able to automate the data extraction and decision-making tasks that are typically relegated to clinical research study coordinators. However, a major obstacle is the vast amount of patient data available as unstructured free-form text in electronic health records. Here we propose an information extraction-based approach that first automatically converts unstructured text into a structured form. The structured data are then compared against a list of eligibility criteria using a rule-based system to determine which patients qualify for enrollment in a heart failure clinical trial. We show that we can achieve highly accurate results, with recall and precision values of 0.95 and 0.86, respectively. Our system allowed us to significantly reduce the time needed for prescreening patients from a few weeks to a few minutes. Our open-source information extraction modules are available for researchers and could be tested and validated in other cardiovascular trials. An approach such as the one we demonstrate here may decrease costs and expedite clinical trials, and could enhance the reproducibility of trials across institutions and populations.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01597",
        "title": "A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical Knowledge Summarization: Combining Concept Extraction, Vector Similarity and Query Expansion for High Precision",
        "authors": [
            "Kalpana Raja",
            "Andrew J Sauer",
            "Ravi P Garg",
            "Melanie R Klerer",
            "Siddhartha R Jonnalagadda"
        ],
        "abstract": "Novel information retrieval methods to identify citations relevant to a clinical topic can overcome the knowledge gap existing between the primary literature (MEDLINE) and online clinical knowledge resources such as UpToDate. Searching the MEDLINE database directly or with query expansion methods returns a large number of citations that are not relevant to the query. The current study presents a citation retrieval system that retrieves citations for evidence-based clinical knowledge summarization. This approach combines query expansion, concept-based screening algorithm, and concept-based vector similarity. We also propose an information extraction framework for automated concept (Population, Intervention, Comparison, and Disease) extraction. We evaluated our proposed system on all topics (as queries) available from UpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib). The system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib topics on a gold standard of citations available in UpToDate. This is significantly high when compared to a query-expansion based baseline (F-score of 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with disease hyponyms and journal names, concept-based screening, and term-based vector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating the system with top K relevant citations, where K is the number of citations in the gold standard achieved a much higher overall F-score of 69.9% on HF topics and 75.1% on AFib topics. In addition, the system retrieved up to 18 new relevant citations per topic when tested on ten HF and six AFib clinical topics.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01933",
        "title": "Sentiment Classification of Food Reviews",
        "authors": [
            "Hua Feng",
            "Ruixi Lin"
        ],
        "abstract": "Sentiment analysis of reviews is a popular task in natural language processing. In this work, the goal is to predict the score of food reviews on a scale of 1 to 5 with two recurrent neural networks that are carefully tuned. As for baseline, we train a simple RNN for classification. Then we extend the baseline to GRU. In addition, we present two different methods to deal with highly skewed data, which is a common problem for reviews. Models are evaluated using accuracies.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01962",
        "title": "Using Gaussian Processes for Rumour Stance Classification in Social Media",
        "authors": [
            "Michal Lukasik",
            "Kalina Bontcheva",
            "Trevor Cohn",
            "Arkaitz Zubiaga",
            "Maria Liakata",
            "Rob Procter"
        ],
        "abstract": "Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting, denying or questioning the rumour. Using a classifier based on Gaussian Processes, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of Twitter and professional news practitioners when a rumour is being rebutted.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02075",
        "title": "The Social Dynamics of Language Change in Online Networks",
        "authors": [
            "Rahul Goel",
            "Sandeep Soni",
            "Naman Goyal",
            "John Paparrizos",
            "Hanna Wallach",
            "Fernando Diaz",
            "Jacob Eisenstein"
        ],
        "abstract": "Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence. But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers. In this paper, we use a data set of several million Twitter users to track language changes in progress. First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and \"netspeak\" abbreviations (e.g., lol), but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02549",
        "title": "Learning Lexical Entries for Robotic Commands using Crowdsourcing",
        "authors": [
            "Junjie Hu",
            "Jean Oh",
            "Anatole Gershman"
        ],
        "abstract": "Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.\n    ",
        "submission_date": "2016-09-08T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02727",
        "title": "Detecting Singleton Review Spammers Using Semantic Similarity",
        "authors": [
            "Vlad Sandulescu",
            "Martin Ester"
        ],
        "abstract": "Online reviews have increasingly become a very important resource for consumers when making purchases. Though it is becoming more and more difficult for people to make well-informed buying decisions without being deceived by fake reviews. Prior works on the opinion spam problem mostly considered classifying fake reviews using behavioral user patterns. They focused on prolific users who write more than a couple of reviews, discarding one-time reviewers. The number of singleton reviewers however is expected to be high for many review websites. While behavioral patterns are effective when dealing with elite users, for one-time reviewers, the review text needs to be exploited. In this paper we tackle the problem of detecting fake reviews written by the same person using multiple names, posting each review under a different name. We propose two methods to detect similar reviews and show the results generally outperform the vectorial similarity measures used in prior works. The first method extends the semantic similarity between words to the reviews level. The second method is based on topic modeling and exploits the similarity of the reviews topic distributions using two models: bag-of-words and bag-of-opinion-phrases. The experiments were conducted on reviews from three different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset (800 reviews).\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02745",
        "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02746",
        "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02748",
        "title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "This paper describes our deep learning-based approach to multilingual aspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a convolutional neural network (CNN) for both aspect extraction and aspect-based sentiment analysis. We cast aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. To determine the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Our constrained system (unconstrained for English) achieves competitive results across all languages and domains, placing first or second in 5 and 7 out of 11 language-domain pairs for aspect category detection (slot 1) and sentiment polarity (slot 3) respectively, thereby demonstrating the viability of a deep learning-based approach for multilingual aspect-based sentiment analysis.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02809",
        "title": "Harassment detection: a benchmark on the #HackHarassment dataset",
        "authors": [
            "Alexei Bastidas",
            "Edward Dixon",
            "Chris Loo",
            "John Ryan"
        ],
        "abstract": "Online harassment has been a problem to a greater or lesser extent since the early days of the internet. Previous work has applied anti-spam techniques like machine-learning based text classification (Reynolds, 2011) to detecting harassing messages. However, existing public datasets are limited in size, with labels of varying quality. The #HackHarassment initiative (an alliance of 1 tech companies and NGOs devoted to fighting bullying on the internet) has begun to address this issue by creating a new dataset superior to its predecssors in terms of both size and quality. As we (#HackHarassment) complete further rounds of labelling, later iterations of this dataset will increase the available samples by at least an order of magnitude, enabling corresponding improvements in the quality of machine learning models for harassment detection. In this paper, we introduce the first models built on the #HackHarassment dataset v1.0 (a new open dataset, which we are delighted to share with any interested researcherss) as a benchmark for future research.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02846",
        "title": "Dialogue manager domain adaptation using Gaussian process reinforcement learning",
        "authors": [
            "Milica Gasic",
            "Nikola Mrksic",
            "Lina M. Rojas-Barahona",
            "Pei-Hao Su",
            "Stefan Ultes",
            "David Vandyke",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or outperform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02960",
        "title": "A Large Scale Corpus of Gulf Arabic",
        "authors": [
            "Salam Khalifa",
            "Nizar Habash",
            "Dana Abdulrahim",
            "Sara Hassan"
        ],
        "abstract": "Most Arabic natural language processing tools and resources are developed to serve Modern Standard Arabic (MSA), which is the official written language in the Arab World. Some Dialectal Arabic varieties, notably Egyptian Arabic, have received some attention lately and have a growing collection of resources that include annotated corpora and morphological analyzers and taggers. Gulf Arabic, however, lags behind in that respect. In this paper, we present the Gumar Corpus, a large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels. We annotate the corpus for sub-dialect information at the document level. We also present results of a preliminary study in the morphological annotation of Gulf Arabic which includes developing guidelines for a conventional orthography. The text of the corpus is publicly browsable through a web interface we developed for it.\n    ",
        "submission_date": "2016-09-09T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03148",
        "title": "Divide and...conquer? On the limits of algorithmic approaches to syntactic semantic structure",
        "authors": [
            "Diego Gabriel Krivochen"
        ],
        "abstract": "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A D&C algorithm works by recursively and monotonically breaking down a problem into sub problems of the same (or a related) type, until these become simple enough to be solved directly. The solutions to the sub problems are then combined to give a solution to the original problem. The present work identifies D&C algorithms assumed within contemporary syntactic theory, and discusses the limits of their applicability in the realms of the syntax semantics and syntax morphophonology interfaces. We will propose that D&C algorithms, while valid for some processes, fall short on flexibility given a mixed approach to the structure of linguistic phrase markers. Arguments in favour of a computationally mixed approach to linguistic structure will be presented as an alternative that offers advantages to uniform D&C approaches.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03204",
        "title": "On the Similarities Between Native, Non-native and Translated Texts",
        "authors": [
            "Ella Rabinovich",
            "Sergiu Nisioi",
            "Noam Ordan",
            "Shuly Wintner"
        ],
        "abstract": "We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language. Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) non-native language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reflecting, perhaps, unified principles that similarly affect translations and non-native language.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03205",
        "title": "Unsupervised Identification of Translationese",
        "authors": [
            "Ella Rabinovich",
            "Shuly Wintner"
        ],
        "abstract": "Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03357",
        "title": "Modelling Creativity: Identifying Key Components through a Corpus-Based Approach",
        "authors": [
            "Anna Jordanous",
            "Bill Keller"
        ],
        "abstract": "Creativity is a complex, multi-faceted concept encompassing a variety of related aspects, abilities, properties and behaviours. If we wish to study creativity scientifically, then a tractable and well-articulated model of creativity is required. Such a model would be of great value to researchers investigating the nature of creativity and in particular, those concerned with the evaluation of creative practice. This paper describes a unique approach to developing a suitable model of how creative behaviour emerges that is based on the words people use to describe the concept. Using techniques from the field of statistical natural language processing, we identify a collection of fourteen key components of creativity through an analysis of a corpus of academic papers on the topic. Words are identified which appear significantly often in connection with discussions of the concept. Using a measure of lexical similarity to help cluster these words, a number of distinct themes emerge, which collectively contribute to a comprehensive and multi-perspective model of creativity. The components provide an ontology of creativity: a set of building blocks which can be used to model creative practice in a variety of domains. The components have been employed in two case studies to evaluate the creativity of computational systems and have proven useful in articulating achievements of this work and directions for further research.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03376",
        "title": "Morphological Constraints for Phrase Pivot Statistical Machine Translation",
        "authors": [
            "Ahmed El Kholy",
            "Nizar Habash"
        ],
        "abstract": "The lack of parallel data for many language pairs is an important challenge to statistical machine translation (SMT). One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations especially when a poor morphology language is used as the pivot between rich morphology languages. In this paper, we examine the use of synchronous morphology constraint features to improve the quality of phrase pivot SMT. We compare hand-crafted constraints to those learned from limited parallel data between source and target languages. The learned morphology constraints are based on projected align- ments between the source and target phrases in the pivot phrase table. We show positive results on Hebrew-Arabic SMT (pivoting on English). We get 1.5 BLEU points over a phrase pivot baseline and 0.8 BLEU points over a system combination baseline with a direct model built from parallel data.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03441",
        "title": "Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing",
        "authors": [
            "Jan Chorowski",
            "Micha\u0142 Zapotoczny",
            "Pawe\u0142 Rychlikowski"
        ],
        "abstract": "We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn't require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transition- based system trained on perfect POS tags.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03528",
        "title": "The Microsoft 2016 Conversational Speech Recognition System",
        "authors": [
            "W. Xiong",
            "J. Droppo",
            "X. Huang",
            "F. Seide",
            "M. Seltzer",
            "A. Stolcke",
            "D. Yu",
            "G. Zweig"
        ],
        "abstract": "We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.2%, representing an improvement over previously reported results on this benchmark task.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2017-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03632",
        "title": "Joint Extraction of Events and Entities within a Document Context",
        "authors": [
            "Bishan Yang",
            "Tom Mitchell"
        ],
        "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03663",
        "title": "An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification",
        "authors": [
            "Tong Wang",
            "Ping Chen",
            "Kevin Amaral",
            "Jipeng Qiang"
        ],
        "abstract": "Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS.\n    ",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03976",
        "title": "Multimodal Attention for Neural Machine Translation",
        "authors": [
            "Ozan Caglayan",
            "Lo\u00efc Barrault",
            "Fethi Bougares"
        ],
        "abstract": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.\n    ",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04186",
        "title": "Neural Machine Translation with Supervised Attention",
        "authors": [
            "Lemao Liu",
            "Masao Utiyama",
            "Andrew Finch",
            "Eiichiro Sumita"
        ],
        "abstract": "The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04253",
        "title": "Neural Machine Transliteration: Preliminary Results",
        "authors": [
            "Amir H. Jadidinejad"
        ],
        "abstract": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04309",
        "title": "Efficient softmax approximation for GPUs",
        "authors": [
            "Edouard Grave",
            "Armand Joulin",
            "Moustapha Ciss\u00e9",
            "David Grangier",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2017-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04325",
        "title": "Transliteration in Any Language with Surrogate Languages",
        "authors": [
            "Stephen Mayhew",
            "Christos Christodoulopoulos",
            "Dan Roth"
        ],
        "abstract": "We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04417",
        "title": "An Adaptive Psychoacoustic Model for Automatic Speech Recognition",
        "authors": [
            "Peng Dai",
            "Xue Teng",
            "Frank Rudzicz",
            "Ing Yann Soon"
        ],
        "abstract": "Compared with automatic speech recognition (ASR), the human auditory system is more adept at handling noise-adverse situations, including environmental noise and channel distortion. To mimic this adeptness, auditory models have been widely incorporated in ASR systems to improve their robustness. This paper proposes a novel auditory model which incorporates psychoacoustics and otoacoustic emissions (OAEs) into ASR. In particular, we successfully implement the frequency-dependent property of psychoacoustic models and effectively improve resulting system performance. We also present a novel double-transform spectrum-analysis technique, which can qualitatively predict ASR performance for different noise types. Detailed theoretical analysis is provided to show the effectiveness of the proposed algorithm. Experiments are carried out on the AURORA2 database and show that the word recognition rate using our proposed feature extraction method is significantly increased over the baseline. Given models trained with clean speech, our proposed method achieves up to 85.39% word recognition accuracy on noisy data.\n    ",
        "submission_date": "2016-09-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04621",
        "title": "Factored Neural Machine Translation",
        "authors": [
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Lo\u00efc Barrault",
            "Fethi Bougares"
        ],
        "abstract": "We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. The final translation is built using some \\textit{a priori} linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04779",
        "title": "Characterizing the Language of Online Communities and its Relation to Community Reception",
        "authors": [
            "Trang Tran",
            "Mari Ostendorf"
        ],
        "abstract": "This work investigates style and topic aspects of language in online communities: looking at both utility as an identifier of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around specific topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04873",
        "title": "Distant Supervision for Relation Extraction beyond the Sentence Boundary",
        "authors": [
            "Chris Quirk",
            "Hoifung Poon"
        ],
        "abstract": "The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2017-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04904",
        "title": "Long-Term Trends in the Public Perception of Artificial Intelligence",
        "authors": [
            "Ethan Fast",
            "Eric Horvitz"
        ],
        "abstract": "Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04909",
        "title": "An Iterative Transfer Learning Based Ensemble Technique for Automatic Short Answer Grading",
        "authors": [
            "Shourya Roy",
            "Himanshu S. Bhatt",
            "Y. Narahari"
        ],
        "abstract": "Automatic short answer grading (ASAG) techniques are designed to automatically assess short answers to questions in natural language, having a length of a few words to a few sentences. Supervised ASAG techniques have been demonstrated to be effective but suffer from a couple of key practical limitations. They are greatly reliant on instructor provided model answers and need labeled training data in the form of graded student answers for every assessment task. To overcome these, in this paper, we introduce an ASAG technique with two novel features. We propose an iterative technique on an ensemble of (a) a text classifier of student answers and (b) a classifier using numeric features derived from various similarity measures with respect to model answers. Second, we employ canonical correlation analysis based transfer learning on a common feature representation to build the classifier ensemble for questions having no labelled data. The proposed technique handsomely beats all winning supervised entries on the SCIENTSBANK dataset from the Student Response Analysis task of SemEval 2013. Additionally, we demonstrate generalizability and benefits of the proposed technique through evaluation on multiple ASAG datasets from different subject topics and standards.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05180",
        "title": "Grammatical Templates: Improving Text Difficulty Evaluation for Language Learners",
        "authors": [
            "Shuhan Wang",
            "Erik Andersen"
        ],
        "abstract": "Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4%. Moreover, we build a simple and human-understandable text difficulty evaluation approach with 87.7% accuracy, using only 5 grammatical template features.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05234",
        "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning",
        "authors": [
            "Yen-Chen Wu",
            "Tzu-Hsiang Lin",
            "Yang-De Chen",
            "Hung-Yi Lee",
            "Lin-Shan Lee"
        ],
        "abstract": "User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05244",
        "title": "Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis",
        "authors": [
            "Haohan Wang",
            "Aaksha Meghawat",
            "Louis-Philippe Morency",
            "Eric P. Xing"
        ],
        "abstract": "Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews which are now available aplenty on online platforms. However, multimodal sentiment analysis has only a few high-quality data sets annotated for training machine learning algorithms. These limited resources restrict the generalizability of models, where, for example, the unique characteristics of a few speakers (e.g., wearing glasses) may become a confounding factor for the sentiment classification task. In this paper, we propose a Select-Additive Learning (SAL) procedure that improves the generalizability of trained neural networks for multimodal sentiment analysis. In our experiments, we show that our SAL approach improves prediction accuracy significantly in all three modalities (verbal, acoustic, visual), as well as in their fusion. Our results show that SAL, even when trained on one dataset, achieves good generalization across two new test datasets.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2017-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05511",
        "title": "Multilinear Grammar: Ranks and Interpretations",
        "authors": [
            "Dafydd Gibbon",
            "Sascha Griffiths"
        ],
        "abstract": "Multilinear Grammar provides a framework for integrating the many different syntagmatic structures of language into a coherent semiotically based Rank Interpretation Architecture, with default linear grammars at each rank. The architecture defines a Sui Generis Condition on ranks, from discourse through utterance and phrasal structures to the word, with its sub-ranks of morphology and phonology. Each rank has unique structures and its own semantic-pragmatic and prosodic-phonetic interpretation models. Default computational models for each rank are proposed, based on a Procedural Plausibility Condition: incremental processing in linear time with finite working memory. We suggest that the Rank Interpretation Architecture and its multilinear properties provide systematic design features of human languages, contrasting with unordered lists of key properties or single structural properties at one rank, such as recursion, which have previously been been put forward as language design features. The framework provides a realistic background for the gradual development of complexity in the phylogeny and ontogeny of language, and clarifies a range of challenges for the evaluation of realistic linguistic theories and applications. The empirical objective of the paper is to demonstrate unique multilinear properties at each rank and thereby motivate the Multilinear Grammar and Rank Interpretation Architecture framework as a coherent approach to capturing the complexity of human languages in the simplest possible way.\n    ",
        "submission_date": "2016-09-18T00:00:00",
        "last_modified_date": "2017-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05625",
        "title": "The MGB-2 Challenge: Arabic Multi-Dialect Broadcast Media Recognition",
        "authors": [
            "Ahmed Ali",
            "Peter Bell",
            "James Glass",
            "Yacine Messaoui",
            "Hamdy Mubarak",
            "Steve Renals",
            "Yifan Zhang"
        ],
        "abstract": "This paper describes the Arabic Multi-Genre Broadcast (MGB-2) Challenge for SLT-2016. Unlike last year's English MGB Challenge, which focused on recognition of diverse TV genres, this year, the challenge has an emphasis on handling the diversity in dialect in Arabic speech. Audio data comes from 19 distinct programmes from the Aljazeera Arabic TV channel between March 2005 and December 2015. Programmes are split into three groups: conversations, interviews, and reports. A total of 1,200 hours have been released with lightly supervised transcriptions for the acoustic modelling. For language modelling, we made available over 110M words crawled from Aljazeera Arabic website ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2019-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05650",
        "title": "Multi-view Dimensionality Reduction for Dialect Identification of Arabic Broadcast Speech",
        "authors": [
            "Sameer Khurana",
            "Ahmed Ali",
            "Steve Renals"
        ],
        "abstract": "In this work, we present a new Vector Space Model (VSM) of speech utterances for the task of spoken dialect identification. Generally, DID systems are built using two sets of features that are extracted from speech utterances; acoustic and phonetic. The acoustic and phonetic features are used to form vector representations of speech utterances in an attempt to encode information about the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used for the task of DID. The aim of this paper is to construct a single VSM that encodes information about spoken dialects from both the Phonotactic and Acoustic VSMs. Given the two views of the data, we make use of a well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), to form a single vector representation for each speech utterance that encodes dialect specific discriminative information from both the phonetic and acoustic representations. We refer to this approach as feature space combination approach and show that our CCA based feature vector representation performs better on the Arabic DID task than the phonetic and acoustic feature representations used alone. We also present the feature space combination approach as a viable alternative to the model based combination approach, where two DID systems are built using the two VSMs (Phonotactic and Acoustic) and the final prediction score is the output score combination from the two systems.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05935",
        "title": "Advances in All-Neural Speech Recognition",
        "authors": [
            "G. Zweig",
            "C. Yu",
            "J. Droppo",
            "A. Stolcke"
        ],
        "abstract": "This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2017-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06038",
        "title": "Enhanced LSTM for Natural Language Inference",
        "authors": [
            "Qian Chen",
            "Xiaodan Zhu",
            "Zhenhua Ling",
            "Si Wei",
            "Hui Jiang",
            "Diana Inkpen"
        ],
        "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2017-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06049",
        "title": "Automatic Quality Assessment for Speech Translation Using Joint ASR and MT Features",
        "authors": [
            "Ngoc-Tien Le",
            "Benjamin Lecouteux",
            "Laurent Besacier"
        ],
        "abstract": "This paper addresses automatic quality assessment of spoken language translation (SLT). This relatively new task is defined and formalized as a sequence labeling problem where each word in the SLT hypothesis is tagged as good or bad according to a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality, or both (combined ASR+MT). This research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing: ASR output, verbatim transcript, text translation, speech translation and post-edition of translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR feature can bring interesting complementary information. Our robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06082",
        "title": "Learning Robust Representations of Text",
        "authors": [
            "Yitong Li",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "abstract": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06127",
        "title": "A framework for mining process models from emails logs",
        "authors": [
            "Diana Jlailaty",
            "Daniela Grigori",
            "Khalid Belhajjame"
        ],
        "abstract": "Due to its wide use in personal, but most importantly, professional contexts, email represents a valuable source of information that can be harvested for understanding, reengineering and repurposing undocumented business processes of companies and institutions. Towards this aim, a few researchers investigated the problem of extracting process oriented information from email logs in order to take benefit of the many available process mining techniques and tools. In this paper we go further in this direction, by proposing a new method for mining process models from email logs that leverage unsupervised machine learning techniques with little human involvement. Moreover, our method allows to semi-automatically label emails with activity names, that can be used for activity recognition in new incoming emails. A use case demonstrates the usefulness of the proposed solution using a modest in size, yet real-world, dataset containing emails that belong to two different process models.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06204",
        "title": "Italy goes to Stanford: a collection of CoreNLP modules for Italian",
        "authors": [
            "Alessio Palmero Aprosio",
            "Giovanni Moretti"
        ],
        "abstract": "In this we paper present Tint, an easy-to-use set of fast, accurate and extendable Natural Language Processing modules for Italian. It is based on Stanford CoreNLP and is freely available as a standalone software or a library that can be integrated in an existing project.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2017-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06239",
        "title": "Generating Politically-Relevant Event Data",
        "authors": [
            "John Beieler"
        ],
        "abstract": "Automatically generated political event data is an important part of the social science data ecosystem. The approaches for generating this data, though, have remained largely the same for two decades. During this time, the field of computational linguistics has progressed tremendously. This paper presents an overview of political event data, including methods and ontologies, and a set of experiments to determine the applicability of deep neural networks to the extraction of political events from news text.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06380",
        "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention",
        "authors": [
            "Yang Liu",
            "Sujian Li"
        ],
        "abstract": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06490",
        "title": "One Sentence One Model for Neural Machine Translation",
        "authors": [
            "Xiaoqing Li",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Neural machine translation (NMT) becomes a new state-of-the-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06530",
        "title": "Weakly supervised spoken term discovery using cross-lingual side information",
        "authors": [
            "Sameer Bansal",
            "Herman Kamper",
            "Sharon Goldwater",
            "Adam Lopez"
        ],
        "abstract": "Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06577",
        "title": "Semi-supervised knowledge extraction for detection of drugs and their effects",
        "authors": [
            "Fabio Del Vigna",
            "Marinella Petrocchi",
            "Alessandro Tommasi",
            "Cesare Zavattari",
            "Maurizio Tesconi"
        ],
        "abstract": "New Psychoactive Substances (NPS) are drugs that lay in a grey area of legislation, since they are not internationally and officially banned, possibly leading to their not prosecutable trade. The exacerbation of the phenomenon is that NPS can be easily sold and bought online. Here, we consider large corpora of textual posts, published on online forums specialized on drug discussions, plus a small set of known substances and associated effects, which we call seeds. We propose a semi-supervised approach to knowledge extraction, applied to the detection of drugs (comprising NPS) and effects from the corpora under investigation. Based on the very small set of initial seeds, the work highlights how a contrastive approach and context deduction are effective in detecting substances and effects from the corpora. Our promising results, which feature a F1 score close to 0.9, pave the way for shortening the detection time of new psychoactive substances, once these are discussed and advertised on the Internet.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06578",
        "title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by Leveraging Hashtags and Sentiment Lexicon",
        "authors": [
            "Kar Wai Lim",
            "Wray Buntine"
        ],
        "abstract": "Aspect-based opinion mining is widely applied to review data to aggregate or summarize opinions of a product, and the current state-of-the-art is achieved with Latent Dirichlet Allocation (LDA)-based model. Although social media data like tweets are laden with opinions, their \"dirty\" nature (as natural language) has discouraged researchers from applying LDA-based opinion model for product review mining. Tweets are often informal, unstructured and lacking labeled data such as categories and ratings, making it challenging for product opinion mining. In this paper, we propose an LDA-based opinion model named Twitter Opinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM leverages hashtags, mentions, emoticons and strong sentiment words that are present in tweets in its discovery process. It improves opinion prediction by modeling the target-opinion interaction directly, thus discovering target specific opinion words, neglected in existing approaches. Moreover, we propose a new formulation of incorporating sentiment prior information into a topic model, by utilizing an existing public sentiment lexicon. This is novel in that it learns and updates with the data. We conduct experiments on 9 million tweets on electronic products, and demonstrate the improved performance of TOTM in both quantitative evaluations and qualitative analysis. We show that aspect-based opinion analysis on massive volume of tweets provides useful opinions on products.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06616",
        "title": "Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text",
        "authors": [
            "John J. Nay"
        ],
        "abstract": "We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more fine-grained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vector-arithmetic-based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06649",
        "title": "Minimally Supervised Written-to-Spoken Text Normalization",
        "authors": [
            "Ke Wu",
            "Kyle Gorman",
            "Richard Sproat"
        ],
        "abstract": "In speech-applications such as text-to-speech (TTS) or automatic speech recognition (ASR), \\emph{text normalization} refers to the task of converting from a \\emph{written} representation into a representation of how the text is to be \\emph{spoken}. In all real-world speech applications, the text normalization engine is developed---in large part---by hand. For example, a hand-built grammar may be used to enumerate the possible ways of saying a given token in a given language, and a statistical model used to select the most appropriate pronunciation in context. In this study we examine the tradeoffs associated with using more or less language-specific domain knowledge in a text normalization engine. In the most data-rich scenario, we have access to a carefully constructed hand-built normalization grammar that for any given token will produce a set of all possible verbalizations for that token. We also assume a corpus of aligned written-spoken utterances, from which we can train a ranking model that selects the appropriate verbalization for the given context. As a substitute for the carefully constructed grammar, we also consider a scenario with a language-universal normalization \\emph{covering grammar}, where the developer merely needs to provide a set of lexical items particular to the language. As a substitute for the aligned corpus, we also consider a scenario where one only has the spoken side, and the corresponding written side is \"hallucinated\" by composing the spoken side with the inverted normalization grammar. We investigate the accuracy of a text normalization engine under each of these scenarios. We report the results of experiments on English and Russian.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06686",
        "title": "Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "Convolutional neural networks (CNNs) have demonstrated superior capability for extracting information from raw signals in computer vision. Recently, character-level and multi-channel CNNs have exhibited excellent performance for sentence classification tasks. We apply CNNs to large-scale authorship attribution, which aims to determine an unknown text's author among many candidate authors, motivated by their ability to process character-level signals and to differentiate between a large number of classes, while making fast predictions in comparison to state-of-the-art approaches. We extensively evaluate CNN-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches. We show that character-level CNNs outperform the state-of-the-art on four out of five datasets in different domains. Additionally, we present the first application of authorship attribution to reddit.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06773",
        "title": "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning",
        "authors": [
            "Suyoun Kim",
            "Takaaki Hori",
            "Shinji Watanabe"
        ],
        "abstract": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6% relative improvements in Character Error Rate (CER).\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06791",
        "title": "Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling",
        "authors": [
            "Kar Wai Lim",
            "Changyou Chen",
            "Wray Buntine"
        ],
        "abstract": "Twitter data is extremely noisy -- each tweet is short, unstructured and with informal language, a challenge for current topic modeling. On the other hand, tweets are accompanied by extra information such as authorship, hashtags and the user-follower network. Exploiting this additional information, we propose the Twitter-Network (TN) topic model to jointly model the text and the social network in a full Bayesian nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian process random function model for social network modeling. We show that the TN topic model significantly outperforms several existing nonparametric models due to its flexibility. Moreover, the TN topic model enables additional informative inference such as authors' interests, hashtag analysis, as well as leading to further applications such as author recommendation, automatic topic labeling and hashtag suggestion. Note our general inference framework can readily be applied to other topic models with embedded PDP nodes.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07033",
        "title": "Generating Abstractive Summaries from Meeting Transcripts",
        "authors": [
            "Siddhartha Banerjee",
            "Prasenjit Mitra",
            "Kazunari Sugiyama"
        ],
        "abstract": "Summaries of meetings are very important as they convey the essential content of discussions in a concise form. Generally, it is time consuming to read and understand the whole documents. Therefore, summaries play an important role as the readers are interested in only the important context of discussions. In this work, we address the task of meeting document summarization. Automatic summarization systems on meeting conversations developed so far have been primarily extractive, resulting in unacceptable summaries that are hard to read. The extracted utterances contain disfluencies that affect the quality of the extractive summaries. To make summaries much more readable, we propose an approach to generating abstractive summaries by fusing important content from several utterances. We first separate meeting transcripts into various topic segments, and then identify the important utterances in each segment using a supervised learning approach. The important utterances are then combined together to generate a one-sentence summary. In the text generation step, the dependency parses of the utterances in each segment are combined together to create a directed graph. The most informative and well-formed sub-graph obtained by integer linear programming (ILP) is selected to generate a one-sentence summary for each topic segment. The ILP formulation reduces disfluencies by leveraging grammatical relations that are more prominent in non-conversational style of text, and therefore generates summaries that is comparable to human-written abstractive summaries. Experimental results show that our method can generate more informative summaries than the baselines. In addition, readability assessments by human judges as well as log-likelihood estimates obtained from the dependency parser show that our generated summaries are significantly readable and well-formed.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07034",
        "title": "Multi-document abstractive summarization using ILP based multi-sentence compression",
        "authors": [
            "Siddhartha Banerjee",
            "Prasenjit Mitra",
            "Kazunari Sugiyama"
        ],
        "abstract": "Abstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. In this work, we aim at developing an abstractive summarizer. First, our proposed approach identifies the most important document in the multi-document set. The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences. Second, we generate K-shortest paths from the sentences in each cluster using a word-graph structure. Finally, we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming (ILP) model with the objective of maximizing information content and readability of the final summary. Our ILP model represents the shortest paths as binary variables and considers the length of the path, information score and linguistic quality score in the objective function. Experimental results on the DUC 2004 and 2005 multi-document summarization datasets show that our proposed approach outperforms all the baselines and state-of-the-art extractive summarizers as measured by the ROUGE scores. Our method also outperforms a recent abstractive summarization technique. In manual evaluation, our approach also achieves promising results on informativeness and readability.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07035",
        "title": "Abstractive Meeting Summarization UsingDependency Graph Fusion",
        "authors": [
            "Siddhartha Banerjee",
            "Prasenjit Mitra",
            "Kazunari Sugiyama"
        ],
        "abstract": "Automatic summarization techniques on meeting conversations developed so far have been primarily extractive, resulting in poor summaries. To improve this, we propose an approach to generate abstractive summaries by fusing important content from several utterances. Any meeting is generally comprised of several discussion topic segments. For each topic segment within a meeting conversation, we aim to generate a one sentence summary from the most important utterances using an integer linear programming-based sentence fusion approach. Experimental results show that our method can generate more informative summaries than the baselines.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07053",
        "title": "Semantic Tagging with Deep Residual Networks",
        "authors": [
            "Johannes Bjerva",
            "Barbara Plank",
            "Johan Bos"
        ],
        "abstract": "We propose a novel semantic tagging task, sem-tagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71% accuracy on UD v1.2 and 95.67% accuracy on UD v1.3).\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07075",
        "title": "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs",
        "authors": [
            "Jiawei Wu",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07197",
        "title": "Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems",
        "authors": [
            "Shyam Upadhyay",
            "Ming-Wei Chang"
        ],
        "abstract": "We propose a new evaluation for automatic solvers for algebra word problems, which can identify mistakes that existing evaluations overlook. Our proposal is to evaluate such solvers using derivations, which reflect how an equation system was constructed from the word problem. To accomplish this, we develop an algorithm for checking the equivalence between two derivations, and show how derivation an- notations can be semi-automatically added to existing datasets. To make our experiments more comprehensive, we include the derivation annotation for DRAW-1K, a new dataset containing 1000 general algebra word problems. In our experiments, we found that the annotated derivations enable a more accurate evaluation of automatic solvers than previously used metrics. We release derivation annotations for over 2300 algebra word problems for future evaluations.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07222",
        "title": "Deep Multi-Task Learning with Shared Memory",
        "authors": [
            "Pengfei Liu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07317",
        "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
        "authors": [
            "Yishu Miao",
            "Phil Blunsom"
        ],
        "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07451",
        "title": "AMR-to-text generation as a Traveling Salesman Problem",
        "authors": [
            "Linfeng Song",
            "Yue Zhang",
            "Xiaochang Peng",
            "Zhiguo Wang",
            "Daniel Gildea"
        ],
        "abstract": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07479",
        "title": "Incorporating Relation Paths in Neural Relation Extraction",
        "authors": [
            "Wenyuan Zeng",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/PathNRE.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2017-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07561",
        "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser",
        "authors": [
            "Adhiguna Kuncoro",
            "Miguel Ballesteros",
            "Lingpeng Kong",
            "Chris Dyer",
            "Noah A. Smith"
        ],
        "abstract": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \"distillation\" of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07568",
        "title": "A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects",
        "authors": [
            "Yonatan Belinkov",
            "James Glass"
        ],
        "abstract": "Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07585",
        "title": "An Investigation of Recurrent Neural Architectures for Drug Name Recognition",
        "authors": [
            "Raghavendra Chalapathy",
            "Ehsan Zare Borzeshi",
            "Massimo Piccardi"
        ],
        "abstract": "Drug name recognition (DNR) is an essential step in the Pharmacovigilance (PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories. State-of-the-art DNR approaches heavily rely on hand crafted features and domain specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architectures - the Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at performing DNR straight from the text. The experimental results achieved on the authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07680",
        "title": "Existence of Hierarchies and Human's Pursuit of Top Hierarchy Lead to Power Law",
        "authors": [
            "Shuiyuan Yu",
            "Junying Liang",
            "Haitao Liu"
        ],
        "abstract": "The power law is ubiquitous in natural and social phenomena, and is considered as a universal relationship between the frequency and its rank for diverse social systems. However, a general model is still lacking to interpret why these seemingly unrelated systems share great similarity. Through a detailed analysis of natural language texts and simulation experiments based on the proposed 'Hierarchical Selection Model', we found that the existence of hierarchies and human's pursuit of top hierarchy lead to the power law. Further, the power law is a statistical and emergent performance of hierarchies, and it is the universality of hierarchies that contributes to the ubiquity of the power law.\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07681",
        "title": "The distribution of information content in English sentences",
        "authors": [
            "Shuiyuan Yu",
            "Jin Cong",
            "Junying Liang",
            "Haitao Liu"
        ],
        "abstract": "Sentence is a basic linguistic unit, however, little is known about how information content is distributed across different positions of a sentence. Based on authentic language data of English, the present study calculated the entropy and other entropy-related statistics for different sentence positions. The statistics indicate a three-step staircase-shaped distribution pattern, with entropy in the initial position lower than the medial positions (positions other than the initial and final), the medial positions lower than the final position and the medial positions showing no significant difference. The results suggest that: (1) the hypotheses of Constant Entropy Rate and Uniform Information Density do not hold for the sentence-medial positions; (2) the context of a word in a sentence should not be simply defined as all the words preceding it in the same sentence; and (3) the contextual information content in a sentence does not accumulate incrementally but follows a pattern of \"the whole is greater than the sum of parts\".\n    ",
        "submission_date": "2016-09-24T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07701",
        "title": "Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results",
        "authors": [
            "Yonatan Belinkov",
            "James Glass"
        ],
        "abstract": "Machine translation between Arabic and Hebrew has so far been limited by a lack of parallel corpora, despite the political and cultural importance of this language pair. Previous work relied on manually-crafted grammars or pivoting via English, both of which are unsatisfactory for building a scalable and accurate MT system. In this work, we compare standard phrase-based and neural systems on Arabic-Hebrew translation. We experiment with tokenization by external tools and sub-word modeling by character-level neural models, and show that both methods lead to improved translation performance, with a small advantage to the neural models.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07730",
        "title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation",
        "authors": [
            "Jinsong Su",
            "Zhixing Tan",
            "Deyi Xiong",
            "Rongrong Ji",
            "Xiaodong Shi",
            "Yang Liu"
        ],
        "abstract": "Neural machine translation (NMT) heavily relies on word-level modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2016-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07756",
        "title": "A Factorized Model for Transitive Verbs in Compositional Distributional Semantics",
        "authors": [
            "Lilach Edelstein",
            "Roi Reichart"
        ],
        "abstract": "We present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work.\n    ",
        "submission_date": "2016-09-25T00:00:00",
        "last_modified_date": "2016-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07843",
        "title": "Pointer Sentinel Mixture Models",
        "authors": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07876",
        "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation",
        "authors": [
            "Taehwan Kim",
            "Jonathan Keane",
            "Weiran Wang",
            "Hao Tang",
            "Jason Riggle",
            "Gregory Shakhnarovich",
            "Diane Brentari",
            "Karen Livescu"
        ],
        "abstract": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08075",
        "title": "S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking",
        "authors": [
            "Yi Yang",
            "Ming-Wei Chang"
        ],
        "abstract": "Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks. In this paper, we propose S-MART, a tree-based structured learning framework based on multiple additive regression trees. S-MART is especially suitable for handling tasks with dense features, and can be used to learn many different structures under various loss functions.\n",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08084",
        "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities",
        "authors": [
            "Yi Yang",
            "Ming-Wei Chang",
            "Jacob Eisenstein"
        ],
        "abstract": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08097",
        "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision",
        "authors": [
            "Rebecca Sharp",
            "Mihai Surdeanu",
            "Peter Jansen",
            "Peter Clark",
            "Michael Hammond"
        ],
        "abstract": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08139",
        "title": "An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages",
        "authors": [
            "Antonios Anastasopoulos",
            "David Chiang",
            "Long Duong"
        ],
        "abstract": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems. A first step towards making use of such data would be to automatically align spoken words with their translations. We present a model that combines Dyer et al.'s reparameterization of IBM Model 2 (fast-align) and k-means clustering using Dynamic Time Warping as a distance metric. The two components are trained jointly using expectation-maximization. In an extremely low-resource scenario, our model performs significantly better than both a neural model and a strong baseline.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08144",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
        "authors": [
            "Yonghui Wu",
            "Mike Schuster",
            "Zhifeng Chen",
            "Quoc V. Le",
            "Mohammad Norouzi",
            "Wolfgang Macherey",
            "Maxim Krikun",
            "Yuan Cao",
            "Qin Gao",
            "Klaus Macherey",
            "Jeff Klingner",
            "Apurva Shah",
            "Melvin Johnson",
            "Xiaobing Liu",
            "\u0141ukasz Kaiser",
            "Stephan Gouws",
            "Yoshikiyo Kato",
            "Taku Kudo",
            "Hideto Kazawa",
            "Keith Stevens",
            "George Kurian",
            "Nishant Patil",
            "Wei Wang",
            "Cliff Young",
            "Jason Smith",
            "Jason Riesa",
            "Alex Rudnick",
            "Oriol Vinyals",
            "Greg Corrado",
            "Macduff Hughes",
            "Jeffrey Dean"
        ],
        "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08194",
        "title": "Online Segment to Segment Neural Transduction",
        "authors": [
            "Lei Yu",
            "Jan Buys",
            "Phil Blunsom"
        ],
        "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08210",
        "title": "Learning to Translate for Multilingual Question Answering",
        "authors": [
            "Ferhan Ture",
            "Elizabeth Boschee"
        ],
        "abstract": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p<0.05): translating all text into English, then training a classifier based only on English (original or translated) text.\n    ",
        "submission_date": "2016-09-26T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08237",
        "title": "Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment",
        "authors": [
            "Tao Ge",
            "Qing Dou",
            "Xiaoman Pan",
            "Heng Ji",
            "Lei Cui",
            "Baobao Chang",
            "Zhifang Sui",
            "Ming Zhou"
        ],
        "abstract": "Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; (2). following a novel Data-to-Network-to-Knowledge (D2N2K) paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08293",
        "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models",
        "authors": [
            "Magnus Sahlgren",
            "Alessandro Lenci"
        ],
        "abstract": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08337",
        "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition",
        "authors": [
            "Zhiyuan Tang",
            "Lantian Li",
            "Dong Wang"
        ],
        "abstract": "Research on multilingual speech recognition remains attractive yet challenging. Recent studies focus on learning shared structures under the multi-task paradigm, in particular a feature sharing structure. This approach has been found effective to improve performance on each individual language. However, this approach is only useful when the deployed system supports just one language. In a true multilingual scenario where multiple languages are allowed, performance will be significantly reduced due to the competition among languages in the decoding space. This paper presents a multi-task recurrent model that involves a multilingual speech recognition (ASR) component and a language recognition (LR) component, and the ASR component is informed of the language information by the LR component, leading to a language-aware recognition. We tested the approach on an English-Chinese bilingual recognition task. The results show that the proposed multi-task recurrent model can improve performance of multilingual recognition systems.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08359",
        "title": "emoji2vec: Learning Emoji Representations from their Description",
        "authors": [
            "Ben Eisner",
            "Tim Rockt\u00e4schel",
            "Isabelle Augenstein",
            "Matko Bo\u0161njak",
            "Sebastian Riedel"
        ],
        "abstract": "Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard. The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08389",
        "title": "A Hackathon for Classical Tibetan",
        "authors": [
            "Orna Almogi",
            "Lena Dankin",
            "Nachum Dershowitz",
            "Lior Wolf"
        ],
        "abstract": "We describe the course of a hackathon dedicated to the development of linguistic tools for Tibetan Buddhist studies. Over a period of five days, a group of seventeen scholars, scientists, and students developed and compared algorithms for intertextual alignment and text classification, along with some basic language tools, including a stemmer and word segmenter.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2018-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08409",
        "title": "Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks",
        "authors": [
            "Savelie Cornegruta",
            "Robert Bakewell",
            "Samuel Withey",
            "Giovanni Montana"
        ],
        "abstract": "Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08412",
        "title": "OC16-CE80: A Chinese-English Mixlingual Database and A Speech Recognition Baseline",
        "authors": [
            "Dong Wang",
            "Zhiyuan Tang",
            "Difei Tang",
            "Qing Chen"
        ],
        "abstract": "We present the OC16-CE80 Chinese-English mixlingual speech database which was released as a main resource for training, development and test for the Chinese-English mixlingual speech recognition (MixASR-CHEN) challenge on O-COCOSDA 2016. This database consists of 80 hours of speech signals recorded from more than 1,400 speakers, where the utterances are in Chinese but each involves one or several English words. Based on the database and another two free data resources (THCHS30 and the CMU dictionary), a speech recognition (ASR) baseline was constructed with the deep neural network-hidden Markov model (DNN-HMM) hybrid system. We then report the baseline results following the MixASR-CHEN evaluation rules and demonstrate that OC16-CE80 is a reasonable data resource for mixlingual research.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08445",
        "title": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline",
        "authors": [
            "Dong Wang",
            "Lantian Li",
            "Difei Tang",
            "Qing Chen"
        ],
        "abstract": "We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08492",
        "title": "WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies",
        "authors": [
            "Miguel J. Rodrigues",
            "Miguel Fal\u00e9",
            "Andre Lamurias",
            "Francisco M. Couto"
        ],
        "abstract": "This paper describes our system, dubbed WS4A (Web Services for All), that participated in the fourth edition of the BioASQ challenge (2016). We used WS4A to perform the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and ideal answers for each given question. The novelty in our approach consists on the maximum exploitation of existing web services in each step of WS4A, such as the annotation of text, and the retrieval of metadata for each annotation. The information retrieved included concept identifiers, ontologies, ancestors, and most importantly, PubMed identifiers. The paper describes the WS4A pipeline and also presents the precision, recall and f-measure values obtained in task 4b. Our system achieved two second places in two subtasks on one of the five batches.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08496",
        "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings",
        "authors": [
            "Jipeng Qiang",
            "Ping Chen",
            "Tong Wang",
            "Xindong Wu"
        ],
        "abstract": "Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this prob- lem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn se- mantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo- texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08667",
        "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models",
        "authors": [
            "Kevin Clark",
            "Christopher D. Manning"
        ],
        "abstract": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08703",
        "title": "Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee"
        ],
        "abstract": "Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning hyperparameters. Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing hyperparameters using GP further improves the results, and reduces the computational time by a factor of 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08777",
        "title": "Character Sequence Models for ColorfulWords",
        "authors": [
            "Kazuya Kawakami\u007f",
            "Chris Dyer\u007f",
            "Bryan R. Routledge",
            "Noah A. Smith"
        ],
        "abstract": "We present a neural network architecture to predict a point in color space from the sequence of characters in the color's name. Using large scale color--name pairs obtained from an online color design forum, we evaluate our model on a \"color Turing test\" and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08810",
        "title": "Effective Combination of Language and Vision Through Model Composition and the R-CCA Method",
        "authors": [
            "Hagar Loeub",
            "Roi Reichart"
        ],
        "abstract": "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08824",
        "title": "Equation Parsing: Mapping Sentences to Grounded Equations",
        "authors": [
            "Subhro Roy",
            "Shyam Upadhyay",
            "Dan Roth"
        ],
        "abstract": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$ of the time, it also identifies the correct noun phrase $\\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09004",
        "title": "Byte-based Language Identification with Deep Convolutional Networks",
        "authors": [
            "Johannes Bjerva"
        ],
        "abstract": "We report on our system for the shared task on discriminating between similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09007",
        "title": "Unsupervised Neural Hidden Markov Models",
        "authors": [
            "Ke Tran",
            "Yonatan Bisk",
            "Ashish Vaswani",
            "Daniel Marcu",
            "Kevin Knight"
        ],
        "abstract": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09019",
        "title": "Psychologically Motivated Text Mining",
        "authors": [
            "Ekaterina Shutova",
            "Patricia Lichtenstein"
        ],
        "abstract": "Natural language processing techniques are increasingly applied to identify social trends and predict behavior based on large text collections. Existing methods typically rely on surface lexical and syntactic information. Yet, research in psychology shows that patterns of human conceptualisation, such as metaphorical framing, are reliable predictors of human expectations and decisions. In this paper, we present a method to learn patterns of metaphorical framing from large text collections, using statistical techniques. We apply the method to data in three different languages and evaluate the identified patterns, demonstrating their psychological validity.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09028",
        "title": "Stance Classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations",
        "authors": [
            "Arkaitz Zubiaga",
            "Elena Kochkina",
            "Maria Liakata",
            "Rob Procter",
            "Michal Lukasik"
        ],
        "abstract": "Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users' replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09171",
        "title": "Empirical Evaluation of RNN Architectures on Sentence Classification Task",
        "authors": [
            "Lei Shen",
            "Junlin Zhang"
        ],
        "abstract": "Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are Tail Model and Pooling Model. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the Max Pooling Model or Hybrid Max Pooling Model achieves the best performance on most datasets, while Tail Model does not outperform other models.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09188",
        "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis",
        "authors": [
            "Leonard K.M. Poon",
            "Nevin L. Zhang"
        ],
        "abstract": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09189",
        "title": "Learning Sentence Representation with Guidance of Human Attention",
        "authors": [
            "Shaonan Wang",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Recently, much progress has been made in learning general-purpose sentence representations that can be used across domains. However, most of the existing models typically treat each word in a sentence equally. In contrast, extensive studies have proven that human read sentences efficiently by making a sequence of fixation and saccades. This motivates us to improve sentence representations by assigning different weights to the vectors of the component words, which can be treated as an attention mechanism on single sentences. To that end, we propose two novel attention models, in which the attention weights are derived using significant predictors of human reading time, i.e., Surprisal, POS tags and CCG supertags. The extensive experiments demonstrate that the proposed methods significantly improve upon the state-of-the-art sentence representation models.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09247",
        "title": "Training Dependency Parsers with Partial Annotation",
        "authors": [
            "Zhenghua Li",
            "Yue Zhang",
            "Jiayuan Chao",
            "Min Zhang"
        ],
        "abstract": "Recently, these has been a surge on studying how to obtain partially annotated data for model supervision. However, there still lacks a systematic study on how to train statistical models with partial annotation (PA). Taking dependency parsing as our case study, this paper describes and compares two straightforward approaches for three mainstream dependency parsers. The first approach is previously proposed to directly train a log-linear graph-based parser (LLGPar) with PA based on a forest-based objective. This work for the first time proposes the second approach to directly training a linear graph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA based on the idea of constrained decoding. We conduct extensive experiments on Penn Treebank under three different settings for simulating PA, i.e., random dependencies, most uncertain dependencies, and dependencies with divergent outputs from the three parsers. The results show that LLGPar is most effective in learning from PA and LTPar lags behind the graph-based counterparts by large margin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar to complete PA into full annotation (FA).\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09315",
        "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders",
        "authors": [
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "G\u00e1bor Melis",
            "Edward Grefenstette",
            "Chris Dyer",
            "Wang Ling",
            "Phil Blunsom",
            "Karl Moritz Hermann"
        ],
        "abstract": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09382",
        "title": "Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks",
        "authors": [
            "Othman Zennaki",
            "Nasredine Semmar",
            "Laurent Besacier"
        ],
        "abstract": "This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. We investigate both uni- and bi-directional RNN models and propose a method to include external information (for instance low level information from POS) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers.\n    ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2016-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09405",
        "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing",
        "authors": [
            "Yonatan Bisk",
            "Siva Reddy",
            "John Blitzer",
            "Julia Hockenmaier",
            "Mark Steedman"
        ],
        "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation provides a unique window to explore the strengths and weaknesses of semantics captured by unsupervised grammar induction systems. We release a new Freebase semantic parsing dataset called SPADES (Semantic PArsing of DEclarative Sentences) containing 93K cloze-style questions paired with answers. We evaluate all our models on this dataset. Our code and data are available at ",
        "submission_date": "2016-09-29T00:00:00",
        "last_modified_date": "2017-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09552",
        "title": "Controlling Output Length in Neural Encoder-Decoders",
        "authors": [
            "Yuta Kikuchi",
            "Graham Neubig",
            "Ryohei Sasano",
            "Hiroya Takamura",
            "Manabu Okumura"
        ],
        "abstract": "Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.09580",
        "title": "Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces",
        "authors": [
            "Michael Spranger",
            "Katrien Beuls"
        ],
        "abstract": "This paper discusses lexicon word learning in high-dimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00030",
        "title": "Modeling Language Change in Historical Corpora: The Case of Portuguese",
        "authors": [
            "Marcos Zampieri",
            "Shervin Malmasi",
            "Mark Dras"
        ],
        "abstract": "This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00031",
        "title": "Discriminating Similar Languages: Evaluations and Explorations",
        "authors": [
            "Cyril Goutte",
            "Serge L\u00e9ger",
            "Shervin Malmasi",
            "Marcos Zampieri"
        ],
        "abstract": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00072",
        "title": "Vocabulary Selection Strategies for Neural Machine Translation",
        "authors": [
            "Gurvan L'Hostis",
            "David Grangier",
            "Michael Auli"
        ],
        "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.\n    ",
        "submission_date": "2016-10-01T00:00:00",
        "last_modified_date": "2016-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00211",
        "title": "Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks",
        "authors": [
            "Marcos Vin\u00edcius Treviso",
            "Christopher Shulby",
            "Sandra Maria Alu\u00edsio"
        ],
        "abstract": "Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech, as well as, normal, prepared speech, and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2017-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00277",
        "title": "Very Deep Convolutional Neural Networks for Robust Speech Recognition",
        "authors": [
            "Yanmin Qian",
            "Philip C Woodland"
        ],
        "abstract": "This paper describes the extension and optimization of our previous work on very deep convolutional neural networks (CNNs) for effective recognition of noisy speech in the Aurora 4 task. The appropriate number of convolutional layers, the sizes of the filters, pooling operations and input feature maps are all modified: the filter and pooling sizes are reduced and dimensions of input feature maps are extended to allow adding more convolutional layers. Furthermore appropriate input padding and input feature map selection strategies are developed. In addition, an adaptation framework using joint training of very deep CNN with auxiliary features i-vector and fMLLR features is developed. These modifications give substantial word error rate reductions over the standard CNN used as baseline. Finally the very deep CNN is combined with an LSTM-RNN acoustic model and it is shown that state-level weighted log likelihood score combination in a joint acoustic model decoding scheme is very effective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%, further 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN joint decoding.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2016-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00311",
        "title": "Syntactic Structures and Code Parameters",
        "authors": [
            "Kevin Shu",
            "Matilde Marcolli"
        ],
        "abstract": "We assign binary and ternary error-correcting codes to the data of syntactic structures of world languages and we study the distribution of code points in the space of code parameters. We show that, while most codes populate the lower region approximating a superposition of Thomae functions, there is a substantial presence of codes above the Gilbert-Varshamov bound and even above the asymptotic bound and the Plotkin bound. We investigate the dynamics induced on the space of code parameters by spin glass models of language change, and show that, in the presence of entailment relations between syntactic parameters the dynamics can sometimes improve the code. For large sets of languages and syntactic data, one can gain information on the spin glass dynamics from the induced dynamics in the space of code parameters.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2016-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00369",
        "title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep Recurrent models",
        "authors": [
            "A. Hassan",
            "M. R. Amin",
            "N. Mohammed",
            "A. K. A. Azad"
        ],
        "abstract": "Sentiment Analysis (SA) is an action research area in the digital age. With rapid and constant growth of online social media sites and services, and the increasing amount of textual data such as - statuses, comments, reviews etc. available in them, application of automatic SA is on the rise. However, most of the research works on SA in natural language processing (NLP) are based on English language. Despite being the sixth most widely spoken language in the world, Bangla still does not have a large and standard dataset. Because of this, recent research works in Bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field. Therefore, we first tried to provide a textual dataset - that includes not just Bangla, but Romanized Bangla texts as well, is substantial, post-processed and multiple validated, ready to be used in SA experiments. We tested this dataset in Deep Recurrent model, specifically, Long Short Term Memory (LSTM), using two types of loss functions - binary crossentropy and categorical crossentropy, and also did some experimental pre-training by using data from one validation to pre-train the other and vice versa. Lastly, we documented the results along with some analysis on them, which were promising.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00388",
        "title": "Learning to Translate in Real-time with Neural Machine Translation",
        "authors": [
            "Jiatao Gu",
            "Graham Neubig",
            "Kyunghyun Cho",
            "Victor O.K. Li"
        ],
        "abstract": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00479",
        "title": "Nonsymbolic Text Representation",
        "authors": [
            "Hinrich Schuetze",
            "Heike Adel",
            "Ehsaneddin Asgari"
        ],
        "abstract": "We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text. We show that our model performs better than prior work on an information extraction and a text denoising task.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2017-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00552",
        "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks",
        "authors": [
            "Minjae Lee",
            "Kyuyeon Hwang",
            "Jinhwan Park",
            "Sungwook Choi",
            "Sungho Shin",
            "Wonyong Sung"
        ],
        "abstract": "In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-to-character RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.\n    ",
        "submission_date": "2016-09-30T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00572",
        "title": "An Arabic-Hebrew parallel corpus of TED talks",
        "authors": [
            "Mauro Cettolo"
        ],
        "abstract": "We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00602",
        "title": "Multimodal Semantic Simulations of Linguistically Underspecified Motion Events",
        "authors": [
            "Nikhil Krishnaswamy",
            "James Pustejovsky"
        ],
        "abstract": "In this paper, we describe a system for generating three-dimensional visual simulations of natural language motion expressions. We use a rich formal model of events and their participants to generate simulations that satisfy the minimal constraints entailed by the associated utterance, relying on semantic knowledge of physical objects and motion events. This paper outlines technical considerations and discusses implementing the aforementioned semantic models into such a system.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00634",
        "title": "Orthographic Syllable as basic unit for SMT between Related Languages",
        "authors": [
            "Anoop Kunchukuttan",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00765",
        "title": "Distributed Representations of Lexical Sets and Prototypes in Causal Alternation Verbs",
        "authors": [
            "Edoardo Maria Ponti",
            "Elisabetta Jezek",
            "Bernardo Magnini"
        ],
        "abstract": "Lexical sets contain the words filling an argument slot of a verb, and are in part determined by selectional preferences. The purpose of this paper is to unravel the properties of lexical sets through distributional semantics. We investigate 1) whether lexical set behave as prototypical categories with a centre and a periphery; 2) whether they are polymorphic, i.e. composed by subcategories; 3) whether the distance between lexical sets of different arguments is explanatory of verb properties. In particular, our case study are lexical sets of causative-inchoative verbs in Italian. Having studied several vector models, we find that 1) based on spatial distance from the centroid, object fillers are scattered uniformly across the category, whereas intransitive subject fillers lie on its edge; 2) a correlation exists between the amount of verb senses and that of clusters discovered automatically, especially for intransitive subjects; 3) the distance between the centroids of object and intransitive subject is correlated with other properties of verbs, such as their cross-lingual tendency to appear in the intransitive pattern rather than transitive one. This paper is noncommittal with respect to the hypothesis that this connection is underpinned by a semantic reason, namely the spontaneity of the event denoted by the verb.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2020-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00842",
        "title": "Chinese Event Extraction Using DeepNeural Network with Word Embedding",
        "authors": [
            "Yandi Xia",
            "Yang Liu"
        ],
        "abstract": "A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well; 2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation in order to greatly reduce the effort of feature engineering. In addition, in this framework, we leverage large amount of unlabeled data, which can address the problem of limited labeled corpus for this task. Our experiments show that our proposed method performs better compared to the system using rich language features, and using unlabeled data benefits the word embeddings. This study suggests the potential of DNN and word embedding for the event extraction task.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00879",
        "title": "A Computational Approach to Automatic Prediction of Drunk Texting",
        "authors": [
            "Aditya Joshi",
            "Abhijit Mishra",
            "Balamurali AR",
            "Pushpak Bhattacharyya",
            "Mark Carman"
        ],
        "abstract": "Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00883",
        "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
        "authors": [
            "Aditya Joshi",
            "Vaibhav Tripathi",
            "Kevin Patel",
            "Pushpak Bhattacharyya",
            "Mark Carman"
        ],
        "abstract": "This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00956",
        "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
        "authors": [
            "Ondrej Bajgar",
            "Rudolf Kadlec",
            "Jan Kleindienst"
        ],
        "abstract": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01030",
        "title": "Applications of Online Deep Learning for Crisis Response Using Social Media Information",
        "authors": [
            "Dat Tien Nguyen",
            "Shafiq Joty",
            "Muhammad Imran",
            "Hassan Sajjad",
            "Prasenjit Mitra"
        ],
        "abstract": "During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01108",
        "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Tomasz Dwojak",
            "Hieu Hoang"
        ],
        "abstract": "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01247",
        "title": "ECAT: Event Capture Annotation Tool",
        "authors": [
            "Tuan Do",
            "Nikhil Krishnaswamy",
            "James Pustejovsky"
        ],
        "abstract": "This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01291",
        "title": "Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or Lexical Resources?",
        "authors": [
            "Christophe Servan",
            "Alexandre Berard",
            "Zied Elloumi",
            "Herv\u00e9 Blanchon",
            "Laurent Besacier"
        ],
        "abstract": "This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. This metric enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01367",
        "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models",
        "authors": [
            "Mahdi Khademian",
            "Mohammad Mehdi Homayounpour"
        ],
        "abstract": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past super-human results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first super-human system and 2.7% absolute task performance improvement compared to its recent competitor.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01486",
        "title": "A tentative model for dimensionless phoneme distance from binary distinctive features",
        "authors": [
            "Tiago Tresoldi"
        ],
        "abstract": "This work proposes a tentative model for the calculation of dimensionless distances between phonemes; sounds are described with binary distinctive features and distances show linear consistency in terms of such features. The model can be used as a scoring function for local and global pairwise alignment of phoneme sequences, and the distances can be used as prior probabilities for Bayesian analyses on the phylogenetic relationship between languages, particularly for cognate identification in cases where no empirical prior probability is available.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01508",
        "title": "VoxML: A Visualization Modeling Language",
        "authors": [
            "James Pustejovsky",
            "Nikhil Krishnaswamy"
        ],
        "abstract": "We present the specification for a modeling language, VoxML, which encodes semantic knowledge of real-world objects represented as three-dimensional models, and of events and attributes related to and enacted over these objects. VoxML is intended to overcome the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a variety of systems and platforms, leading to multimodal simulations of real-world scenarios using conceptual objects that represent their semantic values.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01520",
        "title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database",
        "authors": [
            "Edgar Altszyler",
            "Mariano Sigman",
            "Sidarta Ribeiro",
            "Diego Fern\u00e1ndez Slezak"
        ],
        "abstract": "Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01546",
        "title": "Conversational Recommendation System with Unsupervised Learning",
        "authors": [
            "Yueming Sun",
            "Yi Zhang",
            "Yunfei Chen",
            "Roger Jin"
        ],
        "abstract": "We will demonstrate a conversational products recommendation agent. This system shows how we combine research in personalized recommendation systems with research in dialogue systems to build a virtual sales agent. Based on new deep learning technologies we developed, the virtual agent is capable of learning how to interact with users, how to answer user questions, what is the next question to ask, and what to recommend when chatting with a human user.\n",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01588",
        "title": "Neural Structural Correspondence Learning for Domain Adaptation",
        "authors": [
            "Yftah Ziser",
            "Roi Reichart"
        ],
        "abstract": "Domain adaptation, adapting models from domains rich in labeled training data to domains poor in such data, is a fundamental NLP challenge. We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks. Particularly, our model is a three-layer neural network that learns to encode the nonpivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. On the task of cross-domain product sentiment classification (Blitzer et al., 2007), consisting of 12 domain pairs, our model outperforms both the SCL and the marginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods by 3.77% and 2.17% respectively, on average across domain pairs.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01713",
        "title": "Generating Simulations of Motion Events from Verbal Descriptions",
        "authors": [
            "James Pustejovsky",
            "Nikhil Krishnaswamy"
        ],
        "abstract": "In this paper, we describe a computational model for motion events in natural language that maps from linguistic expressions, through a dynamic event interpretation, into three-dimensional temporal simulations in a model. Starting with the model from (Pustejovsky and Moszkowicz, 2011), we analyze motion events using temporally-traced Labelled Transition Systems. We model the distinction between path- and manner-motion in an operational semantics, and further distinguish different types of manner-of-motion verbs in terms of the mereo-topological relations that hold throughout the process of movement. From these representations, we generate minimal models, which are realized as three-dimensional simulations in software developed with the game engine, Unity. The generated simulations act as a conceptual \"debugger\" for the semantics of different motion verbs: that is, by testing for consistency and informativeness in the model, simulations expose the presuppositions associated with linguistic expressions and their compositions. Because the model generation component is still incomplete, this paper focuses on an implementation which maps directly from linguistic interpretations into the Unity code snippets that create the simulations.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01720",
        "title": "Automatic Detection of Small Groups of Persons, Influential Members, Relations and Hierarchy in Written Conversations Using Fuzzy Logic",
        "authors": [
            "French Pope III",
            "Rouzbeh A. Shirvani",
            "Mugizi Robert Rwebangira",
            "Mohamed Chouikha",
            "Ayo Taylor",
            "Andres Alarcon Ramirez",
            "Amirsina Torfi"
        ],
        "abstract": "Nowadays a lot of data is collected in online forums. One of the key tasks is to determine the social structure of these online groups, for example the identification of subgroups within a larger group. We will approach the grouping of individual as a classification problem. The classifier will be based on fuzzy logic. The input to the classifier will be linguistic features and degree of relationships (among individuals). The output of the classifiers are the groupings of individuals. We also incorporate a method that ranks the members of the detected subgroup to identify the hierarchies in each subgroup. Data from the HBO television show The Wire is used to analyze the efficacy and usefulness of fuzzy logic based methods as alternative methods to classical statistical methods usually used for these problems. The proposed methodology could detect automatically the most influential members of each organization The Wire with 90% accuracy.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01858",
        "title": "A Robust Framework for Classifying Evolving Document Streams in an Expert-Machine-Crowd Setting",
        "authors": [
            "Muhammad Imran",
            "Sanjay Chawla",
            "Carlos Castillo"
        ],
        "abstract": "An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01874",
        "title": "Neural-based Noise Filtering from Word Embeddings",
        "authors": [
            "Kim Anh Nguyen",
            "Sabine Schulte im Walde",
            "Ngoc Thang Vu"
        ],
        "abstract": "Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01891",
        "title": "A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text",
        "authors": [
            "Sadikin Mujiono",
            "Mohamad Ivan Fanany",
            "Chan Basaruddin"
        ],
        "abstract": "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01910",
        "title": "Toward Automatic Understanding of the Function of Affective Language in Support Groups",
        "authors": [
            "Amit Navindgi",
            "Caroline Brun",
            "C\u00e9cile Boulard Masson",
            "Scott Nowson"
        ],
        "abstract": "Understanding expressions of emotions in support forums has considerable value and NLP methods are key to automating this. Many approaches understandably use subjective categories which are more fine-grained than a straightforward polarity-based spectrum. However, the definition of such categories is non-trivial and, in fact, we argue for a need to incorporate communicative elements even beyond subjectivity. To support our position, we report experiments on a sentiment-labelled corpus of posts taken from a medical support forum. We argue that not only is a more fine-grained approach to text analysis important, but simultaneously recognising the social function behind affective expressions enable a more accurate and valuable level of understanding.\n    ",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02003",
        "title": "Scalable Machine Translation in Memory Constrained Environments",
        "authors": [
            "Paul Baltescu"
        ],
        "abstract": "Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems.\n",
        "submission_date": "2016-10-06T00:00:00",
        "last_modified_date": "2016-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02124",
        "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction",
        "authors": [
            "Courtney Napoles",
            "Keisuke Sakaguchi",
            "Joel Tetreault"
        ],
        "abstract": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02209",
        "title": "Morphology Generation for Statistical Machine Translation using Deep Learning Techniques",
        "authors": [
            "Marta R. Costa-juss\u00e0",
            "Carlos Escolano"
        ],
        "abstract": "Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2017-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02213",
        "title": "Challenges of Computational Processing of Code-Switching",
        "authors": [
            "\u00d6zlem \u00c7etino\u011flu",
            "Sarah Schulz",
            "Ngoc Thang Vu"
        ],
        "abstract": "This paper addresses challenges of Natural Language Processing (NLP) on non-canonical multilingual data in which two or more languages are mixed. It refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community. We report our experience that cov- ers not only core NLP tasks such as normalisation, language identification, language modelling, part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition. We highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02493",
        "title": "A Semantic Analyzer for the Comprehension of the Spontaneous Arabic Speech",
        "authors": [
            "Mourad Mars",
            "Mounir Zrigui",
            "Mohamed Belgacem",
            "Anis Zouaghi"
        ],
        "abstract": "This work is part of a large research project entitled \"Or\u00e9odule\" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to improve the probabilistic model on which our semantic decoder is based. To achieve this goal, we have decided to test the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The findings are quite satisfactory.\n    ",
        "submission_date": "2016-10-08T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02544",
        "title": "Computational linking theory",
        "authors": [
            "Aaron Steven White",
            "Drew Reisinger",
            "Rachel Rudinger",
            "Kyle Rawlins",
            "Benjamin Van Durme"
        ],
        "abstract": "A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes.\n    ",
        "submission_date": "2016-10-08T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02567",
        "title": "Mining the Web for Pharmacovigilance: the Case Study of Duloxetine and Venlafaxine",
        "authors": [
            "Abbas Chokor",
            "Abeed Sarker",
            "Graciela Gonzalez"
        ],
        "abstract": "Adverse reactions caused by drugs following their release into the market are among the leading causes of death in many countries. The rapid growth of electronically available health related information, and the ability to process large volumes of them automatically, using natural language processing (NLP) and machine learning algorithms, have opened new opportunities for pharmacovigilance. Survey found that more than 70% of US Internet users consult the Internet when they require medical information. In recent years, research in this area has addressed for Adverse Drug Reaction (ADR) pharmacovigilance using social media, mainly Twitter and medical forums and websites. This paper will show the information which can be collected from a variety of Internet data sources and search engines, mainly Google Trends and Google Correlate. While considering the case study of two popular Major depressive Disorder (MDD) drugs, Duloxetine and Venlafaxine, we will provide a comparative analysis for their reactions using publicly-available alternative data sources.\n    ",
        "submission_date": "2016-10-08T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02633",
        "title": "Enabling Medical Translation for Low-Resource Languages",
        "authors": [
            "Ahmad Musleh",
            "Nadir Durrani",
            "Irina Temnikova",
            "Preslav Nakov",
            "Stephan Vogel",
            "Osama Alsaad"
        ],
        "abstract": "We present research towards bridging the language gap between migrant workers in Qatar and medical staff. In particular, we present the first steps towards the development of a real-world Hindi-English machine translation system for doctor-patient communication. As this is a low-resource language pair, especially for speech and for the medical domain, our initial focus has been on gathering suitable training data from various sources. We applied a variety of methods ranging from fully automatic extraction from the Web to manual annotation of test data. Moreover, we developed a method for automatically augmenting the training data with synthetically generated variants, which yielded a very sizable improvement of more than 3 BLEU points absolute.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02683",
        "title": "Interpreting Neural Networks to Improve Politeness Comprehension",
        "authors": [
            "Malika Aubakirova",
            "Mohit Bansal"
        ],
        "abstract": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02692",
        "title": "Open-Ended Visual Question-Answering",
        "authors": [
            "Issey Masuda",
            "Santiago Pascual de la Puente",
            "Xavier Giro-i-Nieto"
        ],
        "abstract": "This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02749",
        "title": "A Dynamic Window Neural Network for CCG Supertagging",
        "authors": [
            "Huijia Wu",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02751",
        "title": "A New Theoretical and Technological System of Imprecise-Information Processing",
        "authors": [
            "Shiyou Lian"
        ],
        "abstract": "Imprecise-information processing will play an indispensable role in intelligent systems, especially in the anthropomorphic intelligent systems (as intelligent robots). A new theoretical and technological system of imprecise-information processing has been founded in Principles of Imprecise-Information Processing: A New Theoretical and Technological System[1] which is different from fuzzy technology. The system has clear hierarchy and rigorous structure, which results from the formation principle of imprecise information and has solid mathematical and logical bases, and which has many advantages beyond fuzzy technology. The system provides a technological platform for relevant applications and lays a theoretical foundation for further research.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02806",
        "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder",
        "authors": [
            "Yao Zhou",
            "Cong Liu",
            "Yan Pan"
        ],
        "abstract": "We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03017",
        "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
        "authors": [
            "Jason Lee",
            "Kyunghyun Cho",
            "Thomas Hofmann"
        ],
        "abstract": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2017-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03022",
        "title": "Very Deep Convolutional Networks for End-to-End Speech Recognition",
        "authors": [
            "Yu Zhang",
            "William Chan",
            "Navdeep Jaitly"
        ],
        "abstract": "Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5\\% word error rate without any dictionary or language using a 15 layer deep network.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03098",
        "title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks",
        "authors": [
            "Aaditya Prakash",
            "Sadid A. Hasan",
            "Kathy Lee",
            "Vivek Datla",
            "Ashequl Qadir",
            "Joey Liu",
            "Oladimeji Farri"
        ],
        "abstract": "In this paper, we propose a novel neural approach for paraphrase generation. Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03106",
        "title": "Supervised Term Weighting Metrics for Sentiment Analysis in Short Text",
        "authors": [
            "Hussam Hamdan",
            "Patrice Bellot",
            "Frederic Bechet"
        ],
        "abstract": "Term weighting metrics assign weights to terms in order to discriminate the important terms from the less crucial ones. Due to this characteristic, these metrics have attracted growing attention in text classification and recently in sentiment analysis. Using the weights given by such metrics could lead to more accurate document representation which may improve the performance of the classification. While previous studies have focused on proposing or comparing different weighting metrics at two-classes document level sentiment analysis, this study propose to analyse the results given by each metric in order to find out the characteristics of good and bad weighting metrics. Therefore we present an empirical study of fifteen global supervised weighting metrics with four local weighting metrics adopted from information retrieval, we also give an analysis to understand the behavior of each metric by observing and analysing how each metric distributes the terms and deduce some characteristics which may distinguish the good and bad metrics. The evaluation has been done using Support Vector Machine on three different datasets: Twitter, restaurant and laptop reviews.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03112",
        "title": "Leveraging Recurrent Neural Networks for Multimodal Recognition of Social Norm Violation in Dialog",
        "authors": [
            "Tiancheng Zhao",
            "Ran Zhao",
            "Zhao Meng",
            "Justine Cassell"
        ],
        "abstract": "Social norms are shared rules that govern and facilitate social interaction. Violating such social norms via teasing and insults may serve to upend power imbalances or, on the contrary reinforce solidarity and rapport in conversation, rapport which is highly situated and context-dependent. In this work, we investigate the task of automatically identifying the phenomena of social norm violation in discourse. Towards this goal, we leverage the power of recurrent neural networks and multimodal information present in the interaction, and propose a predictive model to recognize social norm violation. Using long-term temporal and contextual information, our model achieves an F1 score of 0.705. Implications of our work regarding developing a social-aware agent are discussed.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03120",
        "title": "Correlation-Based Method for Sentiment Classification",
        "authors": [
            "Hussam Hamdan"
        ],
        "abstract": "The classic supervised classification algorithms are efficient, but time-consuming, complicated and not interpretable, which makes it difficult to analyze their results that limits the possibility to improve them based on real observations. In this paper, we propose a new and a simple classifier to predict a sentiment label of a short text. This model keeps the capacity of human interpret-ability and can be extended to integrate NLP techniques in a more interpretable way. Our model is based on a correlation metric which measures the degree of association between a sentiment label and a word. Ten correlation metrics are proposed and evaluated intrinsically. And then a classifier based on each metric is proposed, evaluated and compared to the classic classification algorithms which have proved their performance in many studies. Our model outperforms these algorithms with several correlation metrics.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2018-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03165",
        "title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for Large Vocabulary Speech Recognition",
        "authors": [
            "Xiangang Li",
            "Xihong Wu"
        ],
        "abstract": "Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03167",
        "title": "An Empirical Exploration of Skip Connections for Sequential Tagging",
        "authors": [
            "Huijia Wu",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03246",
        "title": "Toward a new instances of NELL",
        "authors": [
            "Maisa C. Duarte",
            "Pierre Maret"
        ],
        "abstract": "We are developing the method to start new instances of NELL in various languages and develop then NELL multilingualism. We base our method on our experience on NELL Portuguese and NELL French. This reports explain our method and develops some research perspectives.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03256",
        "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training",
        "authors": [
            "G\u00e1bor Gosztolya",
            "Tam\u00e1s Gr\u00f3sz",
            "L\u00e1szl\u00f3 T\u00f3th"
        ],
        "abstract": "Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03321",
        "title": "Keystroke dynamics as signal for shallow syntactic parsing",
        "authors": [
            "Barbara Plank"
        ],
        "abstract": "Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models?\n",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03342",
        "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning",
        "authors": [
            "Lieke Gelderloos",
            "Grzegorz Chrupa\u0142a"
        ],
        "abstract": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03349",
        "title": "Survey on the Use of Typological Information in Natural Language Processing",
        "authors": [
            "Helen O'Horan",
            "Yevgeni Berzak",
            "Ivan Vuli\u0107",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "abstract": "In recent years linguistic typology, which classifies the world's languages according to their functional and structural properties, has been widely used to support multilingual NLP. While the growing importance of typological information in supporting multilingual tasks has been recognised, no systematic survey of existing typological resources and their use in NLP has been published. This paper provides such a survey as well as discussion which we hope will both inform and inspire future work in the area.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03585",
        "title": "A Paradigm for Situated and Goal-Driven Language Learning",
        "authors": [
            "Jon Gauthier",
            "Igor Mordatch"
        ],
        "abstract": "A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03750",
        "title": "Semi-supervised Discovery of Informative Tweets During the Emerging Disasters",
        "authors": [
            "Shanshan Zhang",
            "Slobodan Vucetic"
        ],
        "abstract": "The first objective towards the effective use of microblogging services such as Twitter for situational awareness during the emerging disasters is discovery of the disaster-related postings. Given the wide range of possible disasters, using a pre-selected set of disaster-related keywords for the discovery is suboptimal. An alternative that we focus on in this work is to train a classifier using a small set of labeled postings that are becoming available as a disaster is emerging. Our hypothesis is that utilizing large quantities of historical microblogs could improve the quality of classification, as compared to training a classifier only on the labeled data. We propose to use unlabeled microblogs to cluster words into a limited number of clusters and use the word clusters as features for classification. To evaluate the proposed semi-supervised approach, we used Twitter data from 6 different disasters. Our results indicate that when the number of labeled tweets is 100 or less, the proposed approach is superior to the standard classification based on the bag or words feature representation. Our results also reveal that the choice of the unlabeled corpus, the choice of word clustering algorithm, and the choice of hyperparameters can have a significant impact on the classification accuracy.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03759",
        "title": "Language Models with Pre-Trained (GloVe) Word Embeddings",
        "authors": [
            "Victor Makarenkov",
            "Bracha Shapira",
            "Lior Rokach"
        ],
        "abstract": "In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2017-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03771",
        "title": "SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods",
        "authors": [
            "Marzieh Saeidi",
            "Guillaume Bouchard",
            "Maria Liakata",
            "Sebastian Riedel"
        ],
        "abstract": "In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03807",
        "title": "Question Generation from a Knowledge Base with Web Exploration",
        "authors": [
            "Linfeng Song",
            "Lin Zhao"
        ],
        "abstract": "Question generation from a knowledge base (KB) is the task of generating questions related to the domain of the input KB. We propose a system for generating fluent and natural questions from a KB, which significantly reduces the human effort by leveraging massive web resources. In more detail, a seed question set is first generated by applying a small number of hand-crafted templates on the input KB, then more questions are retrieved by iteratively forming already obtained questions as search queries into a standard search engine, before finally questions are selected by estimating their fluency and domain relevance. Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of \\newcite{serban-EtAl:2016:P16-1} by human graders.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03934",
        "title": "A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder",
        "authors": [
            "Hans Krupakar",
            "Keerthika Rajvel",
            "Bharathi B",
            "Angel Deborah S",
            "Vallidevi Krishnamurthy"
        ],
        "abstract": "Speech Translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice ear-piece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03946",
        "title": "A Neural Network for Coordination Boundary Prediction",
        "authors": [
            "Jessica Ficler",
            "Yoav Goldberg"
        ],
        "abstract": "We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03950",
        "title": "Compressing Neural Language Models by Sparse Word Representations",
        "authors": [
            "Yunchuan Chen",
            "Lili Mou",
            "Yan Xu",
            "Ge Li",
            "Zhi Jin"
        ],
        "abstract": "Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03955",
        "title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling",
        "authors": [
            "Yiping Song",
            "Lili Mou",
            "Rui Yan",
            "Li Yi",
            "Zinan Zhu",
            "Xiaohua Hu",
            "Ming Zhang"
        ],
        "abstract": "In human-computer conversation systems, the context of a user-issued utterance is particularly important because it provides useful background information of the conversation. However, it is unwise to track all previous utterances in the current session as not all of them are equally important. In this paper, we address the problem of session segmentation. We propose an embedding-enhanced TextTiling approach, inspired by the observation that conversation utterances are highly noisy, and that word embeddings provide a robust way of capturing semantics. Experimental results show that our approach achieves better performance than the TextTiling, MMD approaches.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04211",
        "title": "Gated End-to-End Memory Networks",
        "authors": [
            "Julien Perez",
            "Fei Liu"
        ],
        "abstract": "Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04265",
        "title": "Fast, Scalable Phrase-Based SMT Decoding",
        "authors": [
            "Hieu Hoang",
            "Nikolay Bogoychev",
            "Lane Schwartz",
            "Marcin Junczys-Dowmunt"
        ],
        "abstract": "The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers.\n",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04345",
        "title": "A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts",
        "authors": [
            "Fei Liu",
            "Julien Perez",
            "Scott Nowson"
        ],
        "abstract": "Many methods have been used to recognize author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines. This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04377",
        "title": "Civique: Using Social Media to Detect Urban Emergencies",
        "authors": [
            "Diptesh Kanojia",
            "Vishwajeet Kumar",
            "Krithi Ramamritham"
        ],
        "abstract": "We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like \"fire\", \"accident\", \"earthquake\", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique using a web interface and on an Android application, in realtime, and show its use for both tweet detection and visualization.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04416",
        "title": "Distributional Inclusion Hypothesis for Tensor-based Composition",
        "authors": [
            "Dimitri Kartsaklis",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2016-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04841",
        "title": "Translation Quality Estimation using Recurrent Neural Network",
        "authors": [
            "Raj Nath Patel",
            "Sasikumar M"
        ],
        "abstract": "This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system.\n    ",
        "submission_date": "2016-10-16T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04989",
        "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification",
        "authors": [
            "Jiacheng Xu",
            "Danlu Chen",
            "Xipeng Qiu",
            "Xuangjing Huang"
        ],
        "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05011",
        "title": "Interactive Attention for Neural Machine Translation",
        "authors": [
            "Fandong Meng",
            "Zhengdong Lu",
            "Hang Li",
            "Qun Liu"
        ],
        "abstract": "Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05150",
        "title": "Neural Machine Translation Advised by Statistical Machine Translation",
        "authors": [
            "Xing Wang",
            "Zhengdong Lu",
            "Zhaopeng Tu",
            "Hang Li",
            "Deyi Xiong",
            "Min Zhang"
        ],
        "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu et al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05243",
        "title": "Pre-Translation for Neural Machine Translation",
        "authors": [
            "Jan Niehues",
            "Eunah Cho",
            "Thanh-Le Ha",
            "Alex Waibel"
        ],
        "abstract": "Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur.\n",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05256",
        "title": "Achieving Human Parity in Conversational Speech Recognition",
        "authors": [
            "W. Xiong",
            "J. Droppo",
            "X. Huang",
            "F. Seide",
            "M. Seltzer",
            "A. Stolcke",
            "D. Yu",
            "G. Zweig"
        ],
        "abstract": "Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and LSTM acoustic model architectures, combined with a novel spatial smoothing method and lattice-free MMI acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05361",
        "title": "End-to-end attention-based distant speech recognition with Highway LSTM",
        "authors": [
            "Hassan Taherian"
        ],
        "abstract": "End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attention-based models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task.\n    ",
        "submission_date": "2016-10-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05461",
        "title": "Personalized Machine Translation: Preserving Original Author Traits",
        "authors": [
            "Ella Rabinovich",
            "Shachar Mirkin",
            "Raj Nath Patel",
            "Lucia Specia",
            "Shuly Wintner"
        ],
        "abstract": "The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author's gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2017-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05522",
        "title": "Addressing Community Question Answering in English and Arabic",
        "authors": [
            "Giovanni Da San Martino",
            "Alberto Barr\u00f3n-Cede\u00f1o",
            "Salvatore Romeo",
            "Alessandro Moschitti",
            "Shafiq Joty",
            "Fahad A. Al Obaidli",
            "Kateryna Tymoshenko",
            "Antonio Uva"
        ],
        "abstract": "This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval-2016 Task 3 on \"Community Question Answering\". Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that i) the shallow structures used in our TKs are robust enough to noisy data and ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05540",
        "title": "SYSTRAN's Pure Neural Machine Translation Systems",
        "authors": [
            "Josep Crego",
            "Jungi Kim",
            "Guillaume Klein",
            "Anabel Rebollo",
            "Kathy Yang",
            "Jean Senellart",
            "Egor Akhanov",
            "Patrice Brunelle",
            "Aurelien Coquard",
            "Yongchao Deng",
            "Satoshi Enoue",
            "Chiyo Geiss",
            "Joshua Johanson",
            "Ardas Khalsa",
            "Raoum Khiari",
            "Byeongil Ko",
            "Catherine Kobus",
            "Jean Lorieux",
            "Leidiana Martins",
            "Dang-Chuan Nguyen",
            "Alexandra Priori",
            "Thomas Riccardi",
            "Natalia Segal",
            "Christophe Servan",
            "Cyril Tiquet",
            "Bo Wang",
            "Jin Yang",
            "Dakun Zhang",
            "Jing Zhou",
            "Peter Zoldan"
        ],
        "abstract": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work.\n",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05652",
        "title": "Vietnamese Named Entity Recognition using Token Regular Expressions and Bidirectional Inference",
        "authors": [
            "Phuong Le-Hong"
        ],
        "abstract": "This paper describes an efficient approach to improve the accuracy of a named entity recognition system for Vietnamese. The approach combines regular expressions over tokens and a bidirectional inference method in a sequence labelling model. The proposed method achieves an overall $F_1$ score of 89.66% on a test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05670",
        "title": "Stylometric Analysis of Early Modern Period English Plays",
        "authors": [
            "Mark Eisen",
            "Santiago Segarra",
            "Gabriel Egan",
            "Alejandro Ribeiro"
        ],
        "abstract": "Function word adjacency networks (WANs) are used to study the authorship of plays from the Early Modern English period. In these networks, nodes are function words and directed edges between two nodes represent the relative frequency of directed co-appearance of the two words. For every analyzed play, a WAN is constructed and these are aggregated to generate author profile networks. We first study the similarity of writing styles between Early English playwrights by comparing the profile WANs. The accuracy of using WANs for authorship attribution is then demonstrated by attributing known plays among six popular playwrights. Moreover, the WAN method is shown to outperform other frequency-based methods on attributing Early English plays. In addition, WANs are shown to be reliable classifiers even when attributing collaborative plays. For several plays of disputed co-authorship, a deeper analysis is performed by attributing every act and scene separately, in which we both corroborate existing breakdowns and provide evidence of new assignments.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2017-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05688",
        "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models",
        "authors": [
            "Pranay Dighe",
            "Afsaneh Asaei",
            "Herve Bourlard"
        ],
        "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05812",
        "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition",
        "authors": [
            "Liang Lu",
            "Steve Renals"
        ],
        "abstract": "State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05858",
        "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction",
        "authors": [
            "Raghavendra Chalapathy",
            "Ehsan Zare Borzeshi",
            "Massimo Piccardi"
        ],
        "abstract": "Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06053",
        "title": "Chinese Restaurant Process for cognate clustering: A threshold free approach",
        "authors": [
            "Taraka Rama"
        ],
        "abstract": "In this paper, we introduce a threshold free approach, motivated from Chinese Restaurant Process, for the purpose of cognate clustering. We show that our approach yields similar results to a linguistically motivated cognate clustering system known as LexStat. Our Chinese Restaurant Process system is fast and does not require any threshold and can be applied to any language family of the world.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06210",
        "title": "A Theme-Rewriting Approach for Generating Algebra Word Problems",
        "authors": [
            "Rik Koncel-Kedziorski",
            "Ioannis Konstas",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\\it rewriting} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06227",
        "title": "Cross-Lingual Syntactic Transfer with Limited Resources",
        "authors": [
            "Mohammad Sadegh Rasooli",
            "Michael Collins"
        ],
        "abstract": "We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the state-of-the-art in several languages used in previous work, in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets from the Universal Dependencies corpora.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2017-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06272",
        "title": "Lexicon Integrated CNN Models with Attention for Sentiment Analysis",
        "authors": [
            "Bonggun Shin",
            "Timothy Lee",
            "Jinho D. Choi"
        ],
        "abstract": "With the advent of word embeddings, lexicons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting. This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention. Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank, and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow to build high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2017-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06370",
        "title": "Clinical Text Prediction with Numerically Grounded Conditional Language Models",
        "authors": [
            "Georgios P. Spithourakis",
            "Steffen E. Petersen",
            "Sebastian Riedel"
        ],
        "abstract": "Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06454",
        "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Hong Yu"
        ],
        "abstract": "Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2017-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06498",
        "title": "Authorship Attribution Based on Life-Like Network Automata",
        "authors": [
            "Jeaneth Machicao",
            "Edilson A. Corr\u00eaa Jr.",
            "Gisele H. B. Miranda",
            "Diego R. Amancio",
            "Odemir M. Bruno"
        ],
        "abstract": "The authorship attribution is a problem of considerable practical and technical interest. Several methods have been designed to infer the authorship of disputed documents in multiple contexts. While traditional statistical methods based solely on word counts and related measurements have provided a simple, yet effective solution in particular cases; they are prone to manipulation. Recently, texts have been successfully modeled as networks, where words are represented by nodes linked according to textual similarity measurements. Such models are useful to identify informative topological patterns for the authorship recognition task. However, there is no consensus on which measurements should be used. Thus, we proposed a novel method to characterize text networks, by considering both topological and dynamical aspects of networks. Using concepts and methods from cellular automata theory, we devised a strategy to grasp informative spatio-temporal patterns from this model. Our experiments revealed an outperformance over traditional analysis relying only on topological measurements. Remarkably, we have found a dependence of pre-processing steps (such as the lemmatization) on the obtained results, a feature that has mostly been disregarded in related works. The optimized results obtained here pave the way for a better characterization of textual networks.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06510",
        "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding",
        "authors": [
            "Anoop Kunchukuttan",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06540",
        "title": "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural Attention Models",
        "authors": [
            "Shubham Toshniwal",
            "Karen Livescu"
        ],
        "abstract": "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06542",
        "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016",
        "authors": [
            "Graham Neubig"
        ],
        "abstract": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06550",
        "title": "Neural Machine Translation with Characters and Hierarchical Encoding",
        "authors": [
            "Alexander Rosenberg Johansen",
            "Jonas Meinertz Hansen",
            "Elias Khazen Obeid",
            "Casper Kaae S\u00f8nderby",
            "Ole Winther"
        ],
        "abstract": "Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06601",
        "title": "An Approach to Speed-up the Word Sense Disambiguation Procedure through Sense Filtering",
        "authors": [
            "Alok Ranjan Pal",
            "Anupam Munshi",
            "Diganta Saha"
        ],
        "abstract": "In this paper, we are going to focus on speed up of the Word Sense Disambiguation procedure by filtering the relevant senses of an ambiguous word through Part-of-Speech Tagging. First, this proposed approach performs the Part-of-Speech Tagging operation before the disambiguation procedure using Bigram approximation. As a result, the exact Part-of-Speech of the ambiguous word at a particular text instance is derived. In the next stage, only those dictionary definitions (glosses) are retrieved from an online dictionary, which are associated with that particular Part-of-Speech to disambiguate the exact sense of the ambiguous word. In the training phase, we have used Brown Corpus for Part-of-Speech Tagging and WordNet as an online dictionary. The proposed approach reduces the execution time upto half (approximately) of the normal execution time for a text, containing around 200 sentences. Not only that, we have found several instances, where the correct sense of an ambiguous word is found for using the Part-of-Speech Tagging before the Disambiguation procedure.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06602",
        "title": "Iterative Refinement for Machine Translation",
        "authors": [
            "Roman Novak",
            "Michael Auli",
            "David Grangier"
        ],
        "abstract": "Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2018-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06620",
        "title": "Proposing Plausible Answers for Open-ended Visual Question Answering",
        "authors": [
            "Omid Bakhshandeh",
            "Trung Bui",
            "Zhe Lin",
            "Walter Chang"
        ],
        "abstract": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.\n    ",
        "submission_date": "2016-10-20T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06700",
        "title": "End-to-End Training Approaches for Discriminative Segmental Models",
        "authors": [
            "Hao Tang",
            "Weiran Wang",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "abstract": "Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses.\n",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07091",
        "title": "Automatic Identification of Sarcasm Target: An Introductory Approach",
        "authors": [
            "Aditya Joshi",
            "Pranav Goel",
            "Pushpak Bhattacharyya",
            "Mark Carman"
        ],
        "abstract": "Past work in computational sarcasm deals primarily with sarcasm detection. In this paper, we introduce a novel, related problem: sarcasm target identification i.e., extracting the target of ridicule in a sarcastic sentence). We present an introductory approach for sarcasm target identification. Our approach employs two types of extractors: one based on rules, and another consisting of a statistical classifier. To compare our approach, we use two baselines: a na\u00efve baseline and another baseline based on work in sentiment target identification. We perform our experiments on book snippets and tweets, and show that our hybrid approach performs better than the two baselines and also, in comparison with using the two extractors individually. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work.\n    ",
        "submission_date": "2016-10-22T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07149",
        "title": "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems",
        "authors": [
            "Yiping Song",
            "Rui Yan",
            "Xiang Li",
            "Dongyan Zhao",
            "Ming Zhang"
        ],
        "abstract": "Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.\n    ",
        "submission_date": "2016-10-23T00:00:00",
        "last_modified_date": "2016-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07272",
        "title": "Bridging Neural Machine Translation and Bilingual Dictionaries",
        "authors": [
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "abstract": "Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07363",
        "title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media",
        "authors": [
            "Arkaitz Zubiaga",
            "Maria Liakata",
            "Rob Procter"
        ],
        "abstract": "Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07365",
        "title": "Introduction: Cognitive Issues in Natural Language Processing",
        "authors": [
            "Thierry Poibeau",
            "Shravan Vasishth"
        ],
        "abstract": "This special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science. It specifically raises two questions: \"what is the potential contribution of computational language modeling to cognitive science?\" and conversely: \"what is the influence of cognitive science in contemporary computational linguistics?\"\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07418",
        "title": "Statistical Machine Translation for Indian Languages: Mission Hindi",
        "authors": [
            "Raj Nath Patel",
            "Prakash B. Pimpale",
            "Sasikumar M"
        ],
        "abstract": "This paper discusses Centre for Development of Advanced Computing Mumbai's (CDACM) submission to the NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of the contest was to explore the effectiveness of Statistical Machine Translation (SMT) for Indian language to Indian language and English-Hindi machine translation. In this paper, we have proposed that suffix separation and word splitting for SMT from agglutinative languages to Hindi significantly improves over the baseline (BL). We have also shown that the factored model with reordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We report our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi), Marathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for Health, Tourism, and General domains.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07420",
        "title": "Reordering rules for English-Hindi SMT",
        "authors": [
            "Raj Nath Patel",
            "Rohit Gupta",
            "Prakash B. Pimpale",
            "Sasikumar M"
        ],
        "abstract": "Reordering is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07569",
        "title": "Geometry of Polysemy",
        "authors": [
            "Jiaqi Mu",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07647",
        "title": "Learning to Reason With Adaptive Computation",
        "authors": [
            "Mark Neumann",
            "Pontus Stenetorp",
            "Sebastian Riedel"
        ],
        "abstract": "Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07651",
        "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation",
        "authors": [
            "Chunlei Zhang",
            "Fahimeh Bahmaninezhad",
            "Shivesh Ranjan",
            "Chengzhu Yu",
            "Navid Shokouhi",
            "John H.L. Hansen"
        ],
        "abstract": "This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled in-domain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07710",
        "title": "EmojiNet: Building a Machine Readable Sense Inventory for Emoji",
        "authors": [
            "Sanjaya Wijeratne",
            "Lakshika Balasuriya",
            "Amit Sheth",
            "Derek Doran"
        ],
        "abstract": "Emoji are a contemporary and extremely popular way to enhance electronic communication. Without rigid semantics attached to them, emoji symbols take on different meanings based on the context of a message. Thus, like the word sense disambiguation task in natural language processing, machines also need to disambiguate the meaning or sense of an emoji. In a first step toward achieving this goal, this paper presents EmojiNet, the first machine readable sense inventory for emoji. EmojiNet is a resource enabling systems to link emoji with their context-specific meaning. It is automatically constructed by integrating multiple emoji resources with BabelNet, which is the most comprehensive multilingual sense inventory available to date. The paper discusses its construction, evaluates the automatic resource creation process, and presents a use case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is available online for use at ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07796",
        "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks",
        "authors": [
            "Carsten Schnober",
            "Steffen Eger",
            "Erik-L\u00e2n Do Dinh",
            "Iryna Gurevych"
        ],
        "abstract": "We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07809",
        "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
        "authors": [
            "Florian Boudin",
            "Hugo Mougard",
            "Damien Cram"
        ],
        "abstract": "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07844",
        "title": "Improving historical spelling normalization with bi-directional LSTMs and multi-task learning",
        "authors": [
            "Marcel Bollmann",
            "Anders S\u00f8gaard"
        ],
        "abstract": "Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model's performance further.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07918",
        "title": "Sequence Segmentation Using Joint RNN and Structured Prediction Models",
        "authors": [
            "Yossi Adi",
            "Joseph Keshet",
            "Emily Cibelli",
            "Matthew Goldrick"
        ],
        "abstract": "We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results sug- gest the proposed model is superior to previous methods, ob- taining state-of-the-art results on the tested datasets.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08000",
        "title": "Statistical Machine Translation for Indian Languages: Mission Hindi 2",
        "authors": [
            "Raj Nath Patel",
            "Prakash B. Pimpale"
        ],
        "abstract": "This paper presents Centre for Development of Advanced Computing Mumbai's (CDACM) submission to NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the contest was to collectively explore the effectiveness of Statistical Machine Translation (SMT) while translating within Indian languages and between English and Indian languages. In this paper, we report our work on all five language pairs, namely Bengali-Hindi (\\bnhi), Marathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and English-Hindi (\\enhi) for Health, Tourism, and General domains. We have used suffix separation, compound splitting and preordering prior to SMT training and testing.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08078",
        "title": "Dis-S2V: Discourse Informed Sen2Vec",
        "authors": [
            "Tanay Kumar Saha",
            "Shafiq Joty",
            "Naeemul Hassan",
            "Mohammad Al Hasan"
        ],
        "abstract": "Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large.\n",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08375",
        "title": "Content Selection in Data-to-Text Systems: A Survey",
        "authors": [
            "Dimitra Gkatzia"
        ],
        "abstract": "Data-to-text systems are powerful in generating reports from data automatically and thus they simplify the presentation of complex data. Rather than presenting data using visualisation techniques, data-to-text systems use natural (human) language, which is the most common way for human-human communication. In addition, data-to-text systems can adapt their output content to users' preferences, background or interests and therefore they can be pleasant for users to interact with. Content selection is an important part of every data-to-text system, because it is the module that determines which from the available information should be conveyed to the user. This survey initially introduces the field of data-to-text generation, describes the general data-to-text system architecture and then it reviews the state-of-the-art content selection methods. Finally, it provides recommendations for choosing an approach and discusses opportunities for future research.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08431",
        "title": "Broad Context Language Modeling as Reading Comprehension",
        "authors": [
            "Zewei Chu",
            "Hai Wang",
            "Kevin Gimpel",
            "David McAllester"
        ],
        "abstract": "Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2017-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08462",
        "title": "Distraction-Based Neural Networks for Document Summarization",
        "authors": [
            "Qian Chen",
            "Xiaodan Zhu",
            "Zhenhua Ling",
            "Si Wei",
            "Hui Jiang"
        ],
        "abstract": "Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08557",
        "title": "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept Embeddings",
        "authors": [
            "A.K.M. Sabbir",
            "Antonio Jimeno Yepes",
            "Ramakanth Kavuluru"
        ],
        "abstract": "Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledge-based approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the MSH WSD dataset as the test set. Our methods involve weak supervision - we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing well known named entity recognition and concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is an absolute 3% improvement over the best known results obtained via unsupervised or knowledge-based means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves an accuracy of 94.34%. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here, we show that weak supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD. However, external knowledge bases (here sense inventories) play a key role in the improvements achieved.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2017-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08694",
        "title": "CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional Method for the Identification of Semantic Relations",
        "authors": [
            "Vered Shwartz",
            "Ido Dagan"
        ],
        "abstract": "We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08763",
        "title": "CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases",
        "authors": [
            "Xiang Ren",
            "Zeqiu Wu",
            "Wenqi He",
            "Meng Qu",
            "Clare R. Voss",
            "Heng Ji",
            "Tarek F. Abdelzaher",
            "Jiawei Han"
        ],
        "abstract": "Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object \"translation\" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08815",
        "title": "A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks",
        "authors": [
            "Soujanya Poria",
            "Erik Cambria",
            "Devamanyu Hazarika",
            "Prateek Vij"
        ],
        "abstract": "Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an \"apparently positive\" sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network's baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08914",
        "title": "Ex Machina: Personal Attacks Seen at Scale",
        "authors": [
            "Ellery Wulczyn",
            "Nithum Thain",
            "Lucas Dixon"
        ],
        "abstract": "The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09091",
        "title": "Representation Learning Models for Entity Search",
        "authors": [
            "Shijia E",
            "Yang Xiang",
            "Mohan Zhang"
        ],
        "abstract": "We focus on the problem of learning distributed representations for entity search queries, named entities, and their short descriptions. With our representation learning models, the entity search query, named entity and description can be represented as low-dimensional vectors. Our goal is to develop a simple but effective model that can make the distributed representations of query related entities similar to the query in the vector space. Hence, we propose three kinds of learning strategies, and the difference between them mainly lies in how to deal with the relationship between an entity and its description. We analyze the strengths and weaknesses of each learning strategy and validate our methods on public datasets which contain four kinds of named entities, i.e., movies, TV shows, restaurants and celebrities. The experimental results indicate that our proposed methods can adapt to different types of entity search queries, and outperform the current state-of-the-art methods based on keyword matching and vanilla word2vec models. Besides, the proposed methods can be trained fast and be easily extended to other similar tasks.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09158",
        "title": "Towards a continuous modeling of natural language domains",
        "authors": [
            "Sebastian Ruder",
            "Parsa Ghaffari",
            "John G. Breslin"
        ],
        "abstract": "Humans continuously adapt their style and language to a variety of domains. However, a reliable definition of `domain' has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learning-based models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09226",
        "title": "Text Segmentation using Named Entity Recognition and Co-reference Resolution in English and Greek Texts",
        "authors": [
            "Pavlina Fragkou"
        ],
        "abstract": "In this paper we examine the benefit of performing named entity recognition (NER) and co-reference resolution to an English and a Greek corpus used for text segmentation. The aim here is to examine whether the combination of text segmentation and information extraction can be beneficial for the identification of the various topics that appear in a document. NER was performed manually in the English corpus and was compared with the output produced by publicly available annotation tools while, an already existing tool was used for the Greek corpus. Produced annotations from both corpora were manually corrected and enriched to cover four types of named entities. Co-reference resolution i.e., substitution of every reference of the same instance with the same named entity identifier was subsequently performed. The evaluation, using five text segmentation algorithms for the English corpus and four for the Greek corpus leads to the conclusion that, the benefit highly depends on the segment's topic, the number of named entity instances appearing in it, as well as the segment's length.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09333",
        "title": "Word Embeddings for the Construction Domain",
        "authors": [
            "Antoine J.-P. Tixier",
            "Michalis Vazirgiannis",
            "Matthew R. Hallowell"
        ],
        "abstract": "We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful construction-specific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09565",
        "title": "Sequence-to-sequence neural network models for transliteration",
        "authors": [
            "Mihaela Rosca",
            "Thomas Breuel"
        ],
        "abstract": "Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models.\n    ",
        "submission_date": "2016-10-29T00:00:00",
        "last_modified_date": "2016-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09704",
        "title": "Feature-Augmented Neural Networks for Patient Note De-identification",
        "authors": [
            "Ji Young Lee",
            "Franck Dernoncourt",
            "Ozlem Uzuner",
            "Peter Szolovits"
        ],
        "abstract": "Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain.\n    ",
        "submission_date": "2016-10-30T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09722",
        "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources",
        "authors": [
            "Jason Naradowsky",
            "Sebastian Riedel"
        ],
        "abstract": "In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain.\n",
        "submission_date": "2016-10-30T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09756",
        "title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Scarcity",
        "authors": [
            "Vinayak Athavale",
            "Shreenivas Bharadwaj",
            "Monik Pamecha",
            "Ameya Prabhu",
            "Manish Shrivastava"
        ],
        "abstract": "In this paper we describe an end to end Neural Model for Named Entity Recognition NER) which is based on Bi-Directional RNN-LSTM. Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers. Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art performance in both English and Hindi without the use of any morphological analysis or without using gazetteers of any sort.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09799",
        "title": "Experiments with POS Tagging Code-mixed Indian Social Media Text",
        "authors": [
            "Prakash B. Pimpale",
            "Raj Nath Patel"
        ],
        "abstract": "This paper presents Centre for Development of Advanced Computing Mumbai's (CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For Code-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON 2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te) languages mixed with English (en). In this paper, we have described our approaches to the POS tagging techniques, we exploited for this task. Machine learning has been used to POS tag the mixed language text. For POS tagging, distributed representations of words in vector space (word2vec) for feature extraction and Log-linear models have been tried. We report our work on all three languages hi, bn, and te mixed with en.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09889",
        "title": "Chinese Poetry Generation with Planning based Neural Network",
        "authors": [
            "Zhe Wang",
            "Wei He",
            "Hua Wu",
            "Haiyang Wu",
            "Wei Li",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "abstract": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user's writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user's intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09893",
        "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks",
        "authors": [
            "Xiang Li",
            "Tao Qin",
            "Jian Yang",
            "Tie-Yan Liu"
        ],
        "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need $2 \\sqrt{|V|}$ vectors to represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$ vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm \\emph{LightRNN} to reflect its very small model size and very high training speed.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09914",
        "title": "Named Entity Recognition for Novel Types by Transfer Learning",
        "authors": [
            "Lizhen Qu",
            "Gabriela Ferraro",
            "Liyuan Zhou",
            "Weiwei Hou",
            "Timothy Baldwin"
        ],
        "abstract": "In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09935",
        "title": "Knowledge Questions from Knowledge Graphs",
        "authors": [
            "Dominic Seyler",
            "Mohamed Yahya",
            "Klaus Berberich"
        ],
        "abstract": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09975",
        "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
        "authors": [
            "Hagen Soltau",
            "Hank Liao",
            "Hasim Sak"
        ],
        "abstract": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09995",
        "title": "Generating Sentiment Lexicons for German Twitter",
        "authors": [
            "Uladzimir Sidarenka",
            "Manfred Stede"
        ],
        "abstract": "Despite a substantial progress made in developing new sentiment lexicon generation (SLG) methods for English, the task of transferring these approaches to other languages and domains in a sound way still remains open. In this paper, we contribute to the solution of this problem by systematically comparing semi-automatic translations of common English polarity lists with the results of the original automatic SLG algorithms, which were applied directly to German data. We evaluate these lexicons on a corpus of 7,992 manually annotated tweets. In addition to that, we also collate the results of dictionary- and corpus-based SLG methods in order to find out which of these paradigms is better suited for the inherently noisy domain of social media. Our experiments show that semi-automatic translations notably outperform automatic systems (reaching a macro-averaged F1-score of 0.589), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1-scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09996",
        "title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension",
        "authors": [
            "Yang Yu",
            "Wei Zhang",
            "Kazi Hasan",
            "Mo Yu",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves state-of-the-art exact match and F1 scores on the SQuAD dataset.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.10099",
        "title": "Neural Machine Translation in Linear Time",
        "authors": [
            "Nal Kalchbrenner",
            "Lasse Espeholt",
            "Karen Simonyan",
            "Aaron van den Oord",
            "Alex Graves",
            "Koray Kavukcuoglu"
        ],
        "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00020",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision",
        "authors": [
            "Chen Liang",
            "Jonathan Berant",
            "Quoc Le",
            "Kenneth D. Forbus",
            "Ni Lao"
        ],
        "abstract": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00027",
        "title": "CBAS: context based arabic stemmer",
        "authors": [
            "Mahmoud El-Defrawy",
            "Yasser El-Sonbaty",
            "Nahla A. Belal"
        ],
        "abstract": "Arabic morphology encapsulates many valuable features such as word root. Arabic roots are being utilized for many tasks; the process of extracting a word root is referred to as stemming. Stemming is an essential part of most Natural Language Processing tasks, especially for derivative languages such as Arabic. However, stemming is faced with the problem of ambiguity, where two or more roots could be extracted from the same word. On the other hand, distributional semantics is a powerful co-occurrence model. It captures the meaning of a word based on its context. In this paper, a distributional semantics model utilizing Smoothed Pointwise Mutual Information (SPMI) is constructed to investigate its effectiveness on the stemming analysis task. It showed an accuracy of 81.5%, with a at least 9.4% improvement over other stemmers.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00068",
        "title": "RNN Approaches to Text Normalization: A Challenge",
        "authors": [
            "Richard Sproat",
            "Navdeep Jaitly"
        ],
        "abstract": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future.\n",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2017-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00126",
        "title": "Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings",
        "authors": [
            "Shufeng Xiong"
        ],
        "abstract": "Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00179",
        "title": "Dual Learning for Machine Translation",
        "authors": [
            "Yingce Xia",
            "Di He",
            "Tao Qin",
            "Liwei Wang",
            "Nenghai Yu",
            "Tie-Yan Liu",
            "Wei-Ying Ma"
        ],
        "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \\emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00196",
        "title": "Recurrent Neural Network Language Model Adaptation Derived Document Vector",
        "authors": [
            "Wei Li",
            "Brian Kan Wing Mak"
        ],
        "abstract": "In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document, and they can be important in some NLP tasks such as genre classification. This paper proposes a novel distributed vector representation of a document: a simple recurrent-neural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document. The new document vectors are labeled as DV-RNN and DV-LSTM respectively. We believe that our new document vectors can capture some high-level sequential information in the documents, which other current document representations fail to capture. The new document vectors were evaluated in the genre classification of documents in three corpora: the Brown Corpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset. Their classification performances are compared with the performance of TF-IDF vector and the state-of-the-art distributed memory model of paragraph vector (PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and PV-DM in most cases, and combinations of the proposed document vectors with TF-IDF or PV-DM may further improve performance.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00354",
        "title": "Faster decoding for subword level Phrase-based SMT between related languages",
        "authors": [
            "Anoop Kunchukuttan",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00472",
        "title": "Towards Sub-Word Level Compositions for Sentiment Analysis of Hindi-English Code Mixed Text",
        "authors": [
            "Ameya Prabhu",
            "Aditya Joshi",
            "Manish Shrivastava",
            "Vasudeva Varma"
        ],
        "abstract": "Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media.\n",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00483",
        "title": "Detecting Context Dependent Messages in a Conversational Environment",
        "authors": [
            "Chaozhuo Li",
            "Yu Wu",
            "Wei Wu",
            "Chen Xing",
            "Zhoujun Li",
            "Ming Zhou"
        ],
        "abstract": "While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider the linguistic context of an input text in the generation process. The task is challenging, as messages in a conversational environment are short and informal, and evidence that can indicate a message is context dependent is scarce. After a study of social conversation data crawled from the web, we observed that some characteristics estimated from the responses of messages are discriminative for identifying context dependent messages. With the characteristics as weak supervision, we propose using a Long Short Term Memory (LSTM) network to learn a classifier. Our method carries out text representation and classifier learning in a unified framework. Experimental results show that the proposed method can significantly outperform baseline methods on accuracy of classification.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00601",
        "title": "Ordinal Common-sense Inference",
        "authors": [
            "Sheng Zhang",
            "Rachel Rudinger",
            "Kevin Duh",
            "Benjamin Van Durme"
        ],
        "abstract": "Humans have the capacity to draw common-sense inferences from natural language: various things that are likely but not certain to hold based on established discourse, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment: predicting ordinal human responses on the subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge from corpora, which is then used to construct a dataset for this ordinal entailment task. We train a neural sequence-to-sequence model on this dataset, which we use to score and generate possible inferences. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2017-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00674",
        "title": "Fuzzy paraphrases in learning word representations with a lexicon",
        "authors": [
            "Yuanzhi Ke",
            "Masafumi Hagiwara"
        ],
        "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally. In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00801",
        "title": "A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection",
        "authors": [
            "Mingbin Xu",
            "Hui Jiang"
        ],
        "abstract": "In this paper, we study a novel approach for named entity recognition (NER) and mention detection in natural language processing. Instead of treating NER as a sequence labelling problem, we propose a new local detection approach, which rely on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Afterwards, a simple feedforward neural network is used to reject or predict entity label for each individual fragment. The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labelling methods.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00995",
        "title": "An empirical study for Vietnamese dependency parsing",
        "authors": [
            "Dat Quoc Nguyen",
            "Mark Dras",
            "Mark Johnson"
        ],
        "abstract": "This paper presents an empirical comparison of different dependency parsers for Vietnamese, which has some unusual characteristics such as copula drop and verb serialization. Experimental results show that the neural network-based parsers perform significantly better than the traditional parsers. We report the highest parsing scores published to date for Vietnamese with the labeled attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at 80.66%.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01083",
        "title": "A Hybrid Approach to Word Sense Disambiguation Combining Supervised and Unsupervised Learning",
        "authors": [
            "Alok Ranjan Pal",
            "Anirban Kundu",
            "Abhay Singh",
            "Raj Shekhar",
            "Kunal Sinha"
        ],
        "abstract": "In this paper, we are going to find meaning of words based on distinct situations. Word Sense Disambiguation is used to find meaning of words based on live contexts using supervised and unsupervised approaches. Unsupervised approaches use online dictionary for learning, and supervised approaches use manual learning sets. Hand tagged data are populated which might not be effective and sufficient for learning procedure. This limitation of information is main flaw of the supervised approach. Our proposed approach focuses to overcome the limitation using learning set which is enriched in dynamic way maintaining new data. Trivial filtering method is utilized to achieve appropriate training data. We introduce a mixed methodology having Modified Lesk approach and Bag-of-Words having enriched bags using learning methods. Our approach establishes the superiority over individual Modified Lesk and Bag-of-Words approaches based on experimentation.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01101",
        "title": "CogALex-V Shared Task: ROOT18",
        "authors": [
            "Emmanuele Chersoni",
            "Giulia Rambelli",
            "Enrico Santus"
        ],
        "abstract": "In this paper, we describe ROOT 18, a classifier using the scores of several unsupervised distributional measures as features to discriminate between semantically related and unrelated words, and then to classify the related pairs according to their semantic relation (i.e. synonymy, antonymy, hypernymy, part-whole meronymy). Our classifier participated in the CogALex-V Shared Task, showing a solid performance on the first subtask, but a poor performance on the second subtask. The low scores reported on the second subtask suggest that distributional measures are not sufficient to discriminate between multiple semantic relations at once.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2016-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01116",
        "title": "Binary Paragraph Vectors",
        "authors": [
            "Karol Grzegorczyk",
            "Marcin Kurdziel"
        ],
        "abstract": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.\n    ",
        "submission_date": "2016-11-03T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01242",
        "title": "Answering Complicated Question Intents Expressed in Decomposed Question Sequences",
        "authors": [
            "Mohit Iyyer",
            "Wen-tau Yih",
            "Ming-Wei Chang"
        ],
        "abstract": "Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. Existing QA systems face two major problems when evaluated on our dataset: (1) handling questions that contain coreferences to previous questions or answers, and (2) matching words or phrases in a question to corresponding entries in the associated table. We conclude by proposing strategies to handle both of these issues.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01368",
        "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
        "authors": [
            "Tal Linzen",
            "Emmanuel Dupoux",
            "Yoav Goldberg"
        ],
        "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01436",
        "title": "Learning Recurrent Span Representations for Extractive Question Answering",
        "authors": [
            "Kenton Lee",
            "Shimi Salant",
            "Tom Kwiatkowski",
            "Ankur Parikh",
            "Dipanjan Das",
            "Jonathan Berant"
        ],
        "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQuAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.'s baseline by > 50%.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2017-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01487",
        "title": "Morphological Inflection Generation with Hard Monotonic Attention",
        "authors": [
            "Roee Aharoni",
            "Yoav Goldberg"
        ],
        "abstract": "We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention \\cite{bahdanauCB14} models for the task, shedding some light on the features such models extract.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01547",
        "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations",
        "authors": [
            "Philip Blair",
            "Yuval Merhav",
            "Joel Barry"
        ],
        "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2017-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01587",
        "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks",
        "authors": [
            "Kazuma Hashimoto",
            "Caiming Xiong",
            "Yoshimasa Tsuruoka",
            "Richard Socher"
        ],
        "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01603",
        "title": "Bidirectional Attention Flow for Machine Comprehension",
        "authors": [
            "Minjoon Seo",
            "Aniruddha Kembhavi",
            "Ali Farhadi",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2018-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01604",
        "title": "Dynamic Coattention Networks For Question Answering",
        "authors": [
            "Caiming Xiong",
            "Victor Zhong",
            "Richard Socher"
        ],
        "abstract": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2018-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01628",
        "title": "Reference-Aware Language Models",
        "authors": [
            "Zichao Yang",
            "Phil Blunsom",
            "Chris Dyer",
            "Wang Ling"
        ],
        "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks shows our model variants based on deterministic attention.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01702",
        "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency",
        "authors": [
            "Adji B. Dieng",
            "Chong Wang",
            "Jianfeng Gao",
            "John Paisley"
        ],
        "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $6.28\\%$. This is comparable to the state-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01724",
        "title": "Words or Characters? Fine-grained Gating for Reading Comprehension",
        "authors": [
            "Zhilin Yang",
            "Bhuwan Dhingra",
            "Ye Yuan",
            "Junjie Hu",
            "William W. Cohen",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01734",
        "title": "Deep Biaffine Attention for Neural Dependency Parsing",
        "authors": [
            "Timothy Dozat",
            "Christopher D. Manning"
        ],
        "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01747",
        "title": "A Compare-Aggregate Model for Matching Text Sequences",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "abstract": "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general \"compare-aggregate\" framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01783",
        "title": "Domain Adaptation For Formant Estimation Using Deep Learning",
        "authors": [
            "Yehoshua Dissen",
            "Joseph Keshet",
            "Jacob Goldberger",
            "Cynthia Clopper"
        ],
        "abstract": "In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01839",
        "title": "Hierarchical Question Answering for Long Documents",
        "authors": [
            "Eunsol Choi",
            "Daniel Hewlett",
            "Alexandre Lacoste",
            "Illia Polosukhin",
            "Jakob Uszkoreit",
            "Jonathan Berant"
        ],
        "abstract": "We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate the state of the art performance on a challenging subset of the Wikireading and on a new dataset, while speeding up the model by 3.5x-6.7x.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01867",
        "title": "Latent Attention For If-Then Program Synthesis",
        "authors": [
            "Xinyun Chen",
            "Chang Liu",
            "Richard Shin",
            "Dawn Song",
            "Mingcheng Chen"
        ],
        "abstract": "Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01868",
        "title": "Truth Discovery with Memory Network",
        "authors": [
            "Luyang Li",
            "Bing Qin",
            "Wenjing Ren",
            "Ting Liu"
        ],
        "abstract": "Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01874",
        "title": "Neural Machine Translation with Reconstruction",
        "authors": [
            "Zhaopeng Tu",
            "Yang Liu",
            "Lifeng Shang",
            "Xiaohua Liu",
            "Hang Li"
        ],
        "abstract": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01884",
        "title": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification",
        "authors": [
            "Depeng Liang",
            "Yongdong Zhang"
        ],
        "abstract": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long Short-Term Memory network (BLSTM). Experiment results demonstrate that our model achieves state-of-the-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02007",
        "title": "Keyphrase Annotation with Graph Co-Ranking",
        "authors": [
            "Adrien Bougouin",
            "Florian Boudin",
            "B\u00e9atrice Daille"
        ],
        "abstract": "Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02025",
        "title": "Presenting a New Dataset for the Timeline Generation Problem",
        "authors": [
            "Xavier Holt",
            "Will Radford",
            "Ben Hachey"
        ],
        "abstract": "The timeline generation task summarises an entity's biography by selecting stories representing key events from a large pool of relevant documents. This paper addresses the lack of a standard dataset and evaluative methodology for the problem. We present and make publicly available a new dataset of 18,793 news articles covering 39 entities. For each entity, we provide a gold standard timeline and a set of entity-related articles. We propose ROUGE as an evaluation metric and validate our dataset by showing that top Google results outperform straw-man baselines.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02027",
        "title": ":telephone::person::sailboat::whale::okhand:; or \"Call me Ishmael\" - How do you translate emoji?",
        "authors": [
            "Will Radford",
            "Andrew Chisholm",
            "Ben Hachey",
            "Bo Han"
        ],
        "abstract": "We report on an exploratory analysis of Emoji Dick, a project that leverages crowdsourcing to translate Melville's Moby Dick into emoji. This distinctive use of emoji removes textual context, and leads to a varying translation quality. In this paper, we use statistical word alignment and part-of-speech tagging to explore how people use emoji. Despite these simple methods, we observed differences in token and part-of-speech distributions. Experiments also suggest that semantics are preserved in the translation, and repetition is more common in emoji.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02091",
        "title": "Building a comprehensive syntactic and semantic corpus of Chinese clinical texts",
        "authors": [
            "Bin He",
            "Bin Dong",
            "Yi Guan",
            "Jinfeng Yang",
            "Zhipeng Jiang",
            "Qiubin Yu",
            "Jianyi Cheng",
            "Chunyan Qu"
        ],
        "abstract": "Objective: To build a comprehensive corpus covering syntactic and semantic annotations of Chinese clinical texts with corresponding annotation guidelines and methods as well as to develop tools trained on the annotated corpus, which supplies baselines for research on Chinese texts in the clinical domain.\n",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02344",
        "title": "A Convolutional Encoder Model for Neural Machine Translation",
        "authors": [
            "Jonas Gehring",
            "Michael Auli",
            "David Grangier",
            "Yann N. Dauphin"
        ],
        "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02360",
        "title": "Cruciform: Solving Crosswords with Natural Language Processing",
        "authors": [
            "Dragomir Radev",
            "Rui Zhang",
            "Steve Wilson",
            "Derek Van Assche",
            "Henrique Spyra Gubert",
            "Alisa Krivokapic",
            "MeiXing Dong",
            "Chongruo Wu",
            "Spruce Bondera",
            "Luke Brandl",
            "Jeremy Dohmann"
        ],
        "abstract": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system. We describe the results of several of our experiments with the system.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02361",
        "title": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents",
        "authors": [
            "Rui Zhang",
            "Honglak Lee",
            "Dragomir Radev"
        ],
        "abstract": "The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a general-purpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentence-level tasks. Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02378",
        "title": "A Surrogate-based Generic Classifier for Chinese TV Series Reviews",
        "authors": [
            "Yufeng Ma",
            "Long Xia",
            "Wenqi Shen",
            "Mi Zhou",
            "Weiguo Fan"
        ],
        "abstract": "With the emerging of various online video platforms like Youtube, Youku and LeTV, online TV series' reviews become more and more important both for viewers and producers. Customers rely heavily on these reviews before selecting TV series, while producers use them to improve the quality. As a result, automatically classifying reviews according to different requirements evolves as a popular research topic and is essential in our daily life. In this paper, we focused on reviews of hot TV series in China and successfully trained generic classifiers based on eight predefined categories. The experimental results showed promising performance and effectiveness of its generalization to different TV series.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02550",
        "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
        "authors": [
            "Shane Settle",
            "Karen Livescu"
        ],
        "abstract": "Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a \"Siamese network\" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02554",
        "title": "The Neural Noisy Channel",
        "authors": [
            "Lei Yu",
            "Phil Blunsom",
            "Chris Dyer",
            "Edward Grefenstette",
            "Tomas Kocisky"
        ],
        "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02588",
        "title": "Contradiction Detection for Rumorous Claims",
        "authors": [
            "Piroska Lendvai",
            "Uwe D. Reichel"
        ],
        "abstract": "The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis- and disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02590",
        "title": "Veracity Computing from Lexical Cues and Perceived Certainty Trends",
        "authors": [
            "Uwe D. Reichel",
            "Piroska Lendvai"
        ],
        "abstract": "We present a data-driven method for determining the veracity of a set of rumorous claims on social media data. Tweets from different sources pertaining to a rumor are processed on three levels: first, factuality values are assigned to each tweet based on four textual cue categories relevant for our journalism use case; these amalgamate speaker support in terms of polarity and commitment in terms of certainty and speculation. Next, the proportions of these lexical cues are utilized as predictors for tweet certainty in a generalized linear regression model. Subsequently, lexical cue proportions, predicted certainty, as well as their time course characteristics are used to compute veracity for each rumor in terms of the identity of the rumor-resolving tweet and its binary resolution value judgment. The system operates without access to extralinguistic resources. Evaluated on the data portion for which hand-labeled examples were available, it achieves .74 F1-score on identifying rumor resolving tweets and .76 F1-score on predicting if a rumor is resolved as true or false.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02654",
        "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks",
        "authors": [
            "Lajanugen Logeswaran",
            "Honglak Lee",
            "Dragomir Radev"
        ],
        "abstract": "Modeling the structure of coherent texts is a key NLP problem. The task of coherently organizing a given set of sentences has been commonly used to build and evaluate models that understand such structure. We propose an end-to-end unsupervised deep learning approach based on the set-to-sequence framework to address this problem. Our model strongly outperforms prior methods in the order discrimination task and a novel task of ordering abstracts from scientific articles. Furthermore, our work shows that useful text representations can be obtained by learning to order sentences. Visualizing the learned sentence representations shows that the model captures high-level logical structure in paragraphs. Our representations perform comparably to state-of-the-art pre-training methods on sentence similarity and paraphrase detection tasks.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2017-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02683",
        "title": "Unsupervised Pretraining for Sequence to Sequence Learning",
        "authors": [
            "Prajit Ramachandran",
            "Peter J. Liu",
            "Quoc V. Le"
        ],
        "abstract": "This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the art results on the WMT English$\\rightarrow$German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2018-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02695",
        "title": "Automatic recognition of child speech for robotic applications in noisy environments",
        "authors": [
            "Samuel Fernando",
            "Roger K. Moore",
            "David Cameron",
            "Emily C. Collins",
            "Abigail Millings",
            "Amanda J. Sharkey",
            "Tony J. Prescott"
        ],
        "abstract": "Automatic speech recognition (ASR) allows a natural and intuitive interface for robotic educational applications for children. However there are a number of challenges to overcome to allow such an interface to operate robustly in realistic settings, including the intrinsic difficulties of recognising child speech and high levels of background noise often present in classrooms. As part of the EU EASEL project we have provided several contributions to address these challenges, implementing our own ASR module for use in robotics applications. We used the latest deep neural network algorithms which provide a leap in performance over the traditional GMM approach, and apply data augmentation methods to improve robustness to noise and speaker variation. We provide a close integration between the ASR module and the rest of the dialogue system, allowing the ASR to receive in real-time the language models relevant to the current section of the dialogue, greatly improving the accuracy. We integrated our ASR module into an interactive, multimodal system using a small humanoid robot to help children learn about exercise and energy. The system was installed at a public museum event as part of a research study where 320 children (aged 3 to 14) interacted with the robot, with our ASR achieving 90% accuracy for fluent and near-fluent speech.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02839",
        "title": "Old Content and Modern Tools - Searching Named Entities in a Finnish OCRed Historical Newspaper Collection 1771-1910",
        "authors": [
            "Kimmo Kettunen",
            "Eetu M\u00e4kel\u00e4",
            "Teemu Ruokolainen",
            "Juha Kuokkala",
            "Laura L\u00f6fberg"
        ],
        "abstract": "Named Entity Recognition (NER), search, classification and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system's performance is genre and domain dependent and also used entity categories vary (Nadeau and Sekine, 2007). The most general set of named entities is usually some version of three partite categorization of locations, persons and organizations. In this paper we report first large scale trials and evaluation of NER with data out of a digitized Finnish historical newspaper collection Digi. Experiments, results and discussion of this research serve development of the Web collection of historical Finnish newspapers.\n",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02944",
        "title": "Increasing the throughput of machine translation systems using clouds",
        "authors": [
            "Jernej Vi\u010di\u010d",
            "Andrej Brodnik"
        ],
        "abstract": "The manuscript presents an experiment at implementation of a Machine Translation system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two machine translation paradigms were studied: shallow transfer Rule Based Machine Translation and Statistical Machine Translation.\n",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02956",
        "title": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation",
        "authors": [
            "Hong Jin Kang",
            "Tao Chen",
            "Muthu Kumar Chandrasekaran",
            "Min-Yen Kan"
        ],
        "abstract": "Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2017-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02988",
        "title": "Distant supervision for emotion detection using Facebook reactions",
        "authors": [
            "Chris Pool",
            "Malvina Nissim"
        ],
        "abstract": "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it's usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03057",
        "title": "When silver glitters more than gold: Bootstrapping an Italian part-of-speech tagger for Twitter",
        "authors": [
            "Barbara Plank",
            "Malvina Nissim"
        ],
        "abstract": "We bootstrap a state-of-the-art part-of-speech tagger to tag Italian Twitter data, in the context of the Evalita 2016 PoSTWITA shared task. We show that training the tagger on native Twitter data enriched with little amounts of specifically selected gold data and additional silver-labelled data scraped from Facebook, yields better results than using large amounts of manually annotated data from a mix of genres.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03279",
        "title": "Tracing metaphors in time through self-distance in vector spaces",
        "authors": [
            "Marco Del Tredici",
            "Malvina Nissim",
            "Andrea Zaninello"
        ],
        "abstract": "From a diachronic corpus of Italian, we build consecutive vector spaces in time and use them to compare a term's cosine similarity to itself in different time spans. We assume that a drop in similarity might be related to the emergence of a metaphorical sense at a given time. Similarity-based observations are matched to the actual year when a figurative meaning was documented in a reference dictionary and through manual inspection of corpus occurrences.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03382",
        "title": "Efficient Summarization with Read-Again and Copy Mechanism",
        "authors": [
            "Wenyuan Zeng",
            "Wenjie Luo",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03466",
        "title": "Syntactic Enhancement to VSIMM for Roadmap Based Anomalous Trajectory Detection: A Natural Language Processing Approach",
        "authors": [
            "Vikram Krishnamurthy",
            "Sijia Gao"
        ],
        "abstract": "The aim of syntactic tracking is to classify spatio-temporal patterns of a target's motion using natural language processing models. In this paper, we generalize earlier work by considering a constrained stochastic context free grammar (CSCFG) for modeling patterns confined to a roadmap. The constrained grammar facilitates modeling specific directions and road names in a roadmap. We present a novel particle filtering algorithm that exploits the CSCFG model for estimating the target's patterns. This meta-level algorithm operates in conjunction with a base-level tracking algorithm. Extensive numerical results using simulated ground moving target indicator (GMTI) radar measurements show substantial improvement in target tracking accuracy.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2018-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03533",
        "title": "Landmark-based consonant voicing detection on multilingual corpora",
        "authors": [
            "Xiang Kong",
            "Xuesong Yang",
            "Mark Hasegawa-Johnson",
            "Jeung-Yoon Choi",
            "Stefanie Shattuck-Hufnagel"
        ],
        "abstract": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred cross-lingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and(3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT),and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNNfeatures outperform both. MFCC-based classifiers suffer an F1reduction of 16% absolute when generalized from English to other languages. Manual features suffer only a 5% F1 reduction,and CNN features actually perform better in Turkish and Span-ish than in the training language, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2016-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03558",
        "title": "Neural Networks Models for Entity Discovery and Linking",
        "authors": [
            "Dan Liu",
            "Wei Lin",
            "Shiliang Zhang",
            "Si Wei",
            "Hui Jiang"
        ],
        "abstract": "This paper describes the USTC_NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03599",
        "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
        "authors": [
            "Wei-Fan Chen",
            "Lun-Wei Ku"
        ],
        "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03641",
        "title": "Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure",
        "authors": [
            "Oded Avraham",
            "Yoav Goldberg"
        ],
        "abstract": "We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account.\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03932",
        "title": "Training IBM Watson using Automatically Generated Question-Answer Pairs",
        "authors": [
            "Jangho Lee",
            "Gyuwan Kim",
            "Jaeyoon Yoo",
            "Changwoo Jung",
            "Minseok Kim",
            "Sungroh Yoon"
        ],
        "abstract": "IBM Watson is a cognitive computing system capable of question answering in natural languages. It is believed that IBM Watson can understand large corpora and answer relevant questions more effectively than any other question-answering system currently available. To unleash the full power of Watson, however, we need to train its instance with a large number of well-prepared question-answer pairs. Obviously, manually generating such pairs in a large quantity is prohibitively time consuming and significantly limits the efficiency of Watson's training. Recently, a large-scale dataset of over 30 million question-answer pairs was reported. Under the assumption that using such an automatically generated dataset could relieve the burden of manual question-answer generation, we tried to use this dataset to train an instance of Watson and checked the training efficiency and accuracy. According to our experiments, using this auto-generated dataset was effective for training Watson, complementing manually crafted question-answer pairs. To the best of the authors' knowledge, this work is the first attempt to use a large-scale dataset of automatically generated question-answer pairs for training IBM Watson. We anticipate that the insights and lessons obtained from our experiments will be useful for researchers who want to expedite Watson training leveraged by automatically generated question-answer pairs.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03949",
        "title": "Linguistically Regularized LSTMs for Sentiment Classification",
        "authors": [
            "Qiao Qian",
            "Minlie Huang",
            "Jinhao Lei",
            "Xiaoyan Zhu"
        ],
        "abstract": "Sentiment understanding has been a long-term goal of AI in the past decades. This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations. In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models' simplicity.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04010",
        "title": "Multi-Language Identification Using Convolutional Recurrent Neural Network",
        "authors": [
            "Vrishabh Ajay Lakhani",
            "Rohan Mahadev"
        ],
        "abstract": "Language Identification, being an important aspect of Automatic Speaker Recognition has had many changes and new approaches to ameliorate performance over the last decade. We compare the performance of using audio spectrum in the log scale and using Polyphonic sound sequences from raw audio samples to train the neural network and to classify speech as either English or Spanish. To achieve this, we use the novel approach of using a Convolutional Recurrent Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit (GRU) for forward propagation of the neural network. Our hypothesis is that the performance of using polyphonic sound sequence as features and both LSTM and GRU as the gating mechanisms for the neural network outperform the traditional MFCC features using a unidirectional Deep Neural Network.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04033",
        "title": "1.5 billion words Arabic Corpus",
        "authors": [
            "Ibrahim Abu El-khair"
        ],
        "abstract": "This study is an attempt to build a contemporary linguistic corpus for Arabic language. The corpus produced, is a text corpus includes more than five million newspaper articles. It contains over a billion and a half words in total, out of which, there is about three million unique words. The data were collected from newspaper articles in ten major news sources from eight Arabic countries, over a period of fourteen years. The corpus was encoded with two types of encoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two mark-up languages, namely: SGML, and XML.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04052",
        "title": "Semi-automatic Simultaneous Interpreting Quality Evaluation",
        "authors": [
            "Xiaojun Zhang"
        ],
        "abstract": "Increasing interpreting needs a more objective and automatic measurement. We hold a basic idea that 'translating means translating meaning' in that we can assessment interpretation quality by comparing the meaning of the interpreting output with the source input. That is, a translation unit of a 'chunk' named Frame which comes from frame semantics and its components named Frame Elements (FEs) which comes from Frame Net are proposed to explore their matching rate between target and source texts. A case study in this paper verifies the usability of semi-automatic graded semantic-scoring measurement for human simultaneous interpreting and shows how to use frame and FE matches to score. Experiments results show that the semantic-scoring metrics have a significantly correlation coefficient with human judgment.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2016-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04122",
        "title": "Cross-lingual Dataless Classification for Languages with Small Wikipedia Presence",
        "authors": [
            "Yangqiu Song",
            "Stephen Mayhew",
            "Dan Roth"
        ],
        "abstract": "This paper presents an approach to classify documents in any language into an English topical label space, without any text categorization training data. The approach, Cross-Lingual Dataless Document Classification (CLDDC) relies on mapping the English labels or short category description into a Wikipedia-based semantic representation, and on the use of the target language Wikipedia. Consequently, performance could suffer when Wikipedia in the target language is small. In this paper, we focus on languages with small Wikipedias, (Small-Wikipedia languages, SWLs). We use a word-level dictionary to convert documents in a SWL to a large-Wikipedia language (LWLs), and then perform CLDDC based on the LWL's Wikipedia. This approach can be applied to thousands of languages, which can be contrasted with machine translation, which is a supervision heavy approach and can be done for about 100 languages. We also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good LWL, and show that this significantly improves classification of SWLs' documents, performing comparably to the best bridge possible.\n    ",
        "submission_date": "2016-11-13T00:00:00",
        "last_modified_date": "2016-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04125",
        "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion",
        "authors": [
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.\n    ",
        "submission_date": "2016-11-13T00:00:00",
        "last_modified_date": "2016-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04230",
        "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
        "authors": [
            "Ramesh Nallapati",
            "Feifei Zhai",
            "Bowen Zhou"
        ],
        "abstract": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04233",
        "title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features",
        "authors": [
            "Shuming Ma",
            "Xu Sun"
        ],
        "abstract": "Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04234",
        "title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media",
        "authors": [
            "Hangfeng He",
            "Xu Sun"
        ],
        "abstract": "We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semi-supervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of F-score driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields 7.44\\% improvement over previous state-of-the-art result.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04244",
        "title": "Classify or Select: Neural Architectures for Extractive Document Summarization",
        "authors": [
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Mingbo Ma"
        ],
        "abstract": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04326",
        "title": "`Who would have thought of that!': A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection",
        "authors": [
            "Aditya Joshi",
            "Prayas Jain",
            "Pushpak Bhattacharyya",
            "Mark Carman"
        ],
        "abstract": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as `work', `gun laws', `weather' are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25\\%.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04358",
        "title": "Character-level Convolutional Network for Text Classification Applied to Chinese Corpus",
        "authors": [
            "Weijie Huang",
            "Jun Wang"
        ],
        "abstract": "This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04361",
        "title": "Attending to Characters in Neural Sequence Labeling Models",
        "authors": [
            "Marek Rei",
            "Gamal K.O. Crichton",
            "Sampo Pyysalo"
        ],
        "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04491",
        "title": "Ranking medical jargon in electronic health record notes by adapted distant supervision",
        "authors": [
            "Jinying Chen",
            "Abhyuday N. Jagannatha",
            "Samah J. Jarad",
            "Hong Yu"
        ],
        "abstract": "Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources.\n",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04496",
        "title": "Multi-view Recurrent Neural Acoustic Word Embeddings",
        "authors": [
            "Wanjia He",
            "Weiran Wang",
            "Karen Livescu"
        ],
        "abstract": "Recent work has begun exploring neural acoustic word embeddings---fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04503",
        "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot",
        "authors": [
            "Hideki Nakayama",
            "Noriki Nishida"
        ],
        "abstract": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \"pivot\", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04558",
        "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
        "authors": [
            "Melvin Johnson",
            "Mike Schuster",
            "Quoc V. Le",
            "Maxim Krikun",
            "Yonghui Wu",
            "Zhifeng Chen",
            "Nikhil Thorat",
            "Fernanda Vi\u00e9gas",
            "Martin Wattenberg",
            "Greg Corrado",
            "Macduff Hughes",
            "Jeffrey Dean"
        ],
        "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04684",
        "title": "Knowledge Enhanced Hybrid Neural Network for Text Matching",
        "authors": [
            "Yu Wu",
            "Wei Wu",
            "Zhoujun Li",
            "Ming Zhou"
        ],
        "abstract": "Long text brings a big challenge to semantic matching due to their complicated semantic and syntactic structures. To tackle the challenge, we consider using prior knowledge to help identify useful information and filter out noise to matching in long text. To this end, we propose a knowledge enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into word representations by knowledge gates and establishes three matching channels with words, sequential structures of sentences given by Gated Recurrent Units (GRU), and knowledge enhanced representations. The three channels are processed by a convolutional neural network to generate high level features for matching, and the features are synthesized as a matching score by a multilayer perceptron. The model extends the existing methods by conducting matching on words, local structures of sentences, and global context of sentences. Evaluation results from extensive experiments on public data sets for question answering and conversation show that KEHNN can significantly outperform the-state-of-the-art matching models and particularly improve the performance on pairs with long text.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04741",
        "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference",
        "authors": [
            "Biswajit Paria",
            "K. M. Annervaz",
            "Ambedkar Dukkipati",
            "Ankush Chatterjee",
            "Sanjay Podder"
        ],
        "abstract": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2017-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04798",
        "title": "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder",
        "authors": [
            "Thanh-Le Ha",
            "Jan Niehues",
            "Alexander Waibel"
        ],
        "abstract": "In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04822",
        "title": "SimDoc: Topic Sequence Alignment based Document Similarity Framework",
        "authors": [
            "Gaurav Maheshwari",
            "Priyansh Trivedi",
            "Harshita Sahijwani",
            "Kunal Jha",
            "Sourish Dasgupta",
            "Jens Lehmann"
        ],
        "abstract": "Document similarity is the problem of estimating the degree to which a given pair of documents has similar semantic content. An accurate document similarity measure can improve several enterprise relevant tasks such as document clustering, text mining, and question-answering. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their similarity. To this end, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of related words. Then, we use a sequence alignment algorithm to estimate their semantic similarity. We further conceptualize a novel mechanism to compute topic-topic similarity to fine tune our system. In our experiments, we show that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2017-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04837",
        "title": "Lost in Space: Geolocation in Event Data",
        "authors": [
            "Sophie J. Lee",
            "Howard Liu",
            "Michael D. Ward"
        ],
        "abstract": "Extracting the \"correct\" location information from text data, i.e., determining the place of event, has long been a goal for automated text processing. To approximate human-like coding schema, we introduce a supervised machine learning algorithm that classifies each location word to be either correct or incorrect. We use news articles collected from around the world (Integrated Crisis Early Warning System [ICEWS] data and Open Event Data Alliance [OEDA] data) to test our algorithm that consists of two stages. In the feature selection stage, we extract contextual information from texts, namely, the N-gram patterns for location words, the frequency of mention, and the context of the sentences containing location words. In the classification stage, we use three classifiers to estimate the model parameters in the training set and then to predict whether a location word in the test set news articles is the place of the event. The validation results show that our algorithm improves the accuracy rate of the current geolocation methods of dictionary approach by as much as 25%.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2016-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04841",
        "title": "Quantitative Entropy Study of Language Complexity",
        "authors": [
            "R.R. Xie",
            "W.B. Deng",
            "D.J. Wang",
            "L.P. Csernai"
        ],
        "abstract": "We study the entropy of Chinese and English texts, based on characters in case of Chinese texts and based on words for both languages. Significant differences are found between the languages and between different personal styles of debating partners. The entropy analysis points in the direction of lower entropy, that is of higher complexity. Such a text analysis would be applied for individuals of different styles, a single individual at different age, as well as different groups of the population.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04887",
        "title": "Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks",
        "authors": [
            "J Ganesh",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "abstract": "Research in social media analysis is experiencing a recent surge with a large number of works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis, semantic textual similarity computation, hashtag prediction and so on. Although the performance of the representation learning models are better than the traditional baselines for the tasks, little is known about the core properties of a tweet encoded within the representations. Understanding these core properties would empower us in making generalizable conclusions about the quality of representations. Our work presented here constitutes the first step in opening the black-box of vector embedding for social media posts, with emphasis on tweets in particular.\n",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04928",
        "title": "Neural Machine Translation with Pivot Languages",
        "authors": [
            "Yong Cheng",
            "Yang Liu",
            "Qian Yang",
            "Maosong Sun",
            "Wei Xu"
        ],
        "abstract": "While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2017-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04953",
        "title": "End-to-End Neural Sentence Ordering Using Pointer Network",
        "authors": [
            "Jingjing Gong",
            "Xinchi Chen",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pair-wise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes and dataset of this paper are available.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04989",
        "title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed Social Media Text",
        "authors": [
            "Raj Nath Patel",
            "Prakash B. Pimpale",
            "M Sasikumar"
        ],
        "abstract": "This paper describes Centre for Development of Advanced Computing's (CDACM) submission to the shared task-'Tool Contest on POS tagging for Code-Mixed Indian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with ICON-2016. The shared task was to predict Part of Speech (POS) tag at word level for a given text. The code-mixed text is generated mostly on social media by multilingual users. The presence of the multilingual words, transliterations, and spelling variations make such content linguistically complex. In this paper, we propose an approach to POS tag code-mixed social media text using Recurrent Neural Network Language Model (RNN-LM) architecture. We submitted the results for Hindi-English (hi-en), Bengali-English (bn-en), and Telugu-English (te-en) code-mixed data.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05104",
        "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs",
        "authors": [
            "Shayne Longpre",
            "Sabeek Pradhan",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of augmentations and modifications to LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, average pooling, and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05239",
        "title": "How to do lexical quality estimation of a large OCRed historical Finnish newspaper collection with scarce resources",
        "authors": [
            "Kimmo Kettunen"
        ],
        "abstract": "The National Library of Finland has digitized the historical newspapers published in Finland between 1771 and 1910. This collection contains approximately 1.95 million pages in Finnish and Swedish. Finnish part of the collection consists of about 2.40 billion words. The National Library's Digital Collections are offered via the ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2019-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05360",
        "title": "The Life of Lazarillo de Tormes and of His Machine Learning Adversities",
        "authors": [
            "Javier de la Rosa",
            "Juan-Luis Su\u00e1rez"
        ],
        "abstract": "Summit work of the Spanish Golden Age and forefather of the so-called picaresque novel, The Life of Lazarillo de Tormes and of His Fortunes and Adversities still remains an anonymous text. Although distinguished scholars have tried to attribute it to different authors based on a variety of criteria, a consensus has yet to be reached. The list of candidates is long and not all of them enjoy the same support within the scholarly community. Analyzing their works from a data-driven perspective and applying machine learning techniques for style and text fingerprinting, we shed light on the authorship of the Lazarillo. As in a state-of-the-art survey, we discuss the methods used and how they perform in our specific case. According to our methodology, the most likely author seems to be Juan Arce de Ot\u00e1lora, closely followed by Alfonso de Vald\u00e9s. The method states that not certain attribution can be made with the given corpus.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05384",
        "title": "A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging",
        "authors": [
            "Xinchi Chen",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this work, we propose a feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging task. Specifically, to simulate the feature templates of traditional discrete feature based models, we use different filters to model the complex compositional features with convolutional and pooling layer, and then utilize long distance dependency information with recurrent layer. Experimental results on five different datasets show the effectiveness of our proposed model.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2017-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05527",
        "title": "Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization",
        "authors": [
            "Tsubasa Ochiai",
            "Shigeki Matsuda",
            "Hideyuki Watanabe",
            "Shigeru Katagiri"
        ],
        "abstract": "We examine the effect of the Group Lasso (gLasso) regularizer in selecting the salient nodes of Deep Neural Network (DNN) hidden layers by applying a DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of gLasso regularization, one for outgoing weight vectors and another for incoming weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096 nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment results demonstrate that our DNN training, in which the gLasso regularizer was embedded, successfully selected the hidden layer nodes that are necessary and sufficient for achieving high classification power.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05774",
        "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?",
        "authors": [
            "Adhiguna Kuncoro",
            "Miguel Ballesteros",
            "Lingpeng Kong",
            "Chris Dyer",
            "Graham Neubig",
            "Noah A. Smith"
        ],
        "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05962",
        "title": "Word and Document Embeddings based on Neural Network Approaches",
        "authors": [
            "Siwei Lai"
        ],
        "abstract": "Data representation is a fundamental task in machine learning. The representation of data affects the performance of the whole machine learning system. In a long history, the representation of data is done by feature engineering, and researchers aim at designing better features for specific tasks. Recently, the rapid development of deep learning and representation learning has brought new inspiration to various domains.\n",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06204",
        "title": "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks",
        "authors": [
            "Volkan Cirik",
            "Eduard Hovy",
            "Louis-Philippe Morency"
        ],
        "abstract": "Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term Memory (LSTM) networks, which have shown strong competency in many Natural Language Processing (NLP) problems. Our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in NLP show that curriculum learning has a positive effect on the LSTM's internal states by biasing the model towards building constructive representations i.e. the internal representation at the previous timesteps are used as building blocks for the final prediction. We also find that smaller models significantly improves when they are trained with curriculum learning. Lastly, we show that curriculum learning helps more when the amount of training data is limited.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06216",
        "title": "Generative Deep Neural Networks for Dialogue: A Short Review",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "abstract": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06320",
        "title": "Tracking Words in Chinese Poetry of Tang and Song Dynasties with the China Biographical Database",
        "authors": [
            "Chao-Lin Liu",
            "Kuo-Feng Luo"
        ],
        "abstract": "Large-scale comparisons between the poetry of Tang and Song dynasties shed light on how words, collocations, and expressions were used and shared among the poets. That some words were used only in the Tang poetry and some only in the Song poetry could lead to interesting research in linguistics. That the most frequent colors are different in the Tang and Song poetry provides a trace of the changing social circumstances in the dynasties. Results of the current work link to research topics of lexicography, semantics, and social transitions. We discuss our findings and present our algorithms for efficient comparisons among the poems, which are crucial for completing billion times of comparisons within acceptable time.\n    ",
        "submission_date": "2016-11-19T00:00:00",
        "last_modified_date": "2017-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06423",
        "title": "Incorporating Pass-Phrase Dependent Background Models for Text-Dependent Speaker Verification",
        "authors": [
            "A. K. Sarkar",
            "Zheng-Hua Tan"
        ],
        "abstract": "In this paper, we propose pass-phrase dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) to integrate the pass-phrase identification process into the conventional TD-SV system, where a PBM is derived from a text-independent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV systems. The performance of the proposed method is compared to conventional text-independent background model based TD-SV systems using either Gaussian mixture model (GMM)-universal background model (UBM) or Hidden Markov model (HMM)-UBM or i-vector paradigms. In addition, we consider two approaches to build PBMs: speaker-independent and speaker-dependent. We show that the proposed method significantly reduces the error rates of text-dependent speaker verification for the non-target types: target-wrong and imposter-wrong while it maintains comparable TD-SV performance when imposters speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases that consist of short utterances.\n    ",
        "submission_date": "2016-11-19T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06478",
        "title": "Visualizing Linguistic Shift",
        "authors": [
            "Salman Mahmood",
            "Rami Al-Rfou",
            "Klaus Mueller"
        ],
        "abstract": "Neural network based models are a very powerful tool for creating word embeddings, the objective of these models is to group similar words together. These embeddings have been used as features to improve results in various applications such as document classification, named entity recognition, etc. Neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography. The objective of this paper is to first identify and then visualize how words change meaning in different text corpus. We will train a neural language model on texts from a diverse set of disciplines philosophy, religion, fiction etc. Each text will alter the embeddings of the words to represent the meaning of the word inside that text. We will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage. We then use enhanced scatterplots and storyline visualization to visualize the linguistic shift.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06639",
        "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling",
        "authors": [
            "Peng Zhou",
            "Zhenyu Qi",
            "Suncong Zheng",
            "Jiaming Xu",
            "Hongyun Bao",
            "Bo Xu"
        ],
        "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06671",
        "title": "Ontology Driven Disease Incidence Detection on Twitter",
        "authors": [
            "Mark Abraham Magumba",
            "Peter Nabende"
        ],
        "abstract": "In this work we address the issue of generic automated disease incidence monitoring on twitter. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques. We also show that word vectors can be learned directly from our concepts to achieve even better results.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06722",
        "title": "False-Friend Detection and Entity Matching via Unsupervised Transliteration",
        "authors": [
            "Yanqing Chen",
            "Steven Skiena"
        ],
        "abstract": "Transliterations play an important role in multilingual entity reference resolution, because proper names increasingly travel between languages in news and social media. Previous work associated with machine translation targets transliteration only single between language pairs, focuses on specific classes of entities (such as cities and celebrities) and relies on manual curation, which limits the expression power of transliteration in multilingual environment.\n",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06788",
        "title": "Bidirectional Tree-Structured LSTM with Head Lexicalization",
        "authors": [
            "Zhiyang Teng",
            "Yue Zhang"
        ],
        "abstract": "Sequential LSTM has been extended to model tree structures, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain reference to input words for each node. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTM structurally. Experiments show that both extensions give better representations of tree structures. Our final model gives the best results on the Standford Sentiment Treebank and highly competitive results on the TREC question type classification task.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06986",
        "title": "Robust end-to-end deep audiovisual speech recognition",
        "authors": [
            "Ramon Sanabria",
            "Florian Metze",
            "Fernando De La Torre"
        ],
        "abstract": "Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging.\n",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06997",
        "title": "Coherent Dialogue with Attention-based Language Models",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We model coherent conversation continuation via RNN-based dialogue models equipped with a dynamic attention mechanism. Our attention-RNN language model dynamically increases the scope of attention on the history as the conversation continues, as opposed to standard attention (or alignment) models with a fixed input scope in a sequence-to-sequence model. This allows each generated word to be associated with the most relevant words in its corresponding conversation history. We evaluate the model on two popular dialogue datasets, the open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot dataset, and achieve significant improvements over the state-of-the-art and baselines on several metrics, including complementary diversity-based metrics, human evaluation, and qualitative visualizations. We also show that a vanilla RNN with dynamic attention outperforms more complex memory models (e.g., LSTM and GRU) by allowing for flexible, long-distance memory. We promote further coherence via topic modeling-based reranking.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07174",
        "title": "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition",
        "authors": [
            "Zewang Zhang",
            "Zheng Sun",
            "Jiaqi Liu",
            "Jingwen Chen",
            "Zhao Huo",
            "Xiao Zhang"
        ],
        "abstract": "A deep learning approach has been widely applied in sequence modeling problems. In terms of automatic speech recognition (ASR), its performance has significantly been improved by increasing large speech corpus and deeper neural network. Especially, recurrent neural network and deep convolutional neural network have been applied in ASR successfully. Given the arising problem of training speed, we build a novel deep recurrent convolutional network for acoustic modeling and then apply deep residual learning to it. Our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. In the experiments, we compare the convergence speed of our novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost the convergence speed of our novel deep recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate (PER) with our proposed bidirectional statistical n-gram language model. Our evaluation results show that our newly proposed deep recurrent convolutional network applied with deep residual learning can reach the best PER of 17.33\\% with the fastest convergence speed on TIMIT database. The outstanding performance of our novel deep recurrent convolutional neural network with deep residual learning indicates that it can be potentially adopted in other sequential problems.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07206",
        "title": "Learning to Distill: The Essence Vector Modeling Framework",
        "authors": [
            "Kuan-Yu Chen",
            "Shih-Hung Liu",
            "Berlin Chen",
            "Hsin-Min Wang"
        ],
        "abstract": "In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is relatively less work focusing on the development of unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07232",
        "title": "Compositional Learning of Relation Path Embedding for Knowledge Base Completion",
        "authors": [
            "Xixun Lin",
            "Yanchun Liang",
            "Fausto Giunchiglia",
            "Xiaoyue Feng",
            "Renchu Guan"
        ],
        "abstract": "Large-scale knowledge bases have currently reached impressive sizes; however, these knowledge bases are still far from complete. In addition, most of the existing methods for knowledge base completion only consider the direct links between entities, ignoring the vital impact of the consistent semantics of relation paths. In this paper, we study the problem of how to better embed entities and relations of knowledge bases into different low-dimensional spaces by taking full advantage of the additional semantics of relation paths, and we propose a compositional learning model of relation path embedding (RPE). Specifically, with the corresponding relation and path projections, RPE can simultaneously embed each entity into two types of latent spaces. It is also proposed that type constraints could be extended from traditional relation-specific constraints to the new proposed path-specific constraints. The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2017-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07804",
        "title": "ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition Methods in Scala",
        "authors": [
            "N. Astrakhantsev"
        ],
        "abstract": "Automatically recognized terminology is widely used for various domain-specific texts processing tasks, such as machine translation, information retrieval or sentiment analysis. However, there is still no agreement on which methods are best suited for particular settings and, moreover, there is no reliable comparison of already developed methods. We believe that one of the main reasons is the lack of state-of-the-art methods implementations, which are usually non-trivial to recreate. In order to address these issues, we present ATR4S, an open-source software written in Scala that comprises more than 15 methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidates scoring, and finally, term candidates ranking. It is highly scalable, modular and configurable tool with support of automatic caching. We also compare 10 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07897",
        "title": "Learning Generic Sentence Representations Using Convolutional Neural Networks",
        "authors": [
            "Zhe Gan",
            "Yunchen Pu",
            "Ricardo Henao",
            "Chunyuan Li",
            "Xiaodong He",
            "Lawrence Carin"
        ],
        "abstract": "We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07954",
        "title": "Emergent Predication Structure in Hidden State Vectors of Neural Readers",
        "authors": [
            "Hai Wang",
            "Takeshi Onishi",
            "Kevin Gimpel",
            "David McAllester"
        ],
        "abstract": "A significant number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \"predication structure\" in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas $\\Phi[c]$ where $\\Phi$ is a semantic property (predicate) and $c$ is a constant symbol entity identifier.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08034",
        "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling",
        "authors": [
            "Zhe Gan",
            "Chunyuan Li",
            "Changyou Chen",
            "Yunchen Pu",
            "Qinliang Su",
            "Lawrence Carin"
        ],
        "abstract": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach over stochastic optimization.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08358",
        "title": "Kannada Spell Checker with Sandhi Splitter",
        "authors": [
            "A N Akshatha",
            "Chandana G Upadhyaya",
            "Rajashekara S Murthy"
        ],
        "abstract": "Spelling errors are introduced in text either during typing, or when the user does not know the correct phoneme or grapheme. If a language contains complex words like sandhi where two or more morphemes join based on some rules, spell checking becomes very tedious. In such situations, having a spell checker with sandhi splitter which alerts the user by flagging the errors and providing suggestions is very useful. A novel algorithm of sandhi splitting is proposed in this paper. The sandhi splitter can split about 7000 most common sandhi words in Kannada language used as test samples. The sandhi splitter was integrated with a Kannada spell checker and a mechanism for generating suggestions was added. A comprehensive, platform independent, standalone spell checker with sandhi splitter application software was thus developed and tested extensively for its efficiency and correctness. A comparative analysis of this spell checker with sandhi splitter was made and results concluded that the Kannada spell checker with sandhi splitter has an improved performance. It is twice as fast, 200 times more space efficient, and it is 90% accurate in case of complex nouns and 50% accurate for complex verbs. Such a spell checker with sandhi splitter will be of foremost significance in machine translation systems, voice processing, etc. This is the first sandhi splitter in Kannada and the advantage of the novel algorithm is that, it can be extended to all Indian languages.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08459",
        "title": "Neural Machine Translation with Latent Semantic of Image and Text",
        "authors": [
            "Joji Toyama",
            "Masanori Misono",
            "Masahiro Suzuki",
            "Kotaro Nakayama",
            "Yutaka Matsuo"
        ],
        "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English--German translation task show that our model outperforms over the baseline.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08562",
        "title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Dan Jurafsky"
        ],
        "abstract": "In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed.\n",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08656",
        "title": "Attention-based Memory Selection Recurrent Network for Language Modeling",
        "authors": [
            "Da-Rong Liu",
            "Shun-Po Chuang",
            "Hung-yi Lee"
        ],
        "abstract": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08661",
        "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
        "authors": [
            "Jiacheng Xu",
            "Kan Chen",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "abstract": "The objective of knowledge graph embedding is to encode both entities and relations of knowledge graphs into continuous low-dimensional vector spaces. Previously, most works focused on symbolic representation of knowledge graph with structure information, which can not handle new entities or entities with few facts well. In this paper, we propose a novel deep architecture to utilize both structural and textual information of entities. Specifically, we introduce three neural models to encode the valuable information from text description of entity, among which an attentive model can select related information as needed. Then, a gating mechanism is applied to integrate representations of structure and text into a unified architecture. Experiments show that our models outperform baseline by margin on link prediction and triplet classification tasks. Source codes of this paper will be available on Github.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08765",
        "title": "Fill it up: Exploiting partial dependency annotations in a minimum spanning tree parser",
        "authors": [
            "Liang Sun",
            "Jason Mielens",
            "Jason Baldridge"
        ],
        "abstract": "Unsupervised models of dependency parsing typically require large amounts of clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect supervision (e.g. language universals and rules) can help, but we show that obtaining small amounts of direct supervision - here, partial dependency annotations - provides a strong balance between zero and full supervision. We adapt the unsupervised ConvexMST dependency parser to learn from partial dependencies expressed in the Graph Fragment Language. With less than 24 hours of total annotation, we obtain 7% and 17% absolute improvement in unlabeled dependency scores for English and Spanish, respectively, compared to the same parser using only universal grammar constraints.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08807",
        "title": "The polysemy of the words that children learn over time",
        "authors": [
            "Bernardino Casas",
            "Neus Catal\u00e0",
            "Ramon Ferrer-i-Cancho",
            "Antoni Hern\u00e1ndez-Fern\u00e1ndez",
            "Jaume Baixeries"
        ],
        "abstract": "Here we study polysemy as a potential learning bias in vocabulary learning in children. Words of low polysemy could be preferred as they reduce the disambiguation effort for the listener. However, such preference could be a side-effect of another bias: the preference of children for nouns in combination with the lower polysemy of nouns with respect to other part-of-speech categories. Our results show that mean polysemy in children increases over time in two phases, i.e. a fast growth till the 31st month followed by a slower tendency towards adult speech. In contrast, this evolution is not found in adults interacting with children. This suggests that children have a preference for non-polysemous words in their early stages of vocabulary acquisition. Interestingly, the evolutionary pattern described above weakens when controlling for syntactic category (noun, verb, adjective or adverb) but it does not disappear completely, suggesting that it could result from acombination of a standalone bias for low polysemy and a preference for nouns.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2019-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08813",
        "title": "Semi Supervised Preposition-Sense Disambiguation using Multilingual Data",
        "authors": [
            "Hila Gonen",
            "Yoav Goldberg"
        ],
        "abstract": "Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised preposition-sense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08945",
        "title": "Learning a Natural Language Interface with Neural Programmer",
        "authors": [
            "Arvind Neelakantan",
            "Quoc V. Le",
            "Martin Abadi",
            "Andrew McCallum",
            "Dario Amodei"
        ],
        "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08987",
        "title": "Exploiting Unlabeled Data for Neural Grammatical Error Detection",
        "authors": [
            "Zhuoran Liu",
            "Yang Liu"
        ],
        "abstract": "Identifying and correcting grammatical errors in the text written by non-native writers has received increasing attention in recent years. Although a number of annotated corpora have been established to facilitate data-driven grammatical error detection and correction approaches, they are still limited in terms of quantity and coverage because human annotation is labor-intensive, time-consuming, and expensive. In this work, we propose to utilize unlabeled data to train neural network based grammatical error detection models. The basic idea is to cast error detection as a binary classification problem and derive positive and negative training examples from unlabeled data. We introduce an attention-based neural network to capture long-distance dependencies that influence the word being detected. Experiments show that the proposed approach significantly outperforms SVMs and convolutional networks with fixed-size context window.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09020",
        "title": "Developing a cardiovascular disease risk factor annotated corpus of Chinese electronic medical records",
        "authors": [
            "Jia Su",
            "Bin He",
            "Yi Guan",
            "Jingchi Jiang",
            "Jinfeng Yang"
        ],
        "abstract": "Cardiovascular disease (CVD) has become the leading cause of death in China, and most of the cases can be prevented by controlling risk factors. The goal of this study was to build a corpus of CVD risk factor annotations based on Chinese electronic medical records (CEMRs). This corpus is intended to be used to develop a risk factor information extraction system that, in turn, can be applied as a foundation for the further study of the progress of risk factors and CVD. We designed a light annotation task to capture CVD risk factors with indicators, temporal attributes and assertions that were explicitly or implicitly displayed in the records. The task included: 1) preparing data; 2) creating guidelines for capturing annotations (these were created with the help of clinicians); 3) proposing an annotation method including building the guidelines draft, training the annotators and updating the guidelines, and corpus construction. Then, a risk factor annotated corpus based on de-identified discharge summaries and progress notes from 600 patients was developed. Built with the help of clinicians, this corpus has an inter-annotator agreement (IAA) F1-measure of 0.968, indicating a high reliability. To the best of our knowledge, this is the first annotated corpus concerning CVD risk factors in CEMRs and the guidelines for capturing CVD risk factor annotations from CEMRs were proposed. The obtained document-level annotations can be applied in future studies to monitor risk factors and CVD over the long term.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09100",
        "title": "Learning to Compose Words into Sentences with Reinforcement Learning",
        "authors": [
            "Dani Yogatama",
            "Phil Blunsom",
            "Chris Dyer",
            "Edward Grefenstette",
            "Wang Ling"
        ],
        "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09207",
        "title": "AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech",
        "authors": [
            "Brian Patton",
            "Yannis Agiomyrgiannakis",
            "Michael Terry",
            "Kevin Wilson",
            "Rif A. Saurous",
            "D. Sculley"
        ],
        "abstract": "Developers of text-to-speech synthesizers (TTS) often make use of human raters to assess the quality of synthesized speech. We demonstrate that we can model human raters' mean opinion scores (MOS) of synthesized speech using a deep recurrent neural network whose inputs consist solely of a raw waveform. Our best models provide utterance-level estimates of MOS only moderately inferior to sampled human ratings, as shown by Pearson and Spearman correlations. When multiple utterances are scored and averaged, a scenario common in synthesizer quality assessment, AutoMOS achieves correlations approaching those of human raters. The AutoMOS model has a number of applications, such as the ability to explore the parameter space of a speech synthesizer without requiring a human-in-the-loop.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09235",
        "title": "Joint Copying and Restricted Generation for Paraphrase",
        "authors": [
            "Ziqiang Cao",
            "Chuwei Luo",
            "Wenjie Li",
            "Sujian Li"
        ],
        "abstract": "Many natural language generation tasks, such as abstractive summarization and text simplification, are paraphrase-orientated. In these tasks, copying and rewriting are two main writing modes. \nMost previous sequence-to-sequence (Seq2Seq) models use a single decoder and neglect this fact. In this paper, we develop a novel Seq2Seq model to fuse a copying decoder and a restricted generative decoder. The copying decoder finds the position to be copied based on a typical attention model. The generative decoder produces words limited in the source-specific vocabulary. To combine the two decoders and determine the final output, we develop a predictor to predict the mode of copying or rewriting. This predictor can be guided by the actual writing mode in the training data. We conduct extensive experiments on two different paraphrase datasets. The result shows that our model outperforms the state-of-the-art approaches in terms of both informativeness and language quality.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09238",
        "title": "Improving Multi-Document Summarization via Text Classification",
        "authors": [
            "Ziqiang Cao",
            "Wenjie Li",
            "Sujian Li",
            "Furu Wei"
        ],
        "abstract": "Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09268",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
        "authors": [
            "Payal Bajaj",
            "Daniel Campos",
            "Nick Craswell",
            "Li Deng",
            "Jianfeng Gao",
            "Xiaodong Liu",
            "Rangan Majumder",
            "Andrew McNamara",
            "Bhaskar Mitra",
            "Tri Nguyen",
            "Mir Rosenberg",
            "Xia Song",
            "Alina Stoica",
            "Saurabh Tiwary",
            "Tong Wang"
        ],
        "abstract": "We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2018-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09288",
        "title": "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech Recognition",
        "authors": [
            "Tom Sercu",
            "Vaibhava Goel"
        ],
        "abstract": "In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of framewise classification offers several advantages and insights, including computational efficiency and the ability to apply batch normalization. When doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution, called time-dilated convolution, that allows for efficient and elegant implementation of pooling in time. We show results using time-dilated convolutions in a very deep VGG-style CNN with batch normalization on the Hub5 Switchboard-2000 benchmark task. With a big n-gram language model, we achieve 7.7% WER which is the best single model single-pass performance reported so far.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09405",
        "title": "An End-to-End Architecture for Keyword Spotting and Voice Activity Detection",
        "authors": [
            "Chris Lengerich",
            "Awni Hannun"
        ],
        "abstract": "We propose a single neural network architecture for two tasks: on-line keyword spotting and voice activity detection. We develop novel inference algorithms for an end-to-end Recurrent Neural Network trained with the Connectionist Temporal Classification loss function which allow our model to achieve high accuracy on both keyword spotting and voice activity detection without retraining. In contrast to prior voice activity detection models, our architecture does not require aligned training data and uses the same parameters as the keyword spotting model. This allows us to deploy a high quality voice activity detector with no additional memory or maintenance requirements.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09441",
        "title": "Sentiment Analysis for Twitter : Going Beyond Tweet Text",
        "authors": [
            "Lahari Poddar",
            "Kishaloy Halder",
            "Xianyan Jia"
        ],
        "abstract": "Analysing sentiment of tweets is important as it helps to determine the users' opinion. Knowing people's opinion is crucial for several purposes starting from gathering knowledge about customer base, e-governance, campaigning and many more. In this report, we aim to develop a system to detect the sentiment from tweets. We employ several linguistic features along with some other external sources of information to detect the sentiment of a tweet. We show that augmenting the 140 character-long tweet with information harvested from external urls shared in the tweet as well as Social Media features enhances the sentiment prediction accuracy significantly.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09703",
        "title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned Corpora and Theorem Proving",
        "authors": [
            "Cezary Kaliszyk",
            "Josef Urban",
            "Ji\u0159\u00ed Vysko\u010dil"
        ],
        "abstract": "We study methods for automated parsing of informal mathematical expressions into formal ones, a main prerequisite for deep computer understanding of informal mathematical texts. We propose a context-based parsing approach that combines efficient statistical learning of deep parse trees with their semantic pruning by type checking and large-theory automated theorem proving. We show that the methods very significantly improve on previous results in parsing theorems from the Flyspeck corpus.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09799",
        "title": "Geometry of Compositionality",
        "authors": [
            "Hongyu Gong",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "abstract": "This paper proposes a simple test for compositionality (i.e., literal usage) of a word or phrase in a context-specific way. The test is computationally simple, relying on no external resources and only uses a set of trained word vectors. Experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena (idiomaticity, sarcasm, metaphor) for different datasets in multiple languages. The key insight is to connect compositionality to a curious geometric property of word embeddings, which is of independent interest.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09830",
        "title": "NewsQA: A Machine Comprehension Dataset",
        "authors": [
            "Adam Trischler",
            "Tong Wang",
            "Xingdi Yuan",
            "Justin Harris",
            "Alessandro Sordoni",
            "Philip Bachman",
            "Kaheer Suleman"
        ],
        "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09878",
        "title": "Identity-sensitive Word Embedding through Heterogeneous Networks",
        "authors": [
            "Jian Tang",
            "Meng Qu",
            "Qiaozhu Mei"
        ],
        "abstract": "Most existing word embedding approaches do not distinguish the same words in different contexts, therefore ignoring their contextual meanings. As a result, the learned embeddings of these words are usually a mixture of multiple meanings. In this paper, we acknowledge multiple identities of the same word in different contexts and learn the \\textbf{identity-sensitive} word embeddings. Based on an identity-labeled text corpora, a heterogeneous network of words and word identities is constructed to model different-levels of word co-occurrences. The heterogeneous network is further embedded into a low-dimensional space through a principled network embedding approach, through which we are able to obtain the embeddings of words and the embeddings of word identities. We study three different types of word identities including topics, sentiments and categories. Experimental results on real-world data sets show that the identity-sensitive word embeddings learned by our approach indeed capture different meanings of words and outperforms competitive methods on tasks including text classification and word similarity computation.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09900",
        "title": "Context-aware Natural Language Generation with Recurrent Neural Networks",
        "authors": [
            "Jian Tang",
            "Yifan Yang",
            "Sam Carton",
            "Ming Zhang",
            "Qiaozhu Mei"
        ],
        "abstract": "This paper studied generating natural languages at particular contexts or situations. We proposed two novel approaches which encode the contexts into a continuous semantic representation and then decode the semantic representation into text sequences with recurrent neural networks. During decoding, the context information are attended through a gating mechanism, addressing the problem of long-range dependency caused by lengthy sequences. We evaluate the effectiveness of the proposed approaches on user review data, in which rich contexts are available and two informative contexts, sentiments and products, are selected for evaluation. Experiments show that the fake reviews generated by our approaches are very natural. Results of fake review detection with human judges show that more than 50\\% of the fake reviews are misclassified as the real reviews, and more than 90\\% are misclassified by existing state-of-the-art fake review detection algorithm.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10038",
        "title": "Towards Accurate Word Segmentation for Chinese Patents",
        "authors": [
            "Si Li",
            "Nianwen Xue"
        ],
        "abstract": "A patent is a property right for an invention granted by the government to the inventor. An invention is a solution to a specific technological problem. So patents often have a high concentration of scientific and technical terms that are rare in everyday language. The Chinese word segmentation model trained on currently available everyday language data sets performs poorly because it cannot effectively recognize these scientific and technical terms. In this paper we describe a pragmatic approach to Chinese word segmentation on patents where we train a character-based semi-supervised sequence labeling model by extracting features from a manually segmented corpus of 142 patents, enhanced with information extracted from the Chinese TreeBank. Experiments show that the accuracy of our model reached 95.08% (F1 score) on a held-out test set and 96.59% on development set, compared with an F1 score of 91.48% on development set if the model is trained on the Chinese TreeBank. We also experimented with some existing domain adaptation techniques, the results show that the amount of target domain data and the selected features impact the performance of the domain adaptation techniques.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10122",
        "title": "Deep encoding of etymological information in TEI",
        "authors": [
            "Jack Bowers",
            "Laurent Romary"
        ],
        "abstract": "This paper aims to provide a comprehensive modeling and representation of etymological data in digital dictionaries. The purpose is to integrate in one coherent framework both digital representations of legacy dictionaries, and also born-digital lexical databases that are constructed manually or semi-automatically. We want to propose a systematic and coherent set of modeling principles for a variety of etymological phenomena that may contribute to the creation of a continuum between existing and future lexical constructs, where anyone interested in tracing the history of words and their meanings will be able to seamlessly query lexical ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.10277",
        "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge",
        "authors": [
            "Ryan J. Gallagher",
            "Kyle Reing",
            "David Kale",
            "Greg Ver Steeg"
        ],
        "abstract": "While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize generative models to incorporate human input. We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an information-theoretic framework. This framework naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.\n    ",
        "submission_date": "2016-11-30T00:00:00",
        "last_modified_date": "2018-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00148",
        "title": "Domain Adaptation for Named Entity Recognition in Online Media with Word Embeddings",
        "authors": [
            "Vivek Kulkarni",
            "Yashar Mehdad",
            "Troy Chevalier"
        ],
        "abstract": "Content on the Internet is heterogeneous and arises from various domains like News, Entertainment, Finance and Technology. Understanding such content requires identifying named entities (persons, places and organizations) as one of the key steps. Traditionally Named Entity Recognition (NER) systems have been built using available annotated datasets (like CoNLL, MUC) and demonstrate excellent performance. However, these models fail to generalize onto other domains like Sports and Finance where conventions and language use can differ significantly. Furthermore, several domains do not have large amounts of annotated labeled data for training robust Named Entity Recognition models. A key step towards this challenge is to adapt models learned on domains where large amounts of annotated training data are available to domains with scarce annotated data.\n",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00246",
        "title": "Multilingual Multiword Expressions",
        "authors": [
            "Lahari Poddar"
        ],
        "abstract": "The project aims to provide a semi-supervised approach to identify Multiword Expressions in a multilingual context consisting of English and most of the major Indian languages. Multiword expressions are a group of words which refers to some conventional or regional way of saying things. If they are literally translated from one language to another the expression will lose its inherent meaning.\n",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00347",
        "title": "Bootstrapping incremental dialogue systems: using linguistic knowledge to learn from minimal data",
        "authors": [
            "Dimitrios Kalatzis",
            "Arash Eshghi",
            "Oliver Lemon"
        ],
        "abstract": "We present a method for inducing new dialogue systems from very small amounts of unannotated dialogue data, showing how word-level exploration using Reinforcement Learning (RL), combined with an incremental and semantic grammar - Dynamic Syntax (DS) - allows systems to discover, generate, and understand many new dialogue variants. The method avoids the use of expensive and time-consuming dialogue act annotations, and supports more natural (incremental) dialogues than turn-based systems. Here, language generation and dialogue management are treated as a joint decision/optimisation problem, and the MDP model for RL is constructed automatically. With an implemented system, we show that this method enables a wide range of dialogue variations to be automatically captured, even when the system is trained from only a single dialogue. The variants include question-answer pairs, over- and under-answering, self- and other-corrections, clarification interaction, split-utterances, and ellipsis. This generalisation property results from the structural knowledge and constraints present within the DS grammar, and highlights some limitations of recent systems built using machine learning techniques only.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00377",
        "title": "Piecewise Latent Variables for Neural Variational Text Processing",
        "authors": [
            "Iulian V. Serban",
            "Alexander G. Ororbia II",
            "Joelle Pineau",
            "Aaron Courville"
        ],
        "abstract": "Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00394",
        "title": "Definition Modeling: Learning to define word embeddings in natural language",
        "authors": [
            "Thanapon Noraset",
            "Chen Liang",
            "Larry Birnbaum",
            "Doug Downey"
        ],
        "abstract": "Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00467",
        "title": "Neural Document Embeddings for Intensive Care Patient Mortality Prediction",
        "authors": [
            "Paulina Grnarova",
            "Florian Schmidt",
            "Stephanie L. Hyland",
            "Carsten Eickhoff"
        ],
        "abstract": "We present an automatic mortality prediction scheme based on the unstructured textual content of clinical notes. Proposing a convolutional document embedding approach, our empirical investigation using the MIMIC-III intensive care database shows significant performance gains compared to previously employed methods such as latent topic distributions or generic doc2vec embeddings. These improvements are especially pronounced for the difficult problem of post-discharge mortality prediction.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00567",
        "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features",
        "authors": [
            "Jiangming Liu",
            "Yue Zhang"
        ],
        "abstract": "Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00584",
        "title": "Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons",
        "authors": [
            "Yuanzhi Ke",
            "Masafumi Hagiwara"
        ],
        "abstract": "Though there are some works on improving distributed word representations using lexicons, the improper overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when lexicons are used, which needs to be solved. An alternative method is to allocate a vector per sense instead of a vector per word. However, the word representations estimated in the former way are not as easy to use as the latter one. Our previous work uses a probabilistic method to alleviate the overfitting, but it is not robust with a small corpus. In this paper, we propose a new neural network to estimate distributed word representations using a lexicon and a corpus. We add a lexicon layer in the continuous bag-of-words model and a threshold node after the output of the lexicon layer. The threshold rejects the unreliable outputs of the lexicon layer that are less likely to be the same with their inputs. In this way, it alleviates the overfitting of the polysemous words. The proposed neural network can be trained using negative sampling, which maximizing the log probabilities of target words given the context words, by distinguishing the target words from random noises. We compare the proposed neural network with the continuous bag-of-words model, the other works improving it, and the previous works estimating distributed word representations using both a lexicon and a corpus. The experimental results show that the proposed neural network is more efficient and balanced for both semantic tasks and syntactic tasks than the previous works, and robust to the size of the corpus.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00694",
        "title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA",
        "authors": [
            "Song Han",
            "Junlong Kang",
            "Huizi Mao",
            "Yiming Hu",
            "Xin Li",
            "Yubin Li",
            "Dongliang Xie",
            "Hong Luo",
            "Song Yao",
            "Yu Wang",
            "Huazhong Yang",
            "William J. Dally"
        ],
        "abstract": "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built larger and larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption and leads to high total cost of ownership (TCO) of a data center. In order to speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose scheduler that encodes and partitions the compressed model to each PE for parallelism, and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the compressed model. Implemented on Xilinx XCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the compressed LSTM network, corresponding to 2.52 TOPS on the uncompressed one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40x and 11.5x higher energy efficiency compared with the CPU and GPU respectively.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00729",
        "title": "Automated assessment of non-native learner essays: Investigating the role of linguistic features",
        "authors": [
            "Sowmya Vajjala"
        ],
        "abstract": "Automatic essay scoring (AES) refers to the process of scoring free text responses to given prompts, considering human grader scores as the gold standard. Writing such essays is an essential component of many language and aptitude exams. Hence, AES became an active and established area of research, and there are many proprietary systems used in real life applications today. However, not much is known about which specific linguistic features are useful for prediction and how much of this is consistent across datasets. This article addresses that by exploring the role of various linguistic features in automatic essay scoring using two publicly available datasets of non-native English essays written in test taking scenarios. The linguistic properties are modeled by encoding lexical, syntactic, discourse and error types of learner language in the feature set. Predictive models are then developed using these features on both datasets and the most predictive features are compared. While the results show that the feature set used results in good predictive models with both datasets, the question \"what are the most predictive features?\" has a different answer for each dataset.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00866",
        "title": "Creating a Real-Time, Reproducible Event Dataset",
        "authors": [
            "John Beieler"
        ],
        "abstract": "The generation of political event data has remained much the same since the mid-1990s, both in terms of data acquisition and the process of coding text into data. Since the 1990s, however, there have been significant improvements in open-source natural language processing software and in the availability of digitized news content. This paper presents a new, next-generation event dataset, named Phoenix, that builds from these and other advances. This dataset includes improvements in the underlying news collection process and event coding software, along with the creation of a general processing pipeline necessary to produce daily-updated data. This paper provides a face validity checks by briefly examining the data for the conflict in Syria, and a comparison between Phoenix and the Integrated Crisis Early Warning System data.\n    ",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00913",
        "title": "End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager",
        "authors": [
            "Xuesong Yang",
            "Yun-Nung Chen",
            "Dilek Hakkani-Tur",
            "Paul Crook",
            "Xiujun Li",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2017-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00969",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "authors": [
            "Subhro Roy",
            "Dan Roth"
        ],
        "abstract": "Math word problems provide a natural abstraction to a range of natural language understanding problems that involve reasoning about quantities, such as interpreting election results, news about casualties, and the financial section of a newspaper. Units associated with the quantities often provide information that is essential to support this reasoning. This paper proposes a principled way to capture and reason about units and shows how it can benefit an arithmetic word problem solver. This paper presents the concept of Unit Dependency Graphs (UDGs), which provides a compact representation of the dependencies between units of numbers mentioned in a given problem. Inducing the UDG alleviates the brittleness of the unit extraction system and allows for a natural way to leverage domain knowledge about unit compatibility, for word problem solving. We introduce a decomposed model for inducing UDGs with minimal additional annotations, and use it to augment the expressions used in the arithmetic word problem solver of (Roy and Roth 2015) via a constrained inference framework. We show that introduction of UDGs reduces the error of the solver by over 10 %, surpassing all existing systems for solving arithmetic word problems. In addition, it also makes the system more robust to adaptation to new vocabulary and equation forms .\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01039",
        "title": "CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews",
        "authors": [
            "Hu Xu",
            "Sihong Xie",
            "Lei Shu",
            "Philip S. Yu"
        ],
        "abstract": "Product reviews contain a lot of useful information about product features and customer opinions. One important product feature is the complementary entity (products) that may potentially work together with the reviewed product. Knowing complementary entities of the reviewed product is very important because customers want to buy compatible products and avoid incompatible ones. In this paper, we address the problem of Complementary Entity Recognition (CER). Since no existing method can solve this problem, we first propose a novel unsupervised method to utilize syntactic dependency paths to recognize complementary entities. Then we expand category-level domain knowledge about complementary entities using only a few general seed verbs on a large amount of unlabeled reviews. The domain knowledge helps the unsupervised method to adapt to different products and greatly improves the precision of the CER task. The advantage of the proposed method is that it does not require any labeled data for training. We conducted experiments on 7 popular products with about 1200 reviews in total to demonstrate that the proposed approach is effective.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01197",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)",
        "authors": [
            "Chen Liang",
            "Jonathan Berant",
            "Quoc Le",
            "Kenneth D. Forbus",
            "Ni Lao"
        ],
        "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.\n    ",
        "submission_date": "2016-12-04T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01340",
        "title": "We used Neural Networks to Detect Clickbaits: You won't believe what happened Next!",
        "authors": [
            "Ankesh Anand",
            "Tanmoy Chakraborty",
            "Noseong Park"
        ],
        "abstract": "Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2019-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01404",
        "title": "Mapping the Dialog Act Annotations of the LEGO Corpus into the Communicative Functions of ISO 24617-2",
        "authors": [
            "Eug\u00e9nio Ribeiro",
            "Ricardo Ribeiro",
            "David Martins de Matos"
        ],
        "abstract": "In this paper we present strategies for mapping the dialog act annotations of the LEGO corpus into the communicative functions of the ISO 24617-2 standard. Using these strategies, we obtained an additional 347 dialogs annotated according to the standard. This is particularly important given the reduced amount of existing data in those conditions due to the recency of the standard. Furthermore, these are dialogs from a widely explored corpus for dialog related tasks. However, its dialog annotations have been neglected due to their high domain-dependency, which renders them unuseful outside the context of the corpus. Thus, through our mapping process, we both obtain more data annotated according to a recent standard and provide useful dialog act annotations for a widely explored corpus in the context of dialog research.\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2016-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01556",
        "title": "The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers",
        "authors": [
            "Mika Viking M\u00e4ntyl\u00e4",
            "Daniel Graziotin",
            "Miikka Kuutila"
        ],
        "abstract": "Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6,996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990's. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis\n    ",
        "submission_date": "2016-12-05T00:00:00",
        "last_modified_date": "2017-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01627",
        "title": "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots",
        "authors": [
            "Yu Wu",
            "Wei Wu",
            "Chen Xing",
            "Ming Zhou",
            "Zhoujun Li"
        ],
        "abstract": "We study response selection for multi-turn conversation in retrieval-based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN. An empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2017-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01744",
        "title": "Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation",
        "authors": [
            "Alexandre Berard",
            "Olivier Pietquin",
            "Christophe Servan",
            "Laurent Besacier"
        ],
        "abstract": "This paper proposes a first attempt to build an end-to-end speech-to-text translation system, which does not use source language transcription during learning or decoding. We propose a model for direct speech-to-text translation, which gives promising results on a small French-English synthetic corpus. Relaxing the need for source language transcription would drastically change the data collection methodology in speech translation, especially in under-resourced scenarios. For instance, in the former project DARPA TRANSTAC (speech translation from spoken Arabic dialects), a large effort was devoted to the collection of speech transcripts (and a prerequisite to obtain transcripts was often a detailed transcription guide for languages with little standardized spelling). Now, if end-to-end approaches for speech-to-text translation are successful, one might consider collecting data by asking bilingual speakers to directly utter speech in the source language from target language text utterances. Such an approach has the advantage to be applicable to any unwritten (source) language.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01848",
        "title": "Condensed Memory Networks for Clinical Diagnostic Inferencing",
        "authors": [
            "Aaditya Prakash",
            "Siyuan Zhao",
            "Sadid A. Hasan",
            "Vivek Datla",
            "Kathy Lee",
            "Ashequl Qadir",
            "Joey Liu",
            "Oladimeji Farri"
        ],
        "abstract": "Diagnosis of a clinical condition is a challenging task, which often requires significant medical investigation. Previous work related to diagnostic inferencing problems mostly consider multivariate observational data (e.g. physiological signals, lab tests etc.). In contrast, we explore the problem using free-text medical notes recorded in an electronic health record (EHR). Complex tasks like these can benefit from structured knowledge bases, but those are not scalable. We instead exploit raw text from Wikipedia as a knowledge source. Memory networks have been demonstrated to be effective in tasks which require comprehension of free-form text. They use the final iteration of the learned representation to predict probable classes. We introduce condensed memory neural networks (C-MemNNs), a novel model with iterative condensation of memory representations that preserves the hierarchy of features in the memory. Experiments on the MIMIC-III dataset show that the proposed model outperforms other variants of memory networks to predict the most probable diagnoses given a complex clinical scenario.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2017-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01928",
        "title": "Invariant Representations for Noisy Speech Recognition",
        "authors": [
            "Dmitriy Serdyuk",
            "Kartik Audhkhasi",
            "Phil\u00e9mon Brakel",
            "Bhuvana Ramabhadran",
            "Samuel Thomas",
            "Yoshua Bengio"
        ],
        "abstract": "Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2016-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02251",
        "title": "When is multitask learning effective? Semantic sequence prediction under varying data conditions",
        "authors": [
            "H\u00e9ctor Mart\u00ednez Alonso",
            "Barbara Plank"
        ],
        "abstract": "Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on when MTL works and whether there are data characteristics that help to determine its success. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary tasks, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02482",
        "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
        "authors": [
            "Krupakar Hans",
            "R S Milton"
        ],
        "abstract": "The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English - Tamil language pair was selected for this analysis. First, the use of Word2Vec embedding for both the English and Tamil words improved the translation results by 0.73 BLEU points over the baseline RNNSearch model with 4.84 BLEU score. The use of morphological segmentation before word vectorization to split the morphologically rich Tamil words into their respective morphemes before the translation, caused a reduction in the target vocabulary size by a factor of 8. Also, this model (RNNMorph) improved the performance of neural machine translation by 7.05 BLEU points over the RNNSearch model used over the same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in the number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism.\n    ",
        "submission_date": "2016-12-07T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02703",
        "title": "Embedding Words and Senses Together via Joint Knowledge-Enhanced Training",
        "authors": [
            "Massimiliano Mancini",
            "Jose Camacho-Collados",
            "Ignacio Iacobacci",
            "Roberto Navigli"
        ],
        "abstract": "Word embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new model which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02706",
        "title": "Entity Identification as Multitasking",
        "authors": [
            "Karl Stratos"
        ],
        "abstract": "Standard approaches in entity identification hard-code boundary detection and type prediction into labels (e.g., John/B-PER Smith/I-PER) and then perform Viterbi. This has two disadvantages: 1. the runtime complexity grows quadratically in the number of types, and 2. there is no natural segment-level representation. In this paper, we propose a novel neural architecture that addresses these disadvantages. We frame the problem as multitasking, separating boundary detection and type prediction but optimizing them jointly. Despite its simplicity, this architecture performs competitively with fully structured models such as BiLSTM-CRFs while scaling linearly in the number of types. Furthermore, by construction, the model induces type-disambiguating embeddings of predicted mentions.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02801",
        "title": "Discovering Conversational Dependencies between Messages in Dialogs",
        "authors": [
            "Wenchao Du",
            "Pascal Poupart",
            "Wei Xu"
        ],
        "abstract": "We investigate the task of inferring conversational dependencies between messages in one-on-one online chat, which has become one of the most popular forms of customer service. We propose a novel probabilistic classifier that leverages conversational, lexical and semantic information. The approach is evaluated empirically on a set of customer service chat logs from a Chinese e-commerce website. It outperforms heuristic baselines.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03205",
        "title": "Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting",
        "authors": [
            "Peter Potash",
            "Alexey Romanov",
            "Anna Rumshisky"
        ],
        "abstract": "Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider creativity, style, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluation methods for one such task, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions of this task. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel evaluation methodology that addresses several complementary aspects of this task, and illustrate how such evaluation can be used to meaningfully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2016-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03216",
        "title": "#HashtagWars: Learning a Sense of Humor",
        "authors": [
            "Peter Potash",
            "Alexey Romanov",
            "Anna Rumshisky"
        ],
        "abstract": "In this work, we present a new dataset for computational humor, specifically comparative humor ranking, which attempts to eschew the ubiquitous binary approach to humor detection. The dataset consists of tweets that are humorous responses to a given hashtag. We describe the motivation for this new dataset, as well as the collection process, which includes a description of our semi-automated system for data collection. We also present initial experiments for this dataset using both unsupervised and supervised approaches. Our best supervised system achieved 63.7% accuracy, suggesting that this task is much more difficult than comparable humor detection tasks. Initial experiments indicate that a character-level model is more suitable for this task than a token-level model, likely due to a large amount of puns that can be captured by a character-level model.\n    ",
        "submission_date": "2016-12-09T00:00:00",
        "last_modified_date": "2017-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03226",
        "title": "Active Learning for Speech Recognition: the Power of Gradients",
        "authors": [
            "Jiaji Huang",
            "Rewon Child",
            "Vinay Rao",
            "Hairong Liu",
            "Sanjeev Satheesh",
            "Adam Coates"
        ],
        "abstract": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03266",
        "title": "A Character-Word Compositional Neural Language Model for Finnish",
        "authors": [
            "Matti Lankinen",
            "Hannes Heikinheimo",
            "Pyry Takala",
            "Tapani Raiko",
            "Juha Karhunen"
        ],
        "abstract": "Inspired by recent research, we explore ways to model the highly morphological Finnish language at the level of characters while maintaining the performance of word-level models. We propose a new Character-to-Word-to-Character (C2W2C) compositional language model that uses characters as input and output while still internally processing word level embeddings. Our preliminary experiments, using the Finnish Europarl V7 corpus, indicate that C2W2C can respond well to the challenges of morphologically rich languages such as high out of vocabulary rates, the prediction of novel words, and growing vocabulary size. Notably, the model is able to correctly score inflectional forms that are not present in the training data and sample grammatically and semantically correct Finnish sentences character by character.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03551",
        "title": "Reading Comprehension using Entity-based Memory Network",
        "authors": [
            "Xun Wang",
            "Katsuhito Sudoh",
            "Masaaki Nagata",
            "Tomohide Shibata",
            "Daisuke Kawahara",
            "Sadao Kurohashi"
        ],
        "abstract": "This paper introduces a novel neural network model for question answering, the \\emph{entity-based memory network}. It enhances neural networks' ability of representing and calculating information over a long period by keeping records of entities contained in text. The core component is a memory pool which comprises entities' states. These entities' states are continuously updated according to the input text. Questions with regard to the input text are used to search the memory pool for related entities and answers are further predicted based on the states of retrieved entities. Compared with previous memory network models, the proposed model is capable of handling fine-grained information and more sophisticated relations based on entities. We formulated several different tasks as question answering problems and tested the proposed model. Experiments reported satisfying results.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03651",
        "title": "FastText.zip: Compressing text classification models",
        "authors": [
            "Armand Joulin",
            "Edouard Grave",
            "Piotr Bojanowski",
            "Matthijs Douze",
            "H\u00e9rve J\u00e9gou",
            "Tomas Mikolov"
        ],
        "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03659",
        "title": "Unraveling reported dreams with text analytics",
        "authors": [
            "Iris Hendrickx",
            "Louis Onrust",
            "Florian Kunneman",
            "Ali H\u00fcrriyeto\u011flu",
            "Antal van den Bosch",
            "Wessel Stoop"
        ],
        "abstract": "We investigate what distinguishes reported dreams from other personal narratives. The continuity hypothesis, stemming from psychological dream analysis work, states that most dreams refer to a person's daily life and personal concerns, similar to other personal narratives such as diary entries. Differences between the two texts may reveal the linguistic markers of dream text, which could be the basis for new dream analysis work and for the automatic detection of dream descriptions. We used three text analytics methods: text classification, topic modeling, and text coherence analysis, and applied these methods to a balanced set of texts representing dreams, diary entries, and other personal stories. We observed that dream texts could be distinguished from other personal narratives nearly perfectly, mostly based on the presence of uncertainty markers and descriptions of scenes. Important markers for non-dream narratives are specific time expressions and conversational expressions. Dream texts also exhibit a lower discourse coherence than other personal narratives.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03762",
        "title": "From narrative descriptions to MedDRA: automagically encoding adverse drug reactions",
        "authors": [
            "Carlo Combi",
            "Margherita Zorzi",
            "Gabriele Pozzani",
            "Ugo Moretti"
        ],
        "abstract": "The collection of narrative spontaneous reports is an irreplaceable source for the prompt detection of suspected adverse drug reactions (ADRs): qualified domain experts manually revise a huge amount of narrative descriptions and then encode texts according to MedDRA standard terminology. The manual annotation of narrative documents with medical terminology is a subtle and expensive task, since the number of reports is growing up day-by-day. MagiCoder, a Natural Language Processing algorithm, is proposed for the automatic encoding of free-text descriptions into MedDRA terms. MagiCoder procedure is efficient in terms of computational complexity (in particular, it is linear in the size of the narrative input and the terminology). We tested it on a large dataset of about 4500 manually revised reports, by performing an automated comparison between human and MagiCoder revisions. For the current base version of MagiCoder, we measured: on short descriptions, an average recall of $86\\%$ and an average precision of $88\\%$; on medium-long descriptions (up to 255 characters), an average recall of $64\\%$ and an average precision of $63\\%$. From a practical point of view, MagiCoder reduces the time required for encoding ADR reports. Pharmacologists have simply to review and validate the MagiCoder terms proposed by the application, instead of choosing the right terms among the 70K low level terms of MedDRA. Such improvement in the efficiency of pharmacologists' work has a relevant impact also on the quality of the subsequent data analysis. We developed MagiCoder for the Italian pharmacovigilance language. However, our proposal is based on a general approach, not depending on the considered language nor the term dictionary.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03769",
        "title": "Context-aware Sentiment Word Identification: sentiword2vec",
        "authors": [
            "Yushi Yao",
            "Guangjian Li"
        ],
        "abstract": "Traditional sentiment analysis often uses sentiment dictionary to extract sentiment information in text and classify documents. However, emerging informal words and phrases in user generated content call for analysis aware to the context. Usually, they have special meanings in a particular context. Because of its great performance in representing inter-word relation, we use sentiment word vectors to identify the special words. Based on the distributed language model word2vec, in this paper we represent a novel method about sentiment representation of word under particular context, to be detailed, to identify the words with abnormal sentiment polarity in long answers. Result shows the improved model shows better performance in representing the words with special meaning, while keep doing well in representing special idiomatic pattern. Finally, we will discuss the meaning of vectors representing in the field of sentiment, which may be different from general object-based conditions.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03791",
        "title": "Neural Machine Translation by Minimising the Bayes-risk with Respect to Syntactic Translation Lattices",
        "authors": [
            "Felix Stahlberg",
            "Adri\u00e0 de Gispert",
            "Eva Hasler",
            "Bill Byrne"
        ],
        "abstract": "We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than $n$-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on English-German and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03929",
        "title": "Deep Active Learning for Dialogue Generation",
        "authors": [
            "Nabiha Asghar",
            "Pascal Poupart",
            "Xin Jiang",
            "Hang Li"
        ],
        "abstract": "We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03969",
        "title": "Tracking the World State with Recurrent Entity Networks",
        "authors": [
            "Mikael Henaff",
            "Jason Weston",
            "Arthur Szlam",
            "Antoine Bordes",
            "Yann LeCun"
        ],
        "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2017-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03975",
        "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
        "authors": [
            "Robyn Speer",
            "Joshua Chin",
            "Catherine Havasi"
        ],
        "abstract": "Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.\n",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2018-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03990",
        "title": "Evaluating Automatic Speech Recognition Systems in Comparison With Human Perception Results Using Distinctive Feature Measures",
        "authors": [
            "Xiang Kong",
            "Jeung-Yoon Choi",
            "Stefanie Shattuck-Hufnagel"
        ],
        "abstract": "This paper describes methods for evaluating automatic speech recognition (ASR) systems in comparison with human perception results, using measures derived from linguistic distinctive features. Error patterns in terms of manner, place and voicing are presented, along with an examination of confusion matrices via a distinctive-feature-distance metric. These evaluation methods contrast with conventional performance criteria that focus on the phone or word level, and are intended to provide a more detailed profile of ASR system performance,as well as a means for direct comparison with human perception results at the sub-phonemic level.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03991",
        "title": "Performance Improvements of Probabilistic Transcript-adapted ASR with Recurrent Neural Network and Language-specific Constraints",
        "authors": [
            "Xiang Kong",
            "Preethi Jyothi",
            "Mark Hasegawa-Johnson"
        ],
        "abstract": "Mismatched transcriptions have been proposed as a mean to acquire probabilistic transcriptions from non-native speakers of a ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04113",
        "title": "Vicinity-Driven Paragraph and Sentence Alignment for Comparable Corpora",
        "authors": [
            "Gustavo Henrique Paetzold",
            "Lucia Specia"
        ],
        "abstract": "Parallel corpora have driven great progress in the field of Text Simplification. However, most sentence alignment algorithms either offer a limited range of alignment types supported, or simply ignore valuable clues present in comparable documents. We address this problem by introducing a new set of flexible vicinity-driven paragraph and sentence alignment algorithms that 1-N, N-1, N-N and long distance null alignments without the need for hard-to-replicate supervised models.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04118",
        "title": "Information Extraction with Character-level Neural Networks and Free Noisy Supervision",
        "authors": [
            "Philipp Meerkamp",
            "Zhengyi Zhou"
        ],
        "abstract": "We present an architecture for information extraction from text that augments an existing parser with a character-level neural network. The network is trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision. Our architecture combines the ability of constraint-based information extraction systems to easily incorporate domain knowledge and constraints with the ability of deep neural networks to leverage large amounts of data to learn complex features. Boosting the existing parser's precision, the system led to large improvements over a mature and highly tuned constraint-based production information extraction system used at Bloomberg for financial language text.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2017-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04174",
        "title": "Models of retrieval in sentence comprehension: A computational evaluation using Bayesian hierarchical modeling",
        "authors": [
            "Bruno Nicenboim",
            "Shravan Vasishth"
        ],
        "abstract": "Research on interference has provided evidence that the formation of dependencies between non-adjacent words relies on a cue-based retrieval mechanism. Two different models can account for one of the main predictions of interference, i.e., a slowdown at a retrieval site, when several items share a feature associated with a retrieval cue: Lewis and Vasishth's (2005) activation-based model and McElree's (2000) direct access model. Even though these two models have been used almost interchangeably, they are based on different assumptions and predict differences in the relationship between reading times and response accuracy. The activation-based model follows the assumptions of ACT-R, and its retrieval process behaves as a lognormal race between accumulators of evidence with a single variance. Under this model, accuracy of the retrieval is determined by the winner of the race and retrieval time by its rate of accumulation. In contrast, the direct access model assumes a model of memory where only the probability of retrieval varies between items; in this model, differences in latencies are a by-product of the possibility and repairing incorrect retrievals. We implemented both models in a Bayesian hierarchical framework in order to evaluate them and compare them. We show that some aspects of the data are better fit under the direct access model than under the activation-based model. We suggest that this finding does not rule out the possibility that retrieval may be behaving as a race model with assumptions that follow less closely the ones from the ACT-R framework. We show that by introducing a modification of the activation model, i.e, by assuming that the accumulation of evidence for retrieval of incorrect items is not only slower but noisier (i.e., different variances for the correct and incorrect items), the model can provide a fit as good as the one of the direct access model.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2017-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04211",
        "title": "Multi-Perspective Context Matching for Machine Comprehension",
        "authors": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Wael Hamza",
            "Radu Florian"
        ],
        "abstract": "Previous machine comprehension (MC) datasets are either too small to train end-to-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our model first adjusts each word-embedding vector in the passage by multiplying a relevancy weight computed against the question. Then, we encode the question and weighted passage by using bi-directional LSTMs. For each point in the passage, our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector. Given those matched vectors, we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04342",
        "title": "Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors",
        "authors": [
            "Radu Soricut",
            "Nan Ding"
        ],
        "abstract": "We present a dual contribution to the task of machine reading-comprehension: a technique for creating large-sized machine-comprehension (MC) datasets using paragraph-vector models; and a novel, hybrid neural-network architecture that combines the representation power of recurrent neural networks with the discriminative power of fully-connected multi-layered networks. We use the MC-dataset generation technique to build a dataset of around 2 million examples, for which we empirically determine the high-ceiling of human performance (around 91% accuracy), as well as the performance of a variety of computer models. Among all the models we have experimented with, our hybrid neural-network architecture achieves the highest performance (83.2% accuracy). The remaining gap to the human-performance ceiling provides enough room for future model improvements.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04426",
        "title": "Improving Neural Language Models with a Continuous Cache",
        "authors": [
            "Edouard Grave",
            "Armand Joulin",
            "Nicolas Usunier"
        ],
        "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04460",
        "title": "Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection",
        "authors": [
            "Vered Shwartz",
            "Enrico Santus",
            "Dominik Schlechtweg"
        ],
        "abstract": "The fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such unsupervised measures, using several distributional semantic models that differ by context type and feature weighting. We analyze the performance of the different methods based on their linguistic motivation. Comparison to the state-of-the-art supervised methods shows that while supervised methods generally outperform the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04499",
        "title": "Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion",
        "authors": [
            "Hu Xu",
            "Lei Shu",
            "Jingyuan Zhang",
            "Philip S. Yu"
        ],
        "abstract": "Product Community Question Answering (PCQA) provides useful information about products and their features (aspects) that may not be well addressed by product descriptions and reviews. We observe that a product's compatibility issues with other products are frequently discussed in PCQA and such issues are more frequently addressed in accessories, i.e., via a yes/no question \"Does this mouse work with windows 10?\". In this paper, we address the problem of extracting compatible and incompatible products from yes/no questions in PCQA. This problem can naturally have a two-stage framework: first, we perform Complementary Entity (product) Recognition (CER) on yes/no questions; second, we identify the polarities of yes/no answers to assign the complementary entities a compatibility label (compatible, incompatible or unknown). We leverage an existing unsupervised method for the first stage and a 3-class classifier by combining a distant PU-learning method (learning from positive and unlabeled examples) together with a binary classifier for the second stage. The benefit of using distant PU-learning is that it can help to expand more implicit yes/no answers without using any human annotated data. We conduct experiments on 4 products to show that the proposed method is effective.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04538",
        "title": "Grammatical Constraints on Intra-sentential Code-Switching: From Theories to Working Models",
        "authors": [
            "Gayatri Bhat",
            "Monojit Choudhury",
            "Kalika Bali"
        ],
        "abstract": "We make one of the first attempts to build working models for intra-sentential code-switching based on the Equivalence-Constraint (Poplack 1980) and Matrix-Language (Myers-Scotton 1993) theories. We conduct a detailed theoretical analysis, and a small-scale empirical study of the two models for Hindi-English CS. Our analyses show that the models are neither sound nor complete. Taking insights from the errors made by the models, we propose a new model that combines features of both the theories.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04609",
        "title": "Neural Emoji Recommendation in Dialogue Systems",
        "authors": [
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Rui Yan",
            "Maosong Sun"
        ],
        "abstract": "Emoji is an essential component in dialogues which has been broadly utilized on almost all social platforms. It could express more delicate feelings beyond plain texts and thus smooth the communications between users, making dialogue systems more anthropomorphic and vivid. In this paper, we focus on automatically recommending appropriate emojis given the contextual information in multi-turn dialogue systems, where the challenges locate in understanding the whole conversations. More specifically, we propose the hierarchical long short-term memory model (H-LSTM) to construct dialogue representations, followed by a softmax classifier for emoji classification. We evaluate our models on the task of emoji classification in a real-world dataset, with some further explorations on parameter sensitivity and case study. Experimental results demonstrate that our method achieves the best performances on all evaluation metrics. It indicates that our method could well capture the contextual information and emotion flow in dialogues, which is significant for emoji recommendation.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04629",
        "title": "How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs",
        "authors": [
            "Rico Sennrich"
        ],
        "abstract": "Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English->German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04675",
        "title": "Recurrent Deep Stacking Networks for Speech Recognition",
        "authors": [
            "Peidong Wang",
            "Zhongqiu Wang",
            "Deliang Wang"
        ],
        "abstract": "This paper presented our work on applying Recurrent Deep Stacking Networks (RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass Stacking Network (BPSN). The main idea of these two models is to add phoneme-level information into acoustic models, transforming an acoustic model to the combination of an acoustic model and a phoneme-level N-gram model. Experiments showed that RDSN and BPsn can substantially improve the performances over conventional DNNs.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2020-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04683",
        "title": "Unsupervised Clustering of Commercial Domains for Adaptive Machine Translation",
        "authors": [
            "Mauro Cettolo",
            "Mara Chinea Rios",
            "Roldano Cattoni"
        ],
        "abstract": "In this paper, we report on domain clustering in the ambit of an adaptive MT architecture. A standard bottom-up hierarchical clustering algorithm has been instantiated with five different distances, which have been compared, on an MT benchmark built on 40 commercial domains, in terms of dendrograms, intrinsic and extrinsic evaluations. The main outcome is that the most expensive distance is also the only one able to allow the MT engine to guarantee good performance even with few, but highly populated clusters of domains.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04732",
        "title": "Multilingual Word Embeddings using Multigraphs",
        "authors": [
            "Radu Soricut",
            "Nan Ding"
        ],
        "abstract": "We present a family of neural-network--inspired models for computing continuous word representations, specifically designed to exploit both monolingual and multilingual text. This framework allows us to perform unsupervised training of embeddings that exhibit higher accuracy on syntactic and semantic compositionality, as well as multilingual semantic similarity, compared to previous models trained in an unsupervised fashion. We also show that such multilingual embeddings, optimized for semantic similarity, can improve the performance of statistical machine translation with respect to how it handles words not present in the parallel data.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04744",
        "title": "Incorporating Language Level Information into Acoustic Models",
        "authors": [
            "Peidong Wang",
            "Deliang Wang"
        ],
        "abstract": "This paper proposed a class of novel Deep Recurrent Neural Networks which can incorporate language-level information into acoustic models. For simplicity, we named these networks Recurrent Deep Language Networks (RDLNs). Multiple variants of RDLNs were considered, including two kinds of context information, two methods to process the context, and two methods to incorporate the language-level information. RDLNs provided possible methods to fine-tune the whole Automatic Speech Recognition (ASR) system in the acoustic modeling process.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2020-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04765",
        "title": "CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization",
        "authors": [
            "Uwe D. Reichel"
        ],
        "abstract": "The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and (2) prosodic feature extraction from syllable to utterance level. CoPaSul stands for contour-based, parametric, superpositional intonation stylization. In this framework intonation is represented as a superposition of global and local contours that are described parametrically in terms of polynomial coefficients. On the global level (usually associated but not necessarily restricted to intonation phrases) the stylization serves to represent register in terms of time-varying F0 level and range. On the local level (e.g. accent groups), local contour shapes are described. From this parameterization several features related to prosodic boundaries and prominence can be derived. Furthermore, by coefficient clustering prosodic contour classes can be obtained in a bottom-up way. Next to the stylization-based feature extraction also standard F0 and energy measures (e.g. mean and variance) as well as rhythmic aspects can be calculated. At the current state automatic annotation comprises: segmentation into interpausal chunks, syllable nucleus extraction, and unsupervised localization of prosodic phrase boundaries and prominent syllables. F0 and partly also energy feature sets can be derived for: standard measurements (as median and IQR), register in terms of F0 level and range, prosodic boundaries, local contour shapes, bottom-up derived contour classes, Gestalt of accent groups in terms of their deviation from higher level prosodic units, as well as for rhythmic aspects quantifying the relation between F0 and energy contours and prosodic event rates.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2023-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04868",
        "title": "Interpretable Semantic Textual Similarity: Finding and explaining differences between sentences",
        "authors": [
            "I. Lopez-Gazpio",
            "M. Maritxalar",
            "A. Gonzalez-Agirre",
            "G. Rigau",
            "L. Uria",
            "E. Agirre"
        ],
        "abstract": "User acceptance of artificial intelligence agents might depend on their ability to explain their reasoning, which requires adding an interpretability layer that fa- cilitates users to understand their behavior. This paper focuses on adding an in- terpretable layer on top of Semantic Textual Similarity (STS), which measures the degree of semantic equivalence between two sentences. The interpretability layer is formalized as the alignment between pairs of segments across the two sentences, where the relation between the segments is labeled with a relation type and a similarity score. We present a publicly available dataset of sentence pairs annotated following the formalization. We then develop a system trained on this dataset which, given a sentence pair, explains what is similar and different, in the form of graded and typed segment alignments. When evaluated on the dataset, the system performs better than an informed baseline, showing that the dataset and task are well-defined and feasible. Most importantly, two user studies show how the system output can be used to automatically produce explanations in natural language. Users performed better when having access to the explanations, pro- viding preliminary evidence that our dataset and method to automatically produce explanations is useful in real applications.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04936",
        "title": "Learning through Dialogue Interactions by Asking Questions",
        "authors": [
            "Jiwei Li",
            "Alexander H. Miller",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Jason Weston"
        ],
        "abstract": "A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such end-to-end learned interactive dialogue agents.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04988",
        "title": "TeKnowbase: Towards Construction of a Knowledge-base of Technical Concepts",
        "authors": [
            "Prajna Upadhyay",
            "Tanuma Patra",
            "Ashwini Purkar",
            "Maya Ramanath"
        ],
        "abstract": "In this paper, we describe the construction of TeKnowbase, a knowledge-base of technical concepts in computer science. Our main information sources are technical websites such as Webopedia and Techtarget as well as Wikipedia and online textbooks. We divide the knowledge-base construction problem into two parts -- the acquisition of entities and the extraction of relationships among these entities. Our knowledge-base consists of approximately 100,000 triples. We conducted an evaluation on a sample of triples and report an accuracy of a little over 90\\%. We additionally conducted classification experiments on StackOverflow data with features from TeKnowbase and achieved improved classification accuracy.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05131",
        "title": "Transition-based Parsing with Context Enhancement and Future Reward Reranking",
        "authors": [
            "Fugen Zhou",
            "Fuxiang Wu",
            "Zhengchen Zhang",
            "Minghui Dong"
        ],
        "abstract": "This paper presents a novel reranking model, future reward reranking, to re-score the actions in a transition-based parser by using a global scorer. Different to conventional reranking parsing, the model searches for the best dependency tree in all feasible trees constraining by a sequence of actions to get the future reward of the sequence. The scorer is based on a first-order graph-based parser with bidirectional LSTM, which catches different parsing view compared with the transition-based parser. Besides, since context enhancement has shown substantial improvement in the arc-stand transition-based parsing over the parsing accuracy, we implement context enhancement on an arc-eager transition-base parser with stack LSTMs, the dynamic oracle and dropout supporting and achieve further improvement. With the global scorer and context enhancement, the results show that UAS of the parser increases as much as 1.20% for English and 1.66% for Chinese, and LAS increases as much as 1.32% for English and 1.63% for Chinese. Moreover, we get state-of-the-art LASs, achieving 87.58% for Chinese and 93.37% for English.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05202",
        "title": "Building a robust sentiment lexicon with (almost) no resource",
        "authors": [
            "Mickael Rouvier",
            "Benoit Favre"
        ],
        "abstract": "Creating sentiment polarity lexicons is labor intensive. Automatically translating them from resourceful languages requires in-domain machine translation systems, which rely on large quantities of bi-texts. In this paper, we propose to replace machine translation by transferring words from the lexicon through word embeddings aligned across languages with a simple linear transform. The approach leads to no degradation, compared to machine translation, when tested on sentiment polarity classification on tweets from four languages.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05251",
        "title": "Neural Networks for Joint Sentence Classification in Medical Paper Abstracts",
        "authors": [
            "Franck Dernoncourt",
            "Ji Young Lee",
            "Peter Szolovits"
        ],
        "abstract": "Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05270",
        "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
        "authors": [
            "Eric S. Tellez",
            "Sabino Miranda Jim\u00e9nez",
            "Mario Graff",
            "Daniela Moctezuma",
            "Ranyart R. Su\u00e1rez",
            "Oscar S. Siordia"
        ],
        "abstract": "Recently, sentiment analysis has received a lot of attention due to the interest in mining opinions of social media users. Sentiment analysis consists in determining the polarity of a given text, i.e., its degree of positiveness or negativeness. Traditionally, Sentiment Analysis algorithms have been tailored to a specific language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05310",
        "title": "Modeling Trolling in Social Media Conversations",
        "authors": [
            "Luis Gerardo Mojica"
        ],
        "abstract": "Social media websites, electronic newspapers and Internet forums allow visitors to leave comments for others to read and interact. This exchange is not free from participants with malicious intentions, who troll others by positing messages that are intended to be provocative, offensive, or menacing. With the goal of facilitating the computational modeling of trolling, we propose a trolling categorization that is novel in the sense that it allows comment-based analysis from both the trolls' and the responders' perspectives, characterizing these two perspectives using four aspects, namely, the troll's intention and his intention disclosure, as well as the responder's interpretation of the troll's intention and her response strategy. Using this categorization, we annotate and release a dataset containing excerpts of Reddit conversations involving suspected trolls and their interactions with other users. Finally, we identify the difficult-to-classify cases in our corpus and suggest potential solutions for them.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05340",
        "title": "Automatic Labelling of Topics with Neural Embeddings",
        "authors": [
            "Shraey Bhatia",
            "Jey Han Lau",
            "Timothy Baldwin"
        ],
        "abstract": "Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05420",
        "title": "A Two-Phase Approach Towards Identifying Argument Structure in Natural Language",
        "authors": [
            "Arkanath Pathak",
            "Pawan Goyal",
            "Plaban Bhowmick"
        ],
        "abstract": "We propose a new approach for extracting argument structure from natural language texts that contain an underlying argument. Our approach comprises of two phases: Score Assignment and Structure Prediction. The Score Assignment phase trains models to classify relations between argument units (Support, Attack or Neutral). To that end, different training strategies have been explored. We identify different linguistic and lexical features for training the classifiers. Through ablation study, we observe that our novel use of word-embedding features is most effective for this task. The Structure Prediction phase makes use of the scores from the Score Assignment phase to arrive at the optimal structure. We perform experiments on three argumentation datasets, namely, AraucariaDB, Debatepedia and Wikipedia. We also propose two baselines and observe that the proposed approach outperforms baseline systems for the final task of Structure Prediction.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05555",
        "title": "Neural Networks Classifier for Data Selection in Statistical Machine Translation",
        "authors": [
            "\u00c1lvaro Peris",
            "Mara Chinea-Rios",
            "Francisco Casacuberta"
        ],
        "abstract": "We address the data selection problem in statistical machine translation (SMT) as a classification task. The new data selection method is based on a neural network classifier. We present a new method description and empirical results proving that our data selection method provides better translation quality, compared to a state-of-the-art method (i.e., Cross entropy). Moreover, the empirical results reported are coherent across different language pairs.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05734",
        "title": "Web-based Semantic Similarity for Emotion Recognition in Web Objects",
        "authors": [
            "Valentina Franzoni",
            "Giulio Biondi",
            "Alfredo Milani",
            "Yuanxi Li"
        ],
        "abstract": "In this project we propose a new approach for emotion recognition using web-based similarity (e.g. confidence, PMI and PMING). We aim to extract basic emotions from short sentences with emotional content (e.g. news titles, tweets, captions), performing a web-based quantitative evaluation of semantic proximity between each word of the analyzed sentence and each emotion of a psychological model (e.g. Plutchik, Ekman, Lovheim). The phases of the extraction include: text preprocessing (tokenization, stop words, filtering), search engine automated query, HTML parsing of results (i.e. scraping), estimation of semantic proximity, ranking of emotions according to proximity measures. The main idea is that, since it is possible to generalize semantic similarity under the assumption that similar concepts co-occur in documents indexed in search engines, therefore also emotions can be generalized in the same way, through tags or terms that express them in a particular language, ranking emotions. Training results are compared to human evaluation, then additional comparative tests on results are performed, both for the global ranking correlation (e.g. Kendall, Spearman, Pearson) both for the evaluation of the emotion linked to each single word. Different from sentiment analysis, our approach works at a deeper level of abstraction, aiming at recognizing specific emotions and not only the positive/negative sentiment, in order to predict emotions as semantic data.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06027",
        "title": "Neural Multi-Source Morphological Reinflection",
        "authors": [
            "Katharina Kann",
            "Ryan Cotterell",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder- decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06043",
        "title": "An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation",
        "authors": [
            "Raphael Shu",
            "Hideki Nakayama"
        ],
        "abstract": "Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term \"vision span\" means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50% with modest loss in accuracy on English-Japanese and German-English translation tasks.% This results indicate that the conventional attention mechanism performs a significant amount of redundant computation.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06062",
        "title": "Improving Tweet Representations using Temporal and User Context",
        "authors": [
            "Ganesh J",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "abstract": "In this work we propose a novel representation learning model which computes semantic representations for tweets accurately. Our model systematically exploits the chronologically adjacent tweets ('context') from users' Twitter timelines for this task. Further, we make our model user-aware so that it can do well in modeling the target tweet by exploiting the rich knowledge about the user such as the way the user writes the post and also summarizing the topics on which the user writes. We empirically demonstrate that the proposed models outperform the state-of-the-art models in predicting the user profile attributes like spouse, education and job by 19.66%, 2.27% and 2.22% respectively.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06138",
        "title": "Boosting Neural Machine Translation",
        "authors": [
            "Dakun Zhang",
            "Jungi Kim",
            "Josep Crego",
            "Jean Senellart"
        ],
        "abstract": "Training efficiency is one of the main problems for Neural Machine Translation (NMT). Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high computation cost, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on data boosting and bootstrap with no modifications to the neural network. It imitates the learning process of humans, which typically spend more time when learning \"difficult\" concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to 1.63 BLEU while saving 20% of training time.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06139",
        "title": "Neural Machine Translation from Simplified Translations",
        "authors": [
            "Josep Crego",
            "Jean Senellart"
        ],
        "abstract": "Text simplification aims at reducing the lexical, grammatical and structural complexity of a text while keeping the same meaning. In the context of machine translation, we introduce the idea of simplified translations in order to boost the learning ability of deep neural translation models. We conduct preliminary experiments showing that translation complexity is actually reduced in a translation of a source bi-text compared to the target reference of the bi-text while using a neural machine translation (NMT) system learned on the exact same bi-text. Based on knowledge distillation idea, we then train an NMT system using the simplified bi-text, and show that it outperforms the initial system that was built over the reference data set. Performance is further boosted when both reference and automatic translations are used to learn the network. We perform an elementary analysis of the translated corpus and report accuracy results of the proposed approach on English-to-French and English-to-German translation tasks.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06140",
        "title": "Domain Control for Neural Machine Translation",
        "authors": [
            "Catherine Kobus",
            "Josep Crego",
            "Jean Senellart"
        ],
        "abstract": "Machine translation systems are very sensitive to the domains they were trained on. Several domain adaptation techniques have been deeply studied. We propose a new technique for neural machine translation (NMT) that we call domain control which is performed at runtime using a unique neural network covering multiple domains. The presented approach shows quality improvements when compared to dedicated domains translating on any of the covered domains and even on out-of-domain data. In addition, model parameters do not need to be re-estimated for each domain, making this effective to real use cases. Evaluation is carried out on English-to-French translation for two different testing scenarios. We first consider the case where an end-user performs translations on a known domain. Secondly, we consider the scenario where the domain is not known and predicted at the sentence level before translating. Results show consistent accuracy improvements for both conditions.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2017-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06141",
        "title": "Domain specialization: a post-training domain adaptation for Neural Machine Translation",
        "authors": [
            "Christophe Servan",
            "Josep Crego",
            "Jean Senellart"
        ],
        "abstract": "Domain adaptation is a key feature in Machine Translation. It generally encompasses terminology, domain and style adaptation, especially for human post-editing workflows in Computer Assisted Translation (CAT). With Neural Machine Translation (NMT), we introduce a new notion of domain adaptation that we call \"specialization\" and which is showing promising results both in the learning speed and in adaptation accuracy. In this paper, we propose to explore this approach under several perspectives.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06475",
        "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles",
        "authors": [
            "James Cross",
            "Liang Huang"
        ],
        "abstract": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06549",
        "title": "Exploring Different Dimensions of Attention for Uncertainty Detection",
        "authors": [
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Neural networks with attention have proven effective for many natural language processing tasks. In this paper, we develop attention mechanisms for uncertainty detection. In particular, we generalize standardly used attention mechanisms by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve sequence information. We compare them to other configurations along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06572",
        "title": "Unsupervised Dialogue Act Induction using Gaussian Mixtures",
        "authors": [
            "Tom\u00e1\u0161 Brychc\u00edn",
            "Pavel Kr\u00e1l"
        ],
        "abstract": "This paper introduces a new unsupervised approach for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue.\n",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06581",
        "title": "Grammar rules for the isiZulu complex verb",
        "authors": [
            "C. Maria Keet",
            "Langa Khumalo"
        ],
        "abstract": "The isiZulu verb is known for its morphological complexity, which is a subject for on-going linguistics research, as well as for prospects of computational use, such as controlled natural language interfaces, machine translation, and spellcheckers. To this end, we seek to answer the question as to what the precise grammar rules for the isiZulu complex verb are (and, by extension, the Bantu verb morphology). To this end, we iteratively specify the grammar as a Context Free Grammar, and evaluate it computationally. The grammar presented in this paper covers the subject and object concords, negation, present tense, aspect, mood, and the causative, applicative, stative, and the reciprocal verbal extensions, politeness, the wh-question modifiers, and aspect doubling, ensuring their correct order as they appear in verbs. The grammar conforms to specification.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06671",
        "title": "Inferring the location of authors from words in their texts",
        "authors": [
            "Max Berggren",
            "Jussi Karlgren",
            "Robert \u00d6stling",
            "Mikael Parkvall"
        ],
        "abstract": "For the purposes of computational dialectology or other geographically bound text analysis tasks, texts must be annotated with their or their authors' location. Many texts are locatable through explicit labels but most have no explicit annotation of place. This paper describes a series of experiments to determine how positionally annotated microblog posts can be used to learn location-indicating words which then can be used to locate blog texts and their authors. A Gaussian distribution is used to model the locational qualities of words. We introduce the notion of placeness to describe how locational words are.\n",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06685",
        "title": "Stateology: State-Level Interactive Charting of Language, Feelings, and Values",
        "authors": [
            "Konstantinos Pappas",
            "Steven Wilson",
            "Rada Mihalcea"
        ],
        "abstract": "People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06778",
        "title": "SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations",
        "authors": [
            "Dheeraj Mekala",
            "Vivek Gupta",
            "Bhargavi Paranjape",
            "Harish Karnick"
        ],
        "abstract": "We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word embedding's are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, NTSG (Liu et al., 2015a). We also show that SCDV embedding's perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06821",
        "title": "User Bias Removal in Review Score Prediction",
        "authors": [
            "Rahul Wadbude",
            "Vivek Gupta",
            "Dheeraj Mekala",
            "Harish Karnick"
        ],
        "abstract": "Review score prediction of text reviews has recently gained a lot of attention in recommendation systems. A major problem in models for review score prediction is the presence of noise due to user-bias in review scores. We propose two simple statistical methods to remove such noise and improve review score prediction. Compared to other methods that use multiple classifiers, one for each user, our model uses a single global classifier to predict review scores. We empirically evaluate our methods on two major categories (\\textit{Electronics} and \\textit{Movies and TV}) of the SNAP published Amazon e-Commerce Reviews data-set and Amazon \\textit{Fine Food} reviews data-set. We obtain improved review score prediction for three commonly used text feature representations.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06897",
        "title": "Fast Domain Adaptation for Neural Machine Translation",
        "authors": [
            "Markus Freitag",
            "Yaser Al-Onaizan"
        ],
        "abstract": "Neural Machine Translation (NMT) is a new approach for automatic translation of text from one human language into another. The basic concept in NMT is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is gaining popularity in the research community because it outperformed traditional SMT approaches in several translation tasks at WMT and other evaluation tasks/benchmarks at least for some language pairs. However, many of the enhancements in SMT over the years have not been incorporated into the NMT framework. In this paper, we focus on one such enhancement namely domain adaptation. We propose an approach for adapting a NMT system to a new domain. The main idea behind domain adaptation is that the availability of large out-of-domain training data and a small in-domain training data. We report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs. With our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly. In addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the NMT model on the combined data which usually takes several days/weeks depending on the volume of the data.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07130",
        "title": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling",
        "authors": [
            "G\u00e1bor Berend"
        ],
        "abstract": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e.~150 sentences per language.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07182",
        "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language",
        "authors": [
            "Angeliki Lazaridou",
            "Alexander Peysakhovich",
            "Marco Baroni"
        ],
        "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the \"word meanings\" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2017-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07215",
        "title": "Inverted Bilingual Topic Models for Lexicon Extraction from Non-parallel Data",
        "authors": [
            "Tengfei Ma",
            "Tetsuya Nasukawa"
        ],
        "abstract": "Topic models have been successfully applied in lexicon extraction. However, most previous methods are limited to document-aligned data. In this paper, we try to address two challenges of applying topic models to lexicon extraction in non-parallel data: 1) hard to model the word relationship and 2) noisy seed dictionary. To solve these two challenges, we propose two new bilingual topic models to better capture the semantic information of each word while discriminating the multiple translations in a noisy seed dictionary. We extend the scope of topic models by inverting the roles of \"word\" and \"document\". In addition, to solve the problem of noise in seed dictionary, we incorporate the probability of translation selection in our models. Moreover, we also propose an effective measure to evaluate the similarity of words in different languages and select the optimal translation pairs. Experimental results using real world data demonstrate the utility and efficacy of the proposed models.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2017-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07411",
        "title": "A Context-aware Attention Network for Interactive Question Answering",
        "authors": [
            "Huayu Li",
            "Martin Renqiang Min",
            "Yong Ge",
            "Asim Kadav"
        ],
        "abstract": "Neural network based sequence-to-sequence models in an encoder-decoder framework have been successfully applied to solve Question Answering (QA) problems, predicting answers from statements and questions. However, almost all previous models have failed to consider detailed context information and unknown states under which systems do not have enough information to answer given questions. These scenarios with incomplete or ambiguous information are very common in the setting of Interactive Question Answering (IQA). To address this challenge, we develop a novel model, employing context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. We also generate unique IQA datasets to test our model, which will be made publicly available. Employing these attention mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input depending on different contexts. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer an answer. Extensive experiments on QA and IQA datasets quantitatively demonstrate the effectiveness of our model with significant improvement over state-of-the-art conventional QA models.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07486",
        "title": "Continuous multilinguality with language vectors",
        "authors": [
            "Robert \u00d6stling",
            "J\u00f6rg Tiedemann"
        ],
        "abstract": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07495",
        "title": "Noise Mitigation for Neural Entity Typing and Relation Extraction",
        "authors": [
            "Yadollah Yaghoobzadeh",
            "Heike Adel",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "In this paper, we address two different types of noise in information extraction models: noise from distant supervision and noise from pipeline input features. Our target tasks are entity typing and relation extraction. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. This gives our models comparable performance with the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into relation extraction. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07600",
        "title": "Re-evaluating Automatic Metrics for Image Captioning",
        "authors": [
            "Mert Kilickaya",
            "Aykut Erdem",
            "Nazli Ikizler-Cinbis",
            "Erkut Erdem"
        ],
        "abstract": "The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover's Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07833",
        "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task",
        "authors": [
            "Nan Ding",
            "Sebastian Goodman",
            "Fei Sha",
            "Radu Soricut"
        ],
        "abstract": "We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing \"keywords\" (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded \"understanding\" of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and well-studied metric: the accuracy in detecting the true target among the decoys.\n",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07843",
        "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
        "authors": [
            "Leila Arras",
            "Franziska Horn",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.\n    ",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07940",
        "title": "Supervised Opinion Aspect Extraction by Exploiting Past Extraction Results",
        "authors": [
            "Lei Shu",
            "Bing Liu",
            "Hu Xu",
            "Annice Kim"
        ],
        "abstract": "One of the key tasks of sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. In this work, we focus on using supervised sequence labeling as the base approach to performing the task. Although several extraction methods using sequence labeling methods such as Conditional Random Fields (CRF) and Hidden Markov Models (HMM) have been proposed, we show that this supervised approach can be significantly improved by exploiting the idea of concept sharing across multiple domains. For example, \"screen\" is an aspect in iPhone, but not only iPhone has a screen, many electronic devices have screens too. When \"screen\" appears in a review of a new domain (or product), it is likely to be an aspect too. Knowing this information enables us to do much better extraction in the new domain. This paper proposes a novel extraction method exploiting this idea in the context of supervised sequence labeling. Experimental results show that it produces markedly better results than without using the past information.\n    ",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07956",
        "title": "A CRF Based POS Tagger for Code-mixed Indian Social Media Text",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "In this work, we describe a conditional random fields (CRF) based system for Part-Of- Speech (POS) tagging of code-mixed Indian social media text as part of our participation in the tool contest on POS tagging for codemixed Indian social media text, held in conjunction with the 2016 International Conference on Natural Language Processing, IIT(BHU), India. We participated only in constrained mode contest for all three language pairs, Bengali-English, Hindi-English and Telegu-English. Our system achieves the overall average F1 score of 79.99, which is the highest overall average F1 score among all 16 systems participated in constrained mode contest.\n    ",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2016-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08083",
        "title": "Language Modeling with Gated Convolutional Networks",
        "authors": [
            "Yann N. Dauphin",
            "Angela Fan",
            "Michael Auli",
            "David Grangier"
        ],
        "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.\n    ",
        "submission_date": "2016-12-23T00:00:00",
        "last_modified_date": "2017-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08171",
        "title": "KS_JU@DPIL-FIRE2016:Detecting Paraphrases in Indian Languages Using Multinomial Logistic Regression Model",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "In this work, we describe a system that detects paraphrases in Indian Languages as part of our participation in the shared Task on detecting paraphrases in Indian Languages (DPIL) organized by Forum for Information Retrieval Evaluation (FIRE) in 2016. Our paraphrase detection method uses a multinomial logistic regression model trained with a variety of features which are basically lexical and semantic level similarities between two sentences in a pair. The performance of the system has been evaluated against the test set released for the FIRE 2016 shared task on DPIL. Our system achieves the highest f-measure of 0.95 on task1 in Punjabi ",
        "submission_date": "2016-12-24T00:00:00",
        "last_modified_date": "2016-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08205",
        "title": "Predicting the Industry of Users on Social Media",
        "authors": [
            "Konstantinos Pappas",
            "Rada Mihalcea"
        ],
        "abstract": "Automatic profiling of social media users is an important task for supporting a multitude of downstream applications. While a number of studies have used social media content to extract and study collective social attributes, there is a lack of substantial research that addresses the detection of a user's industry. We frame this task as classification using both feature engineering and ensemble learning. Our industry-detection system uses both posted content and profile information to detect a user's industry with 64.3% accuracy, significantly outperforming the majority baseline in a taxonomy of fourteen industry classes. Our qualitative analysis suggests that a person's industry not only affects the words used and their perceived meanings, but also the number and type of emotions being expressed.\n    ",
        "submission_date": "2016-12-24T00:00:00",
        "last_modified_date": "2016-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08220",
        "title": "Understanding Neural Networks through Representation Erasure",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Dan Jurafsky"
        ],
        "abstract": "While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.\n    ",
        "submission_date": "2016-12-24T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08333",
        "title": "Text Summarization using Deep Learning and Ridge Regression",
        "authors": [
            "Karthik Bangalore Mani"
        ],
        "abstract": "We develop models and extract relevant features for automatic text summarization and investigate the performance of different models on the DUC 2001 dataset. Two different models were developed, one being a ridge regressor and the other one was a multi-layer perceptron. The hyperparameters were varied and their performance were noted. We segregated the summarization task into 2 main steps, the first being sentence ranking and the second step being sentence selection. In the first step, given a document, we sort the sentences based on their Importance, and in the second step, in order to obtain non-redundant sentences, we weed out the sentences that are have high similarity with the previously selected sentences.\n    ",
        "submission_date": "2016-12-26T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08375",
        "title": "Abstractive Headline Generation for Spoken Content by Attentive Recurrent Neural Networks with ASR Error Modeling",
        "authors": [
            "Lang-Chi Yu",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "Headline generation for spoken content is important since spoken content is difficult to be shown on the screen and browsed by the user. It is a special type of abstractive summarization, for which the summaries are generated word by word from scratch without using any part of the original content. Many deep learning approaches for headline generation from text document have been proposed recently, all requiring huge quantities of training data, which is difficult for spoken document summarization. In this paper, we propose an ASR error modeling approach to learn the underlying structure of ASR error patterns and incorporate this model in an Attentive Recurrent Neural Network (ARNN) architecture. In this way, the model for abstractive headline generation for spoken content can be learned from abundant text data and the ASR data for some recognizers. Experiments showed very encouraging results and verified that the proposed ASR error model works well even when the input spoken content is recognized by a recognizer very different from the one the model learned from.\n    ",
        "submission_date": "2016-12-26T00:00:00",
        "last_modified_date": "2016-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08989",
        "title": "Shamela: A Large-Scale Historical Arabic Corpus",
        "authors": [
            "Yonatan Belinkov",
            "Alexander Magidow",
            "Maxim Romanov",
            "Avi Shmidman",
            "Moshe Koppel"
        ],
        "abstract": "Arabic is a widely-spoken language with a rich and long history spanning more than fourteen centuries. Yet existing Arabic corpora largely focus on the modern period or lack sufficient diachronic information. We develop a large-scale, historical corpus of Arabic of about 1 billion words from diverse periods of time. We clean this corpus, process it with a morphological analyzer, and enhance it by detecting parallel passages and automatically dating undated texts. We demonstrate its utility with selected case-studies in which we show its application to the digital humanities.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08994",
        "title": "Here's My Point: Joint Pointer Architecture for Argument Mining",
        "authors": [
            "Peter Potash",
            "Alexey Romanov",
            "Anna Rumshisky"
        ],
        "abstract": "One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.\n    ",
        "submission_date": "2016-12-28T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09113",
        "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
        "authors": [
            "Jonathan Godwin",
            "Pontus Stenetorp",
            "Sebastian Riedel"
        ],
        "abstract": "In this paper we present a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by junior downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F1 for Chunking compared to a plausible baseline.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09213",
        "title": "Verifying Heaps' law using Google Books Ngram data",
        "authors": [
            "Vladimir V. Bochkarev",
            "Eduard Yu.Lerner",
            "Anna V. Shevlyakova"
        ],
        "abstract": "This article is devoted to the verification of the empirical Heaps law in European languages using Google Books Ngram corpus data. The connection between word distribution frequency and expected dependence of individual word number on text size is analysed in terms of a simple probability model of text generation. It is shown that the Heaps exponent varies significantly within characteristic time intervals of 60-100 years.\n    ",
        "submission_date": "2016-12-29T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09327",
        "title": "Intelligent information extraction based on artificial neural network",
        "authors": [
            "Ahlam Ansari",
            "Moonish Maknojia",
            "Altamash Shaikh"
        ],
        "abstract": "Question Answering System (QAS) is used for information retrieval and natural language processing (NLP) to reduce human effort. There are numerous QAS based on the user documents present today, but they all are limited to providing objective answers and process simple questions only. Complex questions cannot be answered by the existing QAS, as they require interpretation of the current and old data as well as the question asked by the user. The above limitations can be overcome by using deep cases and neural network. Hence we propose a modified QAS in which we create a deep artificial neural network with associative memory from text documents. The modified QAS processes the contents of the text document provided to it and find the answer to even complex questions in the documents.\n    ",
        "submission_date": "2016-04-11T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00025",
        "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text",
        "authors": [
            "Mohamed Elhoseiny",
            "Ahmed Elgammal",
            "Babak Saleh"
        ],
        "abstract": "People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer, that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the parameters of a linear classifier. We also propose a generic kernelized models where a kernel classifier is predicted in the form defined by the representer theorem. The kernelized models allow defining and utilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in the visual space and text space, respectively. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers on two fine-grained and challenging categorization datasets (CU Birds and Flower Datasets), and the results indicate successful predictions of our final model over several baselines that we designed.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2016-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00816",
        "title": "Open challenges in understanding development and evolution of speech forms: The roles of embodied self-organization, motivation and active exploration",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "abstract": "This article discusses open scientific challenges for understanding development and evolution of speech forms, as a commentary to Moulin-Frier et al. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models of the origins of speech forms, with a focus on their assumptions , we study the fundamental question of how speech can be formed out of non--speech, at both developmental and evolutionary scales. In particular, we emphasize the importance of embodied self-organization , as well as the role of mechanisms of motivation and active curiosity-driven exploration in speech formation. Finally , we discuss an evolutionary-developmental perspective of the origins of speech.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.00901",
        "title": "Joint learning of ontology and semantic parser from text",
        "authors": [
            "Janez Starc",
            "Dunja Mladeni\u0107"
        ],
        "abstract": "Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels -- classes, instances, taxonomic and non-taxonomic relations. The approach was evaluated on the first sentences of Wikipedia pages describing people.\n    ",
        "submission_date": "2016-01-05T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.01356",
        "title": "From Word Embeddings to Item Recommendation",
        "authors": [
            "Makbule Gulcin Ozsoy"
        ],
        "abstract": "Social network platforms can use the data produced by their users to serve them better. One of the services these platforms provide is recommendation service. Recommendation systems can predict the future preferences of users using their past preferences. In the recommendation systems literature there are various techniques, such as neighborhood based methods, machine-learning based methods and matrix-factorization based methods. In this work, a set of well known methods from natural language processing domain, namely Word2Vec, is applied to recommendation systems domain. Unlike previous works that use Word2Vec for recommendation, this work uses non-textual features, the check-ins, and it recommends venues to visit/check-in to the target users. For the experiments, a Foursquare check-in dataset is used. The results show that use of continuous vector space representations of items modeled by techniques of Word2Vec is promising for making recommendations.\n    ",
        "submission_date": "2016-01-07T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.03478",
        "title": "Deep Learning Applied to Image and Text Matching",
        "authors": [
            "Afroze Ibrahim Baqapuri"
        ],
        "abstract": "The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.04075",
        "title": "Modification of Question Writing Style Influences Content Popularity in a Social Q&A System",
        "authors": [
            "Igor A. Podgorny"
        ],
        "abstract": "TurboTax AnswerXchange is a social Q&A system supporting users working on federal and state tax returns. Using 2015 data, we demonstrate that content popularity (or number of views per AnswerXchange question) can be predicted with reasonable accuracy based on attributes of the question alone. We also employ probabilistic topic analysis and uplift modeling to identify question features with the highest impact on popularity. We demonstrate that content popularity is driven by behavioral attributes of AnswerXchange users and depends on complex interactions between search ranking algorithms, psycholinguistic factors and question writing style. Our findings provide a rationale for employing popularity predictions to guide the users into formulating better questions and editing the existing ones. For example, starting question title with a question word or adding details to the question increase number of views per question. Similar approach can be applied to promoting AnswerXchange content indexed by Google to drive organic traffic to TurboTax.\n    ",
        "submission_date": "2016-01-15T00:00:00",
        "last_modified_date": "2016-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.05893",
        "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach",
        "authors": [
            "Shawn Brunsting",
            "Hans De Sterck",
            "Remco Dolman",
            "Teun van Sprundel"
        ],
        "abstract": "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each block. Finally, one location is chosen for each block by assigning distance-based scores to each location and repeatedly selecting the location and block with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles' authors, where classification approaches have achieved median errors as low as 11 km, with attainable accuracy limited by the class size. Our approach achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When considering the five location tags with the greatest scores, 50% of articles were assigned at least one tag within 8.5 kilometres of the article's author-assigned true location. We also tested our approach on Twitter messages that are tagged with the location from which the message was sent. Twitter texts are challenging because they are short and unstructured and often do not contain words referring to the location they were sent from, but we obtain potentially useful results. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper accuracy limit, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall.\n    ",
        "submission_date": "2016-01-22T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06303",
        "title": "Undecidability of the Lambek calculus with a relevant modality",
        "authors": [
            "Max Kanovich",
            "Stepan Kuznetsov",
            "Andre Scedrov"
        ],
        "abstract": "Morrill and Valentin in the paper \"Computational coverage of TLG: Nonlinearity\" considered an extension of the Lambek calculus enriched by a so-called \"exponential\" modality. This modality behaves in the \"relevant\" style, that is, it allows contraction and permutation, but not weakening. Morrill and Valentin stated an open problem whether this system is decidable. Here we show its undecidability. Our result remains valid if we consider the fragment where all division operations have one direction. We also show that the derivability problem in a restricted case, where the modality can be applied only to variables (primitive types), is decidable and belongs to the NP class.\n    ",
        "submission_date": "2016-01-23T00:00:00",
        "last_modified_date": "2016-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06732",
        "title": "Concept Generation in Language Evolution",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06738",
        "title": "A Label Semantics Approach to Linguistic Hedges",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts. The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding. We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06755",
        "title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like `very' gives better convergence over time. Secondly, using expansion hedges such as `quite' reduces concept overlap. However, both these improvements come at a cost of slower speed of development.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.06763",
        "title": "Emerging Dimension Weights in a Conceptual Spaces Model of Concept Combination",
        "authors": [
            "Martha Lewis",
            "Jonathan Lawry"
        ],
        "abstract": "We investigate the generation of new concepts from combinations of properties as an artificial language develops. To do so, we have developed a new framework for conjunctive concept combination. This framework gives a semantic grounding to the weighted sum approach to concept combination seen in the literature. We implement the framework in a multi-agent simulation of language evolution and show that shared combination weights emerge. The expected value and the variance of these weights across agents may be predicted from the distribution of elements in the conceptual space, as determined by the underlying environment, together with the rate at which agents adopt others' concepts. When this rate is smaller, the agents are able to converge to weights with lower variance. However, the time taken to converge to a steady state distribution of weights is longer.\n    ",
        "submission_date": "2016-01-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1601.08188",
        "title": "Lipreading with Long Short-Term Memory",
        "authors": [
            "Michael Wand",
            "Jan Koutn\u00edk",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feed-forward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-to-end neural network-based solution (11.6% improvement over the best feature-based solution evaluated).\n    ",
        "submission_date": "2016-01-29T00:00:00",
        "last_modified_date": "2016-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00104",
        "title": "Extracting Keyword for Disambiguating Name Based on the Overlap Principle",
        "authors": [
            "Mahyuddin K. M. Nasution"
        ],
        "abstract": "Name disambiguation has become one of the main themes in the Semantic Web agenda. The semantic web is an extension of the current Web in which information is not only given well-defined meaning, but also has many purposes that contain the ambiguous naturally or a lot of thing came with the overlap, mainly deals with the persons name. Therefore, we develop an approach to extract keywords from web snippet with utilizing the overlap principle, a concept to understand things with ambiguous, whereby features of person are generated for dealing with the variety of web, the web is steadily gaining ground in the semantic research.\n    ",
        "submission_date": "2016-01-30T00:00:00",
        "last_modified_date": "2016-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.00515",
        "title": "Marvin: Semantic annotation using multiple knowledge sources",
        "authors": [
            "Nikola Milosevic"
        ],
        "abstract": "People are producing more written material then anytime in the history. The increase is so high that professionals from the various fields are no more able to cope with this amount of publications. Text mining tools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantic text annotation. In this report we present Marvin, a text annotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotate text using multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.\n    ",
        "submission_date": "2016-02-01T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01103",
        "title": "Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions",
        "authors": [
            "Chenhao Tan",
            "Vlad Niculae",
            "Cristian Danescu-Niculescu-Mizil",
            "Lillian Lee"
        ],
        "abstract": "Changing someone's opinion is arguably one of the most important challenges of social interaction. The underlying process proves difficult to study: it is hard to know how someone's opinions are formed and whether and how someone's views shift. Fortunately, ChangeMyView, an active community on Reddit, provides a platform where users present their own opinions and reasoning, invite others to contest them, and acknowledge when the ensuing discussions change their original views. In this work, we study these interactions to understand the mechanisms behind persuasion.\n",
        "submission_date": "2016-02-02T00:00:00",
        "last_modified_date": "2016-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01208",
        "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
        "authors": [
            "Akira Taniguchi",
            "Tadahiro Taniguchi",
            "Tetsunari Inamura"
        ],
        "abstract": "In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.\n    ",
        "submission_date": "2016-02-03T00:00:00",
        "last_modified_date": "2016-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01248",
        "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report",
        "authors": [
            "Nikolaos Nodarakis",
            "Spyros Sioutas",
            "Athanasios Tsakalidis",
            "Giannis Tzimas"
        ],
        "abstract": "Sentiment analysis (or opinion mining) on Twitter data has attracted much attention recently. One of the system's key features, is the immediacy in communication with other users in an easy, user-friendly and fast way. Consequently, people tend to express their feelings freely, which makes Twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics. This amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics. However, since none can invest an infinite amount of time to read through these tweets, an automated decision making approach is necessary. Nevertheless, most existing solutions are limited in centralized environments only. Thus, they can only process at most a few thousand tweets. Such a sample, is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily. In this paper, we go one step further and develop a novel method for sentiment learning in the MapReduce framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. Through an extensive experimental evaluation, we prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification.\n    ",
        "submission_date": "2016-02-03T00:00:00",
        "last_modified_date": "2016-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.01895",
        "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features",
        "authors": [
            "Shijian Tang",
            "Song Han"
        ],
        "abstract": "Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02089",
        "title": "Harmonic Grammar in a DisCo Model of Meaning",
        "authors": [
            "Martha Lewis",
            "Bob Coecke"
        ],
        "abstract": "The model of cognition developed in (Smolensky and Legendre, 2006) seeks to unify two levels of description of the cognitive process: the connectionist and the symbolic. The theory developed brings together these two levels into the Integrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and Pulman (2007) draw a parallel with semantics where meaning may be modelled on both distributional and symbolic levels, developed by Coecke et al, 2010 into the Distributional Compositional (DisCo) model of meaning. In the current work, we revisit Smolensky and Legendre (S&L)'s model. We describe the DisCo framework, summarise the key ideas in S&L's architecture, and describe how their description of harmony as a graded measure of grammaticality may be applied in the DisCo model.\n    ",
        "submission_date": "2016-02-05T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02332",
        "title": "Scalable Text Mining with Sparse Generative Models",
        "authors": [
            "Antti Puurula"
        ],
        "abstract": "The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods.\n",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02373",
        "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings",
        "authors": [
            "Rie Johnson",
            "Tong Zhang"
        ],
        "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.\n    ",
        "submission_date": "2016-02-07T00:00:00",
        "last_modified_date": "2016-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02499",
        "title": "The \"Sprekend Nederland\" project and its application to accent location",
        "authors": [
            "David A. van Leeuwen",
            "Rosemary Orr"
        ],
        "abstract": "This paper describes the data collection effort that is part of the project Sprekend Nederland (The Netherlands Talking), and discusses its potential use in Automatic Accent Location. We define Automatic Accent Location as the task to describe the accent of a speaker in terms of the location of the speaker and its history. We discuss possible ways of describing accent location, the consequence these have for the task of automatic accent location, and potential evaluation metrics.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02665",
        "title": "The happiness paradox: your friends are happier than you",
        "authors": [
            "Johan Bollen",
            "Bruno Gon\u00e7alves",
            "Ingrid van de Leemput",
            "Guangchen Ruan"
        ],
        "abstract": "Most individuals in social networks experience a so-called Friendship Paradox: they are less popular than their friends on average. This effect may explain recent findings that widespread social network media use leads to reduced happiness. However the relation between popularity and happiness is poorly understood. A Friendship paradox does not necessarily imply a Happiness paradox where most individuals are less happy than their friends. Here we report the first direct observation of a significant Happiness Paradox in a large-scale online social network of $39,110$ Twitter users. Our results reveal that popular individuals are indeed happier and that a majority of individuals experience a significant Happiness paradox. The magnitude of the latter effect is shaped by complex interactions between individual popularity, happiness, and the fact that users cluster assortatively by level of happiness. Our results indicate that the topology of online social networks and the distribution of happiness in some populations can cause widespread psycho-social effects that affect the well-being of billions of individuals.\n    ",
        "submission_date": "2016-02-08T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.02850",
        "title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization",
        "authors": [
            "Bo Tang",
            "Steven Kay",
            "Haibo He"
        ],
        "abstract": "Automated feature selection is important for text categorization to reduce the feature size and to speed up the learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches.\n    ",
        "submission_date": "2016-02-09T00:00:00",
        "last_modified_date": "2016-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03001",
        "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
        "authors": [
            "Miltiadis Allamanis",
            "Hao Peng",
            "Charles Sutton"
        ],
        "abstract": "Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.\n    ",
        "submission_date": "2016-02-09T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.03661",
        "title": "On the emergence of syntactic structures: quantifying and modelling duality of patterning",
        "authors": [
            "Vittorio Loreto",
            "Pietro Gravino",
            "Vito D.P. Servedio",
            "Francesca Tria"
        ],
        "abstract": "The complex organization of syntax in hierarchical structures is one of the core design features of human language. Duality of patterning refers for instance to the organization of the meaningful elements in a language at two distinct levels: a combinatorial level where meaningless forms are combined into meaningful forms and a compositional level where meaningful forms are composed into larger lexical units. The question remains wide open regarding how such a structure could have emerged. Furthermore a clear mathematical framework to quantify this phenomenon is still lacking. The aim of this paper is that of addressing these two aspects in a self-consistent way. First, we introduce suitable measures to quantify the level of combinatoriality and compositionality in a language, and present a framework to estimate these observables in human natural languages. Second, we show that the theoretical predictions of a multi-agents modeling scheme, namely the Blending Game, are in surprisingly good agreement with empirical data. In the Blending Game a population of individuals plays language games aiming at success in communication. It is remarkable that the two sides of duality of patterning emerge simultaneously as a consequence of a pure cultural dynamics in a simulated environment that contains meaningful relations, provided a simple constraint on message transmission fidelity is also considered.\n    ",
        "submission_date": "2016-02-11T00:00:00",
        "last_modified_date": "2016-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04101",
        "title": "An Empirical Study on Academic Commentary and Its Implications on Reading and Writing",
        "authors": [
            "Tai Wang",
            "Xiangen Hu",
            "Keith Shubeck",
            "Zhiqiang Cai",
            "Jie Tang"
        ],
        "abstract": "The relationship between reading and writing (RRW) is one of the major themes in learning science. One of its obstacles is that it is difficult to define or measure the latent background knowledge of the individual. However, in an academic research setting, scholars are required to explicitly list their background knowledge in the citation sections of their manuscripts. This unique opportunity was taken advantage of to observe RRW, especially in the published academic commentary scenario. RRW was visualized under a proposed topic process model by using a state of the art version of latent Dirichlet allocation (LDA). The empirical study showed that the academic commentary is modulated both by its target paper and the author's background knowledge. Although this conclusion was obtained in a unique environment, we suggest its implications can also shed light on other similar interesting areas, such as dialog and conversation, group discussion, and social media.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04709",
        "title": "Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies",
        "authors": [
            "Giancarlo Crocetti",
            "Amir A. Delay",
            "Fatemeh Seyedmendhi"
        ],
        "abstract": "The exploration of social conversations for addressing patient's needs is an important analytical task in which many scholarly publications are contributing to fill the knowledge gap in this area. The main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data, which can be considered as one of the most challenging source of information available today due to its sheer volume and noise. This study is based on the work by Scott Spangler and Jeffrey Kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health-related sites. The mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families. Moreover, a novel method for generating the category's label and the determination of an optimal number of categories is presented which extends Scott and Jeffrey's research in a meaningful way. We assume the reader is familiar with taxonomies, what they are and how they are used.\n    ",
        "submission_date": "2016-02-12T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04853",
        "title": "Complex Networks of Words in Fables",
        "authors": [
            "Yurij Holovatch",
            "Vasyl Palchykov"
        ],
        "abstract": "In this chapter we give an overview of the application of complex network theory to quantify some properties of language. Our study is based on two fables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists of two parts: the analysis of frequency-rank distributions of words and the application of complex-network theory. The first part shows that the text sizes are sufficiently large to observe statistical properties. This supports their selection for the analysis of typical properties of the language networks in the second part of the chapter. In describing language as a complex network, while words are usually associated with nodes, there is more variability in the choice of links and different representations result in different networks. Here, we examine a number of such representations of the language network and perform a comparative analysis of their characteristics. Our results suggest that, irrespective of link representation, the Ukrainian language network used in the selected fables is a strongly correlated, scale-free, small world. We discuss how such empirical approaches may help form a useful basis for a theoretical description of language evolution and how they may be used in analyses of other textual narratives.\n    ",
        "submission_date": "2016-02-04T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04874",
        "title": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation",
        "authors": [
            "Yushi Yao",
            "Zheng Huang"
        ],
        "abstract": "Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04930",
        "title": "Generalized minimum dominating set and application in automatic text summarization",
        "authors": [
            "Yi-Zhi Xu",
            "Hai-Jun Zhou"
        ],
        "abstract": "For a graph formed by vertices and weighted edges, a generalized minimum dominating set (MDS) is a vertex set of smallest cardinality such that the summed weight of edges from each outside vertex to vertices in this set is equal to or larger than certain threshold value. This generalized MDS problem reduces to the conventional MDS problem in the limiting case of all the edge weights being equal to the threshold value. We treat the generalized MDS problem in the present paper by a replica-symmetric spin glass theory and derive a set of belief-propagation equations. As a practical application we consider the problem of extracting a set of sentences that best summarize a given input text document. We carry out a preliminary test of the statistical physics-inspired method to this automatic text summarization problem.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.04983",
        "title": "Contextual Media Retrieval Using Natural Language Queries",
        "authors": [
            "Sreyasi Nag Chowdhury",
            "Mateusz Malinowski",
            "Andreas Bulling",
            "Mario Fritz"
        ],
        "abstract": "The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation.\n    ",
        "submission_date": "2016-02-16T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05765",
        "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
        "authors": [
            "Shoaib Jameel",
            "Steven Schockaert"
        ],
        "abstract": "Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.\n    ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2017-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.05875",
        "title": "Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data",
        "authors": [
            "Gil Keren",
            "Bj\u00f6rn Schuller"
        ],
        "abstract": "Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in ",
        "submission_date": "2016-02-18T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06025",
        "title": "Spectral Learning for Supervised Topic Models",
        "authors": [
            "Yong Ren",
            "Yining Wang",
            "Jun Zhu"
        ],
        "abstract": "Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on variational approximation or Monte Carlo sampling, which often suffers from the local minimum defect. Spectral methods have been applied to learn unsupervised topic models, such as latent Dirichlet allocation (LDA), with provable guarantees. This paper investigates the possibility of applying spectral methods to recover the parameters of supervised LDA (sLDA). We first present a two-stage spectral method, which recovers the parameters of LDA followed by a power update method to recover the regression model parameters. Then, we further present a single-phase spectral algorithm to jointly recover the topic distribution matrix as well as the regression weights. Our spectral algorithms are provably correct and computationally efficient. We prove a sample complexity bound for each algorithm and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the spectral algorithms. In fact, our results on a large-scale review rating dataset demonstrate that our single-phase spectral algorithm alone gets comparable or even better performance than state-of-the-art methods, while previous work on spectral methods has rarely reported such promising performance.\n    ",
        "submission_date": "2016-02-19T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.06727",
        "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training",
        "authors": [
            "Zhizheng Wu",
            "Simon King"
        ],
        "abstract": "We propose two novel techniques --- stacking bottleneck features and minimum generation error training criterion --- to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically--informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07275",
        "title": "Temporal Network Analysis of Literary Texts",
        "authors": [
            "Sandra D. Prado",
            "Silvio R. Dahmen",
            "Ana L.C. Bazzan",
            "Padraig Mac Carron",
            "Ralph Kenna"
        ],
        "abstract": "We study temporal networks of characters in literature focusing on \"Alice's Adventures in Wonderland\" (1865) by Lewis Carroll and the anonymous \"La Chanson de Roland\" (around 1100). The former, one of the most influential pieces of nonsense literature ever written, describes the adventures of Alice in a fantasy world with logic plays interspersed along the narrative. The latter, a song of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during Charlemagne's campaign on the Iberian Peninsula. We apply methods recently developed by Taylor and coworkers \\cite{Taylor+2015} to find time-averaged eigenvector centralities, Freeman indices and vitalities of characters. We show that temporal networks are more appropriate than static ones for studying stories, as they capture features that the time-independent approaches fail to yield.\n    ",
        "submission_date": "2016-02-22T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07291",
        "title": "The IBM 2016 Speaker Recognition System",
        "authors": [
            "Seyed Omid Sadjadi",
            "Sriram Ganapathy",
            "Jason W. Pelecanos"
        ],
        "abstract": "In this paper we describe the recent advancements made in the IBM i-vector speaker recognition system for conversational speech. In particular, we identify key techniques that contribute to significant improvements in performance of our system, and quantify their contributions. The techniques include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is formulated to alleviate some of the limitations associated with the conventional linear discriminant analysis (LDA) that assumes Gaussian class-conditional distributions, 2) the application of speaker- and channel-adapted features, which are derived from an automatic speech recognition (ASR) system, for speaker recognition, and 3) the use of a deep neural network (DNN) acoustic model with a large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 speaker recognition evaluation (SRE) extended core conditions involving telephone and microphone trials. Experimental results indicate that: 1) the NDA is more effective (up to 35% relative improvement in terms of EER) than the traditional parametric LDA for speaker recognition, 2) when compared to raw acoustic features (e.g., MFCCs), the ASR speaker-adapted features provide gains in speaker recognition performance, and 3) increasing the number of output units in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k) provides consistent improvements in performance (for example from 37% to 57% relative EER gains over our baseline GMM i-vector system). To our knowledge, results reported in this paper represent the best performances published to date on the NIST SRE 2010 extended core tasks.\n    ",
        "submission_date": "2016-02-23T00:00:00",
        "last_modified_date": "2016-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07394",
        "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features",
        "authors": [
            "Zhenhao Ge"
        ],
        "abstract": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.07807",
        "title": "Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection",
        "authors": [
            "Michael Bloodgood",
            "Benjamin Strauss"
        ],
        "abstract": "Many important forms of data are stored digitally in XML format. Errors can occur in the textual content of the data in the fields of the XML. Fixing these errors manually is time-consuming and expensive, especially for large amounts of data. There is increasing interest in the research, development, and use of automated techniques for assisting with data cleaning. Electronic dictionaries are an important form of data frequently stored in XML format that frequently have errors introduced through a mixture of manual typographical entry errors and optical character recognition errors. In this paper we describe methods for flagging statistical anomalies as likely errors in electronic dictionaries stored in XML format. We describe six systems based on different sources of information. The systems detect errors using various signals in the data including uncommon characters, text length, character-based language models, word-based language models, tied-field length ratios, and tied-field transliteration models. Four of the systems detect errors based on expectations automatically inferred from content within elements of a single field type. We call these single-field systems. Two of the systems detect errors based on correspondence expectations automatically inferred from content within elements of multiple related field types. We call these tied-field systems. For each system, we provide an intuitive analysis of the type of error that it is successful at detecting. Finally, we describe two larger-scale evaluations using crowdsourcing with Amazon's Mechanical Turk platform and using the annotations of a domain expert. The evaluations consistently show that the systems are useful for improving the efficiency with which errors in XML electronic dictionaries can be detected.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08128",
        "title": "PCA Method for Automated Detection of Mispronounced Words",
        "authors": [
            "Zhenhao Ge",
            "Sudhendu R. Sharma",
            "Mark J. T. Smith"
        ],
        "abstract": "This paper presents a method for detecting mispronunciations with the aim of improving Computer Assisted Language Learning (CALL) tools used by foreign language learners. The algorithm is based on Principle Component Analysis (PCA). It is hierarchical with each successive step refining the estimate to classify the test word as being either mispronounced or correct. Preprocessing before detection, like normalization and time-scale modification, is implemented to guarantee uniformity of the feature vectors input to the detection system. The performance using various features including spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated. Best results were obtained using MFCCs, achieving up to 99% accuracy in word verification and 93% in native/non-native classification. Compared with Hidden Markov Models (HMMs) which are used pervasively in recognition application, this particular approach is computational efficient and effective when training data is limited.\n    ",
        "submission_date": "2016-02-25T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1602.08761",
        "title": "Resource Constrained Structured Prediction",
        "authors": [
            "Tolga Bolukbasi",
            "Kai-Wei Chang",
            "Joseph Wang",
            "Venkatesh Saligrama"
        ],
        "abstract": "We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.\n    ",
        "submission_date": "2016-02-28T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00106",
        "title": "Characterizing Diseases from Unstructured Text: A Vocabulary Driven Word2vec Approach",
        "authors": [
            "Saurav Ghosh",
            "Prithwish Chakraborty",
            "Emily Cohn",
            "John S. Brownstein",
            "Naren Ramakrishnan"
        ],
        "abstract": "Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00260",
        "title": "Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search and Analytics",
        "authors": [
            "Dhruv Gupta"
        ],
        "abstract": "In this article, I present the questions that I seek to answer in my PhD research. I posit to analyze natural language text with the help of semantic annotations and mine important events for navigating large text corpora. Semantic annotations such as named entities, geographic locations, and temporal expressions can help us mine events from the given corpora. These events thus provide us with useful means to discover the locked knowledge in them. I pose three problems that can help unlock this knowledge vault in semantically annotated text corpora: i. identifying important events; ii. semantic search; and iii. event analytics.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.00423",
        "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.\n    ",
        "submission_date": "2016-03-01T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01354",
        "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
        "authors": [
            "Xuezhe Ma",
            "Eduard Hovy"
        ],
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01417",
        "title": "Dynamic Memory Networks for Visual and Textual Question Answering",
        "authors": [
            "Caiming Xiong",
            "Stephen Merity",
            "Richard Socher"
        ],
        "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01520",
        "title": "Optimized Polynomial Evaluation with Semantic Annotations",
        "authors": [
            "Daniel Rubio Bonilla",
            "Colin W. Glass",
            "Jan Kuper"
        ],
        "abstract": "In this paper we discuss how semantic annotations can be used to introduce mathematical algorithmic information of the underlying imperative code to enable compilers to produce code transformations that will enable better performance. By using this approaches not only good performance is achieved, but also better programmability, maintainability and portability across different hardware architectures. To exemplify this we will use polynomial equations of different degrees.\n    ",
        "submission_date": "2016-03-04T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.01987",
        "title": "A matter of words: NLP for quality evaluation of Wikipedia medical articles",
        "authors": [
            "Vittoria Cozza",
            "Marinella Petrocchi",
            "Angelo Spognardi"
        ],
        "abstract": "Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles. In particular, we evaluate the articles adopting an \"actionable\" model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving a given article quality. We rely on Natural Language Processing (NLP) and dictionaries-based techniques in order to extract the bio-medical concepts in a text. We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal, which have been previously manually labeled by the Wiki Project team. The results of our experiments confirm that, by considering domain-oriented features, it is possible to obtain sensible improvements with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. Other than being interesting by their own, the results call for further research in the area of domain specific features suitable for Web data quality assessment.\n    ",
        "submission_date": "2016-03-07T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03153",
        "title": "Zipf's law emerges asymptotically during phase transitions in communicative systems",
        "authors": [
            "Bohdan B. Khomtchouk",
            "Claes Wahlestedt"
        ],
        "abstract": "Zipf's law predicts a power-law relationship between word rank and frequency in language communication systems, and is widely reported in texts yet remains enigmatic as to its origins. Computer simulations have shown that language communication systems emerge at an abrupt phase transition in the fidelity of mappings between symbols and objects. Since the phase transition approximates the Heaviside or step function, we show that Zipfian scaling emerges asymptotically at high rank based on the Laplace transform. We thereby demonstrate that Zipf's law gradually emerges from the moment of phase transition in communicative systems. We show that this power-law scaling behavior explains the emergence of natural languages at phase transitions. We find that the emergence of Zipf's law during language communication suggests that the use of rare words in a lexicon is critical for the construction of an effective communicative system at the phase transition.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03170",
        "title": "Data fluidity in DARIAH -- pushing the agenda forward",
        "authors": [
            "Laurent Romary",
            "Mike Mertens",
            "Anne Baillot"
        ],
        "abstract": "This paper provides both an update concerning the setting up of the European DARIAH infrastructure and a series of strong action lines related to the development of a data centred strategy for the humanities in the coming years. In particular we tackle various aspect of data management: data hosting, the setting up of a DARIAH seal of approval, the establishment of a charter between cultural heritage institutions and scholars and finally a  specific view on certification mechanisms for data.\n    ",
        "submission_date": "2016-03-10T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.03610",
        "title": "A short proof that $O_2$ is an MCFL",
        "authors": [
            "Mark-Jan Nederhof"
        ],
        "abstract": "We present a new proof that $O_2$ is a multiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widening the results to higher dimensions. This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language.\n    ",
        "submission_date": "2016-03-11T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.05962",
        "title": "Document Neural Autoregressive Distribution Estimation",
        "authors": [
            "Stanislas Lauly",
            "Yin Zheng",
            "Alexandre Allauzen",
            "Hugo Larochelle"
        ],
        "abstract": "We present an approach based on feed-forward neural networks for learning the distribution of textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator(NADE) model, which has been shown to be a good estimator of the distribution of discrete-valued igh-dimensional vectors. In this paper, we present how NADE can successfully be adapted to the case of textual data, retaining from NADE the property that sampling or computing the probability of observations can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by the alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-specific context.\n    ",
        "submission_date": "2016-03-18T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06318",
        "title": "Harnessing Deep Neural Networks with Logic Rules",
        "authors": [
            "Zhiting Hu",
            "Xuezhe Ma",
            "Zhengzhong Liu",
            "Eduard Hovy",
            "Eric Xing"
        ],
        "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2020-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.06485",
        "title": "A System for Probabilistic Linking of Thesauri and Classification Systems",
        "authors": [
            "Lisa Posch",
            "Philipp Schaer",
            "Arnim Bleier",
            "Markus Strohmaier"
        ],
        "abstract": "This paper presents a system which creates and visualizes probabilistic semantic links between concepts in a thesaurus and classes in a classification system. For creating the links, we build on the Polylingual Labeled Topic Model (PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in the classification system by using information from the natural language text of documents, their assigned thesaurus descriptors and their designated classes. The links are then presented to users of the system in an interactive visualization, providing them with an automatically generated overview of the relations between the thesaurus and the classification system.\n    ",
        "submission_date": "2016-03-21T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07150",
        "title": "The Anatomy of a Search and Mining System for Digital Archives",
        "authors": [
            "Martyn Harris",
            "Mark Levene",
            "Dell Zhang",
            "Dan Levene"
        ],
        "abstract": "Samtla (Search And Mining Tools with Linguistic Analysis) is a digital humanities system designed in collaboration with historians and linguists to assist them with their research work in quantifying the content of any textual corpora through approximate phrase search and document comparison. The retrieval engine uses a character-based n-gram language model rather than the conventional word-based one so as to achieve great flexibility in language agnostic query processing.\n",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07313",
        "title": "CONDITOR1: Topic Maps and DITA labelling tool for textual documents with historical information",
        "authors": [
            "Piedad Garrido",
            "Jesus Tramullas",
            "Manuel Coll"
        ],
        "abstract": "Conditor is a software tool which works with textual documents containing historical information. The purpose of this work two-fold: firstly to show the validity of the developed engine to correctly identify and label the entities of the universe of discourse with a labelled-combined XTM-DITA model. Secondly to explain the improvements achieved in the information retrieval process thanks to the use of a object-oriented database (JPOX) as well as its integration into the Lucene-type database search process to not only accomplish more accurate searches, but to also help the future development of a recommender system. We finish with a brief demo in a 3D-graph of the results of the aforementioned search.\n    ",
        "submission_date": "2016-03-23T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.07646",
        "title": "Recursive Neural Language Architecture for Tag Prediction",
        "authors": [
            "Saurabh Kataria"
        ],
        "abstract": "We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.\n    ",
        "submission_date": "2016-03-24T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08079",
        "title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities",
        "authors": [
            "Yevgeni Berzak",
            "Andrei Barbu",
            "Daniel Harari",
            "Boris Katz",
            "Shimon Ullman"
        ],
        "abstract": "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.\n    ",
        "submission_date": "2016-03-26T00:00:00",
        "last_modified_date": "2016-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08089",
        "title": "Online shopping behavior study based on multi-granularity opinion mining: China vs. America",
        "authors": [
            "Qingqing Zhou",
            "Rui Xia",
            "Chengzhi Zhang"
        ],
        "abstract": "With the development of e-commerce, many products are now being sold worldwide, and manufacturers are eager to obtain a better understanding of customer behavior in various regions. To achieve this goal, most previous efforts have focused mainly on questionnaires, which are time-consuming and costly. The tremendous volume of product reviews on e-commerce websites has seen a new trend emerge, whereby manufacturers attempt to understand user preferences by analyzing online reviews. Following this trend, this paper addresses the problem of studying customer behavior by exploiting recently developed opinion mining techniques. This work is novel for three reasons. First, questionnaire-based investigation is automatically enabled by employing algorithms for template-based question generation and opinion mining-based answer extraction. Using this system, manufacturers are able to obtain reports of customer behavior featuring a much larger sample size, more direct information, a higher degree of automation, and a lower cost. Second, international customer behavior study is made easier by integrating tools for multilingual opinion mining. Third, this is the first time an automatic questionnaire investigation has been conducted to compare customer behavior in China and America, where product reviews are written and read in Chinese and English, respectively. Our study on digital cameras, smartphones, and tablet computers yields three findings. First, Chinese customers follow the Doctrine of the Mean, and often use euphemistic expressions, while American customers express their opinions more directly. Second, Chinese customers care more about general feelings, while American customers pay more attention to product details. Third, Chinese customers focus on external features, while American customers care more about the internal features of products.\n    ",
        "submission_date": "2016-03-26T00:00:00",
        "last_modified_date": "2016-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08091",
        "title": "Measuring Book Impact Based on the Multi-granularity Online Review Mining",
        "authors": [
            "Qingqing Zhou",
            "Chengzhi Zhang",
            "Star X. Zhao",
            "Bikun Chen"
        ],
        "abstract": "As with articles and journals, the customary methods for measuring books' academic impact mainly involve citations, which is easy but limited to interrogating traditional citation databases and scholarly book reviews, Researchers have attempted to use other metrics, such as Google Books, libcitation, and publisher prestige. However, these approaches lack content-level information and cannot determine the citation intentions of users. Meanwhile, the abundant online review resources concerning academic books can be used to mine deeper information and content utilizing altmetric perspectives. In this study, we measure the impacts of academic books by multi-granularity mining online reviews, and we identify factors that affect a book's impact. First, online reviews of a sample of academic books on ",
        "submission_date": "2016-03-26T00:00:00",
        "last_modified_date": "2016-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08321",
        "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention",
        "authors": [
            "Linlin Chao",
            "Jianhua Tao",
            "Minghao Yang",
            "Ya Li",
            "Zhengqi Wen"
        ],
        "abstract": "This paper focuses on two key problems for audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08507",
        "title": "Generating Visual Explanations",
        "authors": [
            "Lisa Anne Hendricks",
            "Zeynep Akata",
            "Marcus Rohrbach",
            "Jeff Donahue",
            "Bernt Schiele",
            "Trevor Darrell"
        ],
        "abstract": "Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.\n    ",
        "submission_date": "2016-03-28T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.08636",
        "title": "Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems",
        "authors": [
            "Jiri Vinarek",
            "Petr Hnetynka"
        ],
        "abstract": "The Invariant Refinement Method for Self Adaptation (IRM-SA) is a design method targeting development of smart Cyber-Physical Systems (sCPS). It allows for a systematic translation of the system requirements into the system architecture expressed as an ensemble-based component system (EBCS). However, since the requirements are captured using natural language, there exists the danger of their misinterpretation due to natural language requirements' ambiguity, which could eventually lead to design errors. Thus, automation and validation of the design process is desirable. In this paper, we (i) analyze the translation process of natural language requirements into the IRM-SA model, (ii) identify individual steps that can be automated and/or validated using natural language processing techniques, and (iii) propose suitable methods.\n    ",
        "submission_date": "2016-03-29T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1603.09381",
        "title": "Clinical Information Extraction via Convolutional Neural Network",
        "authors": [
            "Peng Li",
            "Heng Huang"
        ],
        "abstract": "We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.\n    ",
        "submission_date": "2016-03-30T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00125",
        "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
        "authors": [
            "Ziqiang Cao",
            "Wenjie Li",
            "Sujian Li",
            "Furu Wei",
            "Yanran Li"
        ],
        "abstract": "Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.\n    ",
        "submission_date": "2016-04-01T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.00790",
        "title": "Image Captioning with Deep Bidirectional LSTMs",
        "authors": [
            "Cheng Wang",
            "Haojin Yang",
            "Christian Bartz",
            "Christoph Meinel"
        ],
        "abstract": "This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \"translate\" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task.\n    ",
        "submission_date": "2016-04-04T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01219",
        "title": "Learning to Generate Posters of Scientific Papers",
        "authors": [
            "Yuting Qiang",
            "Yanwei Fu",
            "Yanwen Guo",
            "Zhi-Hua Zhou",
            "Leonid Sigal"
        ],
        "abstract": "Researchers often summarize their work in the form of posters. Posters provide a coherent and efficient way to convey core ideas from scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including panel layout and attributes of each panel, are learned and inferred from data. Then, given inferred layout and attributes, composition of graphical elements within each panel is synthesized. To learn and validate our model, we collect and make public a Poster-Paper dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01243",
        "title": "Mental Lexicon Growth Modelling Reveals the Multiplexity of the English Language",
        "authors": [
            "Massimo Stella",
            "Markus Brede"
        ],
        "abstract": "In this work we extend previous analyses of linguistic networks by adopting a multi-layer network framework for modelling the human mental lexicon, i.e. an abstract mental repository where words and concepts are stored together with their linguistic patterns. Across a three-layer linguistic multiplex, we model English words as nodes and connect them according to (i) phonological similarities, (ii) synonym relationships and (iii) free word associations. Our main aim is to exploit this multi-layered structure to explore the influence of phonological and semantic relationships on lexicon assembly over time. We propose a model of lexicon growth which is driven by the phonological layer: words are suggested according to different orderings of insertion (e.g. shorter word length, highest frequency, semantic multiplex features) and accepted or rejected subject to constraints. We then measure times of network assembly and compare these to empirical data about the age of acquisition of words. In agreement with empirical studies in psycholinguistics, our results provide quantitative evidence for the hypothesis that word acquisition is driven by features at multiple levels of organisation within language.\n    ",
        "submission_date": "2016-04-05T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.01485",
        "title": "A Focused Dynamic Attention Model for Visual Question Answering",
        "authors": [
            "Ilija Ilievski",
            "Shuicheng Yan",
            "Jiashi Feng"
        ],
        "abstract": "Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.\n    ",
        "submission_date": "2016-04-06T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02027",
        "title": "Combinatorial Topic Models using Small-Variance Asymptotics",
        "authors": [
            "Ke Jiang",
            "Suvrit Sra",
            "Brian Kulis"
        ],
        "abstract": "Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02038",
        "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves",
        "authors": [
            "Fei Tian",
            "Bin Gao",
            "Di He",
            "Tie-Yan Liu"
        ],
        "abstract": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation.\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02125",
        "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes",
        "authors": [
            "Gordon Christie",
            "Ankit Laddha",
            "Aishwarya Agrawal",
            "Stanislaw Antol",
            "Yash Goyal",
            "Kevin Kochersberger",
            "Dhruv Batra"
        ],
        "abstract": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \"I shot an elephant in my pajamas\", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).\n    ",
        "submission_date": "2016-04-07T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.02594",
        "title": "Learning Compact Recurrent Neural Networks",
        "authors": [
            "Zhiyun Lu",
            "Vikas Sindhwani",
            "Tara N. Sainath"
        ],
        "abstract": "Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75%, at a small cost of 0.3% increase in WER, on a 2,000-hr English Voice Search task.\n    ",
        "submission_date": "2016-04-09T00:00:00",
        "last_modified_date": "2016-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03249",
        "title": "Attributes as Semantic Units between Natural Language and Visual Recognition",
        "authors": [
            "Marcus Rohrbach"
        ],
        "abstract": "Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03318",
        "title": "Applying Ontological Modeling on Quranic Nature Domain",
        "authors": [
            "A.B.M. Shamsuzzaman Sadi",
            "Towfique Anam",
            "Mohamed Abdirazak",
            "Abdillahi Hasan Adnan",
            "Sazid Zaman Khan",
            "Mohamed Mahmudur Rahman",
            "Ghassan Samara"
        ],
        "abstract": "The holy Quran is the holy book of the Muslims. It contains information about many domains. Often people search for particular concepts of holy Quran based on the relations among concepts. An ontological modeling of holy Quran can be useful in such a scenario. In this paper, we have modeled nature related concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource Description Framework). Our methodology involves identifying nature related concepts mentioned in holy Quran and identifying relations among those concepts. These concepts and relations are represented as classes/instances and properties of an OWL ontology. Later, in the result section it is shown that, using the Ontological model, SPARQL queries can retrieve verses and concepts of interest. Thus, this modeling helps semantic search and query on the holy Quran. In this work, we have used English translation of the holy Quran by Sahih International, Protege OWL Editor and for querying we have used SPARQL.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03390",
        "title": "Video Description using Bidirectional Recurrent Neural Networks",
        "authors": [
            "\u00c1lvaro Peris",
            "Marc Bola\u00f1os",
            "Petia Radeva",
            "Francisco Casacuberta"
        ],
        "abstract": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.\n    ",
        "submission_date": "2016-04-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.03627",
        "title": "Dissecting a Social Botnet: Growth, Content and Influence in Twitter",
        "authors": [
            "Norah Abokhodair",
            "Daisy Yoo",
            "David W. McDonald"
        ],
        "abstract": "Social botnets have become an important phenomenon on social media. There are many ways in which social bots can disrupt or influence online discourse, such as, spam hashtags, scam twitter users, and astroturfing. In this paper we considered one specific social botnet in Twitter to understand how it grows over time, how the content of tweets by the social botnet differ from regular users in the same dataset, and lastly, how the social botnet may have influenced the relevant discussions. Our analysis is based on a qualitative coding for approximately 3000 tweets in Arabic and English from the Syrian social bot that was active for 35 weeks on Twitter before it was shutdown. We find that the growth, behavior and content of this particular botnet did not specifically align with common conceptions of botnets. Further we identify interesting aspects of the botnet that distinguish it from regular users.\n    ",
        "submission_date": "2016-04-13T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04383",
        "title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding",
        "authors": [
            "Milos Cernak",
            "Alexandros Lazaridis",
            "Afsaneh Asaei",
            "Philip N. Garner"
        ],
        "abstract": "Most current very low bit rate (VLBR) speech coding systems use hidden Markov model (HMM) based speech recognition/synthesis techniques. This allows transmission of information (such as phonemes) segment by segment that decreases the bit rate. However, the encoder based on a phoneme speech recognition may create bursts of segmental errors. Segmental errors are further propagated to optional suprasegmental (such as syllable) information coding. Together with the errors of voicing detection in pitch parametrization, HMM-based speech coding creates speech discontinuities and unnatural speech sound artefacts.\n",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.04661",
        "title": "Parallelizing Word2Vec in Shared and Distributed Memory",
        "authors": [
            "Shihao Ji",
            "Nadathur Satish",
            "Sheng Li",
            "Pradeep Dubey"
        ],
        "abstract": "Word2Vec is a widely used algorithm for extracting low-dimensional vector representations of words. It generated considerable excitement in the machine learning and natural language processing (NLP) communities recently due to its exceptional performance in many NLP applications such as named entity recognition, sentiment analysis, machine translation and question answering. State-of-the-art algorithms including those by Mikolov et al. have been parallelized for multi-core CPU architectures but are based on vector-vector operations that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we improve reuse of various data structures in the algorithm through the use of minibatching, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a compute cluster, and demonstrate good strong scalability up to 32 nodes. In combination, these techniques allow us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge.\n    ",
        "submission_date": "2016-04-15T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05468",
        "title": "Understanding Rating Behaviour and Predicting Ratings by Identifying Representative Users",
        "authors": [
            "Rahul Kamath",
            "Masanao Ochi",
            "Yutaka Matsuo"
        ],
        "abstract": "Online user reviews describing various products and services are now abundant on the web. While the information conveyed through review texts and ratings is easily comprehensible, there is a wealth of hidden information in them that is not immediately obvious. In this study, we unlock this hidden value behind user reviews to understand the various dimensions along which users rate products. We learn a set of users that represent each of these dimensions and use their ratings to predict product ratings. Specifically, we work with restaurant reviews to identify users whose ratings are influenced by dimensions like 'Service', 'Atmosphere' etc. in order to predict restaurant ratings and understand the variation in rating behaviour across different cuisines. While previous approaches to obtaining product ratings require either a large number of user ratings or a few review texts, we show that it is possible to predict ratings with few user ratings and no review text. Our experiments show that our approach outperforms other conventional methods by 16-27% in terms of RMSE.\n    ",
        "submission_date": "2016-04-19T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.05781",
        "title": "What we write about when we write about causality: Features of causal statements across large-scale social discourse",
        "authors": [
            "Thomas C. McAndrew",
            "Joshua C. Bongard",
            "Christopher M. Danforth",
            "Peter S. Dodds",
            "Paul D. H. Hines",
            "James P. Bagrow"
        ],
        "abstract": "Identifying and communicating relationships between causes and effects is important for understanding our world, but is affected by language structure, cognitive and emotional biases, and the properties of the communication medium. Despite the increasing importance of social media, much remains unknown about causal statements made online. To study real-world causal attribution, we extract a large-scale corpus of causal statements made on the Twitter social network platform as well as a comparable random control corpus. We compare causal and control statements using statistical language and sentiment analysis tools. We find that causal statements have a number of significant lexical and grammatical differences compared with controls and tend to be more negative in sentiment than controls. Causal statements made online tend to focus on news and current events, medicine and health, or interpersonal relationships, as shown by topic models. By quantifying the features and potential biases of causality communication, this study improves our understanding of the accuracy of information and opinions found online.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06076",
        "title": "Question Answering via Integer Programming over Semi-Structured Knowledge",
        "authors": [
            "Daniel Khashabi",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Peter Clark",
            "Oren Etzioni",
            "Dan Roth"
        ],
        "abstract": "Answering science questions posed in natural language is an important AI challenge. Answering such questions often requires non-trivial inference and knowledge that goes beyond factoid retrieval. Yet, most systems for this task are based on relatively shallow Information Retrieval (IR) and statistical correlation techniques operating on large unstructured corpora. We propose a structured inference system for this task, formulated as an Integer Linear Program (ILP), that answers natural language questions using a semi-structured knowledge base derived from text, including questions requiring multi-step inference and a combination of multiple facts. On a dataset of real, unseen science questions, our system significantly outperforms (+14%) the best previous attempt at structured reasoning for this task, which used Markov Logic Networks (MLNs). It also improves upon a previous ILP formulation by 17.7%. When combined with unstructured inference methods, the ILP system significantly boosts overall performance (+10%). Finally, we show our approach is substantially more robust to a simple answer perturbation compared to statistical correlation methods.\n    ",
        "submission_date": "2016-04-20T00:00:00",
        "last_modified_date": "2016-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06225",
        "title": "OCR Error Correction Using Character Correction and Feature-Based Word Classification",
        "authors": [
            "Ido Kissos",
            "Nachum Dershowitz"
        ],
        "abstract": "This paper explores the use of a learned classifier for post-OCR text correction. Experiments with the Arabic language show that this approach, which integrates a weighted confusion matrix and a shallow language model, improves the vast majority of segmentation and recognition errors, the most frequent types of error on our dataset.\n    ",
        "submission_date": "2016-04-21T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.06721",
        "title": "Exploiting Deep Semantics and Compositionality of Natural Language for Human-Robot-Interaction",
        "authors": [
            "Manfred Eppe",
            "Sean Trott",
            "Jerome Feldman"
        ],
        "abstract": "We develop a natural language interface for human robot interaction that implements reasoning about deep semantics in natural language. To realize the required deep analysis, we employ methods from cognitive linguistics, namely the modular and compositional framework of Embodied Construction Grammar (ECG) [Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference resolution problems and other issues related to deep semantics and compositionality of natural language. This also includes verbal interaction with humans to clarify commands and queries that are too ambiguous to be executed safely. We implement our NLU framework as a ROS package and present proof-of-concept scenarios with different robots, as well as a survey on the state of the art.\n    ",
        "submission_date": "2016-04-22T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.07236",
        "title": "Towards Real-Time, Country-Level Location Classification of Worldwide Tweets",
        "authors": [
            "Arkaitz Zubiaga",
            "Alex Voss",
            "Rob Procter",
            "Maria Liakata",
            "Bo Wang",
            "Adam Tsakalidis"
        ],
        "abstract": "In contrast to much previous work that has focused on location classification of tweets restricted to a specific country, here we undertake the task in a broader context by classifying global tweets at the country level, which is so far unexplored in a real-time scenario. We analyse the extent to which a tweet's country of origin can be determined by making use of eight tweet-inherent features for classification. Furthermore, we use two datasets, collected a year apart from each other, to analyse the extent to which a model trained from historical tweets can still be leveraged for classification of new tweets. With classification experiments on all 217 countries in our datasets, as well as on the top 25 countries, we offer some insights into the best use of tweet-inherent features for an accurate country-level classification of tweets. We find that the use of a single feature, such as the use of tweet content alone -- the most widely used feature in previous work -- leaves much to be desired. Choosing an appropriate combination of both tweet content and metadata can actually lead to substantial improvements of between 20\\% and 50\\%. We observe that tweet content, the user's self-reported location and the user's real name, all of which are inherent in a tweet and available in a real-time scenario, are particularly useful to determine the country of origin. We also experiment on the applicability of a model trained on historical tweets to classify new tweets, finding that the choice of a particular combination of features whose utility does not fade over time can actually lead to comparable performance, avoiding the need to retrain. However, the difficulty of achieving accurate classification increases slightly for countries with multiple commonalities, especially for English and Spanish speaking countries.\n    ",
        "submission_date": "2016-04-25T00:00:00",
        "last_modified_date": "2017-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1604.08095",
        "title": "Accent Classification with Phonetic Vowel Representation",
        "authors": [
            "Zhenhao Ge",
            "Yingyi Tan",
            "Aravind Ganapathiraju"
        ],
        "abstract": "Previous accent classification research focused mainly on detecting accents with pure acoustic information without recognizing accented speech. This work combines phonetic knowledge such as vowels with acoustic information to build Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP) features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With input about 20-second accented speech, this system achieves classification rate of 51% on a 7-way classification system focusing on the major types of accents in English, which is competitive to the state-of-the-art results in this field.\n    ",
        "submission_date": "2016-02-24T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00122",
        "title": "An Improved System for Sentence-level Novelty Detection in Textual Streams",
        "authors": [
            "Xinyu Fu",
            "Eugene Ch'ng",
            "Uwe Aickelin",
            "Lanyun Zhang"
        ],
        "abstract": "Novelty detection in news events has long been a difficult problem. A number of models performed well on specific data streams but certain issues are far from being solved, particularly in large data streams from the WWW where unpredictability of new terms requires adaptation in the vector space model. We present a novel event detection system based on the Incremental Term Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with Locality Sensitive Hashing (LSH). Our system could efficiently and effectively adapt to the changes within the data streams of any new terms with continual updates to the vector space model. Regarding miss probability, our proposed novelty detection framework outperforms a recognised baseline system by approximately 16% when evaluating a benchmark dataset from Google News.\n    ",
        "submission_date": "2016-04-30T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00223",
        "title": "Text-mining the NeuroSynth corpus using Deep Boltzmann Machines",
        "authors": [
            "Ricardo Pio Monti",
            "Romy Lorenz",
            "Robert Leech",
            "Christoforos Anagnostopoulos",
            "Giovanni Montana"
        ],
        "abstract": "Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.\n    ",
        "submission_date": "2016-05-01T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.00855",
        "title": "Improving Image Captioning by Concept-based Sentence Reranking",
        "authors": [
            "Xirong Li",
            "Qin Jin"
        ],
        "abstract": "This paper describes our winning entry in the ImageCLEF 2015 image sentence generation task. We improve Google's CNN-LSTM model by introducing concept-based sentence reranking, a data-driven approach which exploits the large amounts of concept-level annotations on Flickr. Different from previous usage of concept detection that is tailored to specific image captioning models, the propose approach reranks predicted sentences in terms of their matches with detected concepts, essentially treating the underlying model as a black box. This property makes the approach applicable to a number of existing solutions. We also experiment with fine tuning on the deep language model, which improves the performance further. Scoring METEOR of 0.1875 on the ImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of 0.1687) with a clear margin.\n    ",
        "submission_date": "2016-05-03T00:00:00",
        "last_modified_date": "2016-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01652",
        "title": "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues",
        "authors": [
            "Phong Le",
            "Marc Dymetman",
            "Jean-Michel Renders"
        ],
        "abstract": "We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01661",
        "title": "Parallels of human language in the behavior of bottlenose dolphins",
        "authors": [
            "R. Ferrer-i-Cancho",
            "D. Lusseau",
            "B. McCowan"
        ],
        "abstract": "A short review of similarities between dolphins and humans with the help of quantitative linguistics and information theory.\n    ",
        "submission_date": "2016-05-05T00:00:00",
        "last_modified_date": "2022-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.01919",
        "title": "User Reviews and Language: How Language Influences Ratings",
        "authors": [
            "Scott A. Hale"
        ],
        "abstract": "The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages.\n    ",
        "submission_date": "2016-05-06T00:00:00",
        "last_modified_date": "2016-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02442",
        "title": "Machine Learning Techniques with Ontology for Subjective Answer Evaluation",
        "authors": [
            "M. Syamala Devi",
            "Himani Mittal"
        ],
        "abstract": "Computerized Evaluation of English Essays is performed using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Prot\u00e9g\u00e9, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology.\n    ",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.02697",
        "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.\n",
        "submission_date": "2016-05-09T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03261",
        "title": "Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs",
        "authors": [
            "Junpei Zhong",
            "Martin Peniak",
            "Jun Tani",
            "Tetsuya Ogata",
            "Angelo Cangelosi"
        ],
        "abstract": "The paper presents a neurorobotics cognitive model to explain the understanding and generalisation of nouns and verbs combinations when a vocal command consisting of a verb-noun sentence is provided to a humanoid robot. This generalisation process is done via the grounding process: different objects are being interacted, and associated, with different motor behaviours, following a learning approach inspired by developmental language acquisition in infants. This cognitive model is based on Multiple Time-scale Recurrent Neural Networks (MTRNN).With the data obtained from object manipulation tasks with a humanoid robot platform, the robotic agent implemented with this model can ground the primitive embodied structure of verbs through training with verb-noun combination samples. Moreover, we show that a functional hierarchical architecture, based on MTRNN, is able to generalise and produce novel combinations of noun-verb sentences. Further analyses of the learned network dynamics and representations also demonstrate how the generalisation is possible via the exploitation of this functional hierarchical recurrent network.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03481",
        "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media",
        "authors": [
            "Bhuwan Dhingra",
            "Zhong Zhou",
            "Dylan Fitzpatrick",
            "Michael Muehl",
            "William W. Cohen"
        ],
        "abstract": "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available.\n    ",
        "submission_date": "2016-05-11T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.03705",
        "title": "Movie Description",
        "authors": [
            "Anna Rohrbach",
            "Atousa Torabi",
            "Marcus Rohrbach",
            "Niket Tandon",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville",
            "Bernt Schiele"
        ],
        "abstract": "Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop \"Describing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)\", at ICCV 2015.\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.04227",
        "title": "Relation Schema Induction using Tensor Factorization with Side Information",
        "authors": [
            "Madhav Nimishakavi",
            "Uday Singh Saini",
            "Partha Talukdar"
        ],
        "abstract": "Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).\n    ",
        "submission_date": "2016-05-12T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05087",
        "title": "Word2Vec is a special case of Kernel Correspondence Analysis and Kernels for Natural Language Processing",
        "authors": [
            "Hirotaka Niitsuma",
            "Minho Lee"
        ],
        "abstract": "We show that correspondence analysis (CA) is equivalent to defining a Gini index with appropriately scaled one-hot encoding. Using this relation, we introduce a nonlinear kernel extension to CA. This extended CA gives a known analysis for natural language via specialized kernels that use an appropriate contingency table. We propose a semi-supervised CA, which is a special case of the kernel extension to CA. Because CA requires excessive memory if applied to numerous categories, CA has not been used for natural language processing. We address this problem by introducing delayed evaluation to randomized singular value decomposition. The memory-efficient CA is then applied to a word-vector representation task. We propose a tail-cut kernel, which is an extension to the skip-gram within the kernel extension to CA. Our tail-cut kernel outperforms existing word-vector representation methods.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2018-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05134",
        "title": "A Semi-automatic Method for Efficient Detection of Stories on Social Media",
        "authors": [
            "Soroush Vosoughi",
            "Deb Roy"
        ],
        "abstract": "Twitter has become one of the main sources of news for many people. As real-world events and emergencies unfold, Twitter is abuzz with hundreds of thousands of stories about the events. Some of these stories are harmless, while others could potentially be life-saving or sources of malicious rumors. Thus, it is critically important to be able to efficiently track stories that spread on Twitter during these events. In this paper, we present a novel semi-automatic tool that enables users to efficiently identify and track stories about real-world events on Twitter. We ran a user study with 25 participants, demonstrating that compared to more conventional methods, our tool can increase the speed and the accuracy with which users can track stories about real-world events.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05166",
        "title": "Digital Stylometry: Linking Profiles Across Social Networks",
        "authors": [
            "Soroush Vosoughi",
            "Helen Zhou",
            "Deb Roy"
        ],
        "abstract": "There is an ever growing number of users with accounts on multiple social media and networking sites. Consequently, there is increasing interest in matching user accounts and profiles across different social networks in order to create aggregate profiles of users. In this paper, we present models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We experimented with linguistic, temporal, and combined temporal-linguistic models for matching user accounts, using standard and novel techniques. Using publicly available data, our best model, a combined temporal-linguistic one, was able to correctly match the accounts of 31% of 5,612 distinct users across Twitter and Facebook.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05195",
        "title": "Enhanced Twitter Sentiment Classification Using Contextual Information",
        "authors": [
            "Soroush Vosoughi",
            "Helen Zhou",
            "Deb Roy"
        ],
        "abstract": "The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2021-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.05303",
        "title": "Fuzzy Sets Across the Natural Language Generation Pipeline",
        "authors": [
            "A. Ramos-Soto",
            "A. Bugar\u00edn",
            "S. Barro"
        ],
        "abstract": "We explore the implications of using fuzzy techniques (mainly those commonly used in the linguistic description/summarization of data discipline) from a natural language generation perspective. For this, we provide an extensive discussion of some general convergence points and an exploration of the relationship between the different tasks involved in the standard NLG system pipeline architecture and the most common fuzzy approaches used in linguistic summarization/description of data, such as fuzzy quantified statements, evaluation criteria or aggregation operators. Each individual discussion is illustrated with a related use case. Recent work made in the context of cross-fertilization of both research fields is also referenced. This paper encompasses general ideas that emerged as part of the PhD thesis \"Application of fuzzy sets in data-to-text systems\". It does not present a specific application or a formal approach, but rather discusses current high-level issues and potential usages of fuzzy sets (focused on linguistic summarization of data) in natural language generation.\n    ",
        "submission_date": "2016-05-17T00:00:00",
        "last_modified_date": "2016-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06304",
        "title": "Local communities obstruct global consensus: Naming game on multi-local-world networks",
        "authors": [
            "Yang Lou",
            "Guanrong Chen",
            "Zhengping Fan",
            "Luna Xiang"
        ],
        "abstract": "Community structure is essential for social communications, where individuals belonging to the same community are much more actively interacting and communicating with each other than those in different communities within the human society. Naming game, on the other hand, is a social communication model that simulates the process of learning a name of an object within a community of humans, where the individuals can generally reach global consensus asymptotically through iterative pair-wise conversations. The underlying network indicates the relationships among the individuals. In this paper, three typical topologies, namely random-graph, small-world and scale-free networks, are employed, which are embedded with the multi-local-world community structure, to study the naming game. Simulations show that 1) the convergence process to global consensus is getting slower as the community structure becomes more prominent, and eventually might fail; 2) if the inter-community connections are sufficiently dense, neither the number nor the size of the communities affects the convergence process; and 3) for different topologies with the same average node-degree, local clustering of individuals obstruct or prohibit global consensus to take place. The results reveal the role of local communities in a global naming game in social network studies.\n    ",
        "submission_date": "2016-05-20T00:00:00",
        "last_modified_date": "2018-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.06778",
        "title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
        "authors": [
            "Maximilian Schmitt",
            "Bj\u00f6rn W. Schuller"
        ],
        "abstract": "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.\n    ",
        "submission_date": "2016-05-22T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07268",
        "title": "Classifying discourse in a CSCL platform to evaluate correlations with Teacher Participation and Progress",
        "authors": [
            "Eliana Scheihing",
            "Matthieu Vernier",
            "Javiera Born",
            "Julio Guerra",
            "Luis Carcamo"
        ],
        "abstract": "In Computer-Supported learning, monitoring and engaging a group of learners is a complex task for teachers, especially when learners are working collaboratively: Are my students motivated? What kind of progress are they making? Should I intervene? Is my communication and the didactic design adapted to my students? Our hypothesis is that the analysis of natural language interactions between students, and between students and teachers, provide very valuable information and could be used to produce qualitative indicators to help teachers' decisions. We develop an automatic approach in three steps (1) to explore the discursive functions of messages in a CSCL platform, (2) to classify the messages automatically and (3) to evaluate correlations between discursive attitudes and other variables linked to the learning activity. Results tend to show that some types of discourse are correlated with a notion of Progress on the learning activities and the importance of emotive participation from the Teacher.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07427",
        "title": "Hierarchical Memory Networks",
        "authors": [
            "Sarath Chandar",
            "Sungjin Ahn",
            "Hugo Larochelle",
            "Pascal Vincent",
            "Gerald Tesauro",
            "Yoshua Bengio"
        ],
        "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n    ",
        "submission_date": "2016-05-24T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07733",
        "title": "On model architecture for a children's speech recognition interactive dialog system",
        "authors": [
            "Radoslava Kraleva",
            "Velin Kralev"
        ],
        "abstract": "This report presents a general model of the architecture of information systems for the speech recognition of children. It presents a model of the speech data stream and how it works. The result of these studies and presented veins architectural model shows that research needs to be focused on acoustic-phonetic modeling in order to improve the quality of children's speech recognition and the sustainability of the systems to noise and changes in transmission environment. Another important aspect is the development of more accurate algorithms for modeling of spontaneous child speech.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07844",
        "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation",
        "authors": [
            "Javid Dadashkarimi",
            "Mahsa S. Shahshahani",
            "Amirhossein Tebbifakhr",
            "Heshaam Faili",
            "Azadeh Shakery"
        ],
        "abstract": "Using top-ranked documents in response to a query has been shown to be an effective approach to improve the quality of query translation in dictionary-based cross-language information retrieval. In this paper, we propose a new method for dictionary-based query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional vectors of the words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of translation pairs appeared in the collections. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Italian demonstrate that the proposed method outperforms a word embedding baseline based on bilingual shuffling and a further number of competitive baselines. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07852",
        "title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts",
        "authors": [
            "Javid Dadashkarimi",
            "Hossein Nasr Esfahani",
            "Heshaam Faili",
            "Azadeh Shakery"
        ],
        "abstract": "There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complex texts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in different text mining tasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07891",
        "title": "Query Expansion with Locally-Trained Word Embeddings",
        "authors": [
            "Fernando Diaz",
            "Bhaskar Mitra",
            "Nick Craswell"
        ],
        "abstract": "Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07895",
        "title": "Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey",
        "authors": [
            "Nabiha Asghar"
        ],
        "abstract": "Automatic extraction of cause-effect relationships from natural language texts is a challenging open problem in Artificial Intelligence. Most of the early attempts at its solution used manually constructed linguistic and syntactic rules on small and domain-specific data sets. However, with the advent of big data, the availability of affordable computing power and the recent popularization of machine learning, the paradigm to tackle this problem has slowly shifted. Machines are now expected to learn generic causal extraction rules from labelled data with minimal supervision, in a domain independent-manner. In this paper, we provide a comprehensive survey of causal relation extraction techniques from both paradigms, and analyse their relative strengths and weaknesses, with recommendations for future work.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.07912",
        "title": "Review Networks for Caption Generation",
        "authors": [
            "Zhilin Yang",
            "Ye Yuan",
            "Yuexin Wu",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "abstract": "We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.\n    ",
        "submission_date": "2016-05-25T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.08535",
        "title": "Deep API Learning",
        "authors": [
            "Xiaodong Gu",
            "Hongyu Zhang",
            "Dongmei Zhang",
            "Sunghun Kim"
        ],
        "abstract": "Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query.\n",
        "submission_date": "2016-05-27T00:00:00",
        "last_modified_date": "2017-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09211",
        "title": "Going Deeper for Multilingual Visual Sentiment Detection",
        "authors": [
            "Brendan Jou",
            "Shih-Fu Chang"
        ],
        "abstract": "This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO). The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release, adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet, providing comparable or better performance with reduced network parameter cost.\n",
        "submission_date": "2016-05-30T00:00:00",
        "last_modified_date": "2016-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1605.09553",
        "title": "Attention Correctness in Neural Image Captioning",
        "authors": [
            "Chenxi Liu",
            "Junhua Mao",
            "Fei Sha",
            "Alan Yuille"
        ],
        "abstract": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \"correctness\" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2016-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00061",
        "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
        "authors": [
            "Jiasen Lu",
            "Jianwei Yang",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.\n    ",
        "submission_date": "2016-05-31T00:00:00",
        "last_modified_date": "2017-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00411",
        "title": "Temporal Topic Modeling to Assess Associations between News Trends and Infectious Disease Outbreaks",
        "authors": [
            "Saurav Ghosh",
            "Prithwish Chakraborty",
            "Elaine O. Nsoesie",
            "Emily Cohn",
            "Sumiko R. Mekaru",
            "John S. Brownstein",
            "Naren Ramakrishnan"
        ],
        "abstract": "In retrospective assessments, internet news reports have been shown to capture early reports of unknown infectious disease transmission prior to official laboratory confirmation. In general, media interest and reporting peaks and wanes during the course of an outbreak. In this study, we quantify the extent to which media interest during infectious disease outbreaks is indicative of trends of reported incidence. We introduce an approach that uses supervised temporal topic models to transform large corpora of news articles into temporal topic trends. The key advantages of this approach include, applicability to a wide range of diseases, and ability to capture disease dynamics - including seasonality, abrupt peaks and troughs. We evaluated the method using data from multiple infectious disease outbreaks reported in the United States of America (U.S.), China and India. We noted that temporal topic trends extracted from disease-related news reports successfully captured the dynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue outbreaks in India (2013) and China (2014). Our observations also suggest that efficient modeling of temporal topic trends using time-series regression techniques can estimate disease case counts with increased precision before official reports by health organizations.\n    ",
        "submission_date": "2016-06-01T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.00979",
        "title": "Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information",
        "authors": [
            "Yuanzhe Zhang",
            "Kang Liu",
            "Shizhu He",
            "Guoliang Ji",
            "Zhanyi Liu",
            "Hua Wu",
            "Jun Zhao"
        ],
        "abstract": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01305",
        "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
        "authors": [
            "David Krueger",
            "Tegan Maharaj",
            "J\u00e1nos Kram\u00e1r",
            "Mohammad Pezeshki",
            "Nicolas Ballas",
            "Nan Rosemary Ke",
            "Anirudh Goyal",
            "Yoshua Bengio",
            "Aaron Courville",
            "Chris Pal"
        ],
        "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.\n    ",
        "submission_date": "2016-06-03T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01720",
        "title": "Proof nets for the Displacement calculus",
        "authors": [
            "Richard Moot"
        ],
        "abstract": "We present a proof net calculus for the Displacement calculus and show its correctness. This is the first proof net calculus which models the Displacement calculus directly and not by some sort of translation into another formalism. The proof net calculus opens up new possibilities for parsing and proof search with the Displacement calculus.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.01847",
        "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
        "authors": [
            "Akira Fukui",
            "Dong Huk Park",
            "Daylen Yang",
            "Anna Rohrbach",
            "Trevor Darrell",
            "Marcus Rohrbach"
        ],
        "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.\n    ",
        "submission_date": "2016-06-06T00:00:00",
        "last_modified_date": "2016-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02560",
        "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning",
        "authors": [
            "Tiancheng Zhao",
            "Maxine Eskenazi"
        ],
        "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02562",
        "title": "DialPort: Connecting the Spoken Dialog Research Community to Real User Data",
        "authors": [
            "Tiancheng Zhao",
            "Kyusong Lee",
            "Maxine Eskenazi"
        ],
        "abstract": "This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog academic research community and gives them access to real users. We introduce a distributed, multi-modal, multi-agent prototype dialog framework that affords easy integration with various remote resources, ranging from end-to-end dialog systems to external knowledge APIs. To date, the DialPort portal has successfully connected to the multi-domain spoken dialog system at Cambridge University, the NOAA (National Oceanic and Atmospheric Administration) weather API and the Yelp API.\n    ",
        "submission_date": "2016-06-08T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.02976",
        "title": "Large scale biomedical texts classification: a kNN and an ESA-based approaches",
        "authors": [
            "Khadim Dram\u00e9",
            "Fleur Mougin",
            "Gayo Diallo"
        ],
        "abstract": "With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using only partial information to annotate these documents is promising but remains a very ambitious issue. MethodsWe propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it needs to be improved to perform well in this specific classification problem which deals with partial information. Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for ranking the labels. Additional features are also investigated in order to improve the classifiers' performance. In addition, the combination of several learning algorithms with various techniques for fixing the number of relevant topics is performed. On the other hand, ESA seems promising for this classification task as it yielded interesting results in related issues, such as semantic relatedness computation between texts and text classification.  Unlike existing works, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, our ESA-based method builds a standalone classifier.  Furthermore, we investigate if the results of this method could be useful as a complementary feature of our kNN-based ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03002",
        "title": "MuFuRU: The Multi-Function Recurrent Unit",
        "authors": [
            "Dirk Weissenborn",
            "Tim Rockt\u00e4schel"
        ],
        "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.\n    ",
        "submission_date": "2016-06-09T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03333",
        "title": "Automatic Genre and Show Identification of Broadcast Media",
        "authors": [
            "Mortaza Doulaty",
            "Oscar Saz",
            "Raymond W. M. Ng",
            "Thomas Hain"
        ],
        "abstract": "Huge amounts of digital videos are being produced and broadcast every day, leading to giant media archives. Effective techniques are needed to make such data accessible further. Automatic meta-data labelling of broadcast media is an essential task for multimedia indexing, where it is standard to use multi-modal input for such purposes. This paper describes a novel method for automatic detection of media genre and show identities using acoustic features, textual features or a combination thereof. Furthermore the inclusion of available meta-data, such as time of broadcast, is shown to lead to very high performance. Latent Dirichlet Allocation is used to model both acoustics and text, yielding fixed dimensional representations of media recordings that can then be used in Support Vector Machines based classification. Experiments are conducted on more than 1200 hours of TV broadcasts from the British Broadcasting Corporation (BBC), where the task is to categorise the broadcasts into 8 genres or 133 show identities. On a 200-hour test set, accuracies of 98.6% and 85.7% were achieved for genre and show identification respectively, using a combination of acoustic and textual features with meta-data.\n    ",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03402",
        "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning",
        "authors": [
            "Pavel Sountsov",
            "Sunita Sarawagi"
        ],
        "abstract": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.\n",
        "submission_date": "2016-06-10T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03556",
        "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
        "authors": [
            "Abhishek Das",
            "Harsh Agrawal",
            "C. Lawrence Zitnick",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.\n    ",
        "submission_date": "2016-06-11T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03783",
        "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression",
        "authors": [
            "Pedro Chahuara",
            "Thomas Lampert",
            "Pierre Gancarski"
        ],
        "abstract": "Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.\n    ",
        "submission_date": "2016-06-12T00:00:00",
        "last_modified_date": "2016-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03784",
        "title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection",
        "authors": [
            "Guido Zarrella",
            "Amy Marsh"
        ],
        "abstract": "We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.03864",
        "title": "Neural Associative Memory for Dual-Sequence Modeling",
        "authors": [
            "Dirk Weissenborn"
        ],
        "abstract": "Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.\n    ",
        "submission_date": "2016-06-13T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04217",
        "title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation",
        "authors": [
            "Ekaterina Vylomova",
            "Trevor Cohn",
            "Xuanli He",
            "Gholamreza Haffari"
        ],
        "abstract": "Dealing with the complex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04429",
        "title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation",
        "authors": [
            "Alberto P. Garc\u00eda-Plaza",
            "V\u00edctor Fresno",
            "Raquel Mart\u00ednez",
            "Arkaitz Zubiaga"
        ],
        "abstract": "The selection of a suitable document representation approach plays a crucial role in the performance of a document clustering task. Being able to pick out representative words within a document can lead to substantial improvements in document clustering. In the case of web documents, the HTML markup that defines the layout of the content provides additional structural information that can be further exploited to identify representative words. In this paper we introduce a fuzzy term weighing approach that makes the most of the HTML structure for document clustering. We set forth and build on the hypothesis that a good representation can take advantage of how humans skim through documents to extract the most representative words. The authors of web pages make use of HTML tags to convey the most important message of a web page through page elements that attract the readers' attention, such as page titles or emphasized elements. We define a set of criteria to exploit the information provided by these page elements, and introduce a fuzzy combination of these criteria that we evaluate within the context of a web page clustering task. Our proposed approach, called Abstract Fuzzy Combination of Criteria (AFCC), can adapt to datasets whose features are distributed differently, achieving good results compared to other similar fuzzy logic based approaches and TF-IDF across different datasets.\n    ",
        "submission_date": "2016-06-14T00:00:00",
        "last_modified_date": "2016-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04631",
        "title": "Bidirectional Long-Short Term Memory for Video Description",
        "authors": [
            "Yi Bin",
            "Yang Yang",
            "Zi Huang",
            "Fumin Shen",
            "Xing Xu",
            "Heng Tao Shen"
        ],
        "abstract": "Video captioning has been attracting broad research attention in multimedia community. However, most existing approaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as \\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures bidirectional global temporal structure in video. Specifically, we first devise a joint visual modelling approach to encode video data by combining a forward LSTM pass, a backward LSTM pass, together with visual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The benefits are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense visual features and sparse semantic representations for videos and sentences, respectively. We verify the effectiveness of our proposed video captioning framework on a commonly-used benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.04721",
        "title": "Personality Traits and Echo Chambers on Facebook",
        "authors": [
            "Alessandro Bessi"
        ],
        "abstract": "In online social networks, users tend to select information that adhere to their system of beliefs and to form polarized groups of like minded people. Polarization as well as its effects on online social interactions have been extensively investigated. Still, the relation between group formation and personality traits remains unclear. A better understanding of the cognitive and psychological determinants of online social dynamics might help to design more efficient communication strategies and to challenge the digital misinformation threat. In this work, we focus on users commenting posts published by US Facebook pages supporting scientific and conspiracy-like narratives, and we classify the personality traits of those users according to their online behavior. We show that different and conflicting communities are populated by users showing similar psychological profiles, and that the dominant personality model is the same in both scientific and conspiracy echo chambers. Moreover, we observe that the permanence within echo chambers slightly shapes users' psychological profiles. Our results suggest that the presence of specific personality traits in individuals lead to their considerable involvement in supporting narratives inside virtual echo chambers.\n    ",
        "submission_date": "2016-06-15T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05320",
        "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
        "authors": [
            "Viktoriya Krakovna",
            "Finale Doshi-Velez"
        ],
        "abstract": "As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.\n    ",
        "submission_date": "2016-06-16T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.05925",
        "title": "Graph based manifold regularized deep neural networks for automatic speech recognition",
        "authors": [
            "Vikrant Singh Tomar",
            "Richard C. Rose"
        ],
        "abstract": "Deep neural networks (DNNs) have been successfully applied to a wide variety of acoustic modeling tasks in recent years. These include the applications of DNNs either in a discriminative feature extraction or in a hybrid acoustic modeling scenario. Despite the rapid progress in this area, a number of challenges remain in training DNNs. This paper presents an effective way of training DNNs using a manifold learning based regularization framework. In this framework, the parameters of the network are optimized to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets. This is achieved by incorporating manifold based locality constraints in the objective criterion of DNNs. Empirical evidence is provided to demonstrate that training a network with manifold constraints preserves structural compactness in the hidden layers of the network. Manifold regularization is applied to train bottleneck DNNs for feature extraction in hidden Markov model (HMM) based speech recognition. The experiments in this work are conducted on the Aurora-2 spoken digits and the Aurora-4 read news large vocabulary continuous speech recognition tasks. The performance is measured in terms of word error rate (WER) on these tasks. It is shown that the manifold regularized DNNs result in up to 37% reduction in WER relative to standard DNNs.\n    ",
        "submission_date": "2016-06-19T00:00:00",
        "last_modified_date": "2016-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06061",
        "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices",
        "authors": [
            "Heiga Zen",
            "Yannis Agiomyrgiannakis",
            "Niels Egberts",
            "Fergus Henderson",
            "Przemys\u0142aw Szczepaniak"
        ],
        "abstract": "Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed significant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an {\\epsilon}-contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also presented.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06083",
        "title": "Product Classification in E-Commerce using Distributional Semantics",
        "authors": [
            "Vivek Gupta",
            "Harish Karnick",
            "Ashendra Bansal",
            "Pradhuman Jhala"
        ],
        "abstract": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06137",
        "title": "LSTM-Based Predictions for Proactive Information Retrieval",
        "authors": [
            "Petri Luukkonen",
            "Markus Koskela",
            "Patrik Flor\u00e9en"
        ],
        "abstract": "We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task. In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user's past actions. We focus on the task of writing, in which the user is coalescing previously collected information into a text. Our proactive system automatically recommends the user relevant background information. The proposed system incorporates text input prediction using a long short-term memory (LSTM) network. We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06142",
        "title": "Comparing the hierarchy of keywords in on-line news portals",
        "authors": [
            "Gergely Tib\u00e9ly",
            "David Sousa-Rodrigues",
            "P\u00e9ter Pollner",
            "Gergely Palla"
        ],
        "abstract": "The tagging of on-line content with informative keywords is a widespread phenomenon from scientific article repositories through blogs to on-line news portals. In most of the cases, the tags on a given item are free words chosen by the authors independently. Therefore, relations among keywords in a collection of news items is unknown. However, in most cases the topics and concepts described by these keywords are forming a latent hierarchy, with the more general topics and categories at the top, and more specialised ones at the bottom. Here we apply a recent, cooccurrence-based tag hierarchy extraction method to sets of keywords obtained from four different on-line news portals. The resulting hierarchies show substantial differences not just in the topics rendered as important (being at the top of the hierarchy) or of less interest (categorised low in the hierarchy), but also in the underlying network structure. This reveals discrepancies between the plausible keyword association frameworks in the studied news portals.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06352",
        "title": "Visualizing textual models with in-text and word-as-pixel highlighting",
        "authors": [
            "Abram Handler",
            "Su Lin Blodgett",
            "Brendan O'Connor"
        ],
        "abstract": "We explore two techniques which use color to make sense of statistical text models. One method uses in-text annotations to illustrate a model's view of particular tokens in particular documents. Another uses a high-level, \"words-as-pixels\" graphic to display an entire corpus. Together, these methods offer both zoomed-in and zoomed-out perspectives into a model's understanding of text. We show how these interconnected methods help diagnose a classifier's poor performance on Twitter slang, and make sense of a topic model on historical political texts.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06368",
        "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings",
        "authors": [
            "Fereshte Khani",
            "Martin Rinard",
            "Percy Liang"
        ],
        "abstract": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.\n    ",
        "submission_date": "2016-06-20T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06424",
        "title": "A Novel Framework to Expedite Systematic Reviews by Automatically Building Information Extraction Training Corpora",
        "authors": [
            "Tanmay Basu",
            "Shraman Kumar",
            "Abhishek Kalyan",
            "Priyanka Jayaswal",
            "Pawan Goyal",
            "Stephen Pettifer",
            "Siddhartha R. Jonnalagadda"
        ],
        "abstract": "A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data.A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06622",
        "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions",
        "authors": [
            "Arijit Ray",
            "Gordon Christie",
            "Mohit Bansal",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2016-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06737",
        "title": "Criticality in Formal Languages and Statistical Physics",
        "authors": [
            "Henry W. Lin",
            "Max Tegmark"
        ],
        "abstract": "We show that the mutual information between two symbols, as a function of the number of symbols between the two, decays exponentially in any probabilistic regular grammar, but can decay like a power law for a context-free grammar. This result about formal languages is closely related to a well-known result in classical statistical mechanics that there are no phase transitions in dimensions fewer than two. It is also related to the emergence of power-law correlations in turbulence and cosmological inflation through recursive generative processes. We elucidate these physics connections and comment on potential applications of our results to machine learning tasks like training artificial recurrent neural networks. Along the way, we introduce a useful quantity which we dub the rational mutual information and discuss generalizations of our claims involving more complicated Bayesian networks.\n    ",
        "submission_date": "2016-06-21T00:00:00",
        "last_modified_date": "2017-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06871",
        "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic Modeling in Speech Recognition",
        "authors": [
            "Albert Zeyer",
            "Patrick Doetsch",
            "Paul Voigtlaender",
            "Ralf Schl\u00fcter",
            "Hermann Ney"
        ],
        "abstract": "We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniques such as dropout and $L_2$ regularization, and different gradient clipping variants.\n",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2017-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.06991",
        "title": "Toward Word Embedding for Personalized Information Retrieval",
        "authors": [
            "Nawal Ould-Amer",
            "Philippe Mulhem",
            "Mathias Gery"
        ],
        "abstract": "This paper presents preliminary works on using Word Embedding (word2vec) for query expansion in the context of Personalized Information Retrieval. Traditionally, word embeddings are learned on a general corpus, like Wikipedia. In this work we try to personalize the word embeddings learning, by achieving the learning on the user's profile. The word embeddings are then in the same context than the user interests. Our proposal is evaluated on the CLEF Social Book Search 2016 collection. The results obtained show that some efforts should be made in the way to apply Word Embedding in the context of Personalized Information Retrieval.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07006",
        "title": "Using Word Embeddings in Twitter Election Classification",
        "authors": [
            "Xiao Yang",
            "Craig Macdonald",
            "Iadh Ounis"
        ],
        "abstract": "Word embeddings and convolutional neural networks (CNN) have attracted extensive attention in various classification tasks for Twitter, e.g. sentiment classification. However, the effect of the configuration used to train and generate the word embeddings on the classification performance has not been studied in the existing literature. In this paper, using a Twitter election classification task that aims to detect election-related tweets, we investigate the impact of the background dataset used to train the embedding models, the context window size and the dimensionality of word embeddings on the classification performance. By comparing the classification results of two word embedding models, which are trained using different background corpora (e.g. Wikipedia articles and Twitter microposts), we show that the background data type should align with the Twitter classification dataset to achieve a better performance. Moreover, by evaluating the results of word embeddings models trained using various context window sizes and dimensionalities, we found that large context window and dimension sizes are preferable to improve the performance. Our experimental results also show that using word embeddings and CNN leads to statistically significant improvements over various baselines such as random, SVM with TF-IDF and SVM with word embeddings.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07043",
        "title": "Toward Interpretable Topic Discovery via Anchored Correlation Explanation",
        "authors": [
            "Kyle Reing",
            "David C. Kale",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "Many predictive tasks, such as diagnosing a patient based on their medical chart, are ultimately defined by the decisions of human experts. Unfortunately, encoding experts' knowledge is often time consuming and expensive. We propose a simple way to use fuzzy and informal knowledge from experts to guide discovery of interpretable latent topics in text. The underlying intuition of our approach is that latent factors should be informative about both correlations in the data and a set of relevance variables specified by an expert. Mathematically, this approach is a combination of the information bottleneck and Total Correlation Explanation (CorEx). We give a preliminary evaluation of Anchored CorEx, showing that it produces more coherent and interpretable topics on two distinct corpora.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07056",
        "title": "Emulating Human Conversations using Convolutional Neural Network-based IR",
        "authors": [
            "Abhay Prakash",
            "Chris Brockett",
            "Puneet Agrawal"
        ],
        "abstract": "Conversational agents (\"bots\") are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model (cDSSM) with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07103",
        "title": "Deep Feature Fusion Network for Answer Quality Prediction in Community Question Answering",
        "authors": [
            "Sai Praneeth Suggu",
            "Kushwanth N. Goutham",
            "Manoj K. Chinnakotla",
            "Manish Shrivastava"
        ],
        "abstract": "Community Question Answering (cQA) forums have become a popular medium for soliciting direct answers to specific questions of users from experts or other experienced users on a given topic. However, for a given question, users sometimes have to sift through a large number of low-quality or irrelevant answers to find out the answer which satisfies their information need. To alleviate this, the problem of Answer Quality Prediction (AQP) aims to predict the quality of an answer posted in response to a forum question. Current AQP systems either learn models using - a) various hand-crafted features (HCF) or b) use deep learning (DL) techniques which automatically learn the required feature representations.\n",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07137",
        "title": "Automated Extraction of Number of Subjects in Randomised Controlled Trials",
        "authors": [
            "Abeed Sarker"
        ],
        "abstract": "We present a simple approach for automatically extracting the number of subjects involved in randomised controlled trials (RCT). Our approach first applies a set of rule-based techniques to extract candidate study sizes from the abstracts of the articles. Supervised classification is then performed over the candidates with support vector machines, using a small set of lexical, structural, and contextual features. With only a small annotated training set of 201 RCTs, we obtained an accuracy of 88\\%. We believe that this system will aid complex medical text processing tasks such as summarisation and question answering.\n    ",
        "submission_date": "2016-06-22T00:00:00",
        "last_modified_date": "2016-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07211",
        "title": "Toward a Deep Neural Approach for Knowledge-Based IR",
        "authors": [
            "Gia-Hung Nguyen",
            "Lynda Tamine",
            "Laure Soulier",
            "Nathalie Bricon-Souf"
        ],
        "abstract": "This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases (KBs) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by recent works dealing with deep representation learn ing of texts. With this in mind, we argue that embedding KBs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations. In this paper, we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07287",
        "title": "Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions",
        "authors": [
            "Fabio Carrara",
            "Andrea Esuli",
            "Tiziano Fagni",
            "Fabrizio Falchi",
            "Alejandro Moreo Fern\u00e1ndez"
        ],
        "abstract": "In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higher-level semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07496",
        "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?",
        "authors": [
            "Roberto Camacho Barranco",
            "Laura M. Rodriguez",
            "Rebecca Urbina",
            "M. Shahriar Hossain"
        ],
        "abstract": "While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images.\n    ",
        "submission_date": "2016-06-23T00:00:00",
        "last_modified_date": "2016-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07565",
        "title": "Adaptability of Neural Networks on Varying Granularity IR Tasks",
        "authors": [
            "Daniel Cohen",
            "Qingyao Ai",
            "W. Bruce Croft"
        ],
        "abstract": "Recent work in Information Retrieval (IR) using Deep Learning models has yielded state of the art results on a variety of IR tasks. Deep neural networks (DNN) are capable of learning ideal representations of data during the training process, removing the need for independently extracting features. However, the structures of these DNNs are often tailored to perform on specific datasets. In addition, IR tasks deal with text at varying levels of granularity from single factoids to documents containing thousands of words. In this paper, we examine the role of the granularity on the performance of common state of the art DNN structures in IR.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07572",
        "title": "Enriching Linked Datasets with New Object Properties",
        "authors": [
            "Subhashree S",
            "P Sreenivasa Kumar"
        ],
        "abstract": "Although several RDF knowledge bases are available through the LOD initiative, the ontology schema of such linked datasets is not very rich. In particular, they lack object properties. The problem of finding new object properties (and their instances) between any two given classes has not been investigated in detail in the context of Linked Data. In this paper, we present DART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an unsupervised solution to enrich the LOD cloud with new object properties between two given classes. DART exploits contextual similarity to identify text patterns from the web corpus that can potentially represent relations between individuals. These text patterns are then clustered by means of paraphrase detection to capture the object properties between the two given LOD classes. DART also performs fully automated mapping of the discovered relations to the properties in the linked dataset. This serves many purposes such as identification of completely new relations, elimination of irrelevant relations, and generation of prospective property axioms. We have empirically evaluated our approach on several pairs of classes and found that the system can indeed be used for enriching the linked datasets with new object properties and their instances. We compared DART with newOntExt system which is an offshoot of the NELL (Never-Ending Language Learning) effort. Our experiments reveal that DART gives better results than newOntExt with respect to both the correctness, as well as the number of relations.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2017-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07711",
        "title": "A Game-Theoretic Approach to Word Sense Disambiguation",
        "authors": [
            "Rocco Tripodi",
            "Marcello Pelillo"
        ],
        "abstract": "This paper presents a new model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes. The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose. We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices. With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence. The model is based on two ideas: similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them. The paper provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example. The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems. The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07770",
        "title": "Captioning Images with Diverse Objects",
        "authors": [
            "Subhashini Venugopalan",
            "Lisa Anne Hendricks",
            "Marcus Rohrbach",
            "Raymond Mooney",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Recent captioning models are limited in their ability to scale and describe concepts unseen in paired image-text corpora. We propose the Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Our model takes advantage of external sources -- labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text. We propose minimizing a joint objective which can learn from these diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of image-caption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in MSCOCO image-caption training data, as well as many categories that are observed very rarely. Both automatic evaluations and human judgements show that our model considerably outperforms prior work in being able to describe many more categories of objects.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2017-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07839",
        "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles",
        "authors": [
            "Stefan Lee",
            "Senthil Purushwalkam",
            "Michael Cogswell",
            "Viresh Ranjan",
            "David Crandall",
            "Dhruv Batra"
        ],
        "abstract": "Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.\n    ",
        "submission_date": "2016-06-24T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.07955",
        "title": "X575: writing rengas with web services",
        "authors": [
            "Daniel Winterstein",
            "Joseph Corneli"
        ],
        "abstract": "Our software system simulates the classical collaborative Japanese poetry form, renga, made of linked haikus. We used NLP methods wrapped up as web services. Our experiments were only a partial success, since results fail to satisfy classical constraints. To gather ideas for future work, we examine related research in semiotics, linguistics, and computing.\n    ",
        "submission_date": "2016-06-25T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08207",
        "title": "Semantic homophily in online communication: evidence from Twitter",
        "authors": [
            "Sanja \u0160\u0107epanovi\u0107",
            "Igor Mishkovski",
            "Bruno Gon\u00e7alves",
            "Nguyen Trung Hieu",
            "Pan Hui"
        ],
        "abstract": "People are observed to assortatively connect on a set of traits. This phenomenon, termed assortative mixing or sometimes homophily, can be quantified through assortativity coefficient in social networks. Uncovering the exact causes of strong assortative mixing found in social networks has been a research challenge. Among the main suggested causes from sociology are the tendency of similar individuals to connect (often itself referred as homophily) and the social influence among already connected individuals. An important question to researchers and in practice can be tackled, as we present here: understanding the exact mechanisms of interplay between these tendencies and the underlying social network structure. Namely, in addition to the mentioned assortativity coefficient, there are several other static and temporal network properties and substructures that can be linked to the tendencies of homophily and social influence in the social network and we herein investigate those. Concretely, we tackle a computer-mediated \\textit{communication network} (based on Twitter mentions) and a particular type of assortative mixing that can be inferred from the semantic features of communication content that we term \\textit{semantic homophily}. Our work, to the best of our knowledge, is the first to offer an in-depth analysis on semantic homophily in a communication network and the interplay between them. We quantify diverse levels of semantic homophily, identify the semantic aspects that are the drivers of observed homophily, show insights in its temporal evolution and finally, we present its intricate interplay with the communication network on Twitter. By analyzing these mechanisms we increase understanding on what are the semantic aspects that shape and how they shape the human computer-mediated communication.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08359",
        "title": "Lifted Rule Injection for Relation Embeddings",
        "authors": [
            "Thomas Demeester",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel"
        ],
        "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.\n    ",
        "submission_date": "2016-06-27T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.08970",
        "title": "The rotating normal form of braids is regular",
        "authors": [
            "Jean Fromentin"
        ],
        "abstract": "Defined on Birman-Ko-Lee monoids, the rotating normal form has strong connections with the Dehornoy's braid ordering. It can be seen as a process for selecting between all the representative words of a Birman-Ko-Lee braid a particular one, called rotating word. In this paper we construct, for all n   2, a finite-state automaton which recognizes rotating words on n strands, proving that the rotating normal form is regular. As a consequence we obtain the regularity of a $\\sigma$-definite normal form defined on the whole braid group.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2024-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09222",
        "title": "Penambahan emosi menggunakan metode manipulasi prosodi untuk sistem text to speech bahasa Indonesia",
        "authors": [
            "Salita Ulitia Prini",
            "Ary Setijadi Prihatmanto"
        ],
        "abstract": "Adding an emotions using prosody manipulation method for Indonesian text to speech system. Text To Speech (TTS) is a system that can convert text in one language into speech, accordance with the reading of the text in the language used. The focus of this research is a natural sounding concept, the make \"humanize\" for the pronunciation of voice synthesis system Text To Speech. Humans have emotions / intonation that may affect the sound produced. The main requirement for the system used Text To Speech in this research is eSpeak, the database MBROLA using id1, Human Speech Corpus database from a website that summarizes the words with the highest frequency (Most Common Words) used in a country. And there are 3 types of emotional / intonation designed base. There is a happy, angry and sad emotion. Method for develop the emotional filter is manipulate the relevant features of prosody (especially pitch and duration value) using a predetermined rate factor that has been established by analyzing the differences between the standard output Text To Speech and voice recording with emotional prosody / a particular intonation. The test results for the perception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for angry emotion and 98.75 % for sad emotions. For perception test system carried by intelligibility and naturalness test. Intelligibility test for the accuracy of sound with the original sentence is 93.3%, and for clarity rate for each sentence is 62.8%. For naturalness, accuracy emotional election amounted to 75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions.\n",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1606.09274",
        "title": "Compression of Neural Machine Translation Models via Pruning",
        "authors": [
            "Abigail See",
            "Minh-Thang Luong",
            "Christopher D. Manning"
        ],
        "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.\n    ",
        "submission_date": "2016-06-29T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00167",
        "title": "SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets",
        "authors": [
            "Jo\u00e3o Oliveira",
            "Mike Pinto",
            "Pedro Saleiro",
            "Jorge Teixeira"
        ],
        "abstract": "Social Media users tend to mention entities when reacting to news events. The main purpose of this work is to create entity-centric aggregations of tweets on a daily basis. By applying topic modeling and sentiment analysis, we create data visualization insights about current events and people reactions to those events from an entity-centric perspective.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2018-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00424",
        "title": "Learning Relational Dependency Networks for Relation Extraction",
        "authors": [
            "Dileep Viswanathan",
            "Ameet Soni",
            "Jude Shavlik",
            "Sriraam Natarajan"
        ],
        "abstract": "We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.\n    ",
        "submission_date": "2016-07-01T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00570",
        "title": "Representation learning for very short texts using weighted word embedding aggregation",
        "authors": [
            "Cedric De Boom",
            "Steven Van Canneyt",
            "Thomas Demeester",
            "Bart Dhoedt"
        ],
        "abstract": "Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.\n    ",
        "submission_date": "2016-07-02T00:00:00",
        "last_modified_date": "2016-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.00992",
        "title": "Generic Statistical Relational Entity Resolution in Knowledge Graphs",
        "authors": [
            "Jay Pujara",
            "Lise Getoor"
        ],
        "abstract": "Entity resolution, the problem of identifying the underlying entity of references found in data, has been researched for many decades in many communities. A common theme in this research has been the importance of incorporating relational features into the resolution process. Relational entity resolution is particularly important in knowledge graphs (KGs), which have a regular structure capturing entities and their interrelationships. We identify three major problems in KG entity resolution: (1) intra-KG reference ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending KGs with new facts. We implement a framework that generalizes across these three settings and exploits this regular structure of KGs. Our framework has many advantages over custom solutions widely deployed in industry, including collective inference, scalability, and interpretability. We apply our framework to two real-world KG entity resolution problems, ambiguity in NELL and merging data from Freebase and MusicBrainz, demonstrating the importance of relational features.\n    ",
        "submission_date": "2016-07-04T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01490",
        "title": "Towards Self-explanatory Ontology Visualization with Contextual Verbalization",
        "authors": [
            "Ren\u0101rs Liepi\u0146\u0161",
            "Uldis Boj\u0101rs",
            "Normunds Gr\u016bz\u012btis",
            "K\u0101rlis \u010cer\u0101ns",
            "Edgars Celms"
        ],
        "abstract": "Ontologies are one of the core foundations of the Semantic Web. To participate in Semantic Web projects, domain experts need to be able to understand the ontologies involved. Visual notations can provide an overview of the ontology and help users to understand the connections among entities. However, the users first need to learn the visual notation before they can interpret it correctly. Controlled natural language representation would be readable right away and might be preferred in case of complex axioms, however, the structure of the ontology would remain less apparent. We propose to combine ontology visualizations with contextual ontology verbalizations of selected ontology (diagram) elements, displaying controlled natural language (CNL) explanations of OWL axioms corresponding to the selected visual notation elements. Thus, the domain experts will benefit from both the high-level overview provided by the graphical notation and the detailed textual explanations of particular elements in the diagram.\n    ",
        "submission_date": "2016-07-06T00:00:00",
        "last_modified_date": "2016-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01869",
        "title": "Scalable Semantic Matching of Queries to Ads in Sponsored Search Advertising",
        "authors": [
            "Mihajlo Grbovic",
            "Nemanja Djuric",
            "Vladan Radosavljevic",
            "Fabrizio Silvestri",
            "Ricardo Baeza-Yates",
            "Andrew Feng",
            "Erik Ordentlich",
            "Lee Yang",
            "Gavin Owens"
        ],
        "abstract": "Sponsored search represents a major source of revenue for web search engines. This popular advertising model brings a unique possibility for advertisers to target users' immediate intent communicated through a search query, usually by displaying their ads alongside organic search results for queries deemed relevant to their products or services. However, due to a large number of unique queries it is challenging for advertisers to identify all such relevant queries. For this reason search engines often provide a service of advanced matching, which automatically finds additional relevant queries for advertisers to bid on. We present a novel advanced matching approach based on the idea of semantic embeddings of queries and ads. The embeddings were learned using a large data set of user search sessions, consisting of search queries, clicked ads and search links, while utilizing contextual information such as dwell time and skipped ads. To address the large-scale nature of our problem, both in terms of data and vocabulary size, we propose a novel distributed algorithm for training of the embeddings. Finally, we present an approach for overcoming a cold-start problem associated with new ads and queries. We report results of editorial evaluation and online tests on actual search traffic. The results show that our approach significantly outperforms baselines in terms of relevance, coverage, and incremental revenue. Lastly, we open-source learned query embeddings to be used by researchers in computational advertising and related fields.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.01990",
        "title": "A Maturity Model for Public Administration as Open Translation Data Providers",
        "authors": [
            "N\u00faria Bel",
            "Mikel L. Forcada",
            "Asunci\u00f3n G\u00f3mez-P\u00e9rez"
        ],
        "abstract": "Any public administration that produces translation data can be a provider of useful reusable data to meet its own translation needs and the ones of other public organizations and private companies that work with texts of the same domain. These data can also be crucial to produce domain-tuned Machine Translation systems. The organization's management of the translation process, the characteristics of the archives of the generated resources and of the infrastructure available to support them determine the efficiency and the effectiveness with which the materials produced can be converted into reusable data. However, it is of utmost importance that the organizations themselves first become aware of the goods they are producing and, second, adapt their internal processes to become optimal providers. In this article, we propose a Maturity Model to help these organizations to achieve it by identifying the different stages of the management of translation data that determine the path to the aforementioned goal.\n    ",
        "submission_date": "2016-07-07T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02436",
        "title": "Document Clustering Games in Static and Dynamic Scenarios",
        "authors": [
            "Rocco Tripodi",
            "Marcello Pelillo"
        ],
        "abstract": "In this work we propose a game theoretic model for document clustering. Each document to be clustered is represented as a player and each cluster as a strategy. The players receive a reward interacting with other players that they try to maximize choosing their best strategies. The geometry of the data is modeled with a weighted graph that encodes the pairwise similarity among documents, so that similar players are constrained to choose similar strategies, updating their strategy preferences at each iteration of the games. We used different approaches to find the prototypical elements of the clusters and with this information we divided the players into two disjoint sets, one collecting players with a definite strategy and the other one collecting players that try to learn from others the correct strategy to play. The latter set of players can be considered as new data points that have to be clustered according to previous information. This representation is useful in scenarios in which the data are streamed continuously. The evaluation of the system was conducted on 13 document datasets using different settings. It shows that the proposed method performs well compared to different document clustering algorithms.\n    ",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02467",
        "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior Knowledge",
        "authors": [
            "Marc Dymetman",
            "Chunyang Xiao"
        ],
        "abstract": "We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.\n",
        "submission_date": "2016-07-08T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.02769",
        "title": "Annotation Methodologies for Vision and Language Dataset Creation",
        "authors": [
            "Gitit Kehat",
            "James Pustejovsky"
        ],
        "abstract": "Annotated datasets are commonly used in the training and evaluation of tasks involving natural language and vision (image description generation, action recognition and visual question answering). However, many of the existing datasets reflect problems that emerge in the process of data selection and annotation. Here we point out some of the difficulties and problems one confronts when creating and validating annotated vision and language datasets.\n    ",
        "submission_date": "2016-07-10T00:00:00",
        "last_modified_date": "2016-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03474",
        "title": "Recurrent Highway Networks",
        "authors": [
            "Julian Georg Zilly",
            "Rupesh Kumar Srivastava",
            "Jan Koutn\u00edk",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.\n    ",
        "submission_date": "2016-07-12T00:00:00",
        "last_modified_date": "2017-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03766",
        "title": "AudioPairBank: Towards A Large-Scale Tag-Pair-Based Audio Content Analysis",
        "authors": [
            "Sebastian Sager",
            "Benjamin Elizalde",
            "Damian Borth",
            "Christian Schulze",
            "Bhiksha Raj",
            "Ian Lane"
        ],
        "abstract": "Recently, sound recognition has been used to identify sounds, such as car and river. However, sounds have nuances that may be better described by adjective-noun pairs such as slow car, and verb-noun pairs such as flying insects, which are under explored. Therefore, in this work we investigate the relation between audio content and both adjective-noun pairs and verb-noun pairs. Due to the lack of datasets with these kinds of annotations, we collected and processed the AudioPairBank corpus consisting of a combined total of 1,123 pairs and over 33,000 audio files. One contribution is the previously unavailable documentation of the challenges and implications of collecting audio recordings with these type of labels. A second contribution is to show the degree of correlation between the audio content and the labels through sound recognition experiments, which yielded results of 70% accuracy, hence also providing a performance benchmark. The results and study in this paper encourage further exploration of the nuances in audio and are meant to complement similar research performed on images and text in multimedia analysis.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2018-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.03827",
        "title": "The KIT Motion-Language Dataset",
        "authors": [
            "Matthias Plappert",
            "Christian Mandery",
            "Tamim Asfour"
        ],
        "abstract": "Linking human motion and natural language is of great interest for the generation of semantic representations of human activities as well as for the generation of robot activities based on natural language input. However, while there have been years of research in this area, no standardized and openly available dataset exists to support the development and evaluation of such systems. We therefore propose the KIT Motion-Language Dataset, which is large, open, and extensible. We aggregate data from multiple motion capture databases and include them in our dataset using a unified representation that is independent of the capture system or marker set, making it easy to work with the data regardless of its origin. To obtain motion annotations in natural language, we apply a crowd-sourcing approach and a web-based tool that was specifically build for this purpose, the Motion Annotation Tool. We thoroughly document the annotation process itself and discuss gamification methods that we used to keep annotators motivated. We further propose a novel method, perplexity-based selection, which systematically selects motions for further annotation that are either under-represented in our dataset or that have erroneous annotations. We show that our method mitigates the two aforementioned problems and ensures a systematic annotation process. We provide an in-depth analysis of the structure and contents of our resulting dataset, which, as of October 10, 2016, contains 3911 motions with a total duration of 11.23 hours and 6278 annotations in natural language that contain 52,903 words. We believe this makes our dataset an excellent choice that enables more transparent and comparable research in this important area.\n    ",
        "submission_date": "2016-07-13T00:00:00",
        "last_modified_date": "2018-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04315",
        "title": "Neural Semantic Encoders",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Hong Yu"
        ],
        "abstract": "We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read}, compose and write operations. NSE can also access multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU.\n    ",
        "submission_date": "2016-07-14T00:00:00",
        "last_modified_date": "2017-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.04683",
        "title": "On the efficient representation and execution of deep acoustic models",
        "authors": [
            "Raziel Alvarez",
            "Rohit Prabhavalkar",
            "Anton Bakhtin"
        ],
        "abstract": "In this paper we present a simple and computationally efficient quantization scheme that enables us to reduce the resolution of the parameters of a neural network from 32-bit floating point values to 8-bit integer values. The proposed quantization scheme leads to significant memory savings and enables the use of optimized hardware instructions for integer arithmetic, thus significantly reducing the cost of inference. Finally, we propose a \"quantization aware\" training process that applies the proposed scheme during network training and find that it allows us to recover most of the loss in accuracy introduced by quantization. We validate the proposed techniques by applying them to a long short-term memory-based acoustic model on an open-ended large vocabulary speech recognition task.\n    ",
        "submission_date": "2016-07-15T00:00:00",
        "last_modified_date": "2016-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05108",
        "title": "Neural Machine Translation with Recurrent Attention Modeling",
        "authors": [
            "Zichao Yang",
            "Zhiting Hu",
            "Yuntian Deng",
            "Chris Dyer",
            "Alex Smola"
        ],
        "abstract": "Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05174",
        "title": "Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction",
        "authors": [
            "Roger K. Moore"
        ],
        "abstract": "Recent years have seen significant market penetration for voice-based personal assistants such as Apple's Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: \"is spoken language all-or-nothing\"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction.\n    ",
        "submission_date": "2016-07-18T00:00:00",
        "last_modified_date": "2016-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05422",
        "title": "A Novel Information Theoretic Framework for Finding Semantic Similarity in WordNet",
        "authors": [
            "Abhijit Adhikari",
            "Shivang Singh",
            "Deepjyoti Mondal",
            "Biswanath Dutta",
            "Animesh Dutta"
        ],
        "abstract": "Information content (IC) based measures for finding semantic similarity is gaining preferences day by day. Semantics of concepts can be highly characterized by information theory. The conventional way for calculating IC is based on the probability of appearance of concepts in corpora. Due to data sparseness and corpora dependency issues of those conventional approaches, a new corpora independent intrinsic IC calculation measure has evolved. In this paper, we mainly focus on such intrinsic IC model and several topological aspects of the underlying ontology. Accuracy of intrinsic IC calculation and semantic similarity measure rely on these aspects deeply. Based on these analysis we propose an information theoretic framework which comprises an intrinsic IC calculator and a semantic similarity model. Our approach is compared with state of the art semantic similarity measures based on corpora dependent IC calculation as well as intrinsic IC based methods using several benchmark data set. We also compare our model with the related Edge based, Feature based and Distributional approaches. Experimental results show that our intrinsic IC model gives high correlation value when applied to different semantic similarity models. Our proposed semantic similarity model also achieves significant results when embedded with some state of the art IC models including ours.\n    ",
        "submission_date": "2016-07-19T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.05968",
        "title": "Robust Natural Language Processing - Combining Reasoning, Cognitive Semantics and Construction Grammar for Spatial Language",
        "authors": [
            "Michael Spranger",
            "Jakob Suchan",
            "Mehul Bhatt"
        ],
        "abstract": "We present a system for generating and understanding of dynamic and static spatial relations in robotic interaction setups. Robots describe an environment of moving blocks using English phrases that include spatial relations such as \"across\" and \"in front of\". We evaluate the system in robot-robot interactions and show that the system can robustly deal with visual perception errors, language omissions and ungrammatical utterances.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06025",
        "title": "Constructing a Natural Language Inference Dataset using Generative Neural Networks",
        "authors": [
            "Janez Starc",
            "Dunja Mladeni\u0107"
        ],
        "abstract": "Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.\n    ",
        "submission_date": "2016-07-20T00:00:00",
        "last_modified_date": "2017-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06215",
        "title": "A Comprehensive Survey on Cross-modal Retrieval",
        "authors": [
            "Kaiye Wang",
            "Qiyue Yin",
            "Wei Wang",
            "Shu Wu",
            "Liang Wang"
        ],
        "abstract": "In recent years, cross-modal retrieval has drawn much attention due to the rapid growth of multimodal data. It takes one type of data as the query to retrieve relevant data of another type. For example, a user can use a text to retrieve relevant pictures or videos. Since the query and its retrieved results can be of different modalities, how to measure the content similarity between different modalities of data remains a challenge. Various methods have been proposed to deal with such a problem. In this paper, we first review a number of representative methods for cross-modal retrieval and classify them into two main groups: 1) real-valued representation learning, and 2) binary representation learning. Real-valued representation learning methods aim to learn real-valued common representations for different modalities of data. To speed up the cross-modal retrieval, a number of binary representation learning methods are proposed to map different modalities of data into a common Hamming space. Then, we introduce several multimodal datasets in the community, and show the experimental results on two commonly used multimodal datasets. The comparison reveals the characteristic of different kinds of cross-modal retrieval methods, which is expected to benefit both practical applications and future research. Finally, we discuss open problems and future research directions.\n    ",
        "submission_date": "2016-07-21T00:00:00",
        "last_modified_date": "2016-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.06875",
        "title": "Processing Natural Language About Ongoing Actions",
        "authors": [
            "Steve Doubleday",
            "Sean Trott",
            "Jerome Feldman"
        ],
        "abstract": "Actions may not proceed as planned; they may be interrupted, resumed or overridden. This is a challenge to handle in a natural language understanding system. We describe extensions to an existing implementation for the control of autonomous systems by natural language, to enable such systems to handle incoming language requests regarding actions. Language Communication with Autonomous Systems (LCAS) has been extended with support for X-nets, parameterized executable schemas representing actions. X-nets enable the system to control actions at a desired level of granularity, while providing a mechanism for language requests to be processed asynchronously. Standard semantics supported include requests to stop, continue, or override the existing action. The specific domain demonstrated is the control of motion of a simulated robot, but the approach is general, and could be applied to other domains.\n    ",
        "submission_date": "2016-07-23T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.07602",
        "title": "OntoCat: Automatically categorizing knowledge in API Documentation",
        "authors": [
            "Niraj Kumar",
            "Premkumar Devanbu"
        ],
        "abstract": "Most application development happens in the context of complex APIs; reference documentation for APIs has grown tremendously in variety, complexity, and volume, and can be difficult to navigate. There is a growing need to develop well-organized ways to access the knowledge latent in the documentation; several research efforts deal with the organization (ontology) of API-related knowledge. Extensive knowledge-engineering work, supported by a rigorous qualitative analysis, by Maalej & Robillard [3] has identified a useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain independent technique to extract the knowledge types from the given API reference documentation. Our system, OntoCat, introduces total nine different features and their semantic and statistical combinations to classify the different knowledge types. We tested OntoCat on python API reference documentation. Our experimental results show the effectiveness of the system and opens the scope of probably related research areas (i.e., user behavior, documentation quality, etc.).\n    ",
        "submission_date": "2016-07-26T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08074",
        "title": "Mining Arguments from Cancer Documents Using Natural Language Processing and Ontologies",
        "authors": [
            "Adrian Groza",
            "Oana Popa"
        ],
        "abstract": "In the medical domain, the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments. As medical expertise occurs at different levels, part of the human agents have difficulties to face the huge amount of studies, but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic. To better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers. Our work here aims to fill the above technological gap.\n",
        "submission_date": "2016-07-27T00:00:00",
        "last_modified_date": "2016-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08720",
        "title": "TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for Automatic Measurement in MOOCs",
        "authors": [
            "Jiazhen He",
            "Benjamin I. P. Rubinstein",
            "James Bailey",
            "Rui Zhang",
            "Sandra Milligan"
        ],
        "abstract": "This paper explores the suitability of using automatically discovered topics from MOOC discussion forums for modelling students' academic abilities. The Rasch model from psychometrics is a popular generative probabilistic model that relates latent student skill, latent item difficulty, and observed student-item responses within a principled, unified framework. According to scholarly educational theory, discovered topics can be regarded as appropriate measurement items if (1) students' participation across the discovered topics is well fit by the Rasch model, and if (2) the topics are interpretable to subject-matter experts as being educationally meaningful. Such Rasch-scaled topics, with associated difficulty levels, could be of potential benefit to curriculum refinement, student assessment and personalised feedback. The technical challenge that remains, is to discover meaningful topics that simultaneously achieve good statistical fit with the Rasch model. To address this challenge, we combine the Rasch model with non-negative matrix factorisation based topic modelling, jointly fitting both models. We demonstrate the suitability of our approach with quantitative experiments on data from three Coursera MOOCs, and with qualitative survey results on topic interpretability on a Discrete Optimisation MOOC.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2017-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08822",
        "title": "SPICE: Semantic Propositional Image Caption Evaluation",
        "authors": [
            "Peter Anderson",
            "Basura Fernando",
            "Mark Johnson",
            "Stephen Gould"
        ],
        "abstract": "There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as `which caption-generator best understands colors?' and `can caption-generators count?'\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08883",
        "title": "Labeling of Query Words using Conditional Random Field",
        "authors": [
            "Satanu Ghosh",
            "Souvick Ghosh",
            "Dipankar Das"
        ],
        "abstract": "This paper describes our approach on Query Word Labeling as an attempt in the shared task on Mixed Script Information Retrieval at Forum for Information Retrieval Evaluation (FIRE) 2015. The query is written in Roman script and the words were in English or transliterated from Indian regional languages. A total of eight Indian languages were present in addition to English. We also identified the Named Entities and special symbols as part of our task. A CRF based machine learning framework was used for labeling the individual words with their corresponding language labels. We used a dictionary based approach for language identification. We also took into account the context of the word while identifying the language. Our system demonstrated an overall accuracy of 75.5% for token level language identification. The strict F-measure scores for the identification of token level language labels for Bengali, English and Hindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure of our system was 0.7498.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1607.08898",
        "title": "Personalized Emphasis Framing for Persuasive Message Generation",
        "authors": [
            "Tao Ding",
            "Shimei Pan"
        ],
        "abstract": "In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation.\n    ",
        "submission_date": "2016-07-29T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00104",
        "title": "World Knowledge as Indirect Supervision for Document Clustering",
        "authors": [
            "Chenguang Wang",
            "Yangqiu Song",
            "Dan Roth",
            "Ming Zhang",
            "Jiawei Han"
        ],
        "abstract": "One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this paper, we provide an example of using world knowledge for domain dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features.\n    ",
        "submission_date": "2016-07-30T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00255",
        "title": "Continuation semantics for multi-quantifier sentences: operation-based approaches",
        "authors": [
            "Justyna Grudzinska",
            "Marek Zawadowski"
        ],
        "abstract": "Classical scope-assignment strategies for multi-quantifier sentences involve quantifier phrase (QP)-movement. More recent continuation-based approaches provide a compelling alternative, for they interpret QP's in situ - without resorting to Logical Forms or any structures beyond the overt syntax. The continuation-based strategies can be divided into two groups: those that locate the source of scope-ambiguity in the rules of semantic composition and those that attribute it to the lexical entries for the quantifier words. In this paper, we focus on the former operation-based approaches and the nature of the semantic operations involved. More specifically, we discuss three such possible operation-based strategies for multi-quantifier sentences, together with their relative merits and costs.\n    ",
        "submission_date": "2016-07-31T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00272",
        "title": "Modeling Context in Referring Expressions",
        "authors": [
            "Licheng Yu",
            "Patrick Poirson",
            "Shan Yang",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.\n    ",
        "submission_date": "2016-07-31T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00895",
        "title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent Neural Networks",
        "authors": [
            "Patrick Doetsch",
            "Albert Zeyer",
            "Paul Voigtlaender",
            "Ilya Kulikov",
            "Ralf Schl\u00fcter",
            "Hermann Ney"
        ],
        "abstract": "In this work we release our extensible and easily configurable neural network training software. It provides a rich set of functional layers with a particular focus on efficient training of recurrent neural network topologies on multiple GPUs. The source of the software package is public and freely available for academic research purposes and can be used as a framework or as a standalone tool which supports a flexible configuration. The software allows to train state-of-the-art deep bidirectional long short-term memory (LSTM) models on both one dimensional data like speech or two dimensional data like handwritten text and was used to develop successful submission systems in several evaluation campaigns.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.00938",
        "title": "Evolutionary forces in language change",
        "authors": [
            "Christopher A. Ahern",
            "Mitchell G. Newberry",
            "Robin Clark",
            "Joshua B. Plotkin"
        ],
        "abstract": "Languages and genes are both transmitted from generation to generation, with opportunity for differential reproduction and survivorship of forms. Here we apply a rigorous inference framework, drawn from population genetics, to distinguish between two broad mechanisms of language change: drift and selection. Drift is change that results from stochasticity in transmission and it may occur in the absence of any intrinsic difference between linguistic forms; whereas selection is truly an evolutionary force arising from intrinsic differences -- for example, when one form is preferred by members of the population. Using large corpora of parsed texts spanning the 12th century to the 21st century, we analyze three examples of grammatical changes in English: the regularization of past-tense verbs, the rise of the periphrastic `do', and syntactic variation in verbal negation. We show that we can reject stochastic drift in favor of a selective force driving some of these language changes, but not others. The strength of drift depends on a word's frequency, and so drift provides an alternative explanation for why some words are more prone to change than others. Our results suggest an important role for stochasticity in language change, and they provide a null model against which selective theories of language evolution must be compared.\n    ",
        "submission_date": "2016-08-02T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01247",
        "title": "Query Clustering using Segment Specific Context Embeddings",
        "authors": [
            "S.K Kolluru",
            "Prasenjit Mukherjee"
        ],
        "abstract": "This paper presents a novel query clustering approach to capture the broad interest areas of users querying search engines. We make use of recent advances in NLP - word2vec and extend it to get query2vec, vector representations of queries, based on query contexts, obtained from the top search results for the query and use a highly scalable Divide & Merge clustering algorithm on top of the query vectors, to get the clusters. We have tried this approach on a variety of segments, including Retail, Travel, Health, Phones and found the clusters to be effective in discovering user's interest areas which have high monetization potential.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01281",
        "title": "Learning Online Alignments with Continuous Rewards Policy Gradient",
        "authors": [
            "Yuping Luo",
            "Chung-Cheng Chiu",
            "Navdeep Jaitly",
            "Ilya Sutskever"
        ],
        "abstract": "Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets.\n    ",
        "submission_date": "2016-08-03T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01402",
        "title": "Interacting Conceptual Spaces",
        "authors": [
            "Josef Bolt",
            "Bob Coecke",
            "Fabrizio Genovese",
            "Martha Lewis",
            "Daniel Marsden",
            "Robin Piedeleu"
        ],
        "abstract": "We propose applying the categorical compositional scheme of [6] to conceptual space models of cognition. In order to do this we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We show how conceptual spaces for composite types such as adjectives and verbs can be constructed. We illustrate this new model on detailed examples.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.01884",
        "title": "Winograd Schemas and Machine Translation",
        "authors": [
            "Ernest Davis"
        ],
        "abstract": "A Winograd schema is a pair of sentences that differ in a single word and that contain an ambiguous pronoun whose referent is different in the two sentences and requires the use of commonsense knowledge or world knowledge to disambiguate. This paper discusses how Winograd schemas and other sentence pairs could be used as challenges for machine translation using distinctions between pronouns, such as gender, that appear in the target language but not in the source.\n    ",
        "submission_date": "2016-08-05T00:00:00",
        "last_modified_date": "2016-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02071",
        "title": "Transferring Knowledge from Text to Predict Disease Onset",
        "authors": [
            "Yun Liu",
            "Kun-Ta Chuang",
            "Fu-Wen Liang",
            "Huey-Jen Su",
            "Collin M. Stultz",
            "John V. Guttag"
        ],
        "abstract": "In many domains such as medicine, training data is in short supply. In such cases, external knowledge is often helpful in building predictive models. We propose a novel method to incorporate publicly available domain expertise to build accurate models. Specifically, we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature's text description to the prediction problem. We use these relevance estimates to rescale the features, causing more important features to experience weaker regularization.\n",
        "submission_date": "2016-08-06T00:00:00",
        "last_modified_date": "2016-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02195",
        "title": "Automating Political Bias Prediction",
        "authors": [
            "Felix Biessmann"
        ],
        "abstract": "Every day media generate large amounts of text. An unbiased view on media reports requires an understanding of the political bias of media content. Assistive technology for estimating the political bias of texts can be helpful in this context. This study proposes a simple statistical learning approach to predict political bias from text. Standard text features extracted from speeches and manifestos of political parties are used to predict political bias in terms of political party affiliation and in terms of political views. Results indicate that political bias can be predicted with above chance accuracy. Mistakes of the model can be interpreted with respect to changes of policies of political actors. Two approaches are presented to make the results more interpretable: a) discriminative text features are related to the political orientation of a party and b) sentiment features of texts are correlated with a measure of political power. Political power appears to be strongly correlated with positive sentiment of a text. To highlight some potential use cases a web application shows how the model can be used for texts for which the political bias is not clear such as news articles.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2016-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02254",
        "title": "Reconciling Lambek's restriction, cut-elimination, and substitution in the presence of exponential modalities",
        "authors": [
            "Max Kanovich",
            "Stepan Kuznetsov",
            "Andre Scedrov"
        ],
        "abstract": "The Lambek calculus can be considered as a version of non-commutative intuitionistic linear logic. One of the interesting features of the Lambek calculus is the so-called \"Lambek's restriction,\" that is, the antecedent of any provable sequent should be non-empty. In this paper we discuss ways of extending the Lambek calculus with the linear logic exponential modality while keeping Lambek's restriction. Interestingly enough, we show that for any system equipped with a reasonable exponential modality the following holds: if the system enjoys cut elimination and substitution to the full extent, then the system necessarily violates Lambek's restriction. Nevertheless, we show that two of the three conditions can be implemented. Namely, we design a system with Lambek's restriction and cut elimination and another system with Lambek's restriction and substitution. For both calculi we prove that they are undecidable, even if we take only one of the two divisions provided by the Lambek calculus. The system with cut elimination and substitution and without Lambek's restriction is folklore and known to be undecidable.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2019-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02272",
        "title": "Incorporation of Speech Duration Information in Score Fusion of Speaker Recognition Systems",
        "authors": [
            "Ali Khodabakhsh",
            "Seyyed Saeed Sarfjoo",
            "Umut Uludag",
            "Osman Soyyigit",
            "Cenk Demiroglu"
        ],
        "abstract": "In recent years identity-vector (i-vector) based speaker verification (SV) systems have become very successful. Nevertheless, environmental noise and speech duration variability still have a significant effect on degrading the performance of these systems. In many real-life applications, duration of recordings are very short; as a result, extracted i-vectors cannot reliably represent the attributes of the speaker. Here, we investigate the effect of speech duration on the performance of three state-of-the-art speaker recognition systems. In addition, using a variety of available score fusion methods, we investigate the effect of score fusion for those speaker verification techniques to benefit from the performance difference of different methods under different enrollment and test speech duration conditions. This technique performed significantly better than the baseline score fusion methods.\n    ",
        "submission_date": "2016-08-07T00:00:00",
        "last_modified_date": "2016-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02289",
        "title": "Detecting Sarcasm in Multimodal Social Platforms",
        "authors": [
            "Rossano Schifanella",
            "Paloma de Juan",
            "Joel Tetreault",
            "Liangliang Cao"
        ],
        "abstract": "Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the implied sentiment. The detection of sarcasm in social media platforms has been applied in the past mainly to textual utterances where lexical indicators (such as interjections and intensifiers), linguistic markers, and contextual information (such as user profiles, or past conversations) were used to detect the sarcastic tone. However, modern social media platforms allow to create multimodal messages where audiovisual content is integrated with the text, making the analysis of a mode in isolation partial. In our work, we first study the relationship between the textual and visual aspects in multimodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to quantify the extent to which images are perceived as necessary by human annotators. Moreover, we propose two different computational frameworks to detect sarcasm that integrate the textual and visual modalities. The first approach exploits visual semantics trained on an external dataset, and concatenates the semantics features with state-of-the-art textual features. The second method adapts a visual neural network initialized with parameters trained on ImageNet to multimodal sarcastic posts. Results show the positive effect of combining modalities for the detection of sarcasm across platforms and methods.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02519",
        "title": "Topic Modelling and Event Identification from Twitter Textual Data",
        "authors": [
            "Marina Sokolova",
            "Kanyi Huang",
            "Stan Matwin",
            "Joshua Ramisch",
            "Vera Sazonova",
            "Renee Black",
            "Chris Orwa",
            "Sidney Ochieng",
            "Nanjira Sambuli"
        ],
        "abstract": "The tremendous growth of social media content on the Internet has inspired the development of the text analytics to understand and solve real-life problems. Leveraging statistical topic modelling helps researchers and practitioners in better comprehension of textual content as well as provides useful information for further analysis. Statistical topic modelling becomes especially important when we work with large volumes of dynamic text, e.g., Facebook or Twitter datasets. In this study, we summarize the message content of four data sets of Twitter messages relating to challenging social events in Kenya. We use Latent Dirichlet Allocation (LDA) topic modelling to analyze the content. Our study uses two evaluation measures, Normalized Mutual Information (NMI) and topic coherence analysis, to select the best LDA models. The obtained LDA results show that the tool can be effectively used to extract discussion topics and summarize them for further manual analysis\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02717",
        "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task",
        "authors": [
            "Ashkan Mokarian",
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02893",
        "title": "Syntactically Informed Text Compression with Recurrent Neural Networks",
        "authors": [
            "David Cox"
        ],
        "abstract": "We present a self-contained system for constructing natural language models for use in text compression. Our system improves upon previous neural network based models by utilizing recent advances in syntactic parsing -- Google's SyntaxNet -- to augment character-level recurrent neural networks. RNNs have proven exceptional in modeling sequence data such as text, as their architecture allows for modeling of long-term contextual information.\n    ",
        "submission_date": "2016-08-08T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.02904",
        "title": "TweeTime: A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter",
        "authors": [
            "Jeniya Tabassum",
            "Alan Ritter",
            "Wei Xu"
        ],
        "abstract": "We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems.\n    ",
        "submission_date": "2016-08-09T00:00:00",
        "last_modified_date": "2020-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03192",
        "title": "Growing Graphs with Hyperedge Replacement Graph Grammars",
        "authors": [
            "Salvador Agui\u00f1aga",
            "Rodrigo Palacios",
            "David Chiang",
            "Tim Weninger"
        ],
        "abstract": "Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. In this paper we show that a graph's clique tree can be used to extract a hyperedge replacement grammar. If we store an ordering from the extraction process, the extracted graph grammar is guaranteed to generate an isomorphic copy of the original graph. Or, a stochastic application of the graph grammar rules can be used to quickly create random graphs. In experiments on large real world networks, we show that random graphs, generated from extracted graph grammars, exhibit a wide range of properties that are very similar to the original graphs. In addition to graph properties like degree or eigenvector centrality, what a graph \"looks like\" ultimately depends on small details in local graph substructures that are difficult to define at a global level. We show that our generative graph model is able to preserve these local substructures when generating new graphs and performs well on new and difficult tests of model robustness.\n    ",
        "submission_date": "2016-08-10T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.03785",
        "title": "Compositional Distributional Cognition",
        "authors": [
            "Yaared Al-Mehairi",
            "Bob Coecke",
            "Martha Lewis"
        ],
        "abstract": "We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of [32] within the categorical compositional semantics (CatCo) of [13], forming a model of categorical compositional cognition (CatCog). This resolves intrinsic problems with ICS such as the fact that representations inhabit an unbounded space and that sentences with differing tree structures cannot be directly compared. We do so in a way that makes the most of the grammatical structure available, in contrast to strategies like circular convolution. Using the CatCo model also allows us to make use of tools developed for CatCo such as the representation of ambiguity and logical reasoning via density matrices, structural meanings for words such as relative pronouns, and addressing over- and under-extension, all of which are present in cognitive processes. Moreover the CatCog framework is sufficiently flexible to allow for entirely different representations of meaning, such as conceptual spaces. Interestingly, since the CatCo model was largely inspired by categorical quantum mechanics, so is CatCog.\n    ",
        "submission_date": "2016-08-12T00:00:00",
        "last_modified_date": "2016-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04020",
        "title": "Undecidability of the Lambek calculus with subexponential and bracket modalities",
        "authors": [
            "Max Kanovich",
            "Stepan Kuznetsov",
            "Andre Scedrov"
        ],
        "abstract": "The Lambek calculus is a well-known logical formalism for modelling natural language syntax. The original calculus covered a substantial number of intricate natural language phenomena, but only those restricted to the context-free setting. In order to address more subtle linguistic issues, the Lambek calculus has been extended in various ways. In particular, Morrill and Valentin (2015) introduce an extension with so-called exponential and bracket modalities. Their extension is based on a non-standard contraction rule for the exponential that interacts with the bracket structure in an intricate way. The standard contraction rule is not admissible in this calculus. In this paper we prove undecidability of the derivability problem in their calculus. We also investigate restricted decidable fragments considered by Morrill and Valentin and we show that these fragments belong to the NP class.\n    ",
        "submission_date": "2016-08-13T00:00:00",
        "last_modified_date": "2017-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04808",
        "title": "Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions",
        "authors": [
            "Hao Fang",
            "Hao Cheng",
            "Mari Ostendorf"
        ],
        "abstract": "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.\n    ",
        "submission_date": "2016-08-16T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.04868",
        "title": "Towards Music Captioning: Generating Music Playlist Descriptions",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Brian McFee",
            "Kyunghyun Cho",
            "Mark Sandler"
        ],
        "abstract": "Descriptions are often provided along with recommendations to help users' discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track.\n    ",
        "submission_date": "2016-08-17T00:00:00",
        "last_modified_date": "2017-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06549",
        "title": "Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing",
        "authors": [
            "Jun-Wei Lin",
            "Farn Wang"
        ],
        "abstract": "To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06651",
        "title": "Unsupervised, Efficient and Semantic Expertise Retrieval",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Marcel Worring"
        ],
        "abstract": "We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2017-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.06656",
        "title": "Lexical Query Modeling in Session Search",
        "authors": [
            "Christophe Van Gysel",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "abstract": "Lexical query modeling has been the leading paradigm for session search. In this paper, we analyze TREC session query logs and compare the performance of different lexical matching approaches for session search. Naive methods based on term frequency weighing perform on par with specialized session models. In addition, we investigate the viability of lexical query models in the setting of session search. We give important insights into the potential and limitations of lexical query modeling for session search and propose future directions for the field of session search.\n    ",
        "submission_date": "2016-08-23T00:00:00",
        "last_modified_date": "2016-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07094",
        "title": "A Novel Term_Class Relevance Measure for Text Categorization",
        "authors": [
            "D S Guru",
            "Mahamad Suhil"
        ],
        "abstract": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07187",
        "title": "Semantics derived automatically from language corpora contain human-like biases",
        "authors": [
            "Aylin Caliskan",
            "Joanna J. Bryson",
            "Arvind Narayanan"
        ],
        "abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the {\\em status quo} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07253",
        "title": "Learning Latent Vector Spaces for Product Search",
        "authors": [
            "Christophe Van Gysel",
            "Maarten de Rijke",
            "Evangelos Kanoulas"
        ],
        "abstract": "We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.\n    ",
        "submission_date": "2016-08-25T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.07639",
        "title": "Learning to generalize to new compositions in image understanding",
        "authors": [
            "Yuval Atzmon",
            "Jonathan Berant",
            "Vahid Kezami",
            "Amir Globerson",
            "Gal Chechik"
        ],
        "abstract": "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.\n    ",
        "submission_date": "2016-08-27T00:00:00",
        "last_modified_date": "2016-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08176",
        "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)",
        "authors": [
            "Amritanshu Agrawal",
            "Wei Fu",
            "Tim Menzies"
        ],
        "abstract": "Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeler is Latent Dirichlet allocation. When run on different datasets, LDA suffers from \"order effects\" i.e. different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results;specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands ofSoftware Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using Gibbs sampling). Results were scored via topic stability and text mining classification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its \"off-the-shelf\" settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08188",
        "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
        "authors": [
            "Danna Gurari",
            "Kristen Grauman"
        ],
        "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.\n    ",
        "submission_date": "2016-08-29T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08716",
        "title": "Measuring Machine Intelligence Through Visual Question Answering",
        "authors": [
            "C. Lawrence Zitnick",
            "Aishwarya Agrawal",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08953",
        "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election",
        "authors": [
            "Mehrnoosh Sameki",
            "Mattia Gentil",
            "Kate K. Mays",
            "Lei Guo",
            "Margrit Betke"
        ],
        "abstract": "Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2017-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1608.08974",
        "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models",
        "authors": [
            "Yash Goyal",
            "Akrit Mohapatra",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.\n    ",
        "submission_date": "2016-08-31T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00464",
        "title": "The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain",
        "authors": [
            "Trey Grainger",
            "Khalifeh AlJadda",
            "Mohammed Korayem",
            "Andries Smith"
        ],
        "abstract": "This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes - along with their combined edges - can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system - the Semantic Knowledge Graph - which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00514",
        "title": "On Horizontal and Vertical Separation in Hierarchical Text Classification",
        "authors": [
            "Mostafa Dehghani",
            "Hosein Azarbonyad",
            "Jaap Kamps",
            "Maarten Marx"
        ],
        "abstract": "Hierarchy is a common and effective way of organizing data and representing their relationships at different levels of abstraction. However, hierarchical data dependencies cause difficulties in the estimation of \"separable\" models that can distinguish between the entities in the hierarchy. Extracting separable models of hierarchical entities requires us to take their relative position into account and to consider the different types of dependencies in the hierarchy. In this paper, we present an investigation of the effect of separability in text-based entity classification and argue that in hierarchical classification, a separation property should be established between entities not only in the same layer, but also in different layers. Our main findings are the followings. First, we analyse the importance of separability on the data representation in the task of classification and based on that, we introduce a \"Strong Separation Principle\" for optimizing expected effectiveness of classifiers decision based on separation property. Second, we present Hierarchical Significant Words Language Models (HSWLM) which capture all, and only, the essential features of hierarchical entities according to their relative position in the hierarchy resulting in horizontally and vertically separable models. Third, we validate our claims on real-world data and demonstrate that how HSWLM improves the accuracy of classification and how it provides transferable models over time. Although discussions in this paper focus on the classification problem, the models are applicable to any information access tasks on data that has, or can be mapped to, a hierarchical structure.\n    ",
        "submission_date": "2016-09-02T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.00799",
        "title": "Lexical-Morphological Modeling for Legal Text Analysis",
        "authors": [
            "Danilo S. Carvalho",
            "Minh-Tien Nguyen",
            "Tran Xuan Chien",
            "Minh Le Nguyen"
        ],
        "abstract": "In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.\n    ",
        "submission_date": "2016-09-03T00:00:00",
        "last_modified_date": "2016-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01586",
        "title": "A Bootstrap Machine Learning Approach to Identify Rare Disease Patients from Electronic Health Records",
        "authors": [
            "Ravi Garg",
            "Shu Dong",
            "Sanjiv Shah",
            "Siddhartha R Jonnalagadda"
        ],
        "abstract": "Rare diseases are very difficult to identify among large number of other possible diagnoses. Better availability of patient data and improvement in machine learning algorithms empower us to tackle this problem computationally. In this paper, we target one such rare disease - cardiac amyloidosis. We aim to automate the process of identifying potential cardiac amyloidosis patients with the help of machine learning algorithms and also learn most predictive factors. With the help of experienced cardiologists, we prepared a gold standard with 73 positive (cardiac amyloidosis) and 197 negative instances. We achieved high average cross-validation F1 score of 0.98 using an ensemble machine learning classifier. Some of the predictive variables were: Age and Diagnosis of cardiac arrest, chest pain, congestive heart failure, hypertension, prim open angle glaucoma, and shoulder arthritis. Further studies are needed to validate the accuracy of the system across an entire health system and its generalizability for other diseases.\n    ",
        "submission_date": "2016-09-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.01926",
        "title": "A modular architecture for transparent computation in Recurrent Neural Networks",
        "authors": [
            "Giovanni Sirio Carmantini",
            "Peter beim Graben",
            "Mathieu Desroches",
            "Serafim Rodrigues"
        ],
        "abstract": "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Goedelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02043",
        "title": "Feasibility of Post-Editing Speech Transcriptions with a Mismatched Crowd",
        "authors": [
            "Purushotam Radadia",
            "Shirish Karande"
        ],
        "abstract": "Manual correction of speech transcription can involve a selection from plausible transcriptions. Recent work has shown the feasibility of employing a mismatched crowd for speech transcription. However, it is yet to be established whether a mismatched worker has sufficiently fine-granular speech perception to choose among the phonetically proximate options that are likely to be generated from the trellis of an ASRU. Hence, we consider five languages, Arabic, German, Hindi, Russian and Spanish. For each we generate synthetic, phonetically proximate, options which emulate post-editing scenarios of varying difficulty. We consistently observe non-trivial crowd ability to choose among fine-granular options.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02082",
        "title": "An improved uncertainty decoding scheme with weighted samples for DNN-HMM hybrid systems",
        "authors": [
            "Christian Huemmer",
            "Ram\u00f3n Fern\u00e1ndez Astudillo",
            "Walter Kellermann"
        ],
        "abstract": "In this paper, we advance a recently-proposed uncertainty decoding scheme for DNN-HMM (deep neural network - hidden Markov model) hybrid systems. This numerical sampling concept averages DNN outputs produced by a finite set of feature samples (drawn from a probabilistic distortion model) to approximate the posterior likelihoods of the context-dependent HMM states. As main innovation, we propose a weighted DNN-output averaging based on a minimum classification error criterion and apply it to a probabilistic distortion model for spatial diffuseness features. The experimental evaluation is performed on the 8-channel REVERB Challenge task using a DNN-HMM hybrid system with multichannel front-end signal enhancement. We show that the recognition accuracy of the DNN-HMM hybrid system improves by incorporating uncertainty decoding based on random sampling and that the proposed weighted DNN-output averaging further reduces the word error rate scores.\n    ",
        "submission_date": "2016-08-04T00:00:00",
        "last_modified_date": "2016-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.02116",
        "title": "Ask the GRU: Multi-Task Learning for Deep Text Recommendations",
        "authors": [
            "Trapit Bansal",
            "David Belanger",
            "Andrew McCallum"
        ],
        "abstract": "In a variety of application domains the content to be recommended to users is associated with text. This includes research papers, movies with associated plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can be extended naturally to leverage text by employing an explicit mapping from text to factors. This enables recommendations for new, unseen content, and may generalize better, since the factors for all items are produced by a compactly-parametrized model. Previous work has used topic models or averages of word embeddings for this mapping. In this paper we present a method leveraging deep recurrent neural networks to encode the text sequence into a latent vector, specifically gated recurrent units (GRUs) trained end-to-end on the collaborative filtering task. For the task of scientific paper recommendation, this yields models with significantly higher accuracy. In cold-start scenarios, we beat the previous state-of-the-art, all of which ignore word order. Performance is further improved by multi-task learning, where the text encoder network is trained for a combination of content recommendation and item metadata prediction. This regularizes the collaborative filtering model, ameliorating the problem of sparsity of the observed rating matrix.\n    ",
        "submission_date": "2016-09-07T00:00:00",
        "last_modified_date": "2016-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03193",
        "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System",
        "authors": [
            "Ronan Collobert",
            "Christian Puhrsch",
            "Gabriel Synnaeve"
        ],
        "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03207",
        "title": "Multiplex lexical networks reveal patterns in early word acquisition in children",
        "authors": [
            "Massimo Stella",
            "Nicole M. Beckage",
            "Markus Brede"
        ],
        "abstract": "Network models of language have provided a way of linking cognitive processes to the structure and connectivity of language. However, one shortcoming of current approaches is focusing on only one type of linguistic relationship at a time, missing the complex multi-relational nature of language. In this work, we overcome this limitation by modelling the mental lexicon of English-speaking toddlers as a multiplex lexical network, i.e. a multi-layered network where N=529 words/nodes are connected according to four types of relationships: (i) free associations, (ii) feature sharing, (iii) co-occurrence, and (iv) phonological similarity. We provide analysis of the topology of the resulting multiplex and then proceed to evaluate single layers as well as the full multiplex structure on their ability to predict empirically observed age of acquisition data of English speaking toddlers. We find that the emerging multiplex network topology is an important proxy of the cognitive processes of acquisition, capable of capturing emergent lexicon structure. In fact, we show that the multiplex topology is fundamentally more powerful than individual layers in predicting the ordering with which words are acquired. Furthermore, multiplex analysis allows for a quantification of distinct phases of lexical acquisition in early learners: while initially all the multiplex layers contribute to word learning, after about month 23 free associations take the lead in driving word acquisition.\n    ",
        "submission_date": "2016-09-11T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03286",
        "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks",
        "authors": [
            "Yun-Nung Chen",
            "Dilek Hakkani-Tur",
            "Gokhan Tur",
            "Asli Celikyilmaz",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.\n    ",
        "submission_date": "2016-09-12T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.03777",
        "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks",
        "authors": [
            "Kyuyeon Hwang",
            "Wonyong Sung"
        ],
        "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.\n    ",
        "submission_date": "2016-09-13T00:00:00",
        "last_modified_date": "2017-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04628",
        "title": "Context Aware Nonnegative Matrix Factorization Clustering",
        "authors": [
            "Rocco Tripodi",
            "Sebastiano Vascon",
            "Marcello Pelillo"
        ],
        "abstract": "In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations.\n    ",
        "submission_date": "2016-09-15T00:00:00",
        "last_modified_date": "2016-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.04938",
        "title": "Image-to-Markup Generation with Coarse-to-Fine Attention",
        "authors": [
            "Yuntian Deng",
            "Anssi Kanervisto",
            "Jeffrey Ling",
            "Alexander M. Rush"
        ],
        "abstract": "We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2017-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05104",
        "title": "Intrinsic normalization and extrinsic denormalization of formant data of vowels",
        "authors": [
            "T.V. Ananthapadmanabha",
            "A.G. Ramakrishnan"
        ],
        "abstract": "Using a known speaker-intrinsic normalization procedure, formant data are scaled by the reciprocal of the geometric mean of the first three formant frequencies. This reduces the influence of the talker but results in a distorted vowel space. The proposed speaker-extrinsic procedure re-scales the normalized values by the mean formant values of vowels. When tested on the formant data of vowels published by Peterson and Barney, the combined approach leads to well separated clusters by reducing the spread due to talkers. The proposed procedure performs better than two top-ranked normalization procedures based on the accuracy of vowel classification as the objective measure.\n    ",
        "submission_date": "2016-09-16T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.05600",
        "title": "Graph-Structured Representations for Visual Question Answering",
        "authors": [
            "Damien Teney",
            "Lingqiao Liu",
            "Anton van den Hengel"
        ],
        "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.\n    ",
        "submission_date": "2016-09-19T00:00:00",
        "last_modified_date": "2017-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06268",
        "title": "Semantic Similarity Strategies for Job Title Classification",
        "authors": [
            "Yun Zhu",
            "Faizan Javed",
            "Ozgur Ozturk"
        ],
        "abstract": "Automatic and accurate classification of items enables numerous downstream applications in many domains. These applications can range from faceted browsing of items to product recommendations and big data analytics. In the online recruitment domain, we refer to classifying job ads to pre-defined or custom occupation categories as job title classification. A large-scale job title classification system can power various downstream applications such as semantic search, job recommendations and labor market analytics. In this paper, we discuss experiments conducted to improve our in-house job title classification system. The classification component of the system is composed of a two-stage coarse and fine level classifier cascade that classifies input text such as job title and/or job ads to one of the thousands of job titles in our taxonomy. To improve classification accuracy and effectiveness, we experiment with various semantic representation strategies such as average W2V vectors and document similarity measures such as Word Movers Distance (WMD). Our initial results show an overall improvement in accuracy of Carotene[1].\n    ",
        "submission_date": "2016-09-20T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06404",
        "title": "KU-ISPL Language Recognition System for NIST 2015 i-Vector Machine Learning Challenge",
        "authors": [
            "Suwon Shon",
            "Seongkyu Mun",
            "John H.L. Hansen",
            "Hanseok Ko"
        ],
        "abstract": "In language recognition, the task of rejecting/differentiating closely spaced versus acoustically far spaced languages remains a major challenge. For confusable closely spaced languages, the system needs longer input test duration material to obtain sufficient information to distinguish between languages. Alternatively, if languages are distinct and not acoustically/linguistically similar to others, duration is not a sufficient remedy. The solution proposed here is to explore duration distribution analysis for near/far languages based on the Language Recognition i-Vector Machine Learning Challenge 2015 (LRiMLC15) database. Using this knowledge, we propose a likelihood ratio based fusion approach that leveraged both score and duration information. The experimental results show that the use of duration and score fusion improves language recognition performance by 5% relative in LRiMLC15 cost.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06492",
        "title": "Document Image Coding and Clustering for Script Discrimination",
        "authors": [
            "Darko Brodic",
            "Alessia Amelio",
            "Zoran N. Milivojevic",
            "Milena Jevtic"
        ],
        "abstract": "The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06657",
        "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)",
        "authors": [
            "Andrew Shin",
            "Yoshitaka Ushiku",
            "Tatsuya Harada"
        ],
        "abstract": "Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.\n    ",
        "submission_date": "2016-09-21T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.06783",
        "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes",
        "authors": [
            "Kar Wai Lim",
            "Wray Buntine",
            "Changyou Chen",
            "Lan Du"
        ],
        "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07028",
        "title": "Image-embodied Knowledge Representation Learning",
        "authors": [
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Huanbo Luan",
            "Maosong Sun"
        ],
        "abstract": "Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.\n    ",
        "submission_date": "2016-09-22T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07245",
        "title": "A New Statistic Feature of the Short-Time Amplitude Spectrum Values for Human's Unvoiced Pronunciation",
        "authors": [
            "Xiaodong Zhuang"
        ],
        "abstract": "In this paper, a new statistic feature of the discrete short-time amplitude spectrum is discovered by experiments for the signals of unvoiced pronunciation. For the random-varying short-time spectrum, this feature reveals the relationship between the amplitude's average and its standard for every frequency component. On the other hand, the association between the amplitude distributions for different frequency components is also studied. A new model representing such association is inspired by the normalized histogram of amplitude. By mathematical analysis, the new statistic feature discovered is proved to be necessary evidence which supports the proposed model, and also can be direct evidence for the widely used hypothesis of \"identical distribution of amplitude for all frequencies\".\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.07498",
        "title": "Speaker Recognition for Children's Speech",
        "authors": [
            "Saeid Safavi",
            "Maryam Najafian",
            "Abualsoud Hanani",
            "Martin J Russell",
            "Peter Jancovic",
            "Michael J Carey"
        ],
        "abstract": "This paper presents results on Speaker Recognition (SR) for children's speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions of the spectrum containing important speaker information for children are identified by conducting SR experiments over 21 frequency bands. As for adults, the spectrum can be split into four regions, with the first (containing primary vocal tract resonance information) and third (corresponding to high frequency speech sounds) being most useful for SR. However, the frequencies at which these regions occur are from 11% to 38% higher for children. It is also noted that subband SR rates are lower for younger children. Finally results are presented of SR experiments to identify a child in a class (30 children, similar age) and school (288 children, varying ages). Class performance depends on age, with accuracy varying from 90% for young children to 99% for older children. The identification rate achieved for a child in a school is 81%.\n    ",
        "submission_date": "2016-09-23T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08433",
        "title": "Local Training for PLDA in Speaker Verification",
        "authors": [
            "Chenghui Zhao",
            "Lantian Li",
            "Dong Wang",
            "April Pu"
        ],
        "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain.\n",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08441",
        "title": "Weakly Supervised PLDA Training",
        "authors": [
            "Lantian Li",
            "Yixiang Chen",
            "Dong Wang",
            "Chenghui Zhao"
        ],
        "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training.\n",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08442",
        "title": "Collaborative Learning for Language and Speaker Recognition",
        "authors": [
            "Lantian Li",
            "Zhiyuan Tang",
            "Dong Wang",
            "Andrew Abel",
            "Yang Feng",
            "Shiyue Zhang"
        ],
        "abstract": "This paper presents a unified model to perform language and speaker recognition simultaneously and altogether. The model is based on a multi-task recurrent neural network where the output of one task is fed as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by borrowing information from each other. Our experiments demonstrated that the multi-task model outperforms the task-specific models on both tasks.\n    ",
        "submission_date": "2016-09-27T00:00:00",
        "last_modified_date": "2017-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08779",
        "title": "Using Natural Language Processing and Qualitative Analysis to Intervene in Gang Violence: A Collaboration Between Social Work Researchers and Data Scientists",
        "authors": [
            "Desmond Upton Patton",
            "Kathleen McKeown",
            "Owen Rambow",
            "Jamie Macbeth"
        ],
        "abstract": "The U.S. has the highest rate of firearm-related deaths when compared to other industrialized countries. Violence particularly affects low-income, urban neighborhoods in cities like Chicago, which saw a 40% increase in firearm violence from 2014 to 2015 to more than 3,000 shooting victims. While recent studies have found that urban, gang-involved individuals curate a unique and complex communication style within and between social media platforms, organizations focused on reducing gang violence are struggling to keep up with the growing complexity of social media platforms and the sheer volume of data they present. In this paper, describe the Digital Urban Violence Analysis Approach (DUVVA), a collaborative qualitative analysis method used in a collaboration between data scientists and social work researchers to develop a suite of systems for decoding the high- stress language of urban, gang-involved youth. Our approach leverages principles of grounded theory when analyzing approximately 800 tweets posted by Chicago gang members and participation of youth from Chicago neighborhoods to create a language resource for natural language processing (NLP) methods. In uncovering the unique language and communication style, we developed automated tools with the potential to detect aggressive language on social media and aid individuals and groups in performing violence prevention and interruption.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08789",
        "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition",
        "authors": [
            "Zhiyuan Tang",
            "Ying Shi",
            "Dong Wang",
            "Yang Feng",
            "Shiyue Zhang"
        ],
        "abstract": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2017-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1609.08843",
        "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words",
        "authors": [
            "Jiaming Xu",
            "Jing Shi",
            "Yiqun Yao",
            "Suncong Zheng",
            "Bo Xu",
            "Bo Xu"
        ],
        "abstract": "Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, (k)-max pooling is exploited following reasoning module on the sentence-level memory to sample the (k) most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.\n    ",
        "submission_date": "2016-09-28T00:00:00",
        "last_modified_date": "2016-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00219",
        "title": "Text Network Exploration via Heterogeneous Web of Topics",
        "authors": [
            "Junxian He",
            "Ying Huang",
            "Changfeng Liu",
            "Jiaming Shen",
            "Yuting Jia",
            "Xinbing Wang"
        ],
        "abstract": "A text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges. The proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network, namely text network exploration. In this paper, we address the problem of text network exploration through constructing a heterogeneous web of topics, which allows people to investigate a text network associating word level with document level. To achieve this, a probabilistic generative model for text and links is proposed, where three different relationships in the heterogeneous topic web are quantified. We also develop a prototype demo system named TopicAtlas to exhibit such heterogeneous topic web, and demonstrate how this system can facilitate the task of text network exploration. Extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web. Besides, we validate our model on real-life text networks, showing that it preserves good performance on objective evaluation metrics.\n    ",
        "submission_date": "2016-10-02T00:00:00",
        "last_modified_date": "2016-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00520",
        "title": "Semi-supervised Learning with Sparse Autoencoders in Phone Classification",
        "authors": [
            "Akash Kumar Dhaka",
            "Giampiero Salvi"
        ],
        "abstract": "We propose the application of a semi-supervised learning method to improve the performance of acoustic modelling for automatic speech recognition based on deep neural net- works. As opposed to unsupervised initialisation followed by supervised fine tuning, our method takes advantage of both unlabelled and labelled data simultaneously through mini- batch stochastic gradient descent. We tested the method with varying proportions of labelled vs unlabelled observations in frame-based phoneme classification on the TIMIT database. Our experiments show that the method outperforms standard supervised training for an equal amount of labelled data and provides competitive error rates compared to state-of-the-art graph-based semi-supervised learning techniques.\n    ",
        "submission_date": "2016-10-03T00:00:00",
        "last_modified_date": "2016-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.00852",
        "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction",
        "authors": [
            "Joey Hong",
            "Chris Mattmann",
            "Paul Ramirez"
        ],
        "abstract": "The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and content-based features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the authors exact age.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01076",
        "title": "Tutorial on Answering Questions about Images with Deep Learning",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.\n    ",
        "submission_date": "2016-10-04T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01382",
        "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest",
        "authors": [
            "Abdul Malik Badshah",
            "Jamil Ahmad",
            "Mi Young Lee",
            "Sung Wook Baik"
        ],
        "abstract": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01465",
        "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges",
        "authors": [
            "Kushal Kafle",
            "Christopher Kanan"
        ],
        "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2017-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.01561",
        "title": "Summarizing Situational and Topical Information During Crises",
        "authors": [
            "Koustav Rudra",
            "Siddhartha Banerjee",
            "Niloy Ganguly",
            "Pawan Goyal",
            "Muhammad Imran",
            "Prasenjit Mitra"
        ],
        "abstract": "The use of microblogging platforms such as Twitter during crises has become widespread. More importantly, information disseminated by affected people contains useful information like reports of missing and found people, requests for urgent needs etc. For rapid crisis response, humanitarian organizations look for situational awareness information to understand and assess the severity of the crisis. In this paper, we present a novel framework (i) to generate abstractive summaries useful for situational awareness, and (ii) to capture sub-topics and present a short informative summary for each of these topics. A summary is generated using a two stage framework that first extracts a set of important tweets from the whole set of information through an Integer-linear programming (ILP) based optimization technique and then follows a word graph and concept event based abstractive summarization technique to produce the final summary. High accuracies obtained for all the tasks show the effectiveness of the proposed framework.\n    ",
        "submission_date": "2016-10-05T00:00:00",
        "last_modified_date": "2016-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02424",
        "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models",
        "authors": [
            "Ashwin K Vijayakumar",
            "Michael Cogswell",
            "Ramprasath R. Selvaraju",
            "Qing Sun",
            "Stefan Lee",
            "David Crandall",
            "Dhruv Batra"
        ],
        "abstract": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.\n    ",
        "submission_date": "2016-10-07T00:00:00",
        "last_modified_date": "2018-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02736",
        "title": "Emergence of linguistic laws in human voice",
        "authors": [
            "Ivan Gonzalez Torre",
            "Bartolo Luque",
            "Lucas Lacasa",
            "Jordi Luque",
            "Antoni Hernandez-Fernandez"
        ],
        "abstract": "Linguistic laws constitute one of the quantitative cornerstones of modern cognitive sciences and have been routinely investigated in written corpora, or in the equivalent transcription of oral corpora. This means that inferences of statistical patterns of language in acoustics are biased by the arbitrary, language-dependent segmentation of the signal, and virtually precludes the possibility of making comparative studies between human voice and other animal communication systems. Here we bridge this gap by proposing a method that allows to measure such patterns in acoustic signals of arbitrary origin, without needs to have access to the language corpus underneath. The method has been applied to six different human languages, recovering successfully some well-known laws of human communication at timescales even below the phoneme and finding yet another link between complexity and criticality in a biological system. These methods further pave the way for new comparative studies in animal communication or the analysis of signals of unknown code.\n    ",
        "submission_date": "2016-10-09T00:00:00",
        "last_modified_date": "2016-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02891",
        "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning",
        "authors": [
            "Kaixiang Mo",
            "Shuangyin Li",
            "Yu Zhang",
            "Jiajun Li",
            "Qiang Yang"
        ],
        "abstract": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose \"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2017-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.02906",
        "title": "A General Framework for Content-enhanced Network Representation Learning",
        "authors": [
            "Xiaofei Sun",
            "Jiang Guo",
            "Xiao Ding",
            "Ting Liu"
        ],
        "abstract": "This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03009",
        "title": "Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting",
        "authors": [
            "Ali Khodabakhsh",
            "Cenk Demiroglu"
        ],
        "abstract": "Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03035",
        "title": "Latent Sequence Decompositions",
        "authors": [
            "William Chan",
            "Yu Zhang",
            "Quoc Le",
            "Navdeep Jaitly"
        ],
        "abstract": "We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.6% WER.\n    ",
        "submission_date": "2016-10-10T00:00:00",
        "last_modified_date": "2017-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03164",
        "title": "Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation",
        "authors": [
            "Andrea F. Daniele",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then \"translate\" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans.\n    ",
        "submission_date": "2016-10-11T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03708",
        "title": "Generating captions without looking beyond objects",
        "authors": [
            "Hendrik Heuer",
            "Christof Monz",
            "Arnold W.M. Smeulders"
        ],
        "abstract": "This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.\n    ",
        "submission_date": "2016-10-12T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.03914",
        "title": "Mapping Between fMRI Responses to Movies and their Natural Language Annotations",
        "authors": [
            "Kiran Vodrahalli",
            "Po-Hsuan Chen",
            "Yingyu Liang",
            "Christopher Baldassano",
            "Janice Chen",
            "Esther Yong",
            "Christopher Honey",
            "Uri Hasson",
            "Peter Ramadge",
            "Ken Norman",
            "Sanjeev Arora"
        ],
        "abstract": "Several research groups have shown how to correlate fMRI responses to the meanings of presented stimuli. This paper presents new methods for doing so when only a natural language annotation is available as the description of the stimulus. We study fMRI data gathered from subjects watching an episode of BBCs Sherlock [1], and learn bidirectional mappings between fMRI responses and natural language representations. We show how to leverage data from multiple subjects watching the same movie to improve the accuracy of the mappings, allowing us to succeed at a scene classification task with 72% accuracy (random guessing would give 4%) and at a scene ranking task with average rank in the top 4% (random guessing would give 50%). The key ingredients are (a) the use of the Shared Response Model (SRM) and its variant SRM-ICA [2, 3] to aggregate fMRI data from multiple subjects, both of which are shown to be superior to standard PCA in producing low-dimensional representations for the tasks in this paper; (b) a sentence embedding technique adapted from the natural language processing (NLP) literature [4] that produces semantic vector representation of the annotations; (c) using previous timestep information in the featurization of the predictor data.\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04120",
        "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding",
        "authors": [
            "Lina M. Rojas Barahona",
            "Milica Gasic",
            "Nikola Mrk\u0161i\u0107",
            "Pei-Hao Su",
            "Stefan Ultes",
            "Tsung-Hsien Wen",
            "Steve Young"
        ],
        "abstract": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).\n    ",
        "submission_date": "2016-10-13T00:00:00",
        "last_modified_date": "2016-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04533",
        "title": "A Comprehensive Comparative Study of Word and Sentence Similarity Measures",
        "authors": [
            "Issa Atoum",
            "Ahmed Otoom",
            "Narayanan Kulathuramaiyer"
        ],
        "abstract": "Sentence similarity is considered the basis of many natural language tasks such as information retrieval, question answering and text summarization. The semantic meaning between compared text fragments is based on the words semantic features and their relationships. This article reviews a set of word and sentence similarity measures and compares them on benchmark datasets. On the studied datasets, results showed that hybrid semantic measures perform better than both knowledge and corpus based measures.\n    ",
        "submission_date": "2016-02-17T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04658",
        "title": "Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation",
        "authors": [
            "Yacine Jernite",
            "Anna Choromanska",
            "David Sontag"
        ],
        "abstract": "We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions. We theoret- ically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the al- gorithm on text classification and language mod- eling, respectively, and show that they compare favorably to common baselines in terms of accu- racy and running time.\n    ",
        "submission_date": "2016-10-14T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04718",
        "title": "Generalization of metric classification algorithms for sequences classification and labelling",
        "authors": [
            "Roman Samarev",
            "Andrey Vasnetsov",
            "Elizaveta Smelkova"
        ],
        "abstract": "The article deals with the issue of modification of metric classification algorithms. In particular, it studies the algorithm k-Nearest Neighbours for its application to sequential data. A method of generalization of metric classification algorithms is proposed. As a part of it, there has been developed an algorithm for solving the problem of classification and labelling of sequential data. The advantages of the developed algorithm of classification in comparison with the existing one are also discussed in the article. There is a comparison of the effectiveness of the proposed algorithm with the algorithm of CRF in the task of chunking in the open data set CoNLL2000.\n    ",
        "submission_date": "2016-10-15T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.04814",
        "title": "Term-Class-Max-Support (TCMS): A Simple Text Document Categorization Approach Using Term-Class Relevance Measure",
        "authors": [
            "D S Guru",
            "Mahamad Suhil"
        ],
        "abstract": "In this paper, a simple text categorization method using term-class relevance measures is proposed. Initially, text documents are processed to extract significant terms present in them. For every term extracted from a document, we compute its importance in preserving the content of a class through a novel term-weighting scheme known as Term_Class Relevance (TCR) measure proposed by Guru and Suhil (2015) [1]. In this way, for every term, its relevance for all the classes present in the corpus is computed and stored in the knowledgebase. During testing, the terms present in the test document are extracted and the term-class relevance of each term is obtained from the stored knowledgebase. To achieve quick search of term weights, Btree indexing data structure has been adapted. Finally, the class which receives maximum support in terms of term-class relevance is decided to be the class of the given test document. The proposed method works in logarithmic complexity in testing time and simple to implement when compared to any other text categorization techniques available in literature. The experiments conducted on various benchmarking datasets have revealed that the performance of the proposed method is satisfactory and encouraging.\n    ",
        "submission_date": "2016-10-16T00:00:00",
        "last_modified_date": "2016-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05654",
        "title": "The infochemical core",
        "authors": [
            "Antoni Hern\u00e1ndez-Fern\u00e1ndez",
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Vocalizations and less often gestures have been the object of linguistic research over decades. However, the development of a general theory of communication with human language as a particular case requires a clear understanding of the organization of communication through other means. Infochemicals are chemical compounds that carry information and are employed by small organisms that cannot emit acoustic signals of optimal frequency to achieve successful communication. Here the distribution of infochemicals across species is investigated when they are ranked by their degree or the number of species with which it is associated (because they produce or they are sensitive to it). The quality of the fit of different functions to the dependency between degree and rank is evaluated with a penalty for the number of parameters of the function. Surprisingly, a double Zipf (a Zipf distribution with two regimes with a different exponent each) is the model yielding the best fit although it is the function with the largest number of parameters. This suggests that the world wide repertoire of infochemicals contains a chemical nucleus shared by many species and reminiscent of the core vocabularies found for human language in dictionaries or large corpora.\n    ",
        "submission_date": "2016-10-18T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.05948",
        "title": "A Bayesian Approach to Estimation of Speaker Normalization Parameters",
        "authors": [
            "Dhananjay Ram",
            "Debasis Kundu",
            "Rajesh M. Hegde"
        ],
        "abstract": "In this work, a Bayesian approach to speaker normalization is proposed to compensate for the degradation in performance of a speaker independent speech recognition system. The speaker normalization method proposed herein uses the technique of vocal tract length normalization (VTLN). The VTLN parameters are estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a special type of Markov Chain Monte Carlo method. Additionally the hyperparameters are estimated using maximum likelihood approach. This model is used assuming that human vocal tract can be modeled as a tube of uniform cross section. It captures the variation in length of the vocal tract of different speakers more effectively, than the linear model used in literature. The work has also investigated different methods like minimization of Mean Square Error (MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both single pass and two pass approaches are then used to build a VTLN based speech recognizer. Experimental results on recognition of vowels and Hindi phrases from a medium vocabulary indicate that the Bayesian method improves the performance by a considerable margin.\n    ",
        "submission_date": "2016-10-19T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.06856",
        "title": "Automated Big Text Security Classification",
        "authors": [
            "Khudran Alzhrani",
            "Ethan M. Rudd",
            "Terrance E. Boult",
            "C. Edward Chow"
        ],
        "abstract": "In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled \"confidential documents\" for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity.\n    ",
        "submission_date": "2016-10-21T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07432",
        "title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research",
        "authors": [
            "Douwe Kiela",
            "Luana Bulat",
            "Anita L. Vero",
            "Stephen Clark"
        ],
        "abstract": "Meaning has been called the \"holy grail\" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.\n    ",
        "submission_date": "2016-10-24T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.07708",
        "title": "Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples",
        "authors": [
            "Amit Sheth",
            "Sujan Perera",
            "Sanjaya Wijeratne"
        ],
        "abstract": "Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2019-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08095",
        "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems",
        "authors": [
            "Mengting Wan",
            "Julian McAuley"
        ],
        "abstract": "Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single `correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a `good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions---and over 3.1 million answers---and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.\n    ",
        "submission_date": "2016-10-25T00:00:00",
        "last_modified_date": "2016-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08229",
        "title": "Word Embeddings and Their Use In Sentence Classification Tasks",
        "authors": [
            "Amit Mandelbaum",
            "Adi Shalev"
        ],
        "abstract": "This paper have two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.\n    ",
        "submission_date": "2016-10-26T00:00:00",
        "last_modified_date": "2016-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08597",
        "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
        "authors": [
            "Sanjaya Wijeratne",
            "Lakshika Balasuriya",
            "Derek Doran",
            "Amit Sheth"
        ],
        "abstract": "Gang affiliates have joined the masses who use social media to share thoughts and actions publicly. Interestingly, they use this public medium to express recent illegal actions, to intimidate others, and to share outrageous images and statements. Agencies able to unearth these profiles may thus be able to anticipate, stop, or hasten the investigation of gang-related crimes. This paper investigates the use of word embeddings to help identify gang members on Twitter. Building on our previous work, we generate word embeddings that translate what Twitter users post in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification. Our experimental results show that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts.\n    ",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.08613",
        "title": "Can Active Memory Replace Attention?",
        "authors": [
            "\u0141ukasz Kaiser",
            "Samy Bengio"
        ],
        "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.\n",
        "submission_date": "2016-10-27T00:00:00",
        "last_modified_date": "2017-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09225",
        "title": "Sentiment Analysis of Twitter Data for Predicting Stock Market Movements",
        "authors": [
            "Venkata Sasank Pagolu",
            "Kamal Nayan Reddy Challa",
            "Ganapati Panda",
            "Babita Majhi"
        ],
        "abstract": "Predicting stock market movements is a well-known problem of interest. Now-a-days social media is perfectly representing the public sentiment and opinion about current events. Especially, twitter has attracted a lot of attention from researchers for studying the public sentiments. Stock market prediction on the basis of public sentiments expressed on twitter has been an intriguing field of research. Previous studies have concluded that the aggregate public mood collected from twitter may well be correlated with Dow Jones Industrial Average Index (DJIA). The thesis of this work is to observe how well the changes in stock prices of a company, the rises and falls, are correlated with the public opinions being expressed in tweets about that company. Understanding author's opinion from a piece of text is the objective of sentiment analysis. The present paper have employed two different textual representations, Word2vec and N-gram, for analyzing the public sentiments in tweets. In this paper, we have applied sentiment analysis and supervised machine learning principles to the tweets extracted from twitter and analyze the correlation between stock market movements of a company and sentiments in tweets. In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase. At the end of the paper, it is shown that a strong correlation exists between the rise and falls in stock prices with the public sentiments in tweets.\n    ",
        "submission_date": "2016-10-28T00:00:00",
        "last_modified_date": "2016-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09516",
        "title": "Finding Street Gang Members on Twitter",
        "authors": [
            "Lakshika Balasuriya",
            "Sanjaya Wijeratne",
            "Derek Doran",
            "Amit Sheth"
        ],
        "abstract": "Most street gang members use Twitter to intimidate others, to present outrageous images and statements to the world, and to share recent illegal activities. Their tweets may thus be useful to law enforcement agencies to discover clues about recent crimes or to anticipate ones that may occur. Finding these posts, however, requires a method to discover gang member Twitter profiles. This is a challenging task since gang members represent a very small population of the 320 million Twitter users. This paper studies the problem of automatically finding gang members on Twitter. It outlines a process to curate one of the largest sets of verifiable gang member profiles that have ever been studied. A review of these profiles establishes differences in the language, images, YouTube links, and emojis gang members use compared to the rest of the Twitter population. Features from this review are used to train a series of supervised classifiers. Our classifier achieves a promising F1 score with a low false positive rate.\n    ",
        "submission_date": "2016-10-29T00:00:00",
        "last_modified_date": "2016-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09964",
        "title": "Ontology Verbalization using Semantic-Refinement",
        "authors": [
            "Vinu E.V",
            "P Sreenivasa Kumar"
        ],
        "abstract": "We propose a rule-based technique to generate redundancy-free NL descriptions of OWL ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1610.09982",
        "title": "Sentiment Analysis of Review Datasets Using Naive Bayes and K-NN Classifier",
        "authors": [
            "Lopamudra Dey",
            "Sanjay Chakraborty",
            "Anuraag Biswas",
            "Beepa Bose",
            "Sweta Tiwari"
        ],
        "abstract": "The advent of Web 2.0 has led to an increase in the amount of sentimental content available in the Web. Such content is often found in social media web sites in the form of movie or product reviews, user comments, testimonials, messages in discussion forums etc. Timely discovery of the sentimental or opinionated web content has a number of advantages, the most important of all being monetization. Understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements, recommendation systems and analysis of market trends. The focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same. We use statistical methods to capture elements of subjective style and the sentence polarity. The paper elaborately discusses two supervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naive Bayes and compares their overall accuracy, precisions as well as recall values. It was seen that in case of movie reviews Naive Bayes gave far better results than K-NN but for hotel reviews these algorithms gave lesser, almost same accuracies.\n    ",
        "submission_date": "2016-10-31T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00138",
        "title": "MusicMood: Predicting the mood of music from song lyrics using machine learning",
        "authors": [
            "Sebastian Raschka"
        ],
        "abstract": "Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00356",
        "title": "Using Artificial Intelligence to Identify State Secrets",
        "authors": [
            "Renato Rocha Souza",
            "Flavio Codeco Coelho",
            "Rohan Shah",
            "Matthew Connelly"
        ],
        "abstract": "Whether officials can be trusted to protect national security information has become a matter of great public controversy, reigniting a long-standing debate about the scope and nature of official secrecy. The declassification of millions of electronic records has made it possible to analyze these issues with greater rigor and precision. Using machine-learning methods, we examined nearly a million State Department cables from the 1970s to identify features of records that are more likely to be classified, such as international negotiations, military operations, and high-level communications. Even with incomplete data, algorithms can use such features to identify 90% of classified cables with <11% false positives. But our results also show that there are longstanding problems in the identification of sensitive information. Error analysis reveals many examples of both overclassification and underclassification. This indicates both the need for research on inter-coder reliability among officials as to what constitutes classified material and the opportunity to develop recommender systems to better manage both classification and declassification.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00384",
        "title": "CB2CF: A Neural Multiview Content-to-Collaborative Filtering Model for Completely Cold Item Recommendations",
        "authors": [
            "Oren Barkan",
            "Noam Koenigstein",
            "Eylon Yogev",
            "Ori Katz"
        ],
        "abstract": "In Recommender Systems research, algorithms are often characterized as either Collaborative Filtering (CF) or Content Based (CB). CF algorithms are trained using a dataset of user preferences while CB algorithms are typically based on item profiles. These approaches harness different data sources and therefore the resulting recommended items are generally very different. This paper presents the CB2CF, a deep neural multiview model that serves as a bridge from items content into their CF representations. CB2CF is a real-world algorithm designed for Microsoft Store services that handle around a billion users worldwide. CB2CF is demonstrated on movies and apps recommendations, where it is shown to outperform an alternative CB model on completely cold items.\n    ",
        "submission_date": "2016-11-01T00:00:00",
        "last_modified_date": "2019-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00440",
        "title": "And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election",
        "authors": [
            "Elvyna Tunggawan",
            "Yustinus Eko Soelistio"
        ],
        "abstract": "This paper describes a Naive-Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We introduce a simpler data preprocessing method to label the data and train the model. The model achieves 95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie Sanders as Republican and Democratic nominee respectively. It achieves a comparable result to those in its competitor methods.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00448",
        "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks",
        "authors": [
            "Hao Wang",
            "Xingjian Shi",
            "Dit-Yan Yeung"
        ],
        "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00454",
        "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks",
        "authors": [
            "Hao Wang",
            "Xingjian Shi",
            "Dit-Yan Yeung"
        ],
        "abstract": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00456",
        "title": "Measuring Asymmetric Opinions on Online Social Interrelationship with Language and Network Features",
        "authors": [
            "Bo Wang",
            "Yanshu Yu",
            "Yuan Wang"
        ],
        "abstract": "Instead of studying the properties of social relationship from an objective view, in this paper, we focus on individuals' subjective and asymmetric opinions on their interrelationships. Inspired by the theories from sociolinguistics, we investigate two individuals' opinions on their interrelationship with their interactive language features. Eliminating the difference of personal language style, we clarify that the asymmetry of interactive language feature values can indicate individuals' asymmetric opinions on their interrelationship. We also discuss how the degree of opinions' asymmetry is related to the individuals' personality traits. Furthermore, to measure the individuals' asymmetric opinions on interrelationship concretely, we develop a novel model synthetizing interactive language and social network features. The experimental results with Enron email dataset provide multiple evidences of the asymmetric opinions on interrelationship, and also verify the effectiveness of the proposed model in measuring the degree of opinions' asymmetry.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00457",
        "title": "Structure vs. Language: Investigating the Multi-factors of Asymmetric Opinions on Online Social Interrelationship with a Case Study",
        "authors": [
            "Bo Wang",
            "Yingjun Sun",
            "Yuan Wang"
        ],
        "abstract": "Though current researches often study the properties of online social relationship from an objective view, we also need to understand individuals' subjective opinions on their interrelationships in social computing studies. Inspired by the theories from sociolinguistics, the latest work indicates that interactive language can reveal individuals' asymmetric opinions on their interrelationship. In this work, in order to explain the opinions' asymmetry on interrelationship with more latent factors, we extend the investigation from single relationship to the structural context in online social network. We analyze the correlation between interactive language features and the structural context of interrelationships. The structural context of vertex, edges and triangles in social network are considered. With statistical analysis on Enron email dataset, we find that individuals' opinions (measured by interactive language features) on their interrelationship are related to some of their important structural context in social network. This result can help us to understand and measure the individuals' opinions on their interrelationship with more intrinsic information.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.00514",
        "title": "The Intelligent Voice 2016 Speaker Recognition System",
        "authors": [
            "Abbas Khosravani",
            "Cornelius Glackin",
            "Nazim Dugan",
            "G\u00e9rard Chollet",
            "Nigel Cannings"
        ],
        "abstract": "This paper presents the Intelligent Voice (IV) system submitted to the NIST 2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this year was on developing speaker recognition technology which is robust for novel languages that are much more heterogeneous than those used in the current state-of-the-art, using significantly less training data, that does not contain meta-data from those languages. The system is based on the state-of-the-art i-vector/PLDA which is developed on the fixed training condition, and the results are reported on the protocol defined on the development set of the challenge.\n    ",
        "submission_date": "2016-11-02T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01259",
        "title": "Generalized Topic Modeling",
        "authors": [
            "Avrim Blum",
            "Nika Haghtalab"
        ],
        "abstract": "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution $\\vec a_i$ over words, and a document is generated by first selecting a mixture $\\vec w$ over topics, and then generating words i.i.d. from the associated mixture $A{\\vec w}$. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture.\n",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01400",
        "title": "Learning to Rank Scientific Documents from the Crowd",
        "authors": [
            "Jesse M Lingeman",
            "Hong Yu"
        ],
        "abstract": "Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2016-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01462",
        "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
        "authors": [
            "Hakan Inan",
            "Khashayar Khosravi",
            "Richard Socher"
        ],
        "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.\n    ",
        "submission_date": "2016-11-04T00:00:00",
        "last_modified_date": "2017-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01576",
        "title": "Quasi-Recurrent Neural Networks",
        "authors": [
            "James Bradbury",
            "Stephen Merity",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01599",
        "title": "LipNet: End-to-End Sentence-level Lipreading",
        "authors": [
            "Yannis M. Assael",
            "Brendan Shillingford",
            "Shimon Whiteson",
            "Nando de Freitas"
        ],
        "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).\n    ",
        "submission_date": "2016-11-05T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01714",
        "title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data",
        "authors": [
            "Ark Anderson",
            "Kyle Shaffer",
            "Artem Yankov",
            "Court D. Corley",
            "Nathan O. Hodas"
        ],
        "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.01802",
        "title": "Self-Wiring Question Answering Systems",
        "authors": [
            "Ricardo Usbeck",
            "Jonathan Huthmann",
            "Nico Duldhardt",
            "Axel-Cyrille Ngonga Ngomo"
        ],
        "abstract": "Question answering (QA) has been the subject of a resurgence over the past years. The said resurgence has led to a multitude of question answering (QA) systems being developed both by companies and research facilities. While a few components of QA systems get reused across implementations, most systems do not leverage the full potential of component reuse. Hence, the development of QA systems is currently still a tedious and time-consuming process. We address the challenge of accelerating the creation of novel or tailored QA systems by presenting a concept for a self-wiring approach to composing QA systems. Our approach will allow the reuse of existing, web-based QA systems or modules while developing new QA platforms. To this end, it will rely on QA modules being described using the Web Ontology Language. Based on these descriptions, our approach will be able to automatically compose QA systems using a data-driven approach automatically.\n    ",
        "submission_date": "2016-11-06T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02266",
        "title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering",
        "authors": [
            "Liwen Zhang",
            "John Winn",
            "Ryota Tomioka"
        ],
        "abstract": "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02337",
        "title": "Balotage in Argentina 2015, a sentiment analysis of tweets",
        "authors": [
            "Daniel Robins",
            "Fernando Emmanuel Frati",
            "Jonatan Alvarez",
            "Jose Texier"
        ],
        "abstract": "Twitter social network contains a large amount of information generated by its users. That information is composed of opinions and comments that may reflect trends in social behavior. There is talk of trend when it is possible to identify opinions and comments geared towards the same shared by a lot of people direction. To determine if two or more written opinions share the same address, techniques Natural Language Processing (NLP) are used. This paper proposes a methodology for predicting reflected in Twitter from the use of sentiment analysis functions NLP based on social behaviors. The case study was selected the 2015 Presidential in Argentina, and a software architecture Big Data composed Vertica data base with the component called Pulse was used. Through the analysis it was possible to detect trends in voting intentions with regard to the presidential candidates, achieving greater accuracy in predicting that achieved with traditional systems surveys.\n    ",
        "submission_date": "2016-11-07T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02815",
        "title": "An Automated System for Essay Scoring of Online Exams in Arabic based on Stemming Techniques and Levenshtein Edit Operations",
        "authors": [
            "Emad Fawzi Al-Shalabi"
        ],
        "abstract": "In this article, an automated system is proposed for essay scoring in Arabic language for online exams based on stemming techniques and Levenshtein edit operations. An online exam has been developed on the proposed mechanisms, exploiting the capabilities of light and heavy stemming. The implemented online grading system has shown to be an efficient tool for automated scoring of essay questions.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.02879",
        "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks",
        "authors": [
            "Abhinav Thanda",
            "Shankar M Venkatesan"
        ],
        "abstract": "In this work, we propose a training algorithm for an audio-visual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.\n    ",
        "submission_date": "2016-11-09T00:00:00",
        "last_modified_date": "2016-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03218",
        "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence",
        "authors": [
            "Emilio Jorge",
            "Mikael K\u00e5geb\u00e4ck",
            "Fredrik D. Johansson",
            "Emil Gustavsson"
        ],
        "abstract": "Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.\n    ",
        "submission_date": "2016-11-10T00:00:00",
        "last_modified_date": "2017-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03305",
        "title": "Getting Started with Neural Models for Semantic Matching in Web Search",
        "authors": [
            "Kezban Dilek Onal",
            "Ismail Sengor Altingovde",
            "Pinar Karagoz",
            "Maarten de Rijke"
        ],
        "abstract": "The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work.\n    ",
        "submission_date": "2016-11-08T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03596",
        "title": "Generalized Entropies and the Similarity of Texts",
        "authors": [
            "Eduardo G. Altmann",
            "Laercio Dias",
            "Martin Gerlach"
        ],
        "abstract": "We show how generalized Gibbs-Shannon entropies can provide new insights on the statistical properties of texts. The universal distribution of word frequencies (Zipf's law) implies that the generalized entropies, computed at the word level, are dominated by words in a specific range of frequencies. Here we show that this is the case not only for the generalized entropies but also for the generalized (Jensen-Shannon) divergences, used to compute the similarity between different texts. This finding allows us to identify the contribution of specific words (and word frequencies) for the different generalized entropies and also to estimate the size of the databases needed to obtain a reliable estimation of the divergences. We test our results in large databases of books (from the Google n-gram database) and scientific papers (indexed by Web of Science).\n    ",
        "submission_date": "2016-11-11T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.03954",
        "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment",
        "authors": [
            "Muhao Chen",
            "Yingtao Tian",
            "Mohan Yang",
            "Carlo Zaniolo"
        ],
        "abstract": "Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone. Thus, we propose MTransE, a translation-based model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.\n    ",
        "submission_date": "2016-11-12T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04642",
        "title": "Link Prediction using Embedded Knowledge Graphs",
        "authors": [
            "Yelong Shen",
            "Po-Sen Huang",
            "Ming-Wei Chang",
            "Jianfeng Gao"
        ],
        "abstract": "Since large knowledge bases are typically incomplete, missing facts need to be inferred from observed facts in a task called knowledge base completion. The most successful approaches to this task have typically explored explicit paths through sequences of triples. These approaches have usually resorted to human-designed sampling procedures, since large knowledge graphs produce prohibitively large numbers of possible paths, most of which are uninformative. As an alternative approach, we propose performing a single, short sequence of interactive lookup operations on an embedded knowledge graph which has been trained through end-to-end backpropagation to be an optimized and compressed version of the initial knowledge base. Our proposed model, called Embedded Knowledge Graph Network (EKGN), achieves new state-of-the-art results on popular knowledge base completion benchmarks.\n    ",
        "submission_date": "2016-11-14T00:00:00",
        "last_modified_date": "2018-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.04842",
        "title": "The Role of Word Length in Semantic Topology",
        "authors": [
            "Francesco Fumarola"
        ],
        "abstract": "A topological argument is presented concering the structure of semantic space, based on the negative correlation between polysemy and word length. The resulting graph structure is applied to the modeling of free-recall experiments, resulting in predictions on the comparative values of recall probabilities. Associative recall is found to favor longer words whereas sequential recall is found to favor shorter words. Data from the PEERS experiments of Lohnas et al. (2015) and Healey and Kahana (2016) confirm both predictons, with correlation coefficients $r_{seq}= -0.17$ and $r_{ass}= +0.17$. The argument is then applied to predicting global properties of list recall, which leads to a novel explanation for the word-length effect based on the optimization of retrieval strategies.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05010",
        "title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm",
        "authors": [
            "Kejun Huang",
            "Xiao Fu",
            "Nicholas D. Sidiropoulos"
        ],
        "abstract": "In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.\n    ",
        "submission_date": "2016-11-15T00:00:00",
        "last_modified_date": "2016-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05118",
        "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives",
        "authors": [
            "Mohit Iyyer",
            "Varun Manjunatha",
            "Anupam Guha",
            "Yogarshi Vyas",
            "Jordan Boyd-Graber",
            "Hal Daum\u00e9 III",
            "Larry Davis"
        ],
        "abstract": "Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \"gutters\" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \"closure\". While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2017-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05379",
        "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent' Communicative Systems",
        "authors": [
            "Prof. Roger K. Moore"
        ],
        "abstract": "Recent years have witnessed increasing interest in the potential benefits of `intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot, iRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired the imagination of the general public, and social media buzz with speculation about a utopian world of helpful robot assistants or the coming robot apocalypse! However, there is a long way to go before autonomous systems reach the level of capabilities required for even the simplest of tasks involving human-robot interaction - especially if it involves communicative behaviour such as speech and language. Of course the field of Artificial Intelligence (AI) has made great strides in these areas, and has moved on from abstract high-level rule-based paradigms to embodied architectures whose operations are grounded in real physical environments. What is still missing, however, is an overarching theory of intelligent communicative behaviour that informs system-level design decisions in order to provide a more coherent approach to system integration. This chapter introduces the beginnings of such a framework inspired by the principles of Perceptual Control Theory (PCT). In particular, it is observed that PCT has hitherto tended to view perceptual processes as a relatively straightforward series of transformations from sensation to perception, and has overlooked the potential of powerful generative model-based solutions that have emerged in practical fields such as visual or auditory scene analysis. Starting from first principles, a sequence of arguments is presented which not only shows how these ideas might be integrated into PCT, but which also extend PCT towards a remarkably symmetric architecture for a needs-driven communicative agent. It is concluded that, if behaviour is the control of perception, then perception is the simulation of behaviour.\n    ",
        "submission_date": "2016-11-16T00:00:00",
        "last_modified_date": "2016-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.05546",
        "title": "Zero-Shot Visual Question Answering",
        "authors": [
            "Damien Teney",
            "Anton van den Hengel"
        ],
        "abstract": "Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-the-art performance in the standard VQA evaluation setting.\n    ",
        "submission_date": "2016-11-17T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06188",
        "title": "Variable Computation in Recurrent Neural Networks",
        "authors": [
            "Yacine Jernite",
            "Edouard Grave",
            "Armand Joulin",
            "Tomas Mikolov"
        ],
        "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2017-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06322",
        "title": "Spotting Rumors via Novelty Detection",
        "authors": [
            "Yumeng Qin",
            "Dominik Wurzer",
            "Victor Lavrenko",
            "Cunchen Tang"
        ],
        "abstract": "Rumour detection is hard because the most accurate systems operate retrospectively, only recognizing rumours once they have collected repeated signals. By then the rumours might have already spread and caused harm. We introduce a new category of features based on novelty, tailored to detect rumours early on. To compensate for the absence of repeated signals, we make use of news wire as an additional data source. Unconfirmed (novel) information with respect to the news articles is considered as an indication of rumours. Additionally we introduce pseudo feedback, which assumes that documents that are similar to previous rumours, are more likely to also be a rumour. Comparison with other real-time approaches shows that novelty based features in conjunction with pseudo feedback perform significantly better, when detecting rumours instantly after their publication.\n    ",
        "submission_date": "2016-11-19T00:00:00",
        "last_modified_date": "2016-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06459",
        "title": "Gendered Conversation in a Social Game-Streaming Platform",
        "authors": [
            "Supun Nakandala",
            "Giovanni Luca Ciampaglia",
            "Norman Makoto Su",
            "Yong-Yeol Ahn"
        ],
        "abstract": "Online social media and games are increasingly replacing offline social activities. Social media is now an indispensable mode of communication; online gaming is not only a genuine social activity but also a popular spectator sport. With support for anonymity and larger audiences, online interaction shrinks social and geographical barriers. Despite such benefits, social disparities such as gender inequality persist in online social media. In particular, online gaming communities have been criticized for persistent gender disparities and objectification. As gaming evolves into a social platform, persistence of gender disparity is a pressing question. Yet, there are few large-scale, systematic studies of gender inequality and objectification in social gaming platforms. Here we analyze more than one billion chat messages from Twitch, a social game-streaming platform, to study how the gender of streamers is associated with the nature of conversation. Using a combination of computational text analysis methods, we show that gendered conversation and objectification is prevalent in chats. Female streamers receive significantly more objectifying comments while male streamers receive more game-related comments. This difference is more pronounced for popular streamers. There also exists a large number of users who post only on female or male streams. Employing a neural vector-space embedding (paragraph vector) method, we analyze gendered chat messages and create prediction models that (i) identify the gender of streamers based on messages posted in the channel and (ii) identify the gender a viewer prefers to watch based on their chat messages. Our findings suggest that disparities in social game-streaming platforms is a nuanced phenomenon that involves the gender of streamers as well as those who produce gendered and game-related conversation.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06468",
        "title": "Generating machine-executable plans from end-user's natural-language instructions",
        "authors": [
            "Rui Liu",
            "Xiaoli Zhang"
        ],
        "abstract": "It is critical for advanced manufacturing machines to autonomously execute a task by following an end-user's natural language (NL) instructions. However, NL instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task. To address this NL-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user's NL instructions, we developed a Machine-Executable-Plan-Generation (exePlan) method. The exePlan method conducts task-centered semantic analysis to extract task-related information from ambiguous NL instructions. In addition, the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract NL instructions. To evaluate the exePlan method, an industrial robot Baxter was instructed by NL to perform three types of industrial tasks {'drill a hole', 'clean a spot', 'install a screw'}. The experiment results proved that the exePlan method was effective in generating machine-executable plans from the end-user's NL instructions. Such a method has the promise to endow a machine with the ability of NL-instructed task execution.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06492",
        "title": "Recurrent Memory Addressing for describing videos",
        "authors": [
            "Arnav Kumar Jain",
            "Abhinav Agarwalla",
            "Kumar Krishna Agrawal",
            "Pabitra Mitra"
        ],
        "abstract": "In this paper, we introduce Key-Value Memory Networks to a multimodal setting and a novel key-addressing mechanism to deal with sequence-to-sequence models. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. We propose to find the next step attention weights conditioned on the previous attention distributions for the key-value memory slots in the memory addressing schema. Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Experiments done on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06607",
        "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs",
        "authors": [
            "Jonathan Krause",
            "Justin Johnson",
            "Ranjay Krishna",
            "Li Fei-Fei"
        ],
        "abstract": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2016-11-20T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06933",
        "title": "Unsupervised Learning for Lexicon-Based Classification",
        "authors": [
            "Jacob Eisenstein"
        ],
        "abstract": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.06950",
        "title": "Statistical Learning for OCR Text Correction",
        "authors": [
            "Jie Mei",
            "Aminul Islam",
            "Yajing Wu",
            "Abidalrahman Moh'd",
            "Evangelos E. Milios"
        ],
        "abstract": "The accuracy of Optical Character Recognition (OCR) is crucial to the success of subsequent applications used in text analyzing pipeline. Recent models of OCR post-processing significantly improve the quality of OCR-generated text, but are still prone to suggest correction candidates from limited observations while insufficiently accounting for the characteristics of OCR errors. In this paper, we show how to enlarge candidate suggestion space by using external corpus and integrating OCR-specific features in a regression approach to correct OCR-generated errors. The evaluation results show that our model can correct 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of the OCR-errors (considering the top 3 suggestions), for cases where the theoretical correction upper-bound is 78%.\n    ",
        "submission_date": "2016-11-21T00:00:00",
        "last_modified_date": "2016-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07139",
        "title": "A Natural Language Query Interface for Searching Personal Information on Smartwatches",
        "authors": [
            "Reza Rawassizadeh",
            "Chelsea Dobbins",
            "Manouchehr Nourizadeh",
            "Zahra Ghamchili",
            "Michael Pazzani"
        ],
        "abstract": "Currently, personal assistant systems, run on smartphones and use natural language interfaces. However, these systems rely mostly on the web for finding information. Mobile and wearable devices can collect an enormous amount of contextual personal data such as sleep and physical activities. These information objects and their applications are known as quantified-self, mobile health or personal informatics, and they can be used to provide a deeper insight into our behavior. To our knowledge, existing personal assistant systems do not support all types of quantified-self queries. In response to this, we have undertaken a user study to analyze a set of \"textual questions/queries\" that users have used to search their quantified-self or mobile health data. Through analyzing these questions, we have constructed a light-weight natural language based query interface, including a text parser algorithm and a user interface, to process the users' queries that have been used for searching quantified-self information. This query interface has been designed to operate on small devices, i.e. smartwatches, as well as augmenting the personal assistant systems by allowing them to process end users' natural language queries about their quantified-self data.\n    ",
        "submission_date": "2016-11-22T00:00:00",
        "last_modified_date": "2016-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.07837",
        "title": "Adaptive Feature Abstraction for Translating Video to Text",
        "authors": [
            "Yunchen Pu",
            "Martin Renqiang Min",
            "Zhe Gan",
            "Lawrence Carin"
        ],
        "abstract": "Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video features. However, the variable context-dependent semantics in the video may make it more appropriate to adaptively select features from the multiple CNN layers. We propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task. A novel attention mechanism is developed, that adaptively and sequentially focuses on different layers of CNN features (levels of feature \"abstraction\"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08002",
        "title": "Semantic Compositional Networks for Visual Captioning",
        "authors": [
            "Zhe Gan",
            "Chuang Gan",
            "Xiaodong He",
            "Yunchen Pu",
            "Kenneth Tran",
            "Jianfeng Gao",
            "Lawrence Carin",
            "Li Deng"
        ],
        "abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.\n    ",
        "submission_date": "2016-11-23T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08096",
        "title": "User Personalized Satisfaction Prediction via Multiple Instance Deep Learning",
        "authors": [
            "Zheqian Chen",
            "Ben Gao",
            "Huimin Zhang",
            "Zhou Zhao",
            "Deng Cai"
        ],
        "abstract": "Community based question answering services have arisen as a popular knowledge sharing pattern for netizens. With abundant interactions among users, individuals are capable of obtaining satisfactory information. However, it is not effective for users to attain answers within minutes. Users have to check the progress over time until the satisfying answers submitted. We address this problem as a user personalized satisfaction prediction task. Existing methods usually exploit manual feature selection. It is not desirable as it requires careful design and is labor intensive. In this paper, we settle this issue by developing a new multiple instance deep learning framework. Specifically, in our settings, each question follows a weakly supervised learning multiple instance learning assumption, where its obtained answers can be regarded as instance sets and we define the question resolved with at least one satisfactory answer. We thus design an efficient framework exploiting multiple instance learning property with deep learning to model the question answer pairs. Extensive experiments on large scale datasets from Stack Exchange demonstrate the feasibility of our proposed framework in predicting askers personalized satisfaction. Our framework can be extended to numerous applications such as UI satisfaction Prediction, multi armed bandit problem, expert finding and so on.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08135",
        "title": "Question Retrieval for Community-based Question Answering via Heterogeneous Network Integration Learning",
        "authors": [
            "Zheqian Chen",
            "Chi Zhang",
            "Zhou Zhao",
            "Deng Cai"
        ],
        "abstract": "Community based question answering platforms have attracted substantial users to share knowledge and learn from each other. As the rapid enlargement of CQA platforms, quantities of overlapped questions emerge, which makes users confounded to select a proper reference. It is urgent for us to take effective automated algorithms to reuse historical questions with corresponding answers. In this paper we focus on the problem with question retrieval, which aims to match historical questions that are relevant or semantically equivalent to resolve one s query directly. The challenges in this task are the lexical gaps between questions for the word ambiguity and word mismatch problem. Furthermore, limited words in queried sentences cause sparsity of word features. To alleviate these challenges, we propose a novel framework named HNIL which encodes not only the question contents but also the askers social interactions to enhance the question embedding performance. More specifically, we apply random walk based learning method with recurrent neural network to match the similarities between askers question and historical questions proposed by other users. Extensive experiments on a large scale dataset from a real world CQA site show that employing the heterogeneous social network information outperforms the other state of the art solutions in this task.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08307",
        "title": "Learning Python Code Suggestion with a Sparse Pointer Network",
        "authors": [
            "Avishkar Bhoopchand",
            "Tim Rockt\u00e4schel",
            "Earl Barr",
            "Sebastian Riedel"
        ],
        "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.\n    ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08321",
        "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images",
        "authors": [
            "Junhua Mao",
            "Jiajing Xu",
            "Yushi Jing",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest. This dataset is more than 200 times larger than MS COCO, the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: ",
        "submission_date": "2016-11-24T00:00:00",
        "last_modified_date": "2016-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08373",
        "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction",
        "authors": [
            "Raghavendra Chalapathy",
            "Ehsan Zare Borzeshi",
            "Massimo Piccardi"
        ],
        "abstract": "Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.\n    ",
        "submission_date": "2016-11-25T00:00:00",
        "last_modified_date": "2016-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08669",
        "title": "Visual Dialog",
        "authors": [
            "Abhishek Das",
            "Satwik Kottur",
            "Khushi Gupta",
            "Avi Singh",
            "Deshraj Yadav",
            "Jos\u00e9 M. F. Moura",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.\n",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2017-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08675",
        "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems",
        "authors": [
            "Heriberto Cuay\u00e1huitl",
            "Seunghak Yu",
            "Ashley Williamson",
            "Jacob Carse"
        ],
        "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning---termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08737",
        "title": "Structural Correspondence Learning for Cross-lingual Sentiment Classification with One-to-many Mappings",
        "authors": [
            "Nana Li",
            "Shuangfei Zhai",
            "Zhongfei Zhang",
            "Boying Liu"
        ],
        "abstract": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.\n    ",
        "submission_date": "2016-11-26T00:00:00",
        "last_modified_date": "2016-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.08928",
        "title": "A theory of interpretive clustering in free recall",
        "authors": [
            "Francesco Fumarola"
        ],
        "abstract": "A stochastic model of short-term verbal memory is proposed, in which the psychological state of the subject is encoded as the instantaneous position of a particle diffusing over a semantic graph with a probabilistic structure. The model is particularly suitable for studying the dependence of free-recall observables on semantic properties of the words to be recalled. Besides predicting some well-known experimental features (contiguity effect, forward asymmetry, word-length effect), a novel prediction is obtained on the relationship between the contiguity effect and the syllabic length of words; shorter words, by way of their wider semantic range, are predicted to be characterized by stronger forward contiguity. A fresh analysis of archival data allows to confirm this prediction.\n    ",
        "submission_date": "2016-11-27T00:00:00",
        "last_modified_date": "2017-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09028",
        "title": "Analyzing Features for the Detection of Happy Endings in German Novels",
        "authors": [
            "Fotis Jannidis",
            "Isabella Reger",
            "Albin Zehe",
            "Martin Becker",
            "Lena Hettinger",
            "Andreas Hotho"
        ],
        "abstract": "With regard to a computational representation of literary plot, this paper looks at the use of sentiment analysis for happy ending detection in German novels. Its focus lies on the investigation of previously proposed sentiment features in order to gain insight about the relevance of specific features on the one hand and the implications of their performance on the other hand. Therefore, we study various partitionings of novels, considering the highly variable concept of \"ending\". We also show that our approach, even though still rather simple, can potentially lead to substantial findings relevant to literary studies.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09122",
        "title": "Statistical Properties of European Languages and Voynich Manuscript Analysis",
        "authors": [
            "Andronik Arutyunov",
            "Leonid Borisov",
            "Sergey Fedorov",
            "Anastasiya Ivchenko",
            "Elizabeth Kirina-Lilinskaya",
            "Yurii Orlov",
            "Konstantin Osminin",
            "Sergey Shilin",
            "Dmitriy Zeniuk"
        ],
        "abstract": "The statistical properties of letters frequencies in European literature texts are investigated. The determination of logarithmic dependence of letters sequence for one-language and two-language texts are examined. The pare of languages is suggested for Voynich Manuscript. The internal structure of Manuscript is considered. The spectral portraits of two-letters distribution are constructed.\n    ",
        "submission_date": "2016-11-18T00:00:00",
        "last_modified_date": "2016-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09434",
        "title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability",
        "authors": [
            "Jakob N. Foerster",
            "Justin Gilmer",
            "Jan Chorowski",
            "Jascha Sohl-Dickstein",
            "David Sussillo"
        ],
        "abstract": "There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.\n    ",
        "submission_date": "2016-11-28T00:00:00",
        "last_modified_date": "2017-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09534",
        "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce",
        "authors": [
            "Tom Zahavy",
            "Alessandro Magnani",
            "Abhinandan Krishnan",
            "Shie Mannor"
        ],
        "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09573",
        "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling",
        "authors": [
            "V. S. Anoop",
            "S. Asharaf",
            "P. Deepak"
        ],
        "abstract": "With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \"is-a\" relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09823",
        "title": "Dialogue Learning With Human-In-The-Loop",
        "authors": [
            "Jiwei Li",
            "Alexander H. Miller",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Jason Weston"
        ],
        "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2017-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1611.09921",
        "title": "Less is More: Learning Prominent and Diverse Topics for Data Summarization",
        "authors": [
            "Jian Tang",
            "Cheng Li",
            "Ming Zhang",
            "Qiaozhu Mei"
        ],
        "abstract": "Statistical topic models efficiently facilitate the exploration of large-scale data sets. Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities. However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data. Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge. We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted. With this reinforced random walk as a general process embedded in classical topic models, we obtain \\textit{diverse topic models} that are able to extract the most prominent and diverse topics from data. The inference procedures of these diverse topic models remain as simple and efficient as the classical models. Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.\n    ",
        "submission_date": "2016-11-29T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00227",
        "title": "On Coreferring Text-extracted Event Descriptions with the aid of Ontological Reasoning",
        "authors": [
            "Stefano Borgo",
            "Loris Bozzato",
            "Alessio Palmero Aprosio",
            "Marco Rospocher",
            "Luciano Serafini"
        ],
        "abstract": "Systems for automatic extraction of semantic information about events from large textual resources are now available: these tools are capable to generate RDF datasets about text extracted events and this knowledge can be used to reason over the recognized events. On the other hand, text based tasks for event recognition, as for example event coreference (i.e. recognizing whether two textual descriptions refer to the same event), do not take into account ontological information of the extracted events in their process. In this paper, we propose a method to derive event coreference on text extracted event data using semantic based rule reasoning. We demonstrate our method considering a limited (yet representative) set of event types: we introduce a formal analysis on their ontological properties and, on the base of this, we define a set of coreference criteria. We then implement these criteria as RDF-based reasoning rules to be applied on text extracted event data. We evaluate the effectiveness of our approach over a standard coreference benchmark dataset.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2016-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00370",
        "title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr",
        "authors": [
            "Siqi Liu",
            "Zhenhai Zhu",
            "Ning Ye",
            "Sergio Guadarrama",
            "Kevin Murphy"
        ],
        "abstract": "Current image captioning methods are usually trained via (penalized) maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2018-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00385",
        "title": "Temporal Attention-Gated Model for Robust Sequence Classification",
        "authors": [
            "Wenjie Pei",
            "Tadas Baltru\u0161aitis",
            "David M.J. Tax",
            "Louis-Philippe Morency"
        ],
        "abstract": "Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.\n    ",
        "submission_date": "2016-12-01T00:00:00",
        "last_modified_date": "2017-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00837",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "authors": [
            "Yash Goyal",
            "Tejas Khot",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.\n",
        "submission_date": "2016-12-02T00:00:00",
        "last_modified_date": "2017-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.00944",
        "title": "Using Discourse Signals for Robust Instructor Intervention Prediction",
        "authors": [
            "Muthu Kumar Chandrasekaran",
            "Carrie Demmans Epp",
            "Min-Yen Kan",
            "Diane Litman"
        ],
        "abstract": "We tackle the prediction of instructor intervention in student posts from discussion forums in Massive Open Online Courses (MOOCs). Our key finding is that using automatically obtained discourse relations improves the prediction of when instructors intervene in student discussions, when compared with a state-of-the-art, feature-rich baseline. Our supervised classifier makes use of an automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags that represent in-post discourse features. We show PDTB relation-based features increase the robustness of the classifier and complement baseline features in recalling more diverse instructor intervention patterns. In comprehensive experiments over 14 MOOC offerings from several disciplines, the PDTB discourse features improve performance on average. The resultant models are less dependent on domain-specific vocabulary, allowing them to better generalize to new courses.\n    ",
        "submission_date": "2016-12-03T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.01892",
        "title": "Cross-Lingual Predicate Mapping Between Linked Data Ontologies",
        "authors": [
            "Gautam Singh",
            "Saemi Jang",
            "Mun Y. Yi"
        ],
        "abstract": "Ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links. This difference is markedly visible when comparing a rich English language ontology with a non-English language counterpart. Discovering alignment between them is a useful endeavor as it serves as a starting point in bridging the disparity. In particular, our work is motivated by the absence of inter-language links for predicates in the localised versions of DBpedia. In this paper, we propose and demonstrate an ad-hoc system to find possible owl:equivalentProperty links between predicates in ontologies of different natural languages. We seek to achieve this mapping by using pre-existing inter-language links of the resources connected by the given predicate. Thus, our methodology stresses on semantic similarity rather than lexical. Moreover, through an evaluation, we show that our system is capable of outperforming a baseline system that is similar to the one used in recent OAEI campaigns.\n    ",
        "submission_date": "2016-12-06T00:00:00",
        "last_modified_date": "2016-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02695",
        "title": "Towards better decoding and language model integration in sequence to sequence models",
        "authors": [
            "Jan Chorowski",
            "Navdeep Jaitly"
        ],
        "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2016-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.02741",
        "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries",
        "authors": [
            "Lili Mou",
            "Zhengdong Lu",
            "Hang Li",
            "Zhi Jin"
        ],
        "abstract": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.\n    ",
        "submission_date": "2016-12-08T00:00:00",
        "last_modified_date": "2017-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03231",
        "title": "A natural language interface to a graph-based bibliographic information retrieval system",
        "authors": [
            "Yongjun Zhu",
            "Erjia Yan",
            "Il-Yeol Song"
        ],
        "abstract": "With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. NLI-GIBIR allows users to search for a variety of bibliographic data through natural language. A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis. We find that our framework can effectively represents and addresses complex bibliographic information needs. Thus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrieval. Second, we propose a novel customized natural language processing framework that integrates a few original algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. Third, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systems. Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities.\n    ",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03277",
        "title": "Data Curation APIs",
        "authors": [
            "Seyed-Mehdi-Reza Beheshti",
            "Alireza Tabebordbar",
            "Boualem Benatallah",
            "Reza Nouri"
        ],
        "abstract": "Understanding and analyzing big data is firmly recognized as a powerful and strategic priority. For deeper interpretation of and better intelligence with big data, it is important to transform raw data (unstructured, semi-structured and structured data sources, e.g., text, video, image data sets) into curated data: contextualized data and knowledge that is maintained and made available for use by end-users and applications. In particular, data curation acts as the glue between raw data and analytics, providing an abstraction layer that relieves users from time consuming, tedious and error prone curation tasks. In this context, the data curation process becomes a vital analytics asset for increasing added value and insights.\n",
        "submission_date": "2016-12-10T00:00:00",
        "last_modified_date": "2016-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03494",
        "title": "Flu Detector: Estimating influenza-like illness rates from online user-generated content",
        "authors": [
            "Vasileios Lampos"
        ],
        "abstract": "We provide a brief technical description of an online platform for disease monitoring, titled as the Flu Detector (",
        "submission_date": "2016-12-11T00:00:00",
        "last_modified_date": "2016-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03597",
        "title": "Search Personalization with Embeddings",
        "authors": [
            "Thanh Vu",
            "Dat Quoc Nguyen",
            "Mark Johnson",
            "Dawei Song",
            "Alistair Willis"
        ],
        "abstract": "Recent research has shown that the performance of search personalization depends on the richness of user profiles which normally represent the user's topical interests. In this paper, we propose a new embedding approach to learning user profiles, where users are embedded on a topical interest space. We then directly utilize the user profiles for search personalization. Experiments on query logs from a major commercial web search engine demonstrate that our embedding approach improves the performance of the search engine and also achieves better search performance than other strong baselines.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.03628",
        "title": "VIBIKNet: Visual Bidirectional Kernelized Network for Visual Question Answering",
        "authors": [
            "Marc Bola\u00f1os",
            "\u00c1lvaro Peris",
            "Francisco Casacuberta",
            "Petia Radeva"
        ],
        "abstract": "In this paper, we address the problem of visual question answering by proposing a novel model, called VIBIKNet. Our model is based on integrating Kernelized Convolutional Neural Networks and Long-Short Term Memory units to generate an answer given a question about an image. We prove that VIBIKNet is an optimal trade-off between accuracy and computational load, in terms of memory and time consumption. We validate our method on the VQA challenge dataset and compare it to the top performing methods in order to illustrate its performance and speed.\n    ",
        "submission_date": "2016-12-12T00:00:00",
        "last_modified_date": "2016-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04061",
        "title": "Learning to Hash-tag Videos with Tag2Vec",
        "authors": [
            "Aditya Singh",
            "Saurabh Saini",
            "Rajvi Shah",
            "PJ Narayanan"
        ],
        "abstract": "User-given tags or labels are valuable resources for semantic understanding of visual media such as images and videos. Recently, a new type of labeling mechanism known as hash-tags have become increasingly popular on social media sites. In this paper, we study the problem of generating relevant and useful hash-tags for short video clips. Traditional data-driven approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We attempt to learn a direct low-cost mapping from video to hash-tags using a two step training process. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of 10 million hash-tags. We then train an embedding function to map video features to the low-dimensional Tag2vec space. We learn this embedding for 29 categories of short video clips with hash-tags. A query video without any tag-information can then be directly mapped to the vector space of tags using the learned embedding and relevant tags can be found by performing a simple nearest-neighbor retrieval in the Tag2Vec space. We validate the relevance of the tags suggested by our system qualitatively and quantitatively with a user study.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04403",
        "title": "You Are What You Eat... Listen to, Watch, and Read",
        "authors": [
            "Mason Bretan"
        ],
        "abstract": "This article describes a data driven method for deriving the relationship between personality and media preferences. A qunatifiable representation of such a relationship can be leveraged for use in recommendation systems and ameliorate the \"cold start\" problem. Here, the data is comprised of an original collection of 1,316 Okcupid dating profiles. Of these profiles, 800 are labeled with one of 16 possible Myers-Briggs Type Indicators (MBTI). A personality specific topic model describing a person's favorite books, movies, shows, music, and food was generated using latent Dirichlet allocation (LDA). There were several significant findings, for example, intuitive thinking types preferred sci-fi/fantasy entertainment, extraversion correlated positively with upbeat dance music, and jazz, folk, and international cuisine correlated positively with those characterized by openness to experience. Many other correlations confirmed previous findings describing the relationship among personality, writing style, and personal preferences. (For complete word/personality type assocations see the Appendix).\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04418",
        "title": "User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation",
        "authors": [
            "Alexey Drutsa",
            "Andrey Shutovich",
            "Philipp Pushnyakov",
            "Evgeniy Krokhalyov",
            "Gleb Gusev",
            "Pavel Serdyukov"
        ],
        "abstract": "Despite the growing importance of multilingual aspect of web search, no appropriate offline metrics to evaluate its quality are proposed so far. At the same time, personal language preferences can be regarded as intents of a query. This approach translates the multilingual search problem into a particular task of search diversification. Furthermore, the standard intent-aware approach could be adopted to build a diversified metric for multilingual search on the basis of a classical IR metric such as ERR. The intent-aware approach estimates user satisfaction under a user behavior model. We show however that the underlying user behavior models is not realistic in the multilingual case, and the produced intent-aware metric do not appropriately estimate the user satisfaction. We develop a novel approach to build intent-aware user behavior models, which overcome these limitations and convert to quality metrics that better correlate with standard online metrics of user satisfaction.\n    ",
        "submission_date": "2016-12-13T00:00:00",
        "last_modified_date": "2016-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04757",
        "title": "Attentive Explanations: Justifying Decisions and Pointing to the Evidence",
        "authors": [
            "Dong Huk Park",
            "Lisa Anne Hendricks",
            "Zeynep Akata",
            "Bernt Schiele",
            "Trevor Darrell",
            "Marcus Rohrbach"
        ],
        "abstract": "Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.\n    ",
        "submission_date": "2016-12-14T00:00:00",
        "last_modified_date": "2017-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.04949",
        "title": "Recurrent Image Captioner: Describing Images with Spatial-Invariant Transformation and Attention Filtering",
        "authors": [
            "Hao Liu",
            "Yang Yang",
            "Fumin Shen",
            "Lixin Duan",
            "Heng Tao Shen"
        ],
        "abstract": "Along with the prosperity of recurrent neural network in modelling sequential data and the power of attention mechanism in automatically identify salient information, image captioning, a.k.a., image description, has been remarkably advanced in recent years. Nonetheless, most existing paradigms may suffer from the deficiency of invariance to images with different scaling, rotation, etc.; and effective integration of standalone attention to form a holistic end-to-end system. In this paper, we propose a novel image captioning architecture, termed Recurrent Image Captioner (\\textbf{RIC}), which allows visual encoder and language decoder to coherently cooperate in a recurrent manner. Specifically, we first equip CNN-based visual encoder with a differentiable layer to enable spatially invariant transformation of visual signals. Moreover, we deploy an attention filter module (differentiable) between encoder and decoder to dynamically determine salient visual parts. We also employ bidirectional LSTM to preprocess sentences for generating better textual representations. Besides, we propose to exploit variational inference to optimize the whole architecture. Extensive experimental results on three benchmark datasets (i.e., Flickr8k, Flickr30k and MS COCO) demonstrate the superiority of our proposed architecture as compared to most of the state-of-the-art methods.\n    ",
        "submission_date": "2016-12-15T00:00:00",
        "last_modified_date": "2016-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05348",
        "title": "Machine Reading with Background Knowledge",
        "authors": [
            "Ndapandula Nakashole",
            "Tom M. Mitchell"
        ],
        "abstract": "Intelligent systems capable of automatically understanding natural language text are important for many artificial intelligence applications including mobile phone voice assistants, computer vision, and robotics. Understanding language often constitutes fitting new information into a previously acquired view of the world. However, many machine reading systems rely on the text alone to infer its meaning. In this paper, we pursue a different approach; machine reading methods that make use of background knowledge to facilitate language understanding. To this end, we have developed two methods: The first method addresses prepositional phrase attachment ambiguity. It uses background knowledge within a semi-supervised machine learning algorithm that learns from both labeled and unlabeled data. This approach yields state-of-the-art results on two datasets against strong baselines; The second method extracts relationships from compound nouns. Our knowledge-aware method for compound noun analysis accurately extracts relationships and significantly outperforms a baseline that does not make use of background knowledge.\n    ",
        "submission_date": "2016-12-16T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.05688",
        "title": "A User Simulator for Task-Completion Dialogues",
        "authors": [
            "Xiujun Li",
            "Zachary C. Lipton",
            "Bhuwan Dhingra",
            "Lihong Li",
            "Jianfeng Gao",
            "Yun-Nung Chen"
        ],
        "abstract": "Despite widespread interests in reinforcement-learning for task-oriented dialogue systems, several obstacles can frustrate research and development progress. First, reinforcement learners typically require interaction with the environment, so conventional dialogue corpora cannot be used directly. Second, each task presents specific challenges, requiring separate corpus of task-specific annotated data. Third, collecting and annotating human-machine or human-human conversations for task-oriented dialogues requires extensive domain knowledge. Because building an appropriate dataset can be both financially costly and time-consuming, one popular approach is to build a user simulator based upon a corpus of example dialogues. Then, one can train reinforcement learning agents in an online fashion as they interact with the simulator. Dialogue agents trained on these simulators can serve as an effective starting point. Once agents master the simulator, they may be deployed in a real environment to interact with humans, and continue to be trained online. To ease empirical algorithmic comparisons in dialogues, this paper introduces a new, publicly available simulation framework, where our simulator, designed for the movie-booking domain, leverages both rules and collected data. The simulator supports two tasks: movie ticket booking and movie seeking. Finally, we demonstrate several agents and detail the procedure to add and test your own agent in the proposed framework.\n    ",
        "submission_date": "2016-12-17T00:00:00",
        "last_modified_date": "2017-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06212",
        "title": "A recurrent neural network without chaos",
        "authors": [
            "Thomas Laurent",
            "James von Brecht"
        ],
        "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06391",
        "title": "Talk it up or play it down? (Un)expected correlations between (de-)emphasis and recurrence of discussion points in consequential U.S. economic policy meetings",
        "authors": [
            "Chenhao Tan",
            "Lillian Lee"
        ],
        "abstract": "In meetings where important decisions get made, what items receive more attention may influence the outcome. We examine how different types of rhetorical (de-)emphasis -- including hedges, superlatives, and contrastive conjunctions -- correlate with what gets revisited later, controlling for item frequency and speaker. Our data consists of transcripts of recurring meetings of the Federal Reserve's Open Market Committee (FOMC), where important aspects of U.S. monetary policy are decided on. Surprisingly, we find that words appearing in the context of hedging, which is usually considered a way to express uncertainty, are more likely to be repeated in subsequent meetings, while strong emphasis indicated by superlatives has a slightly negative effect on word recurrence in subsequent meetings. We also observe interesting patterns in how these effects vary depending on social factors such as status and gender of the speaker. For instance, the positive effects of hedging are more pronounced for female speakers than for male speakers.\n    ",
        "submission_date": "2016-12-19T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06530",
        "title": "Automatic Generation of Grounded Visual Questions",
        "authors": [
            "Shijie Zhang",
            "Lizhen Qu",
            "Shaodi You",
            "Zhenglu Yang",
            "Jiawan Zhang"
        ],
        "abstract": "In this paper, we propose the first model to be able to generate visually grounded questions with diverse types for a single image. Visual question generation is an emerging topic which aims to ask questions in natural language based on visual input. To the best of our knowledge, it lacks automatic methods to generate meaningful questions with various types for the same visual input. To circumvent the problem, we propose a model that automatically generates visually grounded questions with varying types. Our model takes as input both images and the captions generated by a dense caption model, samples the most probable question types, and generates the questions in sequel. The experimental results on two real world datasets show that our model outperforms the strongest baseline in terms of both correctness and diversity with a wide margin.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.06890",
        "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
        "authors": [
            "Justin Johnson",
            "Bharath Hariharan",
            "Laurens van der Maaten",
            "Li Fei-Fei",
            "C. Lawrence Zitnick",
            "Ross Girshick"
        ],
        "abstract": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.\n    ",
        "submission_date": "2016-12-20T00:00:00",
        "last_modified_date": "2016-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07040",
        "title": "A deep learning approach for predicting the quality of online health expert question-answering services",
        "authors": [
            "Ze Hu",
            "Zhan Zhang",
            "Qing Chen",
            "Haiqin Yang",
            "Decheng Zuo"
        ],
        "abstract": "Currently, a growing number of health consumers are asking health-related questions online, at any time and from anywhere, which effectively lowers the cost of health care. The most common approach is using online health expert question-answering (HQA) services, as health consumers are more willing to trust answers from professional physicians. However, these answers can be of varying quality depending on circumstance. In addition, as the available HQA services grow, how to predict the answer quality of HQA services via machine learning becomes increasingly important and challenging. In an HQA service, answers are normally short texts, which are severely affected by the data sparsity problem. Furthermore, HQA services lack community features such as best answer and user votes. Therefore, the wisdom of the crowd is not available to rate answer quality. To address these problems, in this paper, the prediction of HQA answer quality is defined as a classification task. First, based on the characteristics of HQA services and feedback from medical experts, a standard for HQA service answer quality evaluation is defined. Next, based on the characteristics of HQA services, several novel non-textual features are proposed, including surface linguistic features and social features. Finally, a deep belief network (DBN)-based HQA answer quality prediction framework is proposed to predict the quality of answers by learning the high-level hidden semantic representation from the physicians' answers. Our results prove that the proposed framework overcomes the problem of overly sparse textual features in short text answers and effectively identifies high-quality answers.\n    ",
        "submission_date": "2016-12-21T00:00:00",
        "last_modified_date": "2016-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.07602",
        "title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking",
        "authors": [
            "Hai Ye",
            "Wenhan Chao",
            "Zhunchen Luo",
            "Zhoujun Li"
        ],
        "abstract": "Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.\n    ",
        "submission_date": "2016-12-22T00:00:00",
        "last_modified_date": "2017-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08178",
        "title": "JU_KS_Group@FIRE 2016: Consumer Health Information Search",
        "authors": [
            "Kamal Sarkar",
            "Debanjan Das",
            "Indra Banerjee",
            "Mamta Kumari",
            "Prasenjit Biswas"
        ],
        "abstract": "In this paper, we describe the methodology used and the results obtained by us for completing the tasks given under the shared task on Consumer Health Information Search (CHIS) collocated with the Forum for Information Retrieval Evaluation (FIRE) 2016, ISI Kolkata. The shared task consists of two sub-tasks - (1) task1: given a query and a document/set of documents associated with that query, the task is to classify the sentences in the document as relevant to the query or not and (2) task 2: the relevant sentences need to be further classified as supporting the claim made in the query, or opposing the claim made in the query. We have participated in both the sub-tasks. The percentage accuracy obtained by our developed system for task1 was 73.39 which is third highest among the 9 teams participated in the shared task.\n    ",
        "submission_date": "2016-12-24T00:00:00",
        "last_modified_date": "2016-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08354",
        "title": "Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation",
        "authors": [
            "Gwangbeen Park",
            "Woobin Im"
        ],
        "abstract": "We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.\n    ",
        "submission_date": "2016-12-26T00:00:00",
        "last_modified_date": "2016-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08504",
        "title": "Classifying Patents Based on their Semantic Content",
        "authors": [
            "Antonin Bergeaud",
            "Yoann Potiron",
            "Juste Raimbault"
        ],
        "abstract": "In this paper, we extend some usual techniques of classification resulting from a large-scale data-mining and network approach. This new technology, which in particular is designed to be suitable to big data, is used to construct an open consolidated database from raw data on 4 million patents taken from the US patent office from 1976 onward. To build the pattern network, not only do we look at each patent title, but we also examine their full abstract and extract the relevant keywords accordingly. We refer to this classification as semantic approach in contrast with the more common technological approach which consists in taking the topology when considering US Patent office technological classes. Moreover, we document that both approaches have highly different topological measures and strong statistical evidence that they feature a different model. This suggests that our method is a useful tool to extract endogenous information.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.08543",
        "title": "Distributed Real-Time Sentiment Analysis for Big Data Social Streams",
        "authors": [
            "Amir Hossein Akhavan Rahnama"
        ],
        "abstract": "Big data trend has enforced the data-centric systems to have continuous fast data streams. In recent years, real-time analytics on stream data has formed into a new research field, which aims to answer queries about what-is-happening-now with a negligible delay. The real challenge with real-time stream data processing is that it is impossible to store instances of data, and therefore online analytical algorithms are utilized. To perform real-time analytics, pre-processing of data should be performed in a way that only a short summary of stream is stored in main memory. In addition, due to high speed of arrival, average processing time for each instance of data should be in such a way that incoming instances are not lost without being captured. Lastly, the learner needs to provide high analytical accuracy measures. Sentinel is a distributed system written in Java that aims to solve this challenge by enforcing both the processing and learning process to be done in distributed form. Sentinel is built on top of Apache Storm, a distributed computing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel decision tree-learning algorithm based on the VFDT, with ability of enabling parallel classification in distributed environments. Sentinel also uses SpaceSaving to keep a summary of the data stream and stores its summary in a synopsis data structure. Application of Sentinel on Twitter Public Stream API is shown and the results are discussed.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09268",
        "title": "The ontogeny of discourse structure mimics the development of literature",
        "authors": [
            "Natalia Bezerra Mota",
            "Sylvia Pinheiro",
            "Mariano Sigman",
            "Diego Fernandez Slezak",
            "Guillermo Cecchi",
            "Mauro Copelli",
            "Sidarta Ribeiro"
        ],
        "abstract": "Discourse varies with age, education, psychiatric state and historical epoch, but the ontogenetic and cultural dynamics of discourse structure remain to be quantitatively characterized. To this end we investigated word graphs obtained from verbal reports of 200 subjects ages 2-58, and 676 literary texts spanning ~5,000 years. In healthy subjects, lexical diversity, graph size, and long-range recurrence departed from initial near-random levels through a monotonic asymptotic increase across ages, while short-range recurrence showed a corresponding decrease. These changes were explained by education and suggest a hierarchical development of discourse structure: short-range recurrence and lexical diversity stabilize after elementary school, but graph size and long-range recurrence only stabilize after high school. This gradual maturation was blurred in psychotic subjects, who maintained in adulthood a near-random structure. In literature, monotonic asymptotic changes over time were remarkable: While lexical diversity, long-range recurrence and graph size increased away from near-randomness, short-range recurrence declined, from above to below random levels. Bronze Age texts are structurally similar to childish or psychotic discourses, but subsequent texts converge abruptly to the healthy adult pattern around the onset of the Axial Age (800-200 BC), a period of pivotal cultural change. Thus, individually as well as historically, discourse maturation increases the range of word recurrence away from randomness.\n    ",
        "submission_date": "2016-12-27T00:00:00",
        "last_modified_date": "2016-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09535",
        "title": "PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese",
        "authors": [
            "Concei\u00e7\u00e3o Rocha",
            "Al\u00edpio Jorge",
            "Roberta Sionara",
            "Paula Brito",
            "Carlos Pimenta",
            "Solange Rezende"
        ],
        "abstract": "This paper deals with the entity extraction task (named entity recognition) of a text mining process that aims at unveiling non-trivial semantic structures, such as relationships and interaction between entities or communities. In this paper we present a simple and efficient named entity extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging based algorithm for NER), relies on flexible pattern matching, part-of-speech tagging and lexical-based rules. It was developed to process texts written in Portuguese, however it is potentially applicable to other languages as well.\n",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09542",
        "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions",
        "authors": [
            "Licheng Yu",
            "Hao Tan",
            "Mohit Bansal",
            "Tamara L. Berg"
        ],
        "abstract": "Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2017-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1612.09574",
        "title": "Automatic Data Deformation Analysis on Evolving Folksonomy Driven Environment",
        "authors": [
            "Massimiliano Dal Mas"
        ],
        "abstract": "The Folksodriven framework makes it possible for data scientists to define an ontology environment where searching for buried patterns that have some kind of predictive power to build predictive models more effectively. It accomplishes this through an abstractions that isolate parameters of the predictive modeling process searching for patterns and designing the feature set, too. To reflect the evolving knowledge, this paper considers ontologies based on folksonomies according to a new concept structure called \"Folksodriven\" to represent folksonomies. So, the studies on the transformational regulation of the Folksodriven tags are regarded to be important for adaptive folksonomies classifications in an evolving environment used by Intelligent Systems to represent the knowledge sharing. Folksodriven tags are used to categorize salient data points so they can be fed to a machine-learning system and \"featurizing\" the data.\n    ",
        "submission_date": "2016-12-30T00:00:00",
        "last_modified_date": "2016-12-30T00:00:00"
    }
]