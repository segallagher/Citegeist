[
    {
        "url": "https://arxiv.org/abs/cmp-lg/9501001",
        "title": "Using default inheritance to describe LTAG",
        "authors": [
            "Roger Evans",
            "Gerald Gazdar",
            "David Weir"
        ],
        "abstract": "We present the results of an investigation into how the set of elementary trees of a Lexicalized Tree Adjoining Grammar can be represented in the lexical knowledge representation language DATR (Evans & Gazdar 1989a,b). The LTAG under consideration is based on the one described in Abeille et al. (1990). Our approach is similar to that of Vijay-Shanker & Schabes (1992) in that we formulate an inheritance hierarchy that efficiently encodes the elementary trees. However, rather than creating a new representation formalism for this task, we employ techniques of established utility in other lexically-oriented frameworks. In particular, we show how DATR's default mechanism can be used to eliminate the need for a non-immediate dominance relation in the descriptions of the surface LTAG entries. This allows us to embed the tree structures in the feature theory in a manner reminiscent of HPSG subcategorisation frames, and hence express lexical rules as relations over feature structures.\n    ",
        "submission_date": "1995-01-09T00:00:00",
        "last_modified_date": "1995-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9501002",
        "title": "NL Understanding with a Grammar of Constructions",
        "authors": [
            "Wlodek Zadrozny",
            "Marcin Szummer",
            "Stanislaw Jarecki",
            "David E. Johnson",
            "Leora Morgenstern"
        ],
        "abstract": "We present an approach to natural language understanding based on a computable grammar of constructions. A \"construction\" consists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key element of Mincal, an implemented natural language, speech-enabled interface to an on-line calendar system. The system consists of a NL grammar, a parser, an on-line calendar, a domain knowledge base (about dates, times and meetings), an application knowledge base (about the calendar), a speech recognizer, a speech generator, and the interfaces between those modules. We claim that this architecture should work in general for spoken interfaces in small domains. In this paper we present two novel aspects of the architecture: (a) the use of constructions, integrating descriptions of form, meaning and context into one whole; and (b) the separation of domain knowledge from application knowledge. We describe the data structures for encoding constructions, the structure of the knowledge bases, and the interactions of the key modules of the system.\n    ",
        "submission_date": "1995-01-17T00:00:00",
        "last_modified_date": "1995-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9501003",
        "title": "An HPSG Parser Based on Description Logics",
        "authors": [
            "J. Joachim Quantz"
        ],
        "abstract": "In this paper I present a parser based on Description Logics (DL) for a German HPSG -style fragment. The specified parser relies mainly on the inferential capabilities of the underlying DL system. Given a preferential default extension for DL disambiguation is achieved by choosing the parse containing a qualitatively minimal number of exceptions.\n    ",
        "submission_date": "1995-01-18T00:00:00",
        "last_modified_date": "1995-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9501004",
        "title": "Lexical Knowledge Representation in an Intelligent Dictionary Help System",
        "authors": [
            "E. Agirre",
            "X. Arregi",
            "X. Artola",
            "A. Diaz de Ilarraza",
            "K. Sarasola"
        ],
        "abstract": "The frame-based knowledge representation model adopted in IDHS (Intelligent Dictionary Help System) is described in this paper. It is used to represent the lexical knowledge acquired automatically from a conventional dictionary. Moreover, the enrichment processes that have been performed on the Dictionary Knowledge Base and the dynamic exploitation of this knowledge - both based on the exploitation of the properties of lexical semantic relations - are also described.\n    ",
        "submission_date": "1995-01-30T00:00:00",
        "last_modified_date": "1995-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9501005",
        "title": "A Tool for Collecting Domain Dependent Sortal Constraints From Corpora",
        "authors": [
            "Francois Andry",
            "Mark Gawron",
            "John Dowding",
            "Robert Moore"
        ],
        "abstract": "  In this paper, we describe a tool designed to generate semi-automatically the sortal constraints specific to a domain to be used in a natural language (NL) understanding system. This tool is evaluated using the SRI Gemini NL understanding system in the ATIS domain.\n    ",
        "submission_date": "1995-01-31T00:00:00",
        "last_modified_date": "1995-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502001",
        "title": "Interlingual Lexical Organisation for Multilingual Lexical Databases in NADIA",
        "authors": [
            "Gilles Serasset"
        ],
        "abstract": "  We propose a lexical organisation for multilingual lexical databases (MLDB). This organisation is based on acceptions (word-senses). We detail this lexical organisation and show a mock-up built to experiment with it. We also present our current work in defining and prototyping a specialised system for the management of acception-based MLDB. Keywords: multilingual lexical database, acception, linguistic structure.\n    ",
        "submission_date": "1995-02-02T00:00:00",
        "last_modified_date": "1995-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502002",
        "title": "Learning Unification-Based Natural Language Grammars",
        "authors": [
            "Miles Osborne"
        ],
        "abstract": "When parsing unrestricted language, wide-covering grammars often undergenerate. Undergeneration can be tackled either by sentence correction, or by grammar correction. This thesis concentrates upon automatic grammar correction (or machine learning of grammar) as a solution to the problem of undergeneration. Broadly speaking, grammar correction approaches can be classified as being either {\\it data-driven}, or {\\it model-based}. Data-driven learners use data-intensive methods to acquire grammar. They typically use grammar formalisms unsuited to the needs of practical text processing and cannot guarantee that the resulting grammar is adequate for subsequent semantic interpretation. That is, data-driven learners acquire grammars that generate strings that humans would judge to be grammatically ill-formed (they {\\it overgenerate}) and fail to assign linguistically plausible parses. Model-based learners are knowledge-intensive and are reliant for success upon the completeness of a {\\it model of grammaticality}. But in practice, the model will be incomplete. Given that in this thesis we deal with undergeneration by learning, we hypothesise that the combined use of data-driven and model-based learning would allow data-driven learning to compensate for model-based learning's incompleteness, whilst model-based learning would compensate for data-driven learning's unsoundness. We describe a system that we have used to test the hypothesis empirically. The system combines data-driven and model-based learning to acquire unification-based grammars that are more suitable for practical text parsing. Using the Spoken English Corpus as data, and by quantitatively measuring undergeneration, overgeneration and parse plausibility, we show that this hypothesis is correct.\n    ",
        "submission_date": "1995-02-03T00:00:00",
        "last_modified_date": "1995-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502003",
        "title": "ProFIT: Prolog with Features, Inheritance and Templates",
        "authors": [
            "Gregor Erbach"
        ],
        "abstract": "ProFIT is an extension of Standard Prolog with Features, Inheritance and Templates. ProFIT allows the programmer or grammar developer to declare an inheritance hierarchy, features and templates. Sorted feature terms can be used in ProFIT programs together with Prolog terms to provide a clearer description language for linguistic structures. ProFIT compiles all sorted feature terms into a Prolog term representation, so that the built-in Prolog term unification can be used for the unification of sorted feature structures, and no special unification algorithm is needed. ProFIT programs are compiled into Prolog programs, so that no meta-interpreter is needed for their execution. ProFIT thus provides a direct step from grammars developed with sorted feature terms to Prolog programs usable for practical NLP systems.\n    ",
        "submission_date": "1995-02-05T00:00:00",
        "last_modified_date": "1995-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502004",
        "title": "Bottom-Up Earley Deduction",
        "authors": [
            "Gregor Erbach"
        ],
        "abstract": "We propose a bottom-up variant of Earley deduction. Bottom-up deduction is preferable to top-down deduction because it allows incremental processing (even for head-driven grammars), it is data-driven, no subsumption check is needed, and preference values attached to lexical items can be used to guide best-first search. We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps.\n    ",
        "submission_date": "1995-02-05T00:00:00",
        "last_modified_date": "1995-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502005",
        "title": "Off-line Optimization for Earley-style HPSG Processing",
        "authors": [
            "Guido Minnen",
            "Dale Gerdemann",
            "Thilo Goetz"
        ],
        "abstract": "A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and inputs the primed grammar to an advanced Earley-style processor. This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation. Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar.\n    ",
        "submission_date": "1995-02-07T00:00:00",
        "last_modified_date": "1995-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502006",
        "title": "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "authors": [
            "David Carter"
        ],
        "abstract": "I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system. The compiler is optimized for a class of languages including many or most European ones, and for rapid development and debugging of descriptions of new languages. The key design decision is to compose morphophonological and morphosyntactic information, but not the lexicon, when compiling the description. This results in typical compilation times of about a minute, and has allowed a reasonably full, feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system.\n    ",
        "submission_date": "1995-02-08T00:00:00",
        "last_modified_date": "1995-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502007",
        "title": "Utilization of a Lexicon for Spelling Correction in Modern Greek",
        "authors": [
            "A.Vagelatos",
            "T.Triantopoulou",
            "C.Tsalidis",
            "D.Christodoulakis"
        ],
        "abstract": "In this paper we present an interactive spelling correction system for Modern Greek. The entire system is based on a morphological lexicon. Emphasis is given to the development of the lexicon, especially as far as storage economy, speed efficiency and dictionary coverage is concerned. Extensive research was conducted from both the computer engineering and linguisting fields, in order to describe inflectional morphology as economically as possible.\n    ",
        "submission_date": "1995-02-09T00:00:00",
        "last_modified_date": "1995-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502008",
        "title": "A Robust and Efficient Three-Layered Dialogue Component for a Speech-to-Speech Translation System",
        "authors": [
            "Jan Alexandersson",
            "Elisabeth Maier",
            "Norbert Reithinger"
        ],
        "abstract": "We present the dialogue component of the speech-to-speech translation system VERBMOBIL. In contrast to conventional dialogue systems it mediates the dialogue while processing maximally 50% of the dialogue in depth. Special requirements like robustness and efficiency lead to a 3-layered hybrid architecture for the dialogue module, using statistics, an automaton and a planner. A dialogue memory is constructed incrementally.\n    ",
        "submission_date": "1995-02-10T00:00:00",
        "last_modified_date": "1995-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502009",
        "title": "On Learning More Appropriate Selectional Restrictions",
        "authors": [
            "Francesc Ribas"
        ],
        "abstract": "We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora. It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes. Evaluation measures for the Selectional Restrictions learning task are discussed. Finally, an experimental evaluation of these variations is reported.\n    ",
        "submission_date": "1995-02-09T00:00:00",
        "last_modified_date": "1995-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502010",
        "title": "NPtool, a detector of English noun phrases",
        "authors": [
            "Atro Voutilainen"
        ],
        "abstract": "NPtool is a fast and accurate system for extracting noun phrases from English texts for the purposes of e.g. information retrieval, translation unit discovery, and corpus studies. After a general introduction, the system architecture is presented in outline. Then follows an examination of a recently written Constraint Syntax. An evaluation report concludes the paper.\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502011",
        "title": "Specifying a shallow grammatical representation for parsing purposes",
        "authors": [
            "Atro Voutilainen",
            "Timo Jarvinen"
        ],
        "abstract": "Is it possible to specify a grammatical representation (descriptors and their application guidelines) to such a degree that it can be consistently applied by different grammarians e.g. for producing a benchmark corpus for parser evaluation? Arguments for and against have been given, but very little empirical evidence. In this article we report on a double-blind experiment with a surface-oriented morphosyntactic grammatical representation used in a large-scale English parser. We argue that a consistently applicable representation for morphology and also shallow syntax can be specified. A grammatical representation with a near-100% coverage of running text can be specified with a reasonable effort, especially if the representation is based on structural distinctions (i.e. it is structurally resolvable).\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502012",
        "title": "A syntax-based part-of-speech analyser",
        "authors": [
            "Atro Voutilainen"
        ],
        "abstract": "There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven. Recent state-of-the-art part-of-speech taggers are based on the data-driven approach. Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data-driven approach in part-of-speech analysis may appear surprising. In this paper, a case is made for the syntactic nature of part-of-speech tagging. A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated. Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%. Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis.\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502013",
        "title": "Ambiguity resolution in a reductionistic parser",
        "authors": [
            "Atro Voutilainen",
            "Pasi Tapanainen"
        ],
        "abstract": "  We are concerned with dependency-oriented morphosyntactic parsing of running text. While a parsing grammar should avoid introducing structurally unresolvable distinctions in order to optimise on the accuracy of the parser, it also is beneficial for the grammarian to have as expressive a structural representation available as possible. In a reductionistic parsing system this policy may result in considerable ambiguity in the input; however, even massive ambiguity can be tackled efficiently with an accurate parsing description and effective parsing technology.\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502014",
        "title": "Ellipsis and Quantification: a substitutional approach",
        "authors": [
            "Richard Crouch"
        ],
        "abstract": "The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple, Shieber and Pereira (1991), but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution. It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition, instead of the more common view of interpretation as actually performing the composition\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502015",
        "title": "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "authors": [
            "Andrew Kehler",
            "Mary Dalrymple",
            "John Lamping",
            "Vijay Saraswat"
        ],
        "abstract": "We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures. We provide an extension to the previous account of LFG semantics (Dalrymple et al., 1993b) according to which dependencies between f-structures are viewed as resources; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained. The resulting system is sufficiently restricted in cases where other approaches overgenerate; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation.\n    ",
        "submission_date": "1995-02-13T00:00:00",
        "last_modified_date": "1995-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502016",
        "title": "Higher-order Linear Logic Programming of Categorial Deduction",
        "authors": [
            "Glyn Morrill"
        ],
        "abstract": "We show how categorial deduction can be implemented in higher-order (linear) logic programming, thereby realising parsing as deduction for the associative and non-associative Lambek calculi. This provides a method of solution to the parsing problem of Lambek categorial grammar applicable to a variety of its extensions.\n    ",
        "submission_date": "1995-02-14T00:00:00",
        "last_modified_date": "1995-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502017",
        "title": "Deterministic Consistency Checking of LP Constraints",
        "authors": [
            "Suresh Manandhar"
        ],
        "abstract": "We provide a constraint based computational model of linear precedence as employed in the HPSG grammar formalism. An extended feature logic which adds a wide range of constraints involving precedence is described. A sound, complete and terminating deterministic constraint solving procedure is given. Deterministic computational model is achieved by weakening the logic such that it is sufficient for linguistic applications involving word-order.\n    ",
        "submission_date": "1995-02-14T00:00:00",
        "last_modified_date": "1995-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502018",
        "title": "Algorithms for Analysing the Temporal Structure of Discourse",
        "authors": [
            "Janet Hitzeman",
            "Marc Moens",
            "Claire Grover"
        ],
        "abstract": "We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure. It is part of a discourse grammar implemented in Carpenter's ALE formalism. The method for building up the temporal structure of the discourse combines constraints and preferences: we use constraints to reduce the number of possible structures, exploiting the HPSG type hierarchy and unification for this purpose; and we apply preferences to choose between the remaining options using a temporal centering mechanism. We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal/rhetorical structure until higher-level information can be used to disambiguate.\n    ",
        "submission_date": "1995-02-15T00:00:00",
        "last_modified_date": "1995-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502019",
        "title": "Integrating \"Free\" Word Order Syntax and Information Structure",
        "authors": [
            "Beryl Hoffman"
        ],
        "abstract": "  This paper describes a combinatory categorial formalism called Multiset-CCG that can capture the syntax and interpretation of ``free'' word order in languages such as Turkish. The formalism compositionally derives the predicate-argument structure and the information structure (e.g. topic, focus) of a sentence in parallel, and uniformly handles word order variation among the arguments and adjuncts within a clause, as well as in complex clauses and across clause boundaries.\n    ",
        "submission_date": "1995-02-15T00:00:00",
        "last_modified_date": "1995-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502020",
        "title": "Formalization and Parsing of Typed Unification-Based ID/LP Grammars",
        "authors": [
            "Frank Morawietz"
        ],
        "abstract": "This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming).\n    ",
        "submission_date": "1995-02-14T00:00:00",
        "last_modified_date": "1995-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502021",
        "title": "A Tractable Extension of Linear Indexed Grammars",
        "authors": [
            "Bill Keller",
            "David Weir"
        ],
        "abstract": "  It has been shown that Linear Indexed Grammars can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that is more powerful than Linear Indexed Grammar, but which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Linear PATR manipulates feature structures rather than stacks.\n    ",
        "submission_date": "1995-02-17T00:00:00",
        "last_modified_date": "1995-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502022",
        "title": "Stochastic HPSG",
        "authors": [
            "Chris Brew"
        ],
        "abstract": "In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag. We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora.\n    ",
        "submission_date": "1995-02-17T00:00:00",
        "last_modified_date": "1995-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502023",
        "title": "Splitting the Reference Time: Temporal Anaphora and Quantification in DRT",
        "authors": [
            "Rani Nelken",
            "Nissim Francez"
        ],
        "abstract": "This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory. The analysis in (Partee 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is \"before\" or \"after\". This problem has been previously analyzed in (de Swart 1991) as an instance of the proportion problem, and given a solution from a Generalized Quantifier approach. By using a careful distinction between the different notions of reference time, based on (Kamp and Reyle 1993), we propose a solution to this problem, within the framework of DRT. We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.\n    ",
        "submission_date": "1995-02-18T00:00:00",
        "last_modified_date": "1995-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502024",
        "title": "A Robust Parser Based on Syntactic Information",
        "authors": [
            "Kong Joo Lee",
            "Cheol Jung Kweon",
            "Jungyun Seo",
            "Gil Chang Kim"
        ],
        "abstract": "In this paper, we propose a robust parser which can parse extragrammatical sentences. This parser can recover them using only syntactic information. It can be easily modified and extended because it utilize only syntactic information.\n    ",
        "submission_date": "1995-02-21T00:00:00",
        "last_modified_date": "1995-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502025",
        "title": "Principle Based Semantics for HPSG",
        "authors": [
            "Anette Frank",
            "Uwe Reyle"
        ],
        "abstract": "  The paper presents a constraint based semantic formalism for HPSG. The syntax-semantics interface directly implements syntactic conditions on quantifier scoping and distributivity. The construction of semantic representations is guided by general principles governing the interaction between syntax and semantics. Each of these principles acts as a constraint to narrow down the set of possible interpretations of a sentence. Meanings of ambiguous sentences are represented by single partial representations (so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which further constraints can be added monotonically to gain more information about the content of a sentence. There is no need to build up a large number of alternative representations of the sentence which are then filtered by subsequent discourse and world knowledge. The advantage of UDRSs is not only that they allow for monotonic incremental interpretation but also that they are equipped with truth conditions and a proof theory that allows for inferences to be drawn directly on structures where quantifier scope is not resolved.\n    ",
        "submission_date": "1995-02-21T00:00:00",
        "last_modified_date": "1995-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502026",
        "title": "On Reasoning with Ambiguities",
        "authors": [
            "Uwe Reyle"
        ],
        "abstract": "  The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations.\n    ",
        "submission_date": "1995-02-21T00:00:00",
        "last_modified_date": "1995-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502027",
        "title": "Towards an Account of Extraposition in HPSG",
        "authors": [
            "Frank Keller"
        ],
        "abstract": "This paper investigates the syntax of extraposition in the HPSG framework. We present English and German data (partly taken from corpora), and provide an analysis using lexical rules and a nonlocal dependency. The condition for binding this dependency is formulated relative to the antecedent of the extraposed phrase, which entails that no fixed site for extraposition exists. Our analysis accounts for the interaction of extraposition with fronting and coordination, and predicts constraints on multiple extraposition.\n    ",
        "submission_date": "1995-02-21T00:00:00",
        "last_modified_date": "1995-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502028",
        "title": "Lexical Acquisition via Constraint Solving",
        "authors": [
            "Ted Pedersen",
            "Weidong Chen"
        ],
        "abstract": "This paper describes a method to automatically acquire the syntactic and semantic classifications of unknown words. Our method reduces the search space of the lexical acquisition problem by utilizing both the left and the right context of the unknown word. Link Grammar provides a convenient framework in which to implement our method.\n    ",
        "submission_date": "1995-02-22T00:00:00",
        "last_modified_date": "1995-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502029",
        "title": "Topic Identification in Discourse",
        "authors": [
            "Kuang-hua Chen"
        ],
        "abstract": "This paper proposes a corpus-based language model for topic identification. We analyze the association of noun-noun and noun-verb pairs in LOB Corpus. The word association norms are based on three factors: 1) word importance, 2) pair co-occurrence, and 3) distance. They are trained on the paragraph and sentence levels for noun-noun and noun-verb pairs, respectively. Under the topic coherence postulation, the nouns that have the strongest connectivities with the other nouns and verbs in the discourse form the preferred topic set. The collocational semantics then is used to identify the topics from paragraphs and to discuss the topic shift phenomenon among paragraphs.\n    ",
        "submission_date": "1995-02-23T00:00:00",
        "last_modified_date": "1995-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502030",
        "title": "Bi-directional memory-based dialog translation: The KEMDT approach",
        "authors": [
            "Geunbae Lee",
            "Hanmin Jung",
            "Jong-Hyeok Lee"
        ],
        "abstract": "A bi-directional Korean/English dialog translation system is designed and implemented using the memory-based translation technique. The system KEMDT (Korean/English Memory-based Dialog Translation system) can perform Korean to English, and English to Korean translation using unified memory network and extended marker passing algorithm. We resolve the word order variation and frequent word omission problems in Korean by classifying the concept sequence element in four different types and extending the marker- passing-based-translation algorithm. Unlike the previous memory-based translation systems, the KEMDT system develops the bilingual memory network and the unified bi-directional marker passing translation algorithm. For efficient language specific processing, we separate the morphological processors from the memory-based translator. The KEMDT technology provides a hierarchical memory network and an efficient marker-based control for the recent example-based MT paradigm.\n    ",
        "submission_date": "1995-02-23T00:00:00",
        "last_modified_date": "1995-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502031",
        "title": "Cooperative Error Handling and Shallow Processing",
        "authors": [
            "Tanya Bowden"
        ],
        "abstract": "This paper is concerned with the detection and correction of sub-sentential English text errors. Previous spelling programs, unless restricted to a very small set of words, have operated as post-processors. And to date, grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse, assuming a complete sentence. Work described below is aimed at evaluating the effectiveness of shallow (sub-sentential) processing and the feasibility of cooperative error checking, through building and testing appropriately an error-processing system. A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parsing. Intended testing is discussed.\n    ",
        "submission_date": "1995-02-23T00:00:00",
        "last_modified_date": "1995-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502032",
        "title": "An NLP Approach to a Specific Type of Texts: Car Accident Reports",
        "authors": [
            "Dominique Estival",
            "Francoise Gayral"
        ],
        "abstract": "The work reported here is the result of a study done within a larger project on the ``Semantics of Natural Languages'' viewed from the field of Artificial Intelligence and Computational Linguistics. In this project, we have chosen a corpus of insurance claim reports. These texts deal with a relatively circumscribed domain, that of road traffic, thereby limiting the extra-linguistic knowledge necessary to understand them. Moreover, these texts present a number of very specific characteristics, insofar as they are written in a quasi-institutional setting which imposes many constraints on their production. We first determine what these constraints are in order to then show how they provide the writer with the means to create as succint a text as possible, and in a symmetric way, how they provide the reader with the means to interpret the text and to distinguish between its factual and argumentative aspects.\n    ",
        "submission_date": "1995-02-23T00:00:00",
        "last_modified_date": "1995-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502033",
        "title": "An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process",
        "authors": [
            "Saliha Azzam"
        ],
        "abstract": "This paper concerns both anaphora resolution and prepositional phrase (PP) attachment that are the most frequent ambiguities in natural language processing. Several methods have been proposed to deal with each phenomenon separately, however none of proposed systems has considered the way of dealing both phenomena. We tackle this issue, proposing an algorithm to co-ordinate the treatment of these two problems efficiently, i.e., the aim is also to exploit at each step all the results that each component can provide.\n    ",
        "submission_date": "1995-02-24T00:00:00",
        "last_modified_date": "1995-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502034",
        "title": "Grouping Words Using Statistical Context",
        "authors": [
            "Christopher C. Huckle"
        ],
        "abstract": "  This paper (cmp-lg/yymmnnn) has been accepted for publication in the student session of EACL-95. It outlines ongoing work using statistical and unsupervised neural network methods for clustering words in untagged corpora. Such approaches are of interest when attempting to understand the development of human intuitive categorization of language as well as for trying to improve computational methods in natural language understanding. Some preliminary results using a simple statistical approach are described, along with work using an unsupervised neural network to distinguish between the sense classes into which words fall.\n    ",
        "submission_date": "1995-02-24T00:00:00",
        "last_modified_date": "1995-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502035",
        "title": "Incorporating \"Unconscious Reanalysis\" into an Incremental, Monotonic Parser",
        "authors": [
            "Patrick Sturt"
        ],
        "abstract": "  This paper describes an implementation based on a recent model in the psycholinguistic literature. We define a parsing operation which allows the reanalysis of dependencies within an incremental and monotonic processing architecture, and discuss search strategies for its application in a head-initial language (English) and a head-final language (Japanese).\n    ",
        "submission_date": "1995-02-24T00:00:00",
        "last_modified_date": "1995-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502036",
        "title": "Literal Movement Grammars",
        "authors": [
            "Annius V. Groenink"
        ],
        "abstract": "Literal movement grammars (LMGs) provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information. LMGs provide a simple and efficient treatment of complex linguistic phenomena such as cross-serial dependencies in German and Dutch---separating the treatment of natural language into a parsing phase closely resembling traditional context-free treatment, and a disambiguation phase which can be carried out using matching, as opposed to full unification employed in most current grammar formalisms of linguistical relevance.\n    ",
        "submission_date": "1995-02-27T00:00:00",
        "last_modified_date": "1995-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502037",
        "title": "A State-Transition Grammar for Data-Oriented Parsing",
        "authors": [
            "David Tugwell"
        ],
        "abstract": "This paper presents a grammar formalism designed for use in data-oriented approaches to language processing. The formalism is best described as a right-linear indexed grammar extended in linguistically interesting ways. The paper goes on to investigate how a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts.\n    ",
        "submission_date": "1995-02-27T00:00:00",
        "last_modified_date": "1995-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502038",
        "title": "Implementation and evaluation of a German HMM for POS disambiguation",
        "authors": [
            "Helmut Feldweg"
        ],
        "abstract": "A German language model for the Xerox HMM tagger is presented. This model's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora. The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French. Finally, the model's error types are described. I argue that although the overall performance of these models for German is comparable to results for English and French, a more exact analysis demonstrates important differences in the types of disambiguation involved for German.\n    ",
        "submission_date": "1995-02-27T00:00:00",
        "last_modified_date": "1995-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9502039",
        "title": "Multilingual Sentence Categorization according to Language",
        "authors": [
            "Emmanuel Giguet"
        ],
        "abstract": "  In this paper, we describe an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency. The implementation is fast, small, robust and textual errors tolerant. Tested for french, english, spanish and german discrimination, the system gives very interesting results, achieving in one test 99.4% correct assignments on real sentences.\n",
        "submission_date": "1995-02-28T00:00:00",
        "last_modified_date": "1995-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503001",
        "title": "Using a Corpus for Teaching Turkish Morphology",
        "authors": [
            "H. Altay Guvenir",
            "Kemal Oflazer"
        ],
        "abstract": "This paper reports on the preliminary phase of our ongoing research towards developing an intelligent tutoring environment for Turkish grammar. One of the components of this environment is a corpus search tool which, among other aspects of the language, will be used to present the learner sample sentences along with their morphological analyses. Following a brief introduction to the Turkish language and its morphology, the paper describes the morphological analysis and ambiguity resolution used to construct the corpus used in the search tool. Finally, implementation issues and details involving the user interface of the tool are discussed.\n    ",
        "submission_date": "1995-03-01T00:00:00",
        "last_modified_date": "1995-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503002",
        "title": "Computational dialectology in Irish Gaelic",
        "authors": [
            "Brett Kessler"
        ],
        "abstract": "Dialect groupings can be discovered objectively and automatically by cluster analysis of phonetic transcriptions such as those found in a linguistic atlas. The first step in the analysis, the computation of linguistic distance between each pair of sites, can be computed as Levenshtein distance between phonetic strings. This correlates closely with the much more laborious technique of determining and counting isoglosses, and is more accurate than the more familiar metric of computing Hamming distance based on whether vocabulary entries match. In the actual clustering step, traditional agglomerative clustering works better than the top-down technique of partitioning around medoids. When agglomerative clustering of phonetic string comparison distances is applied to Gaelic, reasonable dialect boundaries are obtained, corresponding to national and (within Ireland) provincial boundaries.\n    ",
        "submission_date": "1995-03-01T00:00:00",
        "last_modified_date": "1995-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503003",
        "title": "Tagging French -- comparing a statistical and a constraint-based method",
        "authors": [
            "Jean-Pierre Chanod",
            "Pasi Tapanainen"
        ],
        "abstract": "In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development.\n    ",
        "submission_date": "1995-03-02T00:00:00",
        "last_modified_date": "1995-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503004",
        "title": "Creating a tagset, lexicon and guesser for a French tagger",
        "authors": [
            "Jean-Pierre Chanod",
            "Pasi Tapanainen"
        ],
        "abstract": "  We earlier described two taggers for French, a statistical one and a constraint-based one. The two taggers have the same tokeniser and morphological analyser. In this paper, we describe aspects of this work concerned with the definition of the tagset, the building of the lexicon, derived from an existing two-level morphological analyser, and the definition of a lexical transducer for guessing unknown words.\n    ",
        "submission_date": "1995-03-02T00:00:00",
        "last_modified_date": "1995-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503005",
        "title": "A specification language for Lexical Functional Grammars",
        "authors": [
            "Patrick Blackburn",
            "Claire Gardent"
        ],
        "abstract": "This paper defines a language L for specifying LFG grammars. This enables constraints on LFG's composite ontology (c-structures synchronised with f-structures) to be stated directly; no appeal to the LFG construction algorithm is needed. We use L to specify schemata annotated rules and the LFG uniqueness, completeness and coherence principles. Broader issues raised by this work are noted and discussed.\n    ",
        "submission_date": "1995-03-03T00:00:00",
        "last_modified_date": "1995-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503006",
        "title": "ParseTalk about Sentence- and Text-Level Anaphora",
        "authors": [
            "Michael Strube",
            "Udo Hahn"
        ],
        "abstract": "We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model. Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model.\n    ",
        "submission_date": "1995-03-03T00:00:00",
        "last_modified_date": "1995-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503007",
        "title": "The Semantics of Motion",
        "authors": [
            "Pierre Sablayrolles"
        ],
        "abstract": "In this paper we present a semantic study of motion complexes (ie. of a motion verb followed by a spatial preposition). We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs, on the one hand, and of the spatial prepositions, on the other hand. Then, we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components.\n    ",
        "submission_date": "1995-03-07T00:00:00",
        "last_modified_date": "1995-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503008",
        "title": "Ellipsis and Higher-Order Unification",
        "authors": [
            "Mary Dalrymple",
            "Stuart M. Shieber",
            "Fernando C. N. Pereira"
        ],
        "abstract": "We present a new method for characterizing the interpretive possibilities generated by elliptical constructions in natural language. Unlike previous analyses, which postulate ambiguity of interpretation or derivation in the full clause source of the ellipsis, our analysis requires no such hidden ambiguity. Further, the analysis follows relatively directly from an abstract statement of the ellipsis interpretation problem. It predicts correctly a wide range of interactions between ellipsis and other semantic phenomena such as quantifier scope and bound anaphora. Finally, although the analysis itself is stated nonprocedurally, it admits of a direct computational method for generating interpretations.\n    ",
        "submission_date": "1995-03-08T00:00:00",
        "last_modified_date": "1995-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503009",
        "title": "Distributional Part-of-Speech Tagging",
        "authors": [
            "Hinrich Schuetze"
        ],
        "abstract": "  This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.\n    ",
        "submission_date": "1995-03-08T00:00:00",
        "last_modified_date": "1995-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503010",
        "title": "Corpus-based Method for Automatic Identification of Support Verbs for Nominalizations",
        "authors": [
            "Gregory Grefenstette",
            "Simone Teufel"
        ],
        "abstract": "Nominalization is a highly productive phenomena in most languages. The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., \"make a proposal\"). The choice of a support verb for a given nominalization is unpredictable, causing a problem for language learners as well as for natural language processing systems. We present here a method of discovering support verbs from an untagged corpus via low-level syntactic processing and comparison of arguments attached to verbal forms and potential nominalized forms. The result of the process is a list of potential support verbs for the nominalized form of a given predicate.\n    ",
        "submission_date": "1995-03-09T00:00:00",
        "last_modified_date": "1995-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503011",
        "title": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies",
        "authors": [
            "John McMahon",
            "F.J.Smith"
        ],
        "abstract": " An automatic word classification system has been designed which processes word unigram and bigram frequency statistics extracted from a corpus of natural language utterances. The system implements a binary top-down form of word clustering which employs an average class mutual information metric. Resulting classifications are hierarchical, allowing variable class granularity. Words are represented as structural tags --- unique $n$-bit numbers the most significant bit-patterns of which incorporate class information. Access to a structural tag immediately provides access to all classification levels for the corresponding word. The classification system has successfully revealed some of the structure of English, from the phonemic to the semantic level. The system has been compared --- directly and indirectly --- with other recent word classification systems. Class based interpolated language models have been constructed to exploit the extra information supplied by the classifications and some experiments have shown that the new models improve model performance.\n    ",
        "submission_date": "1995-03-09T00:00:00",
        "last_modified_date": "1995-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503012",
        "title": "A Note on Zipf's Law, Natural Languages, and Noncoding DNA regions",
        "authors": [
            "Partha Niyogi",
            "Robert C. Berwick"
        ],
        "abstract": "  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the basis of Zipf rank frequency data that noncoding DNA sequence regions are more like natural languages than coding regions. We argue on the contrary that an empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to natural languages. Although DNA is a presumably an ``organized system of signs'' in Mandelbrot's (1961) sense, an observation of statistical features of the sort presented in the Mantegna et al. paper does not shed light on the similarity between DNA's ``grammar'' and natural language grammars, just as the observation of exact Zipf-like behavior cannot distinguish between the underlying processes of tossing an $M$ sided die or a finite-state branching process.\n    ",
        "submission_date": "1995-03-09T00:00:00",
        "last_modified_date": "1995-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503013",
        "title": "Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Semantics",
        "authors": [
            "David Milward",
            "Robin Cooper"
        ],
        "abstract": "Why should computers interpret language incrementally? In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling, suggesting that humans perform semantic interpretation before constituent boundaries, possibly word by word. However, possible computational applications have received less attention. In this paper we consider various potential applications, in particular graphical interaction and dialogue. We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations. Finally, we tease apart the relationship between dynamic semantics and incremental interpretation.\n    ",
        "submission_date": "1995-03-13T00:00:00",
        "last_modified_date": "1995-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503014",
        "title": "Non-Constituent Coordination: Theory and Practice",
        "authors": [
            "David Milward"
        ],
        "abstract": "Despite the large amount of theoretical work done on non-constituent coordination during the last two decades, many computational systems still treat coordination using adapted parsing strategies, in a similar fashion to the SYSCONJ system developed for ATNs. This paper reviews the theoretical literature, and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing. Finally, it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars.\n    ",
        "submission_date": "1995-03-14T00:00:00",
        "last_modified_date": "1995-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503015",
        "title": "Incremental Interpretation of Categorial Grammar",
        "authors": [
            "David Milward"
        ],
        "abstract": "The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation. The parser does not require fragments of sentences to form constituents, and thereby avoids problems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages.\n    ",
        "submission_date": "1995-03-14T00:00:00",
        "last_modified_date": "1995-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503016",
        "title": "Natural Language Interfaces to Databases - An Introduction",
        "authors": [
            "I.Androutsopoulos",
            "G.D.Ritchie",
            "P.Thanisch"
        ],
        "abstract": "This paper is an introduction to natural language interfaces to databases (NLIDBs). A brief overview of the history of NLIDBs is first given. Some advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBs have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBs. The paper ends with reflections on the current state of the art.\n    ",
        "submission_date": "1995-03-14T00:00:00",
        "last_modified_date": "1995-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503017",
        "title": "Redundancy in Collaborative Dialogue",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "In dialogues in which both agents are autonomous, each agent deliberates whether to accept or reject the contributions of the current speaker. A speaker cannot simply assume that a proposal or an assertion will be accepted. However, an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection. Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer's next dialogue contribution. In this paper, I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance. The model (1) requires a theory of mutual belief that supports mutual beliefs of various strengths; (2) explains the function of a class of informationally redundant utterances that cannot be explained by other accounts; and (3) contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption.\n    ",
        "submission_date": "1995-03-16T00:00:00",
        "last_modified_date": "1995-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503018",
        "title": "Discourse and Deliberation: Testing a Collaborative Strategy",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "A discourse strategy is a strategy for communicating with another agent. Designing effective dialogue systems requires designing agents that can choose among discourse strategies. We claim that the design of effective strategies must take cognitive factors into account, propose a new method for testing the hypothesized factors, and present experimental results on an effective strategy for supporting deliberation. The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics.\n    ",
        "submission_date": "1995-03-16T00:00:00",
        "last_modified_date": "1995-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503019",
        "title": "SATZ - An Adaptive Sentence Segmentation System",
        "authors": [
            "David D. Palmer"
        ],
        "abstract": "  This paper provides a detailed description of the sentence segmentation system first introduced in ",
        "submission_date": "1995-03-20T00:00:00",
        "last_modified_date": "1995-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503020",
        "title": "Different Issues in the Design of a Lemmatizer/Tagger for Basque",
        "authors": [
            "I. Aduriz",
            "I. Alegria",
            "J. M. Arriola",
            "X. Artola",
            "Diaz de Illarraza A.",
            "N. Ezeiza",
            "K. Gojenola",
            "M. Maritxalar"
        ],
        "abstract": "  This paper presents relevant issues that have been considered in the design of a general purpose lemmatizer/tagger for Basque (EUSLEM). The lemmatizer/tagger is conceived as a basic tool necessary for other linguistic applications. It uses the lexical data base and the morphological analyzer previously developed and implemented. Due to the characteristics of the language, the tagset here proposed in structured in for levels, so that each level is a refinement of the previous one in the sense that it adds more detailed information. We will focus on the problems found in designing this tagset and on the strategies for morphological disambiguation that will be used.\n    ",
        "submission_date": "1995-03-20T00:00:00",
        "last_modified_date": "1995-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503021",
        "title": "A Note on the Complexity of Restricted Attribute-Value Grammars",
        "authors": [
            "Leen Torenvliet",
            "Marten Trautwein"
        ],
        "abstract": "  The recognition problem for attribute-value grammars (AVGs) was shown to be undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no practical use. In this paper we study a very restricted form of AVG, for which the recognition problem is decidable (though still NP-complete), the R-AVG. We show that the R-AVG formalism captures all of the context free languages and more, and introduce a variation on the so-called `off-line parsability constraint', the `honest parsability constraint', which lets different types of R-AVG coincide precisely with well-known time complexity classes.\n    ",
        "submission_date": "1995-03-21T00:00:00",
        "last_modified_date": "1995-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503022",
        "title": "Assessing Complexity Results in Feature Theories",
        "authors": [
            "Marten Trautwein"
        ],
        "abstract": "  In this paper, we assess the complexity results of formalisms that describe the feature theories used in computational linguistics. We show that from these complexity results no immediate conclusions can be drawn about the complexity of the recognition problem of unification grammars using these feature theories. On the one hand, the complexity of feature theories does not provide an upper bound for the complexity of such unification grammars.\n",
        "submission_date": "1995-03-21T00:00:00",
        "last_modified_date": "1995-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503023",
        "title": "A fast partial parse of natural language sentences using a connectionist method",
        "authors": [
            "Caroline Lyon",
            "Bob Dickerson"
        ],
        "abstract": "  The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language. This paper describes a fully automated hybrid system, using neural nets operating within a grammatic framework. It addresses the representation of language for connectionist processing, and describes methods of constraining the problem size. The function of the network is briefly explained, and results are given.\n    ",
        "submission_date": "1995-03-22T00:00:00",
        "last_modified_date": "1995-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503024",
        "title": "From compositional to systematic semantics",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "abstract": "  We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when compositional semantics is required to be \"systematic\" (that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and non-compositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic formal comparisons of different systems of grammars.\n    ",
        "submission_date": "1995-03-24T00:00:00",
        "last_modified_date": "1995-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9503025",
        "title": "Co-occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries",
        "authors": [
            "Yoshiki Niwa",
            "Yoshihiko Nitta"
        ],
        "abstract": "  A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors.\n    ",
        "submission_date": "1995-04-01T00:00:00",
        "last_modified_date": "1995-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504001",
        "title": "Automatic processing proper names in texts",
        "authors": [
            "Francis Wolinski",
            "Frantz Vichot",
            "Bruno Dillet"
        ],
        "abstract": "  This paper shows first the problems raised by proper names in natural language processing. Second, it introduces the knowledge representation structure we use based on conceptual graphs. Then it explains the techniques which are used to process known and unknown proper names. At last, it gives the performance of the system and the further works we intend to deal with.\n    ",
        "submission_date": "1995-04-03T00:00:00",
        "last_modified_date": "1995-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504002",
        "title": "Tagset Design and Inflected Languages",
        "authors": [
            "David Elworthy"
        ],
        "abstract": "  An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described, using corpora in English, French and Swedish. In particular, the question of internal versus external criteria for tagset design is considered, with the general conclusion that external (linguistic) criteria should be followed. Some problems associated with tagging unknown words in inflected languages are briefly considered.\n    ",
        "submission_date": "1995-04-03T00:00:00",
        "last_modified_date": "1995-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504003",
        "title": "Collaborating on Referring Expressions",
        "authors": [
            "Peter A. Heeman",
            "Graeme Hirst"
        ],
        "abstract": "  This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation.\n    ",
        "submission_date": "1995-04-04T00:00:00",
        "last_modified_date": "1995-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504004",
        "title": "A Computational Treatment of HPSG Lexical Rules as Covariation in Lexical Entries",
        "authors": [
            "Walt Detmar Meurers",
            "Guido Minnen"
        ],
        "abstract": "  We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application.\n    ",
        "submission_date": "1995-04-04T00:00:00",
        "last_modified_date": "1995-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504005",
        "title": "Constraint Logic Programming for Natural Language Processing",
        "authors": [
            "Philippe Blache",
            "Nabil Hathout"
        ],
        "abstract": "  This paper proposes an evaluation of the adequacy of the constraint logic programming paradigm for natural language processing. Theoretical aspects of this question have been discussed in several works. We adopt here a pragmatic point of view and our argumentation relies on concrete solutions. Using actual contraints (in the CLP sense) is neither easy nor direct. However, CLP can improve parsing techniques in several aspects such as concision, control, efficiency or direct representation of linguistic formalism. This discussion is illustrated by several examples and the presentation of an HPSG parser.\n    ",
        "submission_date": "1995-04-05T00:00:00",
        "last_modified_date": "1995-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504006",
        "title": "Cues and control in Expert-Client Dialogues",
        "authors": [
            "Steve Whittaker",
            "Phil Stenton"
        ],
        "abstract": "  We conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialogues and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not.\n    ",
        "submission_date": "1995-04-05T00:00:00",
        "last_modified_date": "1995-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504007",
        "title": "Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation",
        "authors": [
            "Marilyn Walker",
            "Steve Whittaker"
        ],
        "abstract": "  Conversation between two people is usually of mixed-initiative, with control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles.\n    ",
        "submission_date": "1995-04-05T00:00:00",
        "last_modified_date": "1995-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504008",
        "title": "SKOPE: A connectionist/symbolic architecture of spoken Korean processing",
        "authors": [
            "Geunbae Lee",
            "Jong-Hyeok Lee"
        ],
        "abstract": "  Spoken language processing requires speech and natural language integration. Moreover, spoken Korean calls for unique processing methodology due to its linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic spoken Korean processing engine, which emphasizes that: 1) connectionist and symbolic techniques must be selectively applied according to their relative strength and weakness, and 2) the linguistic characteristics of Korean must be fully considered for phoneme recognition, speech and language integration, and morphological/syntactic processing. The design and implementation of SKOPE demonstrates how connectionist/symbolic hybrid architectures can be constructed for spoken agglutinative language processing. Also SKOPE presents many novel ideas for speech and language processing. The phoneme recognition, morphological analysis, and syntactic analysis experiments show that SKOPE is a viable approach for the spoken Korean processing.\n    ",
        "submission_date": "1995-04-07T00:00:00",
        "last_modified_date": "1995-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504009",
        "title": "Abstract Machine for Typed Feature Structures",
        "authors": [
            "Shuly Wintner",
            "Nissim Francez"
        ],
        "abstract": "  This paper describes an abstract machine for linguistic formalisms that are based on typed feature structures, such as HPSG. The core design of the abstract machine is given in detail, including the compilation process from a high-level language to the abstract machine language and the implementation of the abstract instructions. The machine's engine supports the unification of typed, possibly cyclic, feature structures. A separate module deals with control structures and instructions to accommodate parsing for phrase structure grammars. We treat the linguistic formalism as a high-level declarative programming language, applying methods that were proved useful in computer science to the study of natural languages: a grammar specified using the formalism is endowed with an operational semantics.\n    ",
        "submission_date": "1995-04-13T00:00:00",
        "last_modified_date": "1995-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504010",
        "title": "MAXIMUM LIKELIHOOD AND MINIMUM ENTROPY IDENTIFICATION OF GRAMMARS",
        "authors": [
            "P.Collet",
            "A.Galves",
            "A.Lopes"
        ],
        "abstract": "  Using the Thermodynamic Formalism, we introduce a Gibbsian model for the identification of regular grammars based only on positive evidence. This model mimics the natural language acquisition procedure driven by prosody which is here represented by the thermodynamical potential. The statistical question we face is how to estimate the incidenc e matrix of a subshift of finite type from a sample produced by a Gibbs state whose potential is known. The model acquaints for both the robustness of t he language acquisition procedure and language changes. The probabilistic appr oach we use avoids invoking ad-hoc restrictions as Berwick's Subset Principle.\n    ",
        "submission_date": "1995-04-13T00:00:00",
        "last_modified_date": "1995-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504011",
        "title": "A Processing Model for Free Word Order Languages",
        "authors": [
            "Owen Rambow",
            "Aravind K. Joshi"
        ],
        "abstract": "  Like many verb-final languages, Germn displays considerable word-order freedom: there is no syntactic constraint on the ordering of the nominal arguments of a verb, as long as the verb remains in final position. This effect is referred to as ``scrambling'', and is interpreted in transformational frameworks as leftward movement of the arguments. Furthermore, arguments from an embedded clause may move out of their clause; this effect is referred to as ``long-distance scrambling''. While scrambling has recently received considerable attention in the syntactic literature, the status of long-distance scrambling has only rarely been addressed. The reason for this is the problematic status of the data: not only is long-distance scrambling highly dependent on pragmatic context, it also is strongly subject to degradation due to processing constraints. As in the case of center-embedding, it is not immediately clear whether to assume that observed unacceptability of highly complex sentences is due to grammatical restrictions, or whether we should assume that the competence grammar does not place any restrictions on scrambling (and that, therefore, all such sentences are in fact grammatical), and the unacceptability of some (or most) of the grammatically possible word orders is due to processing limitations. In this paper, we will argue for the second view by presenting a processing model for German.\n    ",
        "submission_date": "1995-04-15T00:00:00",
        "last_modified_date": "1995-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504012",
        "title": "Linear Logic for Meaning Assembly",
        "authors": [
            "Mary Dalrymple",
            "John Lamping",
            "Fernando Pereira",
            "Vijay Saraswat"
        ],
        "abstract": "  Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts. Meanings are often assumed to combine via function application, which works well when constituent structure trees are used to guide semantic composition. However, we believe that the functional structure of Lexical-Functional Grammar is best used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format. It has been difficult, however, to reconcile this approach with the combination of meanings by function application. In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure. Our use of linear logic as a `glue' for assembling meanings allows for a coherent treatment of the LFG requirements of completeness and coherence as well as of modification and quantification.\n    ",
        "submission_date": "1995-04-18T00:00:00",
        "last_modified_date": "1995-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504013",
        "title": "NLG vs. Templates",
        "authors": [
            "Ehud Reiter"
        ],
        "abstract": "  One of the most important questions in applied NLG is what benefits (or `value-added', in business-speak) NLG technology offers over template-based approaches. Despite the importance of this question to the applied NLG community, however, it has not been discussed much in the research NLG community, which I think is a pity. In this paper, I try to summarize the issues involved and recap current thinking on this topic. My goal is not to answer this question (I don't think we know enough to be able to do so), but rather to increase the visibility of this issue in the research community, in the hope of getting some input and ideas on this very important question. I conclude with a list of specific research areas I would like to see more work in, because I think they would increase the `value-added' of NLG over templates.\n    ",
        "submission_date": "1995-04-23T00:00:00",
        "last_modified_date": "1995-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504014",
        "title": "LexGram - a practical categorial grammar formalism -",
        "authors": [
            "Esther Koenig"
        ],
        "abstract": "  We present the LexGram system, an amalgam of (Lambek) categorial grammar and Head Driven Phrase Structure Grammar (HPSG), and show that the grammar formalism it implements is a well-structured and useful tool for actual grammar development.\n    ",
        "submission_date": "1995-04-24T00:00:00",
        "last_modified_date": "1995-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504015",
        "title": "Estimating Lexical Priors for Low-Frequency Syncretic Forms",
        "authors": [
            "Harald Baayen",
            "Richard Sproat"
        ],
        "abstract": "  Given a previously unseen form that is morphologically n-ways ambiguous, what is the best estimator for the lexical prior probabilities for the various functions of the form? We argue that the best estimator is provided by computing the relative frequencies of the various functions among the hapax legomena --- the forms that occur exactly once in a corpus. This result has important implications for the development of stochastic morphological taggers, especially when some initial hand-tagging of a corpus is required: For predicting lexical priors for very low-frequency morphologically ambiguous types (most of which would not occur in any given corpus) one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges.\n    ",
        "submission_date": "1995-04-24T00:00:00",
        "last_modified_date": "1995-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504016",
        "title": "Memoization of Top Down Parsing",
        "authors": [
            "Mark Johnson"
        ],
        "abstract": "  This paper discusses the relationship between memoized top-down recognizers and chart parsers. It presents a version of memoization suitable for continuation-passing style programs. When applied to a simple formalization of a top-down recognizer it yields a terminating parser.\n    ",
        "submission_date": "1995-04-25T00:00:00",
        "last_modified_date": "1995-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504017",
        "title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "authors": [
            "Daniel Marcu",
            "Graeme Hirst"
        ],
        "abstract": "  Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called ``stratified logic'', that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances.\n    ",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504018",
        "title": "An Implemented Formalism for Computing Linguistic Presuppositions and Existential Commitments",
        "authors": [
            "Daniel Marcu",
            "Graeme Hirst"
        ],
        "abstract": "  We rely on the strength of linguistic and philosophical perspectives in constructing a framework that offers a unified explanation for presuppositions and existential commitment. We use a rich ontology and a set of methodological principles that embed the essence of Meinong's philosophy and Grice's conversational principles into a stratified logic, under an unrestricted interpretation of the quantifiers. The result is a logical formalism that yields a tractable computational method that uniformly calculates all the presuppositions of a given utterance, including the existential ones.\n    ",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504019",
        "title": "A Formalism and an Algorithm for Computing Pragmatic Inferences and Detecting Infelicities",
        "authors": [
            "Daniel Marcu"
        ],
        "abstract": "  Since Austin introduced the term ``infelicity'', the linguistic literature has been flooded with its use, but no formal or computational explanation has been given for it. This thesis provides one for those infelicities that occur when a pragmatic inference is cancelled.\n",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504020",
        "title": "Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions",
        "authors": [
            "Robert Dale",
            "Ehud Reiter"
        ],
        "abstract": "  We examine the problem of generating definite noun phrases that are appropriate referring expressions; i.e, noun phrases that (1) successfully identify the intended referent to the hearer whilst (2) not conveying to her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.\n",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504021",
        "title": "Phonological Derivation in Optimality Theory",
        "authors": [
            "T. Mark Ellison"
        ],
        "abstract": "  Optimality Theory is a constraint-based theory of phonology which allows constraints to be violated. Consequently, implementing the theory presents problems for declarative constraint-based processing frameworks. On the basis of two regularity assumptions, that candidate sets are regular and that constraints can be modelled by transducers, this paper presents and proves correct algorithms for computing the action of constraints, and hence deriving surface forms.\n    ",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504022",
        "title": "Constraints, Exceptions and Representations",
        "authors": [
            "T. Mark Ellison"
        ],
        "abstract": "  This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-defaul theories. In achieving this result, I offer a characterisation of Underspecification Theory and Optimality Theory in terms of their methods for ordering defaults. The result means that machine learning techniques for building non-defualt analyses may not provide a suitable basis for morphophonological analysis.\n    ",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504023",
        "title": "TAKTAG: Two-phase learning method for hybrid statistical/rule-based part-of-speech disambiguation",
        "authors": [
            "Geunbae Lee",
            "Jong-Hyeok Lee",
            "Sanghyun Shin"
        ],
        "abstract": "  Both statistical and rule-based approaches to part-of-speech (POS) disambiguation have their own advantages and limitations. Especially for Korean, the narrow windows provided by hidden markov model (HMM) cannot cover the necessary lexical and long-distance dependencies for POS disambiguation. On the other hand, the rule-based approaches are not accurate and flexible to new tag-sets and languages. In this regard, the statistical/rule-based hybrid method that can take advantages of both approaches is called for the robust and flexible POS disambiguation. We present one of such method, that is, a two-phase learning architecture for the hybrid statistical/rule-based POS disambiguation, especially for Korean. In this method, the statistical learning of morphological tagging is error-corrected by the rule-based learning of Brill [1992] style tagger. We also design the hierarchical and flexible Korean tag-set to cope with the multiple tagging applications, each of which requires different tag-set. Our experiments show that the two-phase learning method can overcome the undesirable features of solely HMM-based or solely rule-based tagging, especially for morphologically complex Korean.\n    ",
        "submission_date": "1995-04-26T00:00:00",
        "last_modified_date": "1995-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504024",
        "title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "authors": [
            "Tanya Bowden",
            "George Anton Kiraz"
        ],
        "abstract": "  This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism. Handling of various Semitic error problems is illustrated, with reference to Arabic and Syriac examples. The model handles errors vocalisation, diacritics, phonetic syncopation and morphographemic idiosyncrasies, in addition to Damerau errors. A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined.\n    ",
        "submission_date": "1995-04-27T00:00:00",
        "last_modified_date": "1995-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504025",
        "title": "Discourse Processing of Dialogues with Multiple Threads",
        "authors": [
            "Carolyn Penstein Rose'",
            "Barbara Di Eugenio",
            "Lori S. Levin",
            "Carol Van Ess-Dykema"
        ],
        "abstract": "  In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-to-speech translation system. We will demonstrate that theories of discourse which postulate a strict tree structure of discourse on either the intentional or attentional level are not totally adequate for handling spontaneous dialogues. We will present our extension to this approach along with its implementation in our plan-based discourse processor. We will demonstrate that the implementation of our approach outperforms an implementation based on the strict tree structure approach.\n    ",
        "submission_date": "1995-04-27T00:00:00",
        "last_modified_date": "1995-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504026",
        "title": "The intersection of Finite State Automata and Definite Clause Grammars",
        "authors": [
            "Gertjan van Noord"
        ],
        "abstract": "  Bernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers. In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable). Furthermore we discuss approaches to cope with the problem.\n    ",
        "submission_date": "1995-04-28T00:00:00",
        "last_modified_date": "1995-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504027",
        "title": "An Efficient Generation Algorithm for Lexicalist MT",
        "authors": [
            "Victor Poznanski",
            "John L. Beaven",
            "Pete Whitelock"
        ],
        "abstract": "  The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions. However, the Shake-and-Bake generation algorithm of (Whitelock, COLING-92) is NP-complete. We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism.\n    ",
        "submission_date": "1995-04-28T00:00:00",
        "last_modified_date": "1995-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504028",
        "title": "Memoization of Coroutined Constraints",
        "authors": [
            "Mark Johnson",
            "Jochen D\u00f6rre"
        ],
        "abstract": "  Some linguistic constraints cannot be effectively resolved during parsing at the location in which they are most naturally introduced. This paper shows how constraints can be propagated in a memoizing parser (such as a chart parser) in much the same way that variable bindings are, providing a general treatment of constraint coroutining in memoization. Prolog code for a simple application of our technique to Bouma and van Noord's (1994) categorial grammar analysis of Dutch is provided.\n    ",
        "submission_date": "1995-04-28T00:00:00",
        "last_modified_date": "1995-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504029",
        "title": "Quantifiers, Anaphora, and Intensionality",
        "authors": [
            "Mary Dalrymple",
            "John Lamping",
            "Fernando Pereira",
            "Vijay Saraswat"
        ],
        "abstract": "  The relationship between Lexical-Functional Grammar (LFG) {\\em functional structures} (f-structures) for sentences and their semantic interpretations can be expressed directly in a fragment of linear logic in a way that correctly explains the constrained interactions between quantifier scope ambiguity, bound anaphora and intensionality. This deductive approach to semantic interpretaion obviates the need for additional mechanisms, such as Cooper storage, to represent the possible scopes of a quantified NP, and explains the interactions between quantified NPs, anaphora and intensional verbs such as `seek'. A single specification in linear logic of the argument requirements of intensional verbs is sufficient to derive the correct reading predictions for intensional-verb clauses both with nonquantified and with quantified direct objects. In particular, both de dicto and de re readings are derived for quantified objects. The effects of type-raising or quantifying-in rules in other frameworks here just follow as linear-logic theorems.\n",
        "submission_date": "1995-04-28T00:00:00",
        "last_modified_date": "1995-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504030",
        "title": "Statistical Decision-Tree Models for Parsing",
        "authors": [
            "David M. Magerman"
        ],
        "abstract": "  Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {$n$}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86\\% precision, 86\\% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91\\% precision, 90\\% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.\n    ",
        "submission_date": "1995-04-29T00:00:00",
        "last_modified_date": "1995-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504031",
        "title": "Error-tolerant Finite State Recognition with Applications to Morphological Analysis and Spelling Correction",
        "authors": [
            "Kemal Oflazer"
        ],
        "abstract": "  Error-tolerant recognition enables the recognition of strings that deviate mildly from any string in the regular set recognized by the underlying finite state recognizer. Such recognition has applications in error-tolerant morphological processing, spelling correction, and approximate string matching in information retrieval. After a description of the concepts and algorithms involved, we give examples from two applications: In the context of morphological analysis, error-tolerant recognition allows misspelled input word forms to be corrected, and morphologically analyzed concurrently. We present an application of this to error-tolerant analysis of agglutinative morphology of Turkish words. The algorithm can be applied to morphological analysis of any language whose morphology is fully captured by a single (and possibly very large) finite state transducer, regardless of the word formation processes and morphographemic phenomena involved. In the context of spelling correction, error-tolerant recognition can be used to enumerate correct candidate forms from a given misspelled string within a certain edit distance. Again, it can be applied to any language with a word list comprising all inflected forms, or whose morphology is fully described by a finite state transducer. We present experimental results for spelling correction for a number of languages. These results indicate that such recognition works very efficiently for candidate generation in spelling correction for many European languages such as English, Dutch, French, German, Italian (and others) with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with edit distance 1) on a SparcStation 10/41. For spelling correction in Turkish, error-tolerant\n    ",
        "submission_date": "1995-04-29T00:00:00",
        "last_modified_date": "1995-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504032",
        "title": "The Replace Operator",
        "authors": [
            "Lauri Karttunen",
            "Rank Xerox Research Centre"
        ],
        "abstract": "  This paper introduces to the calculus of regular expressions a replace operator, ->, and defines a set of replacement expressions that concisely encode several alternate variations of the operation.\n",
        "submission_date": "1995-04-29T00:00:00",
        "last_modified_date": "1995-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504033",
        "title": "Corpus Statistics Meet the Noun Compound: Some Empirical Results",
        "authors": [
            "Mark Lauer"
        ],
        "abstract": "  A variety of statistical methods for noun compound analysis are implemented and compared. The results support two main conclusions. First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy. Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature.\n    ",
        "submission_date": "1995-04-28T00:00:00",
        "last_modified_date": "1996-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9504034",
        "title": "Bayesian Grammar Induction for Language Modeling",
        "authors": [
            "Stanley F. Chen"
        ],
        "abstract": "  We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.\n    ",
        "submission_date": "1995-05-01T00:00:00",
        "last_modified_date": "1995-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505001",
        "title": "Response Generation in Collaborative Negotiation",
        "authors": [
            "Jennifer Chu-Carroll",
            "Sandra Carberry"
        ],
        "abstract": "  In collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, our model can successfully handle embedded negotiation subdialogues.\n    ",
        "submission_date": "1995-05-01T00:00:00",
        "last_modified_date": "1995-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505002",
        "title": "New Techniques for Context Modeling",
        "authors": [
            "Eric Sven Ristad",
            "Robert G. Thomas"
        ],
        "abstract": "  We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies. For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters. In contrast, the character 4-gram model requires more than 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char. The fact that our model performs significantly better while using vastly fewer parameters indicates that it is a better probability model of natural language text.\n    ",
        "submission_date": "1995-05-01T00:00:00",
        "last_modified_date": "1995-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505003",
        "title": "Compiling HPSG type constraints into definite clause programs",
        "authors": [
            "Thilo Goetz",
            "Walt Detmar Meurers"
        ],
        "abstract": "  We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs. This provides a clear and computationally useful correspondence between linguistic theories and their implementation. The compiler performs off-line constraint inheritance and code optimization. As a result, we are able to efficiently process with HPSG grammars without having to hand-translate them into definite clause or phrase structure based systems.\n    ",
        "submission_date": "1995-05-02T00:00:00",
        "last_modified_date": "1995-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505004",
        "title": "DATR Theories and DATR Models",
        "authors": [
            "Bill Keller"
        ],
        "abstract": "  Evans and Gazdar introduced DATR as a simple, non-monotonic language for representing natural language lexicons. Although a number of implementations of DATR exist, the full language has until now lacked an explicit, declarative semantics. This paper rectifies the situation by providing a mathematical semantics for DATR. We present a view of DATR as a language for defining certain kinds of partial functions by cases. The formal model provides a transparent treatment of DATR's notion of global context. It is shown that DATR's default mechanism can be accounted for by interpreting value descriptors as families of values indexed by paths.\n    ",
        "submission_date": "1995-05-02T00:00:00",
        "last_modified_date": "1995-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505005",
        "title": "Learning Syntactic Rules and Tags with Genetic Algorithms for Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules",
        "authors": [
            "Robert M. Losee"
        ],
        "abstract": "  The grammars of natural languages may be learned by using genetic algorithms that reproduce and mutate grammatical rules and part-of-speech tags, improving the quality of later generations of grammatical components. Syntactic rules are randomly generated and then evolve; those rules resulting in improved parsing and occasionally improved retrieval and filtering performance are allowed to further propagate. The LUST system learns the characteristics of the language or sublanguage used in document abstracts by learning from the document rankings obtained from the parsed abstracts. Unlike the application of traditional linguistic rules to retrieval and filtering applications, LUST develops grammatical structures and tags without the prior imposition of some common grammatical assumptions (e.g., part-of-speech assumptions), producing grammars that are empirically based and are optimized for this particular application.\n    ",
        "submission_date": "1995-05-02T00:00:00",
        "last_modified_date": "1995-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505006",
        "title": "Treating Coordination with Datalog Grammars",
        "authors": [
            "Veronica Dahl",
            "Paul Tarau",
            "Lidia Moreno",
            "Manolo Palomar"
        ],
        "abstract": "  In previous work we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency was shown to be better than that of their DCG counterparts under (terminating) OLDT-resolution. In this article we motivate a variant of Datalog grammars which allows us a meta-grammatical treatment of coordination. This treatment improves in some respects over previous work on coordination in logic grammars, although more research is needed for testing it in other respects.\n    ",
        "submission_date": "1995-05-03T00:00:00",
        "last_modified_date": "1995-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505007",
        "title": "Parsing a Flexible Word Order Language",
        "authors": [
            "Vladimir Pericliev",
            "Alexander Grigorov"
        ],
        "abstract": "  A logic formalism is presented which increases the expressive power of the ID/LP format of GPSG by enlarging the inventory of ordering relations and extending the domain of their application to non-siblings. This allows a concise, modular and declarative statement of intricate word order regularities.\n    ",
        "submission_date": "1995-05-03T00:00:00",
        "last_modified_date": "1995-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505008",
        "title": "Conciseness through Aggregation in Text Generation",
        "authors": [
            "James Shaw"
        ],
        "abstract": "  Aggregating different pieces of similar information is necessary to generate concise and easy to understand reports in technical domains. This paper presents a general algorithm that combines similar messages in order to generate one or more coherent sentences for them. The process is not as trivial as might be expected. Problems encountered are briefly described.\n    ",
        "submission_date": "1995-05-03T00:00:00",
        "last_modified_date": "1995-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505009",
        "title": "Compilation of HPSG to TAG",
        "authors": [
            "Robert Kasper",
            "Bernd Kiefer",
            "Klaus Netter",
            "K. Vijay-Shanker"
        ],
        "abstract": "  We present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories. While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly. Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify maximal projections, auxiliary trees and foot nodes.\n    ",
        "submission_date": "1995-05-03T00:00:00",
        "last_modified_date": "1995-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505010",
        "title": "Tagset Reduction Without Information Loss",
        "authors": [
            "Thorsten Brants"
        ],
        "abstract": "  A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment. The technique ensures that all information that is provided by the original tagset can be restored from the reduced one. This is crucial, since we are interested in the linguistically motivated tags for part-of-speech disambiguation. The reduced tagset needs fewer parameters for its statistical model and allows more accurate parameter estimation. Additionally, there is a slight but not significant improvement of tagging accuracy.\n    ",
        "submission_date": "1995-05-03T00:00:00",
        "last_modified_date": "1995-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505011",
        "title": "Evaluation of Semantic Clusters",
        "authors": [
            "Rajeev Agarwal"
        ],
        "abstract": "  Semantic clusters of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation. Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques. However, not much progress has been made in evaluating the obtained semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts.\n    ",
        "submission_date": "1995-05-04T00:00:00",
        "last_modified_date": "1995-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505012",
        "title": "A Symbolic and Surgical Acquisition of Terms through Variation",
        "authors": [
            "Christian Jacquemin"
        ],
        "abstract": "  Terminological acquisition is an important issue in learning for NLP due to the constant terminological renewal through technological changes. Terms play a key role in several NLP-activities such as machine translation, automatic indexing or text understanding. In opposition to classical once-and-for-all approaches, we propose an incremental process for terminological enrichment which operates on existing reference lists and large corpora. Candidate terms are acquired by extracting variants of reference terms through {\\em FASTR}, a unification-based partial parser. As acquisition is performed within specific morpho-syntactic contexts (coordinations, insertions or permutations of compounds), rich conceptual links are learned together with candidate terms. A clustering of terms related through coordination yields classes of conceptually close terms while graphs resulting from insertions denote generic/specific relations. A graceful degradation of the volume of acquisition on partial initial lists confirms the robustness of the method to incomplete data.\n    ",
        "submission_date": "1995-05-04T00:00:00",
        "last_modified_date": "1995-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505013",
        "title": "Utilizing Statistical Dialogue Act Processing in Verbmobil",
        "authors": [
            "Norbert Reithinger",
            "Elisabeth Maier"
        ],
        "abstract": "  In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system \\vm. Statistics in dialogue processing is used to predict follow-up dialogue acts. As an application example we show how it supports repair when unexpected dialogue states occur.\n    ",
        "submission_date": "1995-05-05T00:00:00",
        "last_modified_date": "1995-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505014",
        "title": "Compositionality for Presuppositions over Tableaux",
        "authors": [
            "Pablo Gervas"
        ],
        "abstract": "  Tableaux originate as a decision method for a logical language. They can also be extended to obtain a structure that spells out all the information in a set of sentences in terms of truth value assignments to atomic formulas that appear in them. This approach is pursued here. Over such a structure, compositional rules are provided for obtaining the presuppositions of a logical statement from its atomic subformulas and their presuppositions. The rules are based on classical logic semantics and they are shown to model the behaviour of presuppositions observed in natural language sentences built with {\\em if \\ldots then}, {\\em and} and {\\em or}. The advantages of this method over existing frameworks for presuppositions are discussed.\n    ",
        "submission_date": "1995-05-05T00:00:00",
        "last_modified_date": "1995-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505015",
        "title": "Efficient Analysis of Complex Diagrams using Constraint-Based Parsing",
        "authors": [
            "Robert P. Futrelle",
            "Nikos Nikolakis"
        ],
        "abstract": "  This paper describes substantial advances in the analysis (parsing) of diagrams using constraint grammars. The addition of set types to the grammar and spatial indexing of the data make it possible to efficiently parse real diagrams of substantial complexity. The system is probably the first to demonstrate efficient diagram parsing using grammars that easily be retargeted to other domains. The work assumes that the diagrams are available as a flat collection of graphics primitives: lines, polygons, circles, Bezier curves and text. This is appropriate for future electronic documents or for vectorized diagrams converted from scanned images. The classes of diagrams that we have analyzed include x,y data graphs and genetic diagrams drawn from the biological literature, as well as finite state automata diagrams (states and arcs). As an example, parsing a four-part data graph composed of 133 primitives required 35 sec using Macintosh Common Lisp on a Macintosh Quadra 700.\n    ",
        "submission_date": "1995-05-05T00:00:00",
        "last_modified_date": "1995-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505016",
        "title": "A Pattern Matching method for finding Noun and Proper Noun Translations from Noisy Parallel Corpora",
        "authors": [
            "Pascale Fung"
        ],
        "abstract": "  We present a pattern matching method for compiling a bilingual lexicon of nouns and proper nouns from unaligned, noisy parallel texts of Asian/Indo-European language pairs. Tagging information of one language is used. Word frequency and position information for high and low frequency words are represented in two different vector forms for pattern matching. New anchor point finding and noise elimination techniques are introduced. We obtained a 73.1\\% precision. We also show how the results can be used in the compilation of domain-specific noun phrases.\n    ",
        "submission_date": "1995-05-06T00:00:00",
        "last_modified_date": "1995-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505017",
        "title": "Robust Parsing of Spoken Dialogue Using Contextual Knowledge and Recognition Probabilities",
        "authors": [
            "Gerhard Hanrieder",
            "Guenther Goerz"
        ],
        "abstract": "  In this paper we describe the linguistic processor of a spoken dialogue system. The parser receives a word graph from the recognition module as its input. Its task is to find the best path through the graph. If no complete solution can be found, a robust mechanism for selecting multiple partial results is applied. We show how the information content rate of the results can be improved if the selection is based on an integrated quality score combining word recognition scores and context-dependent semantic predictions. Results of parsing word graphs with and without predictions are reported.\n    ",
        "submission_date": "1995-05-08T00:00:00",
        "last_modified_date": "1995-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505018",
        "title": "Acquiring a Lexicon from Unsegmented Speech",
        "authors": [
            "Carl de Marcken"
        ],
        "abstract": "  We present work-in-progress on the machine acquisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition.\n    ",
        "submission_date": "1995-05-08T00:00:00",
        "last_modified_date": "1995-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505019",
        "title": "Measuring semantic complexity",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "abstract": "  We define {\\em semantic complexity} using a new concept of {\\em meaning automata}. We measure the semantic complexity of understanding of prepositional phrases, of an \"in depth understanding system\", and of a natural language interface to an on-line calendar. We argue that it is possible to measure some semantic complexities of natural language processing systems before building them, and that systems that exhibit relatively complex behavior can be built from semantically simple components.\n    ",
        "submission_date": "1995-05-08T00:00:00",
        "last_modified_date": "1995-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505020",
        "title": "CRYSTAL: Inducing a Conceptual Dictionary",
        "authors": [
            "Stephen Soderland",
            "David Fisher",
            "Jonathan Aseltine",
            "Wendy Lehnert"
        ],
        "abstract": "  One of the central knowledge sources of an information extraction system is a dictionary of linguistic patterns that can be used to identify the conceptual content of a text. This paper describes CRYSTAL, a system which automatically induces a dictionary of \"concept-node definitions\" sufficient to identify relevant information from a training corpus. Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances. Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules.\n    ",
        "submission_date": "1995-05-09T00:00:00",
        "last_modified_date": "1995-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505021",
        "title": "Improving the Efficiency of a Generation Algorithm for Shake and Bake Machine Translation Using Head-Driven Phrase Structure Grammar",
        "authors": [
            "Fred Popowich"
        ],
        "abstract": "  A Shake and Bake machine translation algorithm for Head-Driven Phrase Structure Grammar is introduced based on the algorithm proposed by Whitelock for unification categorial grammar. The translation process is then analysed to determine where the potential sources of inefficiency reside, and some proposals are introduced which greatly improve the efficiency of the generation algorithm. Preliminary empirical results from tests involving a small grammar are presented, and suggestions for greater improvement to the algorithm are provided.\n    ",
        "submission_date": "1995-05-09T00:00:00",
        "last_modified_date": "1995-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505022",
        "title": "Generating One-Anaphoric Expressions: Where Does the Decision Lie?",
        "authors": [
            "Robert Dale"
        ],
        "abstract": "  Most natural language generation systems embody mechanisms for choosing whether to subsequently refer to an already-introduced entity by means of a pronoun or a definite noun phrase. Relatively few systems, however, consider referring to entites by means of one-anaphoric expressions such as \\lingform{the small green one}. This paper looks at what is involved in generating referring expressions of this type. Consideration of how to fit this capability into a standard algorithm for referring expression generation leads us to suggest a revision of some of the assumptions that underlie existing approaches. We demonstrate the usefulness of our approach to one-anaphora generation in the context of a simple database interface application, and make some observations about the impact of this approach on referring expression generation more generally.\n    ",
        "submission_date": "1995-05-09T00:00:00",
        "last_modified_date": "1995-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505023",
        "title": "Some Novel Applications of Explanation-Based Learning to Parsing Lexicalized Tree-Adjoining Grammars",
        "authors": [
            "B. Srinivas",
            "Aravind Joshi"
        ],
        "abstract": "  In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars. The novel aspects are (a) immediate generalization of parses in the training set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a ``stapler'' has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach.\n    ",
        "submission_date": "1995-05-10T00:00:00",
        "last_modified_date": "1995-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505024",
        "title": "Exploring the role of Punctuation in Parsing Natural Text",
        "authors": [
            "Bernard Jones"
        ],
        "abstract": "  Few, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars --- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing.\n    ",
        "submission_date": "1995-05-10T00:00:00",
        "last_modified_date": "1995-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505025",
        "title": "Combining Multiple Knowledge Sources for Discourse Segmentation",
        "authors": [
            "Diane J. Litman",
            "Rebecca J. Passonneau"
        ],
        "abstract": "  We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data. We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning).\n    ",
        "submission_date": "1995-05-10T00:00:00",
        "last_modified_date": "1995-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505026",
        "title": "Tagging the Teleman Corpus",
        "authors": [
            "Thorsten Brants",
            "Christer Samuelsson"
        ],
        "abstract": "  Experiments were carried out comparing the Swedish Teleman and the English Susanne corpora using an HMM-based and a novel reductionistic statistical part-of-speech tagger. They indicate that tagging the Teleman corpus is the more difficult task, and that the performance of the two different taggers is comparable.\n    ",
        "submission_date": "1995-05-11T00:00:00",
        "last_modified_date": "1995-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505027",
        "title": "Quantifier Scope and Constituency",
        "authors": [
            "Jong C. Park"
        ],
        "abstract": "  Traditional approaches to quantifier scope typically need stipulation to exclude readings that are unavailable to human understanders. This paper shows that quantifier scope phenomena can be precisely characterized by a semantic representation constrained by surface constituency, if the distinction between referential and quantificational NPs is properly observed. A CCG implementation is described and compared to other approaches.\n    ",
        "submission_date": "1995-05-11T00:00:00",
        "last_modified_date": "1995-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505028",
        "title": "D-Tree Grammars",
        "authors": [
            "Owen Rambow",
            "K. Vijay-Shanker",
            "David Weir"
        ],
        "abstract": "  DTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for em wh-movement in English and Kashmiri, despite the fact that the em wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.\n    ",
        "submission_date": "1995-05-12T00:00:00",
        "last_modified_date": "1995-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505029",
        "title": "Mapping Scrambled Korean Sentences into English Using Synchronous TAGs",
        "authors": [
            "Hyun S. Park"
        ],
        "abstract": "  Synchronous Tree Adjoining Grammars can be used for Machine Translation. However, translating a free order language such as Korean to English is complicated. I present a mechanism to translate scrambled Korean sentences into English by combining the concepts of Multi-Component TAGs (MC-TAGs) and Synchronous TAGs (STAGs).\n    ",
        "submission_date": "1995-05-13T00:00:00",
        "last_modified_date": "1995-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505030",
        "title": "Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy",
        "authors": [
            "Roger Evans",
            "Gerald Gazdar",
            "David Weir"
        ],
        "abstract": "  This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon.\n    ",
        "submission_date": "1995-05-15T00:00:00",
        "last_modified_date": "1995-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505031",
        "title": "The Compactness of Construction Grammars",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "abstract": "  We present an argument for {\\em construction grammars} based on the minimum description length (MDL) principle (a formal version of the Ockham Razor). The argument consists in using linguistic and computational evidence in setting up a formal model, and then applying the MDL principle to prove its superiority with respect to alternative models. We show that construction-based representations are at least an order of magnitude more compact that the corresponding lexicalized representations of the same linguistic data.\n",
        "submission_date": "1995-05-15T00:00:00",
        "last_modified_date": "1995-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505032",
        "title": "Context and ontology in understanding of dialogs",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "abstract": "  We present a model of NLP in which ontology and context are directly included in a grammar. The model is based on the concept of {\\em construction}, consisting of a set of features of form, a set of semantic and pragmatic conditions describing its application context, and a description of its meaning. In this model ontology is embedded into the grammar; e.g. the hierarchy of {\\it np} constructions is based on the corresponding ontology. Ontology is also used in defining contextual parameters; e.g. $\\left[ current\\_question \\ time(\\_) \\right] $.\n",
        "submission_date": "1995-05-15T00:00:00",
        "last_modified_date": "1995-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505033",
        "title": "User-Defined Nonmonotonicity in Unification-Based Formalisms",
        "authors": [
            "Lena Stromback"
        ],
        "abstract": "  A common feature of recent unification-based grammar formalisms is that they give the user the ability to define his own structures. However, this possibility is mostly limited and does not include nonmonotonic operations. In this paper we show how nonmonotonic operations can also be user-defined by applying default logic (Reiter 1980) and generalizing previous results on nonmonotonic sorts (Young & Rounds 1993).\n    ",
        "submission_date": "1995-05-16T00:00:00",
        "last_modified_date": "1995-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505034",
        "title": "Semantic Ambiguity and Perceived Ambiguity",
        "authors": [
            "Massimo Poesio"
        ],
        "abstract": "  I explore some of the issues that arise when trying to establish a connection between the underspecification hypothesis pursued in the NLP literature and work on ambiguity in semantics and in the psychological literature. A theory of underspecification is developed `from the first principles', i.e., starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity. An underspecified language is specified as the translation language of a grammar covering sentences that display three classes of semantic ambiguity: lexical ambiguity, scopal ambiguity, and referential ambiguity. The expressions of this language denote sets of senses. A formalization of defeasible reasoning with underspecified representations is presented, based on Default Logic. Some issues to be confronted by such a formalization are discussed.\n    ",
        "submission_date": "1995-05-16T00:00:00",
        "last_modified_date": "1995-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505035",
        "title": "Development of a Spanish Version of the Xerox Tagger",
        "authors": [
            "Fernando S\u00e1nchez Le\u00f3n",
            "Amalio F. Nieto Serrano"
        ],
        "abstract": "  This paper describes work performed withing the CRATER ({\\em C}orpus {\\em R}esources {\\em A}nd {\\em T}erminology {\\em E}xt{\\em R}action, MLAP-93/20) project, funded by the Commission of the European Communities. In particular, it addresses the issue of adapting the Xerox Tagger to Spanish in order to tag the Spanish version of the ITU (International Telecommunications Union) corpus. The model implemented by this tagger is briefly presented along with some modifications performed on it in order to use some parameters not probabilistically estimated. Initial decisions, like the tagset, the lexicon and the training corpus are also discussed. Finally, results are presented and the benefits of the {\\em mixed model} justified.\n    ",
        "submission_date": "1995-05-19T00:00:00",
        "last_modified_date": "1995-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505036",
        "title": "Integrating Gricean and Attentional Constraints",
        "authors": [
            "Rebecca J. Passonneau"
        ],
        "abstract": "  This paper concerns how to generate and understand discourse anaphoric noun phrases. I present the results of an analysis of all discourse anaphoric noun phrases (N=1,233) in a corpus of ten narrative monologues, where the choice between a definite pronoun or phrasal NP conforms largely to Gricean constraints on informativeness. I discuss Dale and Reiter's [To appear] recent model and show how it can be augmented for understanding as well as generating the range of data presented here. I argue that integrating centering [Grosz et al., 1983] [Kameyama, 1985] with this model can be applied uniformly to discourse anaphoric pronouns and phrasal NPs. I conclude with a hypothesis for addressing the interaction between local and global discourse processing.\n    ",
        "submission_date": "1995-05-19T00:00:00",
        "last_modified_date": "1995-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505037",
        "title": "Identifying Word Translations in Non-Parallel Texts",
        "authors": [
            "Reinhard Rapp"
        ],
        "abstract": "  Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages.\n    ",
        "submission_date": "1995-05-22T00:00:00",
        "last_modified_date": "1995-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505038",
        "title": "Ubiquitous Talker: Spoken Language Interaction with Real World Objects",
        "authors": [
            "Katashi Nagao",
            "Jun Rekimoto"
        ],
        "abstract": "  Augmented reality is a research area that tries to embody an electronic information space within the real world, through computational devices. A crucial issue within this area, is the recognition of real world objects or situations.\n",
        "submission_date": "1995-05-23T00:00:00",
        "last_modified_date": "1995-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505039",
        "title": "Generating efficient belief models for task-oriented dialogues",
        "authors": [
            "Jasper Taylor"
        ],
        "abstract": "  We have shown that belief modelling for dialogue can be simplified if the assumption is made that the participants are cooperating, i.e., they are not committed to any goals requiring deception. In such domains, there is no need to maintain individual representations of deeply nested beliefs; instead, three specific types of belief can be used to summarize all the states of nested belief that can exist about a domain entity.\n",
        "submission_date": "1995-05-23T00:00:00",
        "last_modified_date": "1995-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505040",
        "title": "Text Chunking using Transformation-Based Learning",
        "authors": [
            "Lance A. Ramshaw",
            "Mitchell P. Marcus"
        ],
        "abstract": "  Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.\n    ",
        "submission_date": "1995-05-23T00:00:00",
        "last_modified_date": "1995-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505041",
        "title": "On Descriptive Complexity, Language Complexity, and GB",
        "authors": [
            "James Rogers"
        ],
        "abstract": "  We introduce $L^2_{K,P}$, a monadic second-order language for reasoning about trees which characterizes the strongly Context-Free Languages in the sense that a set of finite trees is definable in $L^2_{K,P}$ iff it is (modulo a projection) a Local Set---the set of derivation trees generated by a CFG. This provides a flexible approach to establishing language-theoretic complexity results for formalisms that are based on systems of well-formedness constraints on trees. We demonstrate this technique by sketching two such results for Government and Binding Theory. First, we show that {\\em free-indexation\\/}, the mechanism assumed to mediate a variety of agreement and binding relationships in GB, is not definable in $L^2_{K,P}$ and therefore not enforcible by CFGs. Second, we show how, in spite of this limitation, a reasonably complete GB account of English can be defined in $L^2_{K,P}$. Consequently, the language licensed by that account is strongly context-free. We illustrate some of the issues involved in establishing this result by looking at the definition, in $L^2_{K,P}$, of chains. The limitations of this definition provide some insight into the types of natural linguistic principles that correspond to higher levels of language complexity. We close with some speculation on the possible significance of these results for generative linguistics.\n    ",
        "submission_date": "1995-05-23T00:00:00",
        "last_modified_date": "1995-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505042",
        "title": "Robust Parsing Based on Discourse Information: Completing partial parses of ill-formed sentences on the basis of discourse information",
        "authors": [
            "Tetsuya Nasukawa"
        ],
        "abstract": "  In a consistent text, many words and phrases are repeatedly used in more than one sentence. When an identical phrase (a set of consecutive words) is repeated in different sentences, the constituent words of those sentences tend to be associated in identical modification patterns with identical parts of speech and identical modifiee-modifier relationships. Thus, when a syntactic parser cannot parse a sentence as a unified structure, parts of speech and modifiee-modifier relationships among morphologically identical words in complete parses of other sentences within the same text provide useful information for obtaining partial parses of the sentence. In this paper, we describe a method for completing partial parses by maintaining consistency among morphologically identical words within the same text as regards their part of speech and their modifiee-modifier relationship. The experimental results obtained by using this method with technical documents offer good prospects for improving the accuracy of sentence analysis in a broad-coverage natural language processing system such as a machine translation system.\n    ",
        "submission_date": "1995-05-24T00:00:00",
        "last_modified_date": "1995-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505043",
        "title": "Using Decision Trees for Coreference Resolution",
        "authors": [
            "Joseph F. McCarthy",
            "Wendy G. Lehnert"
        ],
        "abstract": "  This paper describes RESOLVE, a system that uses decision trees to learn how to classify coreferent phrases in the domain of business joint ventures. An experiment is presented in which the performance of RESOLVE is compared to the performance of a manually engineered set of rules for the same task. The results show that decision trees achieve higher performance than the rules in two of three evaluation metrics developed for the coreference task. In addition to achieving better performance than the rules, RESOLVE provides a framework that facilitates the exploration of the types of knowledge that are useful for solving the coreference problem.\n    ",
        "submission_date": "1995-05-24T00:00:00",
        "last_modified_date": "1995-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505044",
        "title": "Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons",
        "authors": [
            "I. Dan Melamed"
        ],
        "abstract": "  This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.\n    ",
        "submission_date": "1995-05-25T00:00:00",
        "last_modified_date": "1995-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9505045",
        "title": "Hybrid Transfer in an English-French Spoken Language Translator",
        "authors": [
            "Manny Rayner",
            "Pierrette Bouillon"
        ],
        "abstract": "  The paper argues the importance of high-quality translation for spoken language translation systems. It describes an architecture suitable for rapid development of high-quality limited-domain translation systems, which has been implemented within an advanced prototype English to French spoken language translator. The focus of the paper is the hybrid transfer model which combines unification-based rules and a set of trainable statistical preferences; roughly, rules encode domain-independent grammatical information and preferences encode domain-dependent distributional information. The preferences are trained from sets of examples produced by the system, which have been annotated by human judges as correct or incorrect. An experiment is described in which the model was tested on a 2000 utterance sample of previously unseen data.\n    ",
        "submission_date": "1995-05-26T00:00:00",
        "last_modified_date": "1995-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506001",
        "title": "Ma(r)king concessions in English and German",
        "authors": [
            "Brigitte Grote",
            "Nils Lenke",
            "Manfred Stede"
        ],
        "abstract": "  In order to generate cohesive discourse, many of the relations holding between text segments need to be signalled to the reader by means of cue words, or {\\em discourse markers}. Programs usually do this in a simplistic way, e.g., by using one marker per relation. In reality, however, language offers a very wide range of markers from which informed choices should be made. In order to account for the variety and to identify the parameters governing the choices, detailled linguistic analyses are necessary. We worked with one area of discourse relations, the Concession family, identified its underlying pragmatics and semantics, and undertook extensive corpus studies to examine the range of markers used in both English and German. On the basis of an initial classification of these markers, we propose a generation model for producing bilingual text that can incorporate marker choice into its overall decision framework.\n    ",
        "submission_date": "1995-06-01T00:00:00",
        "last_modified_date": "1995-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506002",
        "title": "Weak subsumption Constraints for Type Diagnosis: An Incremental Algorithm",
        "authors": [
            "Martin Mueller",
            "Joachim Niehren"
        ],
        "abstract": "  We introduce constraints necessary for type checking a higher-order concurrent constraint language, and solve them with an incremental algorithm. Our constraint system extends rational unification by constraints x$\\subseteq$ y saying that ``$x$ has at least the structure of $y$'', modelled by a weak instance relation between trees. This notion of instance has been carefully chosen to be weaker than the usual one which renders semi-unification undecidable. Semi-unification has more than once served to link unification problems arising from type inference and those considered in computational linguistics. Just as polymorphic recursion corresponds to subsumption through the semi-unification problem, our type constraint problem corresponds to weak subsumption of feature graphs in linguistics. The decidability problem for \\WhatsIt for feature graphs has been settled by D\u00f6rre~\\cite{Doerre:WeakSubsumption:94}. \\nocite{RuppRosnerJohnson:94} In contrast to D\u00f6rre's, our algorithm is fully incremental and does not refer to finite state automata. Our algorithm also is a lot more flexible. It allows a number of extensions (records, sorts, disjunctive types, type declarations, and others) which make it suitable for type inference of a full-fledged programming language.\n    ",
        "submission_date": "1995-06-02T00:00:00",
        "last_modified_date": "1995-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506003",
        "title": "Syllable parsing in English and French",
        "authors": [
            "Michael Hammond"
        ],
        "abstract": "  In this paper I argue that Optimality Theory provides for an explanatory model of syllabic parsing in English and French. The argument is based on psycholinguistic facts that have been mysterious up to now. This argument is further buttressed by the computational implementation developed here. This model is important for several reasons. First, it provides a demonstration of how OT can be used in a performance domain. Second, it suggests a new relationship between phonological theory and psycholinguistics. (Code in Perl is included and a WWW-interface is running at ",
        "submission_date": "1995-06-02T00:00:00",
        "last_modified_date": "1995-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506004",
        "title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "authors": [
            "Seth Kulick"
        ],
        "abstract": "  Many theories of semantic interpretation use lambda-term manipulation to compositionally compute the meaning of a sentence. These theories are usually implemented in a language such as Prolog that can simulate lambda-term operations with first-order unification. However, for some interesting cases, such as a Combinatory Categorial Grammar account of coordination constructs, this can only be done by obscuring the underlying linguistic theory with the ``tricks'' needed for implementation. This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar, including its handling of coordination constructs.\n    ",
        "submission_date": "1995-06-06T00:00:00",
        "last_modified_date": "1995-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506005",
        "title": "A Support Tool for Tagset Mapping",
        "authors": [
            "Simone Teufel"
        ],
        "abstract": "  Many different tagsets are used in existing corpora; these tagsets vary according to the objectives of specific projects (which may be as far apart as robust parsing vs. spelling correction). In many situations, however, one would like to have uniform access to the linguistic information encoded in corpus annotations without having to know the classification schemes in detail. This paper describes a tool which maps unstructured morphosyntactic tags to a constraint-based, typed, configurable specification language, a ``standard tagset''. The mapping relies on a manually written set of mapping rules, which is automatically checked for consistency. In certain cases, unsharp mappings are unavoidable, and noise, i.e. groups of word forms {\\sl not} conforming to the specification, will appear in the output of the mapping. The system automatically detects such noise and informs the user about it. The tool has been tested with rules for the UPenn tagset \\cite{up} and the SUSANNE tagset \\cite{garside}, in the framework of the EAGLES\\footnote{LRE project EAGLES, cf. \\cite{eagles}.} validation phase for standardised tagsets for European languages.\n    ",
        "submission_date": "1995-06-08T00:00:00",
        "last_modified_date": "1995-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506006",
        "title": "Automatic Extraction of Tagset Mappings from Parallel-Annotated Corpora",
        "authors": [
            "John Hughes",
            "Clive Souter",
            "Eric Atwell"
        ],
        "abstract": "  This paper describes some of the recent work of project AMALGAM (automatic mapping among lexico-grammatical annotation models). We are investigating ways to map between the leading corpus annotation schemes in order to improve their resuability. Collation of all the included corpora into a single large annotated corpus will provide a more detailed language model to be developed for tasks such as speech and handwriting recognition. In particular, we focus here on a method of extracting mappings from corpora that have been annotated according to more than one annotation scheme.\n    ",
        "submission_date": "1995-06-08T00:00:00",
        "last_modified_date": "1995-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506007",
        "title": "Features and Agreement",
        "authors": [
            "Sam Bayer",
            "Mark Johnson"
        ],
        "abstract": "  This paper compares the consistency-based account of agreement phenomena in `unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar (LCG). We show that the LCG treatment accounts for constructions that have been recognized as problematic for `unification-based' treatments.\n    ",
        "submission_date": "1995-06-08T00:00:00",
        "last_modified_date": "1995-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506008",
        "title": "CLiFF Notes: Research in the Language, Information and Computation Laboratory of the University of Pennsylvania",
        "authors": [
            "Editors",
            "Matthew Stone",
            "Libby Levison"
        ],
        "abstract": "  Short abstracts by computational linguistics researchers at the University of Pennsylvania describing ongoing individual and joint projects.\n    ",
        "submission_date": "1995-06-09T00:00:00",
        "last_modified_date": "1995-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506009",
        "title": "Filling Knowledge Gaps in a Broad-Coverage Machine Translation System",
        "authors": [
            "Kevin Knight",
            "Ishwar Chander",
            "Matthew Haines",
            "Vasileios Hatzivassiloglou",
            "Eduard Hovy",
            "Masayo Iida",
            "Steve K. Luk",
            "Richard Whitney",
            "Kenji Yamada"
        ],
        "abstract": "  Knowledge-based machine translation (KBMT) techniques yield high quality in domains with detailed semantic models, limited vocabulary, and controlled input grammar. Scaling up along these dimensions means acquiring large knowledge resources. It also means behaving reasonably when definitive knowledge is not yet available. This paper describes how we can fill various KBMT knowledge gaps, often using robust statistical techniques. We describe quantitative and qualitative results from JAPANGLOSS, a broad-coverage Japanese-English MT system.\n    ",
        "submission_date": "1995-06-10T00:00:00",
        "last_modified_date": "1995-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506010",
        "title": "Two-level, Many-Paths Generation",
        "authors": [
            "Kevin Knight",
            "Vasileios Hatzivassiloglou"
        ],
        "abstract": "  Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.\n    ",
        "submission_date": "1995-06-10T00:00:00",
        "last_modified_date": "1995-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506011",
        "title": "Unification-Based Glossing",
        "authors": [
            "Vasileios Hatzivassiloglou",
            "Kevin Knight"
        ],
        "abstract": "  We present an approach to syntax-based machine translation that combines unification-style interpretation with statistical processing. This approach enables us to translate any Japanese newspaper article into English, with quality far better than a word-for-word translation. Novel ideas include the use of feature structures to encode word lattices and the use of unification to compose and manipulate lattices. Unification also allows us to specify abstract features that delay target-language synthesis until enough source-language information is assembled. Our statistical component enables us to search efficiently among competing translations and locate those with high English fluency.\n    ",
        "submission_date": "1995-06-10T00:00:00",
        "last_modified_date": "1995-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506012",
        "title": "Presenting Punctuation",
        "authors": [
            "Michael White"
        ],
        "abstract": "  Until recently, punctuation has received very little attention in the linguistics and computational linguistics literature. Since the publication of Nunberg's (1990) monograph on the topic, however, punctuation has seen its stock begin to rise: spurred in part by Nunberg's ground-breaking work, a number of valuable inquiries have been subsequently undertaken, including Hovy and Arens (1991), Dale (1991), Pascual (1993), Jones (1994), and Briscoe (1994). Continuing this line of research, I investigate in this paper how Nunberg's approach to presenting punctuation (and other formatting devices) might be incorporated into NLG systems. Insofar as the present paper focuses on the proper syntactic treatment of punctuation, it differs from these other subsequent works in that it is the first to examine this issue from the generation perspective.\n    ",
        "submission_date": "1995-06-10T00:00:00",
        "last_modified_date": "1995-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506013",
        "title": "A Study of the Context(s) in a Specific Type of Texts: Car Accident Reports",
        "authors": [
            "Dominique Estival",
            "Francoise Gayral"
        ],
        "abstract": "  This paper addresses the issue of defining context, and more specifically the different contexts needed for understanding a particular type of texts. The corpus chosen is homogeneous and allows us to determine characteristic properties of the texts from which certain inferences can be drawn by the reader. These characteristic properties come from the real world domain (K-context), the type of events the texts describe (F-context) and the genre of the texts (E-context). Together, these three contexts provide elements for the resolution of anaphoric expressions and for several types of disambiguation. We show in particular that the argumentation aspect of these texts is an essential part of the context and explains some of the inferences that can be drawn.\n    ",
        "submission_date": "1995-06-13T00:00:00",
        "last_modified_date": "1995-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506014",
        "title": "Inducing Features of Random Fields",
        "authors": [
            "S. Della Pietra",
            "V. Della Pietra",
            "J. Lafferty"
        ],
        "abstract": "  We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights.\n",
        "submission_date": "1995-06-13T00:00:00",
        "last_modified_date": "1995-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506015",
        "title": "Ambiguity in the Acquisition of Lexical Information",
        "authors": [
            "Lucy Vanderwende"
        ],
        "abstract": "  This paper describes an approach to the automatic identification of lexical information in on-line dictionaries. This approach uses bootstrapping techniques, specifically so that ambiguity in the dictionary text can be treated properly. This approach consists of processing an on-line dictionary multiple times, each time refining the lexical information previously acquired and identifying new lexical information. The strength of this approach is that lexical information can be acquired from definitions which are syntactically ambiguous, given that information acquired during the first pass can be used to improve the syntactic analysis of definitions in subsequent passes. In the context of a lexical knowledge base, the types of lexical information that need to be represented cannot be viewed as a fixed set, but rather as a set that will change given the resources of the lexical knowledge base and the requirements of analysis systems which access it.\n    ",
        "submission_date": "1995-06-14T00:00:00",
        "last_modified_date": "1995-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506016",
        "title": "Indefeasible Semantics and Defeasible Pragmatics",
        "authors": [
            "Megumi Kameyama"
        ],
        "abstract": "  An account of utterance interpretation in discourse needs to face the issue of how the discourse context controls the space of interacting preferences. Assuming a discourse processing architecture that distinguishes the grammar and pragmatics subsystems in terms of monotonic and nonmonotonic inferences, I will discuss how independently motivated default preferences interact in the interpretation of intersentential pronominal anaphora. In the framework of a general discourse processing model that integrates both the grammar and pragmatics subsystems, I will propose a fine structure of the preferential interpretation in pragmatics in terms of defeasible rule interactions. The pronoun interpretation preferences that serve as the empirical ground draw from the survey data specifically obtained for the present purpose.\n    ",
        "submission_date": "1995-06-14T00:00:00",
        "last_modified_date": "1995-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506017",
        "title": "The Effect of Pitch Accenting on Pronoun Referent Resolution",
        "authors": [
            "Janet Cahn"
        ],
        "abstract": "  By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.\n    ",
        "submission_date": "1995-06-18T00:00:00",
        "last_modified_date": "1995-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506018",
        "title": "Intelligent Voice Prosthesis: Converting Icons into Natural Language Sentences",
        "authors": [
            "Pascal Vaillant",
            "Michael Checler"
        ],
        "abstract": "  The Intelligent Voice Prosthesis is a communication tool which reconstructs the meaning of an ill-structured sequence of icons or symbols, and expresses this meaning into sentences of a Natural Language (French). It has been developed for the use of people who cannot express themselves orally in natural language, and further, who are not able to comply to grammatical rules such as those of natural language. We describe how available corpora of iconic communication by children with Cerebral Palsy has led us to implement a simple and relevant semantic description of the symbol lexicon. We then show how a unification-based, bottom-up semantic analysis allows the system to uncover the meaning of the user's utterances by computing proper dependencies between the symbols. The result of the analysis is then passed to a lexicalization module which chooses the right words of natural language to use, and builds a linguistic semantic network. This semantic network is then generated into French sentences via hierarchization into trees, using a lexicalized Tree Adjoining Grammar. Finally we describe the modular, customizable interface which has been developed for this system.\n    ",
        "submission_date": "1995-06-21T00:00:00",
        "last_modified_date": "1995-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506019",
        "title": "Review of Charniak's \"Statistical Language Learning\"",
        "authors": [
            "David M. Magerman"
        ],
        "abstract": "  This article is an in-depth review of Eugene Charniak's book, \"Statistical Language Learning\". The review evaluates the appropriateness of the book as an introductory text for statistical language learning for a variety of audiences. It also includes an extensive bibliography of articles and papers which might be used as a supplement to this book for learning or teaching statistical language modeling.\n    ",
        "submission_date": "1995-06-21T00:00:00",
        "last_modified_date": "1995-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506020",
        "title": "GLR-Parsing of Word Lattices Using a Beam Search Method",
        "authors": [
            "Steffen Staab"
        ],
        "abstract": "  This paper presents an approach that allows the efficient integration of speech recognition and language understanding using Tomita's generalized LR-parsing algorithm. For this purpose the GLRP-algorithm is revised so that an agenda mechanism can be used to control the flow of computation of the parsing process. This new approach is used to integrate speech recognition and speech understanding incrementally with a beam search method. These considerations have been implemented and tested on ten word lattices.\n    ",
        "submission_date": "1995-06-22T00:00:00",
        "last_modified_date": "1995-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506021",
        "title": "Prepositional Phrase Attachment through a Backed-Off Model",
        "authors": [
            "Michael Collins",
            "James Brooks"
        ],
        "abstract": "  Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form {v np1 p np2} are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events - ignoring events which occur less than 5 times in training data reduces performance to 81.6%.\n    ",
        "submission_date": "1995-06-22T00:00:00",
        "last_modified_date": "1995-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506022",
        "title": "Deriving Procedural and Warning Instructions from Device and Environment Models",
        "authors": [
            "Daniel Ansari"
        ],
        "abstract": "  This study is centred on the generation of instructions for household appliances. We show how knowledge about a device, together with knowledge about the environment, can be used for reasoning about instructions. The information communicated by the instructions can be planned from a version of the knowledge of the artifact and environment. We present the latter, which we call the {\\it planning knowledge}, in the form of axioms in the {\\it situation calculus}. This planning knowledge formally characterizes the behaviour of the artifact, and it is used to produce a basic plan of actions that the device and user take to accomplish a given goal. We explain how both procedural and warning instructions can be generated from this basic plan.\n",
        "submission_date": "1995-06-24T00:00:00",
        "last_modified_date": "1995-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506023",
        "title": "Empirical Discovery in Linguistics",
        "authors": [
            "Vladimir Pericliev"
        ],
        "abstract": "  A discovery system for detecting correspondences in data is described, based on the familiar induction methods of J. S. Mill. Given a set of observations, the system induces the ``causally'' related facts in these observations. Its application to empirical linguistic discovery is described.\n    ",
        "submission_date": "1995-06-26T00:00:00",
        "last_modified_date": "1995-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506024",
        "title": "An Approach to Proper Name Tagging for German",
        "authors": [
            "Christine Thielen"
        ],
        "abstract": "  This paper presents an incremental method for the tagging of proper names in German newspaper texts. The tagging is performed by the analysis of the syntactic and textual contexts of proper names together with a morphological analysis. The proper names selected by this process supply new contexts which can be used for finding new proper names, and so on. This procedure was applied to a small German corpus (50,000 words) and correctly disambiguated 65% of the capitalized words, which should improve when it is applied to a very large corpus.\n    ",
        "submission_date": "1995-06-28T00:00:00",
        "last_modified_date": "1995-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506025",
        "title": "A Categorial Framework for Composition in Multiple Linguistic Domains",
        "authors": [
            "Cem Bozsahin",
            "Elvan Gocmen"
        ],
        "abstract": "  This paper describes a computational framework for a grammar architecture in which different linguistic domains such as morphology, syntax, and semantics are treated not as separate components but compositional domains. Word and phrase formation are modeled as uniform processes contributing to the derivation of the semantic form. The morpheme, as well as the lexeme, has lexical representation in the form of semantic content, tactical constraints, and phonological realization. The motivation for this work is to handle morphology-syntax interaction (e.g., valency change in causatives, subcategorization imposed by case-marking affixes) in an incremental way. The model is based on Combinatory Categorial Grammars.\n    ",
        "submission_date": "1995-06-29T00:00:00",
        "last_modified_date": "1995-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9506026",
        "title": "A Computational Approach to Aspectual Composition",
        "authors": [
            "Michael White"
        ],
        "abstract": "  In this paper, I argue, contrary to the prevailing opinion in the linguistics and philosophy literature, that a sortal approach to aspectual composition can indeed be explanatory. In support of this view, I develop a synthesis of competing proposals by Hinrichs, Krifka and Jackendoff which takes Jackendoff's cross-cutting sortal distinctions as its point of departure. To show that the account is well-suited for computational purposes, I also sketch an implemented calculus of eventualities which yields many of the desired inferences. Further details on both the model-theoretic semantics and the implementation can be found in (White, 1994).\n    ",
        "submission_date": "1995-07-01T00:00:00",
        "last_modified_date": "1995-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507001",
        "title": "Constraint Categorial Grammars",
        "authors": [
            "Luis Damas",
            "Nelma Moreira"
        ],
        "abstract": "  Although unification can be used to implement a weak form of $\\beta$-reduction, several linguistic phenomena are better handled by using some form of $\\lambda$-calculus. In this paper we present a higher order feature description calculus based on a typed $\\lambda$-calculus. We show how the techniques used in \\CLG for resolving complex feature constraints can be efficiently extended. \\CCLG is a simple formalism, based on categorial grammars, designed to test the practical feasibility of such a calculus.\n    ",
        "submission_date": "1995-07-04T00:00:00",
        "last_modified_date": "1995-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507002",
        "title": "A framework for lexical representation",
        "authors": [
            "Jos\u00e9 M. Go\u00f1i",
            "Jos\u00e9 C. Gonz\u00e1lez"
        ],
        "abstract": "  In this paper we present a unification-based lexical platform designed for highly inflected languages (like Roman ones). A formalism is proposed for encoding a lemma-based lexical source, well suited for linguistic generalizations. From this source, we automatically generate an allomorph indexed dictionary, adequate for efficient processing. A set of software tools have been implemented around this formalism: access libraries, morphological processors, etc.\n    ",
        "submission_date": "1995-07-06T00:00:00",
        "last_modified_date": "1995-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507003",
        "title": "Robust Processing of Natural Language",
        "authors": [
            "Wolfgang Menzel"
        ],
        "abstract": "  Previous approaches to robustness in natural language processing usually treat deviant input by relaxing grammatical constraints whenever a successful analysis cannot be provided by ``normal'' means. This schema implies, that error detection always comes prior to error handling, a behaviour which hardly can compete with its human model, where many erroneous situations are treated without even noticing them.\n",
        "submission_date": "1995-07-13T00:00:00",
        "last_modified_date": "1995-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507004",
        "title": "GRAMPAL: A Morphological Processor for Spanish implemented in Prolog",
        "authors": [
            "Antonio Moreno",
            "Jos\u00e9 M. Go\u00f1i"
        ],
        "abstract": "  A model for the full treatment of Spanish inflection for verbs, nouns and adjectives is presented. This model is based on feature unification and it relies upon a lexicon of allomorphs both for stems and morphemes. Word forms are built by the concatenation of allomorphs by means of special contextual features. We make use of standard Definite Clause Grammars (DCG) included in most Prolog implementations, instead of the typical finite-state approach. This allows us to take advantage of the declarativity and bidirectionality of Logic Programming for NLP.\n",
        "submission_date": "1995-07-18T00:00:00",
        "last_modified_date": "1995-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507005",
        "title": "Comparative Ellipsis and Variable Binding",
        "authors": [
            "Jan Lerner",
            "Manfred Pinkal"
        ],
        "abstract": "  In this paper, we discuss the question whether phrasal comparatives should be given a direct interpretation, or require an analysis as elliptic constructions, and answer it with Yes and No. The most adequate analysis of wide reading attributive (WRA) comparatives seems to be as cases of ellipsis, while a direct (but asymmetric) analysis fits the data for narrow scope attributive comparatives. The question whether it is a syntactic or a semantic process which provides the missing linguistic material in the complement of WRA comparatives is also given a complex answer: Linguistic context is accessed by combining a reconstruction operation and a mechanism of anaphoric reference. The analysis makes only few and straightforward syntactic assumptions. In part, this is made possible because the use of Generalized Functional Application as a semantic operation allows us to model semantic composition in a flexible way.\n    ",
        "submission_date": "1995-07-19T00:00:00",
        "last_modified_date": "1995-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507006",
        "title": "Transfer in a Connectionist Model of the Acquisition of Morphology",
        "authors": [
            "Michael Gasser"
        ],
        "abstract": "  The morphological systems of natural languages are replete with examples of the same devices used for multiple purposes: (1) the same type of morphological process (for example, suffixation for both noun case and verb tense) and (2) identical morphemes (for example, the same suffix for English noun plural and possessive). These sorts of similarity would be expected to convey advantages on language learners in the form of transfer from one morphological category to another. Connectionist models of morphology acquisition have been faulted for their supposed inability to represent phonological similarity across morphological categories and hence to facilitate transfer. This paper describes a connectionist model of the acquisition of morphology which is shown to exhibit transfer of this type. The model treats the morphology acquisition problem as one of learning to map forms onto meanings and vice versa. As the network learns these mappings, it makes phonological generalizations which are embedded in connection weights. Since these weights are shared by different morphological categories, transfer is enabled. In a set of experiments with artificial stimuli, networks were trained first on one morphological task (e.g., tense) and then on a second (e.g., number). It is shown that in the context of suffixation, prefixation, and template rules, the second task is facilitated when the second category either makes use of the same forms or the same general process type (e.g., prefixation) as the first.\n    ",
        "submission_date": "1995-07-20T00:00:00",
        "last_modified_date": "1995-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507007",
        "title": "An Efficient Algorithm for Surface Generation",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  A method is given that \"inverts\" a logic grammar and displays it from the point of view of the logical form, rather than from that of the word string. LR-compiling techniques are used to allow a recursive-descent generation algorithm to perform \"functor merging\" much in the same way as an LR parser performs prefix merging.\n",
        "submission_date": "1995-07-21T00:00:00",
        "last_modified_date": "1995-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507008",
        "title": "A Constraint-based Case Frame Lexicon Architecture",
        "authors": [
            "Kemal Oflazer",
            "Okan Yilmaz"
        ],
        "abstract": "  In Turkish, (and possibly in many other languages) verbs often convey several meanings (some totally unrelated) when they are used with subjects, objects, oblique objects, adverbial adjuncts, with certain lexical, morphological, and semantic features, and co-occurrence restrictions. In addition to the usual sense variations due to selectional restrictions on verbal arguments, in most cases, the meaning conveyed by a case frame is idiomatic and not compositional, with subtle constraints. In this paper, we present an approach to building a constraint-based case frame lexicon for use in natural language processing in Turkish, whose prototype we have implemented under the TFS system developed at Univ. of Stuttgart.\n",
        "submission_date": "1995-07-21T00:00:00",
        "last_modified_date": "1995-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507009",
        "title": "Specifying Logic Programs in Controlled Natural Language",
        "authors": [
            "Norbert E. Fuchs",
            "Rolf Schwitter"
        ],
        "abstract": "  Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists. Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable. The translation uses a definite clause grammar (DCG) enhanced by feature structures. Inter-text references of the specification, e.g. anaphora, are resolved with the help of discourse representation theory (DRT). The generated Prolog clauses are added to a knowledge base. We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine.\n    ",
        "submission_date": "1995-07-21T00:00:00",
        "last_modified_date": "1995-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507010",
        "title": "On-line Learning of Binary Lexical Relations Using Two-dimensional Weighted Majority Algorithms",
        "authors": [
            "Naoki Abe",
            "Hang Li",
            "Atsuyoshi Nakamura"
        ],
        "abstract": "  We consider the problem of learning a certain type of lexical semantic knowledge that can be expressed as a binary relation between words, such as the so-called sub-categorization of verbs (a verb-noun relation) and the compound noun phrase relation (a noun-noun relation). Specifically, we view this problem as an on-line learning problem in the sense of Littlestone's learning model in which the learner's goal is to minimize the total number of prediction mistakes. In the computational learning theory literature, Goldman, Rivest and Schapire and subsequently Goldman and Warmuth have considered the on-line learning problem for binary relations R : X * Y -> {0, 1} in which one of the domain sets X can be partitioned into a relatively small number of types, namely clusters consisting of behaviorally indistinguishable members of X. In this paper, we extend this model and suppose that both of the sets X, Y can be partitioned into a small number of types, and propose a host of prediction algorithms which are two-dimensional extensions of Goldman and Warmuth's weighted majority type algorithm proposed for the original model. We apply these algorithms to the learning problem for the `compound noun phrase' relation, in which a noun is related to another just in case they can form a noun phrase together. Our experimental results show that all of our algorithms out-perform Goldman and Warmuth's algorithm. We also theoretically analyze the performance of one of our algorithms, in the form of an upper bound on the worst case number of prediction mistakes it makes.\n    ",
        "submission_date": "1995-07-24T00:00:00",
        "last_modified_date": "1995-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507011",
        "title": "Generalizing Case Frames Using a Thesaurus and the MDL Principle",
        "authors": [
            "Hang Li",
            "Naoki Abe"
        ],
        "abstract": "  We address the problem of automatically acquiring case-frame patterns from large corpus data. In particular, we view this problem as the problem of estimating a (conditional) distribution over a partition of words, and propose a new generalization method based on the MDL (Minimum Description Length) principle. In order to assist with the efficiency, our method makes use of an existing thesaurus and restricts its attention on those partitions that are present as `cuts' in the thesaurus tree, thus reducing the generalization problem to that of estimating the `tree cut models' of the thesaurus. We then give an efficient algorithm which provably obtains the optimal tree cut model for the given frequency data, in the sense of MDL. We have used the case-frame patterns obtained using our method to resolve pp-attachment ",
        "submission_date": "1995-07-24T00:00:00",
        "last_modified_date": "1996-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507012",
        "title": "A Grammar Formalism and Cross-Serial Dependencies",
        "authors": [
            "Tore Burheim"
        ],
        "abstract": "  First we define a unification grammar formalism called the Tree Homomorphic Feature Structure Grammar. It is based on Lexical Functional Grammar (LFG), but has a strong restriction on the syntax of the equations. We then show that this grammar formalism defines a full abstract family of languages, and that it is capable of describing cross-serial dependencies of the type found in Swiss German.\n    ",
        "submission_date": "1995-07-24T00:00:00",
        "last_modified_date": "1995-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507013",
        "title": "Indexed Languages and Unification Grammars",
        "authors": [
            "Tore Burheim"
        ],
        "abstract": "  Indexed languages are interesting in computational linguistics because they are the least class of languages in the Chomsky hierarchy that has not been shown not to be adequate to describe the string set of natural language sentences. We here define a class of unification grammars that exactly describe the class of indexed languages.\n    ",
        "submission_date": "1995-07-24T00:00:00",
        "last_modified_date": "1995-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9507014",
        "title": "Co-Indexing Labelled DRSs to Represent and Reason with Ambiguities",
        "authors": [
            "Uwe Reyle"
        ],
        "abstract": "  The paper addresses the problem of representing ambiguities in a way that allows for monotonic disambiguation and for direct deductive computation. The paper focuses on an extension of the formalism of underspecified DRSs to ambiguities introduced by plural NPs. It deals with the collective/distributive distinction, and also with generic and cumulative readings. In addition it provides a systematic account for an underspecified treatment of plural pronoun resolution.\n    ",
        "submission_date": "1995-07-25T00:00:00",
        "last_modified_date": "1995-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508001",
        "title": "Bridging as Coercive Accommodation",
        "authors": [
            "Johan Bos",
            "Paul Buitelaar",
            "Anne-Marie Mineur"
        ],
        "abstract": "  In this paper we discuss the notion of \"bridging\" in Discourse Representation Theory as a tool to account for discourse referents that have only been established implicitly, through the lexical semantics of other referents. In doing so, we use ideas from Generative Lexicon theory, to introduce antecedents for anaphoric expressions that cannot be \"linked\" to a proper antecedent, but that do not need to be \"accommodated\" because they have some connection to the network of discourse referents that is already established.\n    ",
        "submission_date": "1995-08-02T00:00:00",
        "last_modified_date": "1995-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508002",
        "title": "A Compositional Treatment of Polysemous Arguments in Categorial Grammar",
        "authors": [
            "Anne-Marie Mineur",
            "Paul Buitelaar"
        ],
        "abstract": "  We discuss an extension of the standard logical rules (functional application and abstraction) in Categorial Grammar (CG), in order to deal with some specific cases of polysemy. We borrow from Generative Lexicon theory which proposes the mechanism of {\\em coercion}, next to a rich nominal lexical semantic structure called {\\em qualia structure}.\n",
        "submission_date": "1995-08-02T00:00:00",
        "last_modified_date": "1995-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508003",
        "title": "A Robust Parsing Algorithm For Link Grammars",
        "authors": [
            "Dennis Grinberg",
            "John Lafferty",
            "Daniel Sleator"
        ],
        "abstract": "  In this paper we present a robust parsing algorithm based on the link grammar formalism for parsing natural languages. Our algorithm is a natural extension of the original dynamic programming recognition algorithm which recursively counts the number of linkages between two words in the input sentence. The modified algorithm uses the notion of a null link in order to allow a connection between any pair of adjacent words, regardless of their dictionary definitions. The algorithm proceeds by making three dynamic programming passes. In the first pass, the input is parsed using the original algorithm which enforces the constraints on links to ensure grammaticality. In the second pass, the total cost of each substring of words is computed, where cost is determined by the number of null links necessary to parse the substring. The final pass counts the total number of parses with minimal cost. All of the original pruning techniques have natural counterparts in the robust algorithm. When used together with memoization, these techniques enable the algorithm to run efficiently with cubic worst-case complexity. We have implemented these ideas and tested them by parsing the Switchboard corpus of conversational English. This corpus is comprised of approximately three million words of text, corresponding to more than 150 hours of transcribed speech collected from telephone conversations restricted to 70 different topics. Although only a small fraction of the sentences in this corpus are \"grammatical\" by standard criteria, the robust link grammar parser is able to extract relevant structure for a large portion of the sentences. We present the results of our experiments using this system, including the analyses of selected and random sentences from the corpus.\n    ",
        "submission_date": "1995-08-02T00:00:00",
        "last_modified_date": "1995-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508004",
        "title": "Parsing English with a Link Grammar",
        "authors": [
            "Daniel D. K.Sleator",
            "Davy Temperley"
        ],
        "abstract": "  We develop a formal grammatical system called a link grammar, show how English grammar can be encoded in such a system, and give algorithms for efficiently parsing with a link grammar. Although the expressive power of link grammars is equivalent to that of context free grammars, encoding natural language grammars appears to be much easier with the new system. We have written a program for general link parsing and written a link grammar for the English language. The performance of this preliminary system -- both in the breadth of English phenomena that it captures and in the computational resources used -- indicates that the approach may have practical uses as well as linguistic significance. Our program is written in C and may be obtained through the internet.\n    ",
        "submission_date": "1995-08-02T00:00:00",
        "last_modified_date": "1995-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508005",
        "title": "A Matching Technique in Example-Based Machine Translation",
        "authors": [
            "Lambros Cranias",
            "Harris Papageorgiou",
            "Stelios Piperidis"
        ],
        "abstract": "  This paper addresses an important problem in Example-Based Machine Translation (EBMT), namely how to measure similarity between a sentence fragment and a set of stored examples. A new method is proposed that measures similarity according to both surface structure and content. A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient. Results on a large number of test cases from the CELEX database are presented.\n    ",
        "submission_date": "1995-08-10T00:00:00",
        "last_modified_date": "1995-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508006",
        "title": "Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT",
        "authors": [
            "Arturo Trujillo"
        ],
        "abstract": "  The paper presents a prototype lexicalist Machine Translation system (based on the so-called `Shake-and-Bake' approach of Whitelock (1992) consisting of an analysis component, a dynamic bilingual lexicon, and a generation component, and shows how it is applied to a range of MT problems. Multi-Lexeme translations are handled through bi-lexical rules which map bilingual lexical signs into new bilingual lexical signs. It is argued that much translation can be handled by equating translationally equivalent lists of lexical signs, either directly in the bilingual lexicon, or by deriving them through bi-lexical rules. Lexical semantic information organized as Qualia structures (Pustejovsky 1991) is used as a mechanism for restricting the domain of the rules.\n    ",
        "submission_date": "1995-08-12T00:00:00",
        "last_modified_date": "1995-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508007",
        "title": "A Dynamic Approach to Rhythm in Language: Toward a Temporal Phonology",
        "authors": [
            "Robert Port",
            "Fred Cummins",
            "Michael Gasser"
        ],
        "abstract": "  It is proposed that the theory of dynamical systems offers appropriate tools to model many phonological aspects of both speech production and perception. A dynamic account of speech rhythm is shown to be useful for description of both Japanese mora timing and English timing in a phrase repetition task. This orientation contrasts fundamentally with the more familiar symbolic approach to phonology, in which time is modeled only with sequentially arrayed symbols. It is proposed that an adaptive oscillator offers a useful model for perceptual entrainment (or `locking in') to the temporal patterns of speech production. This helps to explain why speech is often perceived to be more regular than experimental measurements seem to justify. Because dynamic models deal with real time, they also help us understand how languages can differ in their temporal detail---contributing to foreign accents, for example. The fact that languages differ greatly in their temporal detail suggests that these effects are not mere motor universals, but that dynamical models are intrinsic components of the phonological characterization of language.\n    ",
        "submission_date": "1995-08-13T00:00:00",
        "last_modified_date": "1995-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508008",
        "title": "On Constraint-Based Lambek Calculi",
        "authors": [
            "Jochen Doerre",
            "Suresh Manandhar"
        ],
        "abstract": "  We explore the consequences of layering a Lambek proof system over an arbitrary (constraint) logic. A simple model-theoretic semantics for our hybrid language is provided for which a particularly simple combination of Lambek's and the proof system of the base logic is complete. Furthermore the proof system for the underlying base logic can be assumed to be a black box. The essential reasoning needed to be performed by the black box is that of {\\em entailment checking}. Assuming feature logic as the base logic entailment checking amounts to a {\\em subsumption} test which is a well-known quasi-linear time decidable problem.\n    ",
        "submission_date": "1995-08-15T00:00:00",
        "last_modified_date": "1995-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508009",
        "title": "A Labelled Analytic Theorem Proving Environment for Categorial Grammar",
        "authors": [
            "Saturnino F. Luz-Filho",
            "Patrick Sturt"
        ],
        "abstract": "  We present a system for the investigation of computational properties of categorial grammar parsing based on a labelled analytic tableaux theorem prover. This proof method allows us to take a modular approach, in which the basic grammar can be kept constant, while a range of categorial calculi can be captured by assigning different properties to the labelling algebra. The theorem proving strategy is particularly well suited to the treatment of categorial grammar, because it allows us to distribute the computational cost between the algorithm which deals with the grammatical types and the algebraic checker which constrains the derivation.\n    ",
        "submission_date": "1995-08-15T00:00:00",
        "last_modified_date": "1995-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508010",
        "title": "Heuristics and Parse Ranking",
        "authors": [
            "B. Srinivas",
            "Christine Doran",
            "Seth Kulick"
        ],
        "abstract": "  There are currently two philosophies for building grammars and parsers -- Statistically induced grammars and Wide-coverage grammars. One way to combine the strengths of both approaches is to have a wide-coverage grammar with a heuristic component which is domain independent but whose contribution is tuned to particular domains. In this paper, we discuss a three-stage approach to disambiguation in the context of a lexicalized grammar, using a variety of domain independent heuristic techniques. We present a training algorithm which uses hand-bracketed treebank parses to set the weights of these heuristics. We compare the performance of our grammar against the performance of the IBM statistical grammar, using both untrained and trained weights for the heuristics.\n    ",
        "submission_date": "1995-08-28T00:00:00",
        "last_modified_date": "1995-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508011",
        "title": "The Use of Knowledge Preconditions in Language Processing",
        "authors": [
            "Karen E. Lochbaum"
        ],
        "abstract": "  If an agent does not possess the knowledge needed to perform an action, it may privately plan to obtain the required information on its own, or it may involve another agent in the planning process by engaging it in a dialogue. In this paper, we show how the requirements of knowledge preconditions can be used to account for information-seeking subdialogues in discourse. We first present an axiomatization of knowledge preconditions for the SharedPlan model of collaborative activity (Grosz & Kraus, 1993), and then provide an analysis of information-seeking subdialogues within a general framework for discourse processing. In this framework, SharedPlans and relationships among them are used to model the intentional component of Grosz and Sidner's (1986) theory of discourse structure.\n    ",
        "submission_date": "1995-08-29T00:00:00",
        "last_modified_date": "1995-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9508012",
        "title": "A Natural Law of Succession",
        "authors": [
            "Eric Sven Ristad"
        ],
        "abstract": "  Consider the problem of multinomial estimation. You are given an alphabet of k distinct symbols and are told that the i-th symbol occurred exactly n_i times in the past. On the basis of this information alone, you must now estimate the conditional probability that the next symbol will be i. In this report, we present a new solution to this fundamental problem in statistics and demonstrate that our solution outperforms standard approaches, both in theory and in practice.\n    ",
        "submission_date": "1995-08-30T00:00:00",
        "last_modified_date": "1995-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9509001",
        "title": "How much is enough?: Data requirements for statistical NLP",
        "authors": [
            "Mark Lauer"
        ],
        "abstract": "  In this paper I explore a number of issues in the analysis of data requirements for statistical NLP systems. A preliminary framework for viewing such systems is proposed and a sample of existing works are compared within this framework. The first steps toward a theory of data requirements are made by establishing some results relevant to bounding the expected error rate of a class of simplified statistical language learners as a function of the volume of training data.\n    ",
        "submission_date": "1995-09-07T00:00:00",
        "last_modified_date": "1995-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9509002",
        "title": "Conserving Fuel in Statistical Language Learning: Predicting Data Requirements",
        "authors": [
            "Mark Lauer"
        ],
        "abstract": "  In this paper I address the practical concern of predicting how much training data is sufficient for a statistical language learning system. First, I briefly review earlier results and show how these can be combined to bound the expected accuracy of a mode-based learner as a function of the volume of training data. I then develop a more accurate estimate of the expected accuracy function under the assumption that inputs are uniformly distributed. Since this estimate is expensive to compute, I also give a close but cheaply computable approximation to it. Finally, I report on a series of simulations exploring the effects of inputs that are not uniformly distributed. Although these results are based on simplistic assumptions, they are a tentative step toward a useful theory of data requirements for SLL systems.\n    ",
        "submission_date": "1995-09-07T00:00:00",
        "last_modified_date": "1995-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9509003",
        "title": "Cluster Expansions and Iterative Scaling for Maximum Entropy Language Models",
        "authors": [
            "John D. Lafferty",
            "Bernhard Suhm"
        ],
        "abstract": "  The maximum entropy method has recently been successfully introduced to a variety of natural language applications. In each of these applications, however, the power of the maximum entropy method is achieved at the cost of a considerable increase in computational requirements. In this paper we present a technique, closely related to the classical cluster expansion from statistical mechanics, for reducing the computational demands necessary to calculate conditional maximum entropy language models.\n    ",
        "submission_date": "1995-09-09T00:00:00",
        "last_modified_date": "1995-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9509004",
        "title": "The Development and Migration of Concepts from Donor to Borrower Disciplines: Sublanguage Term Use in Hard & Soft Sciences",
        "authors": [
            "Robert M. Losee"
        ],
        "abstract": "  Academic disciplines, often divided into hard and soft sciences, may be understood as \"donor disciplines\" if they produce more concepts than they borrow from other disciplines, or \"borrower disciplines\" if they import more than they originate. Terms used to describe these concepts can be used to distinguish between hard and soft, donor and borrower, as well as individual discipline-specific sublanguages. Using term frequencies, the birth, growth, death, and migration of concepts and their associated terms are examined.\n    ",
        "submission_date": "1995-09-13T00:00:00",
        "last_modified_date": "1995-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9509005",
        "title": "ParseTalk about Textual Ellipsis",
        "authors": [
            "Michael Strube",
            "Udo Hahn"
        ],
        "abstract": "  A hybrid methodology for the resolution of text-level ellipsis is presented in this paper. It incorporates conceptual proximity criteria applied to ontologically well-engineered domain knowledge bases and an approach to centering based on functional topic/comment patterns. We state text grammatical predicates for ellipsis and then turn to the procedural aspects of their evaluation within the framework of an actor-based implementation of a lexically distributed parser.\n    ",
        "submission_date": "1995-09-28T00:00:00",
        "last_modified_date": "1995-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510001",
        "title": "POS Tagging Using Relaxation Labelling",
        "authors": [
            "Lluis Padro"
        ],
        "abstract": "  Relaxation labelling is an optimization technique used in many fields to solve constraint satisfaction problems. The algorithm finds a combination of values for a set of variables such that satisfies -to the maximum possible degree- a set of given constraints. This paper describes some experiments performed applying it to POS tagging, and the results obtained. It also ponders the possibility of applying it to word sense disambiguation.\n    ",
        "submission_date": "1995-10-02T00:00:00",
        "last_modified_date": "1995-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510002",
        "title": "Using Chinese Text Processing Technique for the Processing of Sanskrit Based Indian Languages: Maximum Resource Utilization and Maximum Compatibility",
        "authors": [
            "Md Maruf Hasan"
        ],
        "abstract": "  Chinese text processing systems are using Double Byte Coding , while almost all existing Sanskrit Based Indian Languages have been using Single Byte coding for text processing. Through observation, Chinese Information Processing Technique has already achieved great technical development both in east and west. In contrast,Indian Languages are being processed by computer, more or less, for word processing purpose. This paper mainly emphasizes the method of processing Indian languages from a Computational Linguistic point of view. An overall design method is illustrated in this ",
        "submission_date": "1995-09-27T00:00:00",
        "last_modified_date": "1995-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510003",
        "title": "A Proposal for Word Sense Disambiguation using Conceptual Distance",
        "authors": [
            "Eneko Agirre",
            "German Rigau"
        ],
        "abstract": "  This paper presents a method for the resolution of lexical ambiguity and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiment have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.\n    ",
        "submission_date": "1995-10-04T00:00:00",
        "last_modified_date": "1995-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510004",
        "title": "Disambiguating bilingual nominal entries against WordNet",
        "authors": [
            "German Rigau",
            "Eneko Agirre"
        ],
        "abstract": "  This paper explores the acquisition of conceptual knowledge from bilingual dictionaries (French/English, Spanish/English and English/Spanish) using a pre-existing broad coverage Lexical Knowledge Base (LKB) WordNet. Bilingual nominal entries are disambiguated agains WordNet, therefore linking the bilingual dictionaries to WordNet yielding a multilingual LKB (MLKB). The resulting MLKB has the same structure as WordNet, but some nodes are attached additionally to disambiguated vocabulary of other languages.\n",
        "submission_date": "1995-10-04T00:00:00",
        "last_modified_date": "1995-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510005",
        "title": "Developing and Evaluating a Probabilistic LR Parser of Part-of-Speech and Punctuation Labels",
        "authors": [
            "Ted Briscoe",
            "John Carroll"
        ],
        "abstract": "  We describe an approach to robust domain-independent syntactic parsing of unrestricted naturally-occurring (English) input. The technique involves parsing sequences of part-of-speech and punctuation labels using a unification-based grammar coupled with a probabilistic LR parser. We describe the coverage of several corpora using this grammar and report the results of a parsing experiment using probabilities derived from bracketed training data. We report the first substantial experiments to assess the contribution of punctuation to deriving an accurate syntactic analysis, by parsing identical texts both with and without naturally-occurring punctuation marks.\n    ",
        "submission_date": "1995-10-09T00:00:00",
        "last_modified_date": "1995-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510006",
        "title": "Incorporating Discourse Aspects in English -- Polish MT: Towards Robust Implementation",
        "authors": [
            "Malgorzata E. Stys",
            "Stefan S. Zemke"
        ],
        "abstract": "  The main aim of translation is an accurate transfer of meaning so that the result is not only grammatically and lexically correct but also communicatively adequate. This paper stresses the need for discourse analysis the aim of which is to preserve the communicative meaning in English--Polish machine translation. Unlike English, which is a positional language with word order grammatically determined, Polish displays a strong tendency to order constituents according to their degree of salience, so that the most informationally salient elements are placed towards the end of the clause regardless of their grammatical function. The Centering Theory developed for tracking down given information units in English and the Theory of Functional Sentence Perspective predicting informativeness of subsequent constituents provide theoretical background for this work. The notion of {\\em center} is extended to accommodate not only for pronominalisation and exact reiteration but also for definiteness and other center pointing constructs. Center information is additionally graded and applicable to all primary constituents in a given utterance. This information is used to order the post-transfer constituents correctly, relying on statistical regularities and some syntactic clues.\n    ",
        "submission_date": "1995-10-15T00:00:00",
        "last_modified_date": "1995-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510007",
        "title": "Automatic Identification of Support Verbs: A Step Towards a Definition of Semantic Weight",
        "authors": [
            "Mark Dras"
        ],
        "abstract": "  Current definitions of notions of lexical density and semantic weight are based on the division of words into closed and open classes, and on intuition. This paper develops a computationally tractable definition of semantic weight, concentrating on what it means for a word to be semantically light; the definition involves looking at the frequency of a word in particular syntactic constructions which are indicative of lightness. Verbs such as \"make\" and \"take\", when they function as support verbs, are often considered to be semantically light. To test our definition, we carried out an experiment based on that of Grefenstette and Teufel (1995), where we automatically identify light instances of these words in a corpus; this was done by incorporating our frequency-related definition of semantic weight into a statistical approach similar to that of Grefenstette and Teufel. The results show that this is a plausible definition of semantic lightness for verbs, which can possibly be extended to defining semantic lightness for other classes of words.\n    ",
        "submission_date": "1995-10-25T00:00:00",
        "last_modified_date": "1995-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9510008",
        "title": "Toward an MT System without Pre-Editing --- Effects of New Methods in ALT-J/E ---",
        "authors": [
            "Satoru Ikehara",
            "Satoshi Shirai",
            "Akio Yokoo",
            "Hiromi Nakaiwa"
        ],
        "abstract": "  Recently, several types of Japanese-to-English machine translation systems have been developed, but all of them require an initial process of rewriting the original text into easily translatable Japanese. Therefore these systems are unsuitable for translating information that needs to be speedily disseminated. To overcome this limitation, a Multi-Level Translation Method based on the Constructive Process Theory has been proposed. This paper describes the benefits of using this method in the Japanese-to-English machine translation system ALT-J/E.\n",
        "submission_date": "1995-10-31T00:00:00",
        "last_modified_date": "1995-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511001",
        "title": "Countability and Number in Japanese-to-English Machine Translation",
        "authors": [
            "Francis Bond",
            "Kentaro Ogura",
            "Satoru Ikehara"
        ],
        "abstract": "  This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases. Incorporating this method into the machine translation system ALT-J/E, helped to raise the percentage of noun phrases generated with correct use of articles and number from 65% to 73%.\n    ",
        "submission_date": "1995-11-03T00:00:00",
        "last_modified_date": "1995-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511002",
        "title": "Letting the Cat out of the Bag: Generation for Shake-and-Bake MT",
        "authors": [
            "Chris Brew"
        ],
        "abstract": "  Describes an algorithm for the generation phase of a Shake-and-Bake Machine Translation system. Since the problem is NP-complete, it is unlikely that the algorithm will be efficient in all cases, but for the cases tested it offers an improvement over Whitelock's previously published algorithm. The work was carried out while the author was employed at Sharp Laboratories of Europe Ltd.\n    ",
        "submission_date": "1995-11-13T00:00:00",
        "last_modified_date": "1995-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511003",
        "title": "The Effect of Resource Limits and Task Complexity on Collaborative Planning in Dialogue",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "  This paper shows how agents' choice in communicative action can be designed to mitigate the effect of their resource limits in the context of particular features of a collaborative planning task. I first motivate a number of hypotheses about effective language behavior based on a statistical analysis of a corpus of natural collaborative planning dialogues. These hypotheses are then tested in a dialogue testbed whose design is motivated by the corpus analysis. Experiments in the testbed examine the interaction between (1) agents' resource limits in attentional capacity and inferential capacity; (2) agents' choice in communication; and (3) features of communicative tasks that affect task difficulty such as inferential complexity, degree of belief coordination required, and tolerance for errors. The results show that good algorithms for communication must be defined relative to the agents' resource limits and the features of the task. Algorithms that are inefficient for inferentially simple, low coordination or fault-tolerant tasks are effective when tasks require coordination or complex inferences, or are fault-intolerant. The results provide an explanation for the occurrence of utterances in human dialogues that, prima facie, appear inefficient, and provide the basis for the design of effective algorithms for communicative choice for resource limited agents.\n    ",
        "submission_date": "1995-11-15T00:00:00",
        "last_modified_date": "1995-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511004",
        "title": "An investigation into the correlation of cue phrases, unfilled pauses and the structuring of spoken discourse",
        "authors": [
            "Janet Cahn"
        ],
        "abstract": "  Expectations about the correlation of cue phrases, the duration of unfilled pauses and the structuring of spoken discourse are framed in light of Grosz and Sidner's theory of discourse and are tested for a directions-giving dialogue. The results suggest that cue phrase and discourse structuring tasks may align, and show a correlation for pause length and some of the modifications that speakers can make to discourse structure.\n    ",
        "submission_date": "1995-11-22T00:00:00",
        "last_modified_date": "1995-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511005",
        "title": "Chart-driven Connectionist Categorial Parsing of Spoken Korean",
        "authors": [
            "WonIl Lee",
            "Geunbae Lee",
            "Jong-Hyeok Lee"
        ],
        "abstract": "  While most of the speech and natural language systems which were developed for English and other Indo-European languages neglect the morphological processing and integrate speech and natural language at the word level, for the agglutinative languages such as Korean and Japanese, the morphological processing plays a major role in the language processing since these languages have very complex morphological phenomena and relatively simple syntactic functionality. Obviously degenerated morphological processing limits the usable vocabulary size for the system and word-level dictionary results in exponential explosion in the number of dictionary entries. For the agglutinative languages, we need sub-word level integration which leaves rooms for general morphological processing. In this paper, we developed a phoneme-level integration model of speech and linguistic processings through general morphological analysis for agglutinative languages and a efficient parsing scheme for that integration. Korean is modeled lexically based on the categorial grammar formalism with unordered argument and suppressed category extensions, and chart-driven connectionist parsing method is introduced.\n    ",
        "submission_date": "1995-11-29T00:00:00",
        "last_modified_date": "1995-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511006",
        "title": "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "authors": [
            "Philip Resnik"
        ],
        "abstract": "  Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word {\\em senses}, not words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns --- the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.\n    ",
        "submission_date": "1995-11-29T00:00:00",
        "last_modified_date": "1995-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9511007",
        "title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy",
        "authors": [
            "Philip Resnik"
        ],
        "abstract": "  This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).\n    ",
        "submission_date": "1995-11-29T00:00:00",
        "last_modified_date": "1995-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9512001",
        "title": "Analysis of the Arabic Broken Plural and Diminutive",
        "authors": [
            "George A. Kiraz"
        ],
        "abstract": "  This paper demonstrates how the challenging problem of the Arabic broken plural and diminutive can be handled under a multi-tape two-level model, an extension to two-level morphology.\n    ",
        "submission_date": "1995-12-09T00:00:00",
        "last_modified_date": "1995-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9512002",
        "title": "The Unsupervised Acquisition of a Lexicon from Continuous Speech",
        "authors": [
            "Carl de Marcken"
        ],
        "abstract": "  We present an unsupervised learning algorithm that acquires a natural-language lexicon from raw speech. The algorithm is based on the optimal encoding of symbol sequences in an MDL framework, and uses a hierarchical representation of language that overcomes many of the problems that have stymied previous grammar-induction procedures. The forward mapping from symbol sequences to the speech stream is modeled using features based on articulatory gestures. We present results on the acquisition of lexicons and language models from raw speech, text, and phonetic transcripts, and demonstrate that our algorithm compares very favorably to other reported results with respect to segmentation performance and statistical efficiency.\n    ",
        "submission_date": "1995-12-13T00:00:00",
        "last_modified_date": "1995-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9512003",
        "title": "Limited Attention and Discourse Structure",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "  This squib examines the role of limited attention in a theory of discourse structure and proposes a model of attentional state that relates current hierarchical theories of discourse structure to empirical evidence about human discourse processing capabilities. First, I present examples that are not predicted by Grosz and Sidner's stack model of attentional state. Then I consider an alternative model of attentional state, the cache model, which accounts for the examples, and which makes particular processing predictions. Finally I suggest a number of ways that future research could distinguish the predictions of the cache model and the stack model.\n    ",
        "submission_date": "1995-12-18T00:00:00",
        "last_modified_date": "1996-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9512004",
        "title": "Natural language processing: she needs something old and something new (maybe something borrowed and something blue, too)",
        "authors": [
            "Karen Sparck Jones"
        ],
        "abstract": "  Given the present state of work in natural language processing, this address argues first, that advance in both science and applications requires a revival of concern about what language is about, broadly speaking the world; and second, that an attack on the summarising task, which is made ever more important by the growth of electronic text resources and requires an understanding of the role of large-scale discourse structure in marking important text content, is a good way forward.\n    ",
        "submission_date": "1995-12-21T00:00:00",
        "last_modified_date": "1995-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9512005",
        "title": "Term Encoding of Typed Feature Structures",
        "authors": [
            "Dale Gerdemann"
        ],
        "abstract": "  This paper presents an approach to Prolog-style term encoding of typed feature structures. The type feature structures to be encoded are constrained by appropriateness conditions as in Carpenter's ALE system. But unlike ALE, we impose a further independently motivated closed-world assumption. This assumption allows us to apply term encoding in cases that were problematic for previous approaches. In particular, previous approaches have ruled out multiple inheritance and further specification of feature-value declarations on subtypes. In the present approach, these spececial cases can be handled as well, though with some increase in complexity. For grammars without multiple inheritance and specification of feature values, the encoding presented here reduces to that of previous approaches.\n    ",
        "submission_date": "1995-12-22T00:00:00",
        "last_modified_date": "1995-12-22T00:00:00"
    }
]