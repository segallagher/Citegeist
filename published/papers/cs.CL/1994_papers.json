[
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404001",
        "title": "An Alternative Conception of Tree-Adjoining Derivation",
        "authors": [
            "Yves Schabes",
            "Stuart M. Shieber"
        ],
        "abstract": "  The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable through a definition of TAG derivations as equivalence classes of ordered derivation trees, and computationally operational, by virtue of a compilation to linear indexed grammars together with an efficient algorithm for recognition and parsing according to the compiled grammar.\n    ",
        "submission_date": "1994-04-04T00:00:00",
        "last_modified_date": "1994-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404002",
        "title": "Lessons from a Restricted Turing Test",
        "authors": [
            "Stuart M. Shieber"
        ],
        "abstract": "  We report on the recent Loebner prize competition inspired by Turing's test of intelligent behavior. The presentation covers the structure of the competition and the outcome of its first instantiation in an actual event, and an analysis of the purpose, design, and appropriateness of such a competition. We argue that the competition has no clear purpose, that its design prevents any useful outcome, and that such a competition is inappropriate given the current level of technology. We then speculate as to suitable alternatives to the Loebner prize.\n    ",
        "submission_date": "1994-04-04T00:00:00",
        "last_modified_date": "1994-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404003",
        "title": "Restricting the Weak-Generative Capacity of Synchronous Tree-Adjoining Grammars",
        "authors": [
            "Stuart M. Shieber"
        ],
        "abstract": "  The formalism of synchronous tree-adjoining grammars, a variant of standard tree-adjoining grammars (TAG), was intended to allow the use of TAGs for language transduction in addition to language specification. In previous work, the definition of the transduction relation defined by a synchronous TAG was given by appeal to an iterative rewriting process. The rewriting definition of derivation is problematic in that it greatly extends the expressivity of the formalism and makes the design of parsing algorithms difficult if not impossible. We introduce a simple, natural definition of synchronous tree-adjoining derivation, based on isomorphisms between standard tree-adjoining derivations, that avoids the expressivity and implementability problems of the original rewriting definition. The decrease in expressivity, which would otherwise make the method unusable, is offset by the incorporation of an alternative definition of standard tree-adjoining derivation, previously proposed for completely separate reasons, thereby making it practical to entertain using the natural definition of synchronous derivation. Nonetheless, some remaining problematic cases call for yet more flexibility in the definition; the isomorphism requirement may have to be relaxed. It remains for future research to tune the exact requirements on the allowable mappings.\n    ",
        "submission_date": "1994-04-04T00:00:00",
        "last_modified_date": "1994-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404004",
        "title": "An Empirically Motivated Reinterpretation of Dependency Grammar",
        "authors": [
            "Michael A. Covington"
        ],
        "abstract": "  Dependency grammar is usually interpreted as equivalent to a strict form of X--bar theory that forbids the stacking of nodes of the same bar level (e.g., N' immediately dominating N' with the same head). But adequate accounts of _one_--anaphora and of the semantics of multiple modifiers require such stacking and accordingly argue against dependency grammar. Dependency grammar can be salvaged by reinterpreting its claims about phrase structure, so that modifiers map onto binary--branching X--bar trees rather than ``flat'' ones.\n    ",
        "submission_date": "1994-04-06T00:00:00",
        "last_modified_date": "1994-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404005",
        "title": "Memoization in Constraint Logic Programming",
        "authors": [
            "Mark Johnson"
        ],
        "abstract": "  This paper shows how to apply memoization (caching of subgoals and associated answer substitutions) in a constraint logic programming setting. The research is is motivated by the desire to apply constraint logic programming (CLP) to problems in natural language processing that involve (constraint) interleaving or coroutining, such as GB and HPSG parsing.\n    ",
        "submission_date": "1994-04-11T00:00:00",
        "last_modified_date": "1994-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404006",
        "title": "SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for medical purposes",
        "authors": [
            "R.M. CHANDLER-BURNS"
        ],
        "abstract": "  S92 research was begun in 1987 to analyze word frequencies in present-day Spanish for making speech pathology evaluation tools. 500 2,000-word samples of children, adolescents and adults' language were input between 1988-1991, calculations done in 1992; statistical and Lewandowski analyses were carried out in 1993.\n    ",
        "submission_date": "1994-04-15T00:00:00",
        "last_modified_date": "1994-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404007",
        "title": "Constraint-Based Categorial Grammar",
        "authors": [
            "Gosse Bouma",
            "Gertjan van Noord"
        ],
        "abstract": "  We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints. In particular, the introduction of relational constraints allows one to capture the effects of (recursive) lexical rules in a computationally attractive manner. We illustrate the linguistic merits of the new approach by showing how it accounts for the syntax of Dutch cross-serial dependencies and the position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints.\n    ",
        "submission_date": "1994-04-19T00:00:00",
        "last_modified_date": "1994-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404008",
        "title": "Principles and Implementation of Deductive Parsing",
        "authors": [
            "Stuart M. Shieber",
            "Yves Schabes",
            "Fernando C. N. Pereira"
        ],
        "abstract": "  We present a system for generating parsers based directly on the metaphor of parsing as deduction. Parsing algorithms can be represented directly as deduction systems, and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser. The method generalizes easily to parsers for augmented phrase structure formalisms, such as definite-clause grammars and other logic grammar formalisms, and has been used for rapid prototyping of parsing algorithms for a variety of formalisms including variants of tree-adjoining grammars, categorial grammars, and lexicalized context-free grammars.\n    ",
        "submission_date": "1994-04-26T00:00:00",
        "last_modified_date": "1994-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404009",
        "title": "A Deductive Account of Quantification in LFG",
        "authors": [
            "Mary Dalrymple",
            "John Lamping",
            "Fernando Pereira",
            "Vijay Saraswat"
        ],
        "abstract": "  The relationship between Lexical-Functional Grammar (LFG) functional structures (f-structures) for sentences and their semantic interpretations can be expressed directly in a fragment of linear logic in a way that explains correctly the constrained interactions between quantifier scope ambiguity and bound anaphora. The use of a deductive framework to account for the compositional properties of quantifying expressions in natural language obviates the need for additional mechanisms, such as Cooper storage, to represent the different scopes that a quantifier might take. Instead, the semantic contribution of a quantifier is recorded as an ordinary logical formula, one whose use in a proof will establish the scope of the quantifier. The properties of linear logic ensure that each quantifier is scoped exactly once. Our analysis of quantifier scope can be seen as a recasting of Pereira's analysis (Pereira, 1991), which was expressed in higher-order intuitionistic logic. But our use of LFG and linear logic provides a much more direct and computationally more flexible interpretation mechanism for at least the same range of phenomena. We have developed a preliminary Prolog implementation of the linear deductions described in this work.\n    ",
        "submission_date": "1994-04-27T00:00:00",
        "last_modified_date": "1994-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404010",
        "title": "Intensional Verbs Without Type-Raising or Lexical Ambiguity",
        "authors": [
            "Mary Dalrymple",
            "John Lamping",
            "Fernando Pereira",
            "Vijay Saraswat"
        ],
        "abstract": "  We present an analysis of the semantic interpretation of intensional verbs such as seek that allows them to take direct objects of either individual or quantifier type, producing both de dicto and de re readings in the quantifier case, all without needing to stipulate type-raising or quantifying-in rules. This simple account follows directly from our use of logical deduction in linear logic to express the relationship between syntactic structures and meanings. While our analysis resembles current categorial approaches in important ways, it differs from them in allowing the greater type flexibility of categorial semantics while maintaining a precise connection to syntax. As a result, we are able to provide derivations for certain readings of sentences with intensional verbs and complex direct objects that are not derivable in current purely categorial accounts of the syntax-semantics interface. The analysis forms a part of our ongoing work on semantic interpretation within the framework of Lexical-Functional Grammar.\n    ",
        "submission_date": "1994-04-27T00:00:00",
        "last_modified_date": "1994-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9404011",
        "title": "Adjuncts and the Processing of Lexical Rules",
        "authors": [
            "Gertjan van Noord",
            "Gosse Bouma"
        ],
        "abstract": "  The standard HPSG analysis of Germanic verb clusters can not explain the observed narrow-scope readings of adjuncts in such verb clusters. We present an extension of the HPSG analysis that accounts for the systematic ambiguity of the scope of adjuncts in verb cluster constructions, by treating adjuncts as members of the subcat list. The extension uses powerful recursive lexical rules, implemented as complex constraints. We show how `delayed evaluation' techniques from constraint-logic programming can be used to process such lexical rules.\n    ",
        "submission_date": "1994-04-28T00:00:00",
        "last_modified_date": "1994-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405001",
        "title": "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "authors": [
            "Ido Dagan",
            "Fernando Pereira",
            "Lillian Lee"
        ],
        "abstract": "  In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.\n    ",
        "submission_date": "1994-05-02T00:00:00",
        "last_modified_date": "1994-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405002",
        "title": "Temporal Relations: Reference or Discourse Coherence?",
        "authors": [
            "Andrew Kehler"
        ],
        "abstract": "  The temporal relations that hold between events described by successive utterances are often left implicit or underspecified. We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations. We account for several facets of the identification of temporal relations through an integration of these.\n    ",
        "submission_date": "1994-05-02T00:00:00",
        "last_modified_date": "1994-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405003",
        "title": "Some Bibliographical References on Intonation and Intonational Meaning",
        "authors": [
            "Julia Hirschberg"
        ],
        "abstract": "  A by-no-means-complete collection of references for those interested in intonational meaning, with other miscellaneous references on intonation included. Additional references are welcome, and should be sent to julia@research.",
        "submission_date": "1994-05-02T00:00:00",
        "last_modified_date": "1994-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405004",
        "title": "Syntactic-Head-Driven Generation",
        "authors": [
            "Esther Koenig"
        ],
        "abstract": "  The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990. This is the case for the semantic analysis rules of certain constraint-based semantic representations, e.g. Underspecified Discourse Representation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven generation in general has its merits, we simply return to a syntactic definition of `head' and demonstrate the feasibility of syntactic-head-driven generation. In addition to its generality, a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of (syntactic) heads, for which only ad-hoc solutions existed, so far.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405005",
        "title": "Pearl: A Probabilistic Chart Parser",
        "authors": [
            "David M. Magerman",
            "Mitchell P. Marcus"
        ],
        "abstract": "  This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405006",
        "title": "Efficiency, Robustness, and Accuracy in Picky Chart Parsing",
        "authors": [
            "David M. Magerman",
            "Carl Weir"
        ],
        "abstract": "  This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called {\\em probabilistic prediction} to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405007",
        "title": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing",
        "authors": [
            "Ezra Black",
            "Fred Jelinek",
            "John Lafferty",
            "David M. Magerman",
            "Robert Mercer",
            "Salim Roukos"
        ],
        "abstract": "  We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405008",
        "title": "A Stochastic Finite-State Word-Segmentation Algorithm for Chinese",
        "authors": [
            "Richard Sproat",
            "Chilin Shih",
            "William Gale",
            "Nancy Chang"
        ],
        "abstract": "  We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system's performance, taking into account the fact that people often do not agree on a single segmentation.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405009",
        "title": "Natural Language Parsing as Statistical Pattern Recognition",
        "authors": [
            "David M. Magerman"
        ],
        "abstract": "  Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians.  A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.\n",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405010",
        "title": "Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Discourse Inference",
        "authors": [
            "Andrew Kehler"
        ],
        "abstract": "  It is claimed that a variety of facts concerning ellipsis, event reference, and interclausal coherence can be explained by two features of the linguistic form in question: (1) whether the form leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. It is proposed that these features interact with one of two types of discourse inference, namely {\\it Common Topic} inference and {\\it Coherent Situation} inference. The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account.\n    ",
        "submission_date": "1994-05-03T00:00:00",
        "last_modified_date": "1994-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405011",
        "title": "A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues",
        "authors": [
            "Jennifer Chu-Carroll",
            "Sandra Carberry"
        ],
        "abstract": "  This paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the {\\em Propose-Evaluate-Modify} cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered.\n    ",
        "submission_date": "1994-05-05T00:00:00",
        "last_modified_date": "1994-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405012",
        "title": "Integration Of Visual Inter-word Constraints And Linguistic Knowledge In Degraded Text Recognition",
        "authors": [
            "Tao Hong"
        ],
        "abstract": "  Degraded text recognition is a difficult task. Given a noisy text image, a word recognizer can be applied to generate several candidates for each word image. High-level knowledge sources can then be used to select a decision from the candidate set for each word image. In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection. Visual inter-word constraints provide a way to link word images inside the text page, and to interpret them systematically.\n    ",
        "submission_date": "1994-05-06T00:00:00",
        "last_modified_date": "1994-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405013",
        "title": "Collaboration on reference to objects that are not mutually known",
        "authors": [
            "Philip G. Edmonds"
        ],
        "abstract": "  In conversation, a person sometimes has to refer to an object that is not previously known to the other participant. We present a plan-based model of how agents collaborate on reference of this sort. In making a reference, an agent uses the most salient attributes of the referent. In understanding a reference, an agent determines his confidence in its adequacy as a means of identifying the referent. To collaborate, the agents use judgment, suggestion, and elaboration moves to refashion an inadequate referring expression.\n    ",
        "submission_date": "1994-05-06T00:00:00",
        "last_modified_date": "1994-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405014",
        "title": "Classifying Cue Phrases in Text and Speech Using Machine Learning",
        "authors": [
            "Diane J. Litman"
        ],
        "abstract": "  Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification rules from sets of pre-classified cue phrases and their features. Machine learning is shown to be an effective technique for not only automating the generation of classification rules, but also for improving upon previous results.\n    ",
        "submission_date": "1994-05-09T00:00:00",
        "last_modified_date": "1994-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405015",
        "title": "Intention-based Segmentation: Human Reliability and Correlation with Linguistic Cues",
        "authors": [
            "Rebecca J. Passonneau",
            "Diane J. Litman"
        ],
        "abstract": "  Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.\n    ",
        "submission_date": "1994-05-09T00:00:00",
        "last_modified_date": "1994-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405016",
        "title": "Precise n-gram Probabilities from Stochastic Context-free Grammars",
        "authors": [
            "Andreas Stolcke",
            "Jonathan Segal"
        ],
        "abstract": "  We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. We discuss efficient implementation of the algorithm and report our practical experience with it.\n    ",
        "submission_date": "1994-05-10T00:00:00",
        "last_modified_date": "1994-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405017",
        "title": "Best-first Model Merging for Hidden Markov Model Induction",
        "authors": [
            "Andreas Stolcke",
            "Stephen M. Omohundro"
        ],
        "abstract": "  This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models.\n    ",
        "submission_date": "1994-05-10T00:00:00",
        "last_modified_date": "1994-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405018",
        "title": "Memory-Based Lexical Acquisition and Processing",
        "authors": [
            "Walter Daelemans"
        ],
        "abstract": "  Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.\n    ",
        "submission_date": "1994-05-16T00:00:00",
        "last_modified_date": "1994-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405019",
        "title": "Determination of referential property and number of nouns in Japanese sentences for machine translation into English",
        "authors": [
            "Masaki Murata",
            "Makoto Nagao"
        ],
        "abstract": "  When translating Japanese nouns into English, we face the problem of articles and numbers which the Japanese language does not have, but which are necessary for the English composition. To solve this difficult problem we classified the referential property and the number of nouns into three types respectively. This paper shows that the referential property and the number of nouns in a sentence can be estimated fairly reliably by the words in the sentence. Many rules for the estimation were written in forms similar to rewriting rules in expert systems. We obtained the correct recognition scores of 85.5\\% and 89.0\\% in the estimation of the referential property and the number respectively for the sentences which were used for the construction of our rules. We tested these rules for some other texts, and obtained the scores of 68.9\\% and 85.6\\% respectively.\n    ",
        "submission_date": "1994-05-19T00:00:00",
        "last_modified_date": "1994-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405020",
        "title": "Capturing CFLs with Tree Adjoining Grammars",
        "authors": [
            "James Rogers"
        ],
        "abstract": "  We define a decidable class of TAGs that is strongly equivalent to CFGs and is cubic-time parsable. This class serves to lexicalize CFGs in the same manner as the LCFGs of Schabes and Waters but with considerably less restriction on the form of the grammars. The class provides a normal form for TAGs that generate local sets in much the same way that regular grammars provide a normal form for CFGs that generate regular sets.\n    ",
        "submission_date": "1994-05-24T00:00:00",
        "last_modified_date": "1994-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405021",
        "title": "Generating Precondition Expressions in Instructional Text",
        "authors": [
            "Keith Vander Linden"
        ],
        "abstract": "  This study employs a knowledge intensive corpus analysis to identify the elements of the communicative context which can be used to determine the appropriate lexical and grammatical form of instructional texts. \\ig, an instructional text generation system based on this analysis, is presented, particularly with reference to its expression of precondition relations.\n    ",
        "submission_date": "1994-05-24T00:00:00",
        "last_modified_date": "1994-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405022",
        "title": "Grammar Specialization through Entropy Thresholds",
        "authors": [
            "Christer Samuelsson"
        ],
        "abstract": "  Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values.\n    ",
        "submission_date": "1994-05-25T00:00:00",
        "last_modified_date": "1994-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405023",
        "title": "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "authors": [
            "Alon Lavie"
        ],
        "abstract": "  GLR* is a recently developed robust version of the Generalized LR Parser, that can parse almost ANY input sentence by ignoring unrecognizable parts of the sentence. On a given input sentence, the parser returns a collection of parses that correspond to maximal, or close to maximal, parsable subsets of the original input. This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed ``best'' from such a collection. We describe the heuristic measures used and their combination scheme. Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported.\n    ",
        "submission_date": "1994-05-26T00:00:00",
        "last_modified_date": "1994-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405024",
        "title": "Abductive Equivalential Translation and its application to Natural Language Database Interfacing",
        "authors": [
            "Manny Rayner"
        ],
        "abstract": "  The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of primitives meaningful to the underlying database engine. We begin by describing the nature of the problem, and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call ``Abductive Equivalential Translation'' (AET); functionalities which can be reduced to this form include answering questions, responding to commands, reasoning about the completeness of answers, answering meta-questions of type ``Do you know...'', and generating assertions and questions. In each case, a ``linguistic domain theory'' (LDT) $\\Gamma$ and an input formula $F$ are given, and the goal is to construct a formula with certain properties which is equivalent to $F$, given $\\Gamma$ and a set of permitted assumptions. If the LDT is of a certain specified type, whose formulas are either conditional equivalences or Horn-clauses, we show that the AET problem can be reduced to a goal-directed inference method. We present an abstract description of this method, and sketch its realization in Prolog. The relationship between AET and several problems previously discussed in the literature is discussed. In particular, we show how AET can provide a simple and elegant solution to the so-called ``Doctor on Board'' problem, and in effect allows a ``relativization'' of the Closed World Assumption. The ideas in the thesis have all been implemented concretely within the SRI CLARE project, using a real projects and payments database. The LDT for the example database is described in detail, and examples of the types of functionality that can be achieved within the example domain are presented.\n    ",
        "submission_date": "1994-05-26T00:00:00",
        "last_modified_date": "1994-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405025",
        "title": "An Optimal Tabular Parsing Algorithm",
        "authors": [
            "Mark-Jan Nederhof"
        ],
        "abstract": "  In this paper we relate a number of parsing algorithms which have been developed in very different areas of parsing theory, and which include deterministic algorithms, tabular algorithms, and a parallel algorithm. We show that these algorithms are based on the same underlying ideas. By relating existing ideas, we hope to provide an opportunity to improve some algorithms based on features of others. A second purpose of this paper is to answer a question which has come up in the area of tabular parsing, namely how to obtain a parsing algorithm with the property that the table will contain as little entries as possible, but without the possibility that two entries represent the same subderivation.\n    ",
        "submission_date": "1994-05-26T00:00:00",
        "last_modified_date": "1994-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405026",
        "title": "An Extended Theory of Head-Driven Parsing",
        "authors": [
            "Mark-Jan Nederhof",
            "Giorgio Satta"
        ],
        "abstract": "  We show that more head-driven parsing algorithms can be formulated than those occurring in the existing literature. These algorithms are inspired by a family of left-to-right parsing algorithms from a recent publication. We further introduce a more advanced notion of ``head-driven parsing'' which allows more detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques.\n    ",
        "submission_date": "1994-05-26T00:00:00",
        "last_modified_date": "1994-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405027",
        "title": "Acquiring Receptive Morphology: A Connectionist Model",
        "authors": [
            "Michael Gasser"
        ],
        "abstract": "  This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules. Separate network modules responsible for syllables enable to the network to learn simple reduplication rules as well. The model also embodies constraints against association-line crossing.\n    ",
        "submission_date": "1994-05-27T00:00:00",
        "last_modified_date": "1994-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405028",
        "title": "Semantics of Complex Sentences in Japanese",
        "authors": [
            "Hiroshi Nakagawa",
            "Shin'ichiro Nishizawa"
        ],
        "abstract": "  The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively. However if there can be relations between every pair of semantic roles, the amount of computation to identify the relations that hold in the given sentence is extremely large. In this paper, for semantics of Japanese complex sentence, we introduce new pragmatic roles called `observer' and `motivated' respectively to bridge semantic roles of subordinate and those of main clauses. By these new roles constraints on the relations among semantic/pragmatic roles are known to be almost local within subordinate or main clause. In other words, as for the semantics of the whole complex sentence, the only role we should deal with is a motivated.\n    ",
        "submission_date": "1994-05-28T00:00:00",
        "last_modified_date": "1994-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405029",
        "title": "Structural Tags, Annealing and Automatic Word Classification",
        "authors": [
            "John McMahon",
            "F.J.Smith"
        ],
        "abstract": "  This paper describes an automatic word classification system which uses a locally optimal annealing algorithm and average class mutual information. A new word-class representation, the structural tag is introduced and its advantages for use in statistical language modelling are presented. A summary of some results with the one million word LOB corpus is given; the algorithm is also shown to discover the vowel-consonant distinction and displays an ability to cluster words syntactically in a Latin corpus. Finally, a comparison is made between the current classification system and several leading alternative systems, which shows that the current system performs respectably well.\n    ",
        "submission_date": "1994-05-30T00:00:00",
        "last_modified_date": "1994-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405030",
        "title": "Priority Union and Generalization in Discourse Grammars",
        "authors": [
            "Claire Grover",
            "Chris Brew",
            "Suresh Manandhar",
            "Marc Moens"
        ],
        "abstract": "  We describe an implementation in Carpenter's typed feature formalism, ALE, of a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of VP-ellipsis which exhibit a strict/sloppy ambiguity.\n    ",
        "submission_date": "1994-05-30T00:00:00",
        "last_modified_date": "1994-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405031",
        "title": "An Attributive Logic of Set Descriptions and Set Operations",
        "authors": [
            "Suresh Manandhar"
        ],
        "abstract": "  This paper provides a model theoretic semantics to feature terms augmented with set descriptions. We provide constraints to specify HPSG style set descriptions, fixed cardinality set descriptions, set-membership constraints, restricted universal role quantifications, set union, intersection, subset and disjointness. A sound, complete and terminating consistency checking procedure is provided to determine the consistency of any given term in the logic. It is shown that determining consistency of terms is a NP-complete problem.\n    ",
        "submission_date": "1994-05-30T00:00:00",
        "last_modified_date": "1994-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405032",
        "title": "Modularity in a Connectionist Model of Morphology Acquisition",
        "authors": [
            "Michael Gasser"
        ],
        "abstract": "  This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs. It is shown that the performance of the two separate modular networks is superior to a single network responsible for both root and inflection identification. In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification.\n    ",
        "submission_date": "1994-05-30T00:00:00",
        "last_modified_date": "1994-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405033",
        "title": "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "authors": [
            "John Carroll"
        ],
        "abstract": "  The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers.\n    ",
        "submission_date": "1994-05-31T00:00:00",
        "last_modified_date": "1994-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405034",
        "title": "Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation",
        "authors": [
            "Kuang-hua Chen",
            "Hsin-Hsi Chen"
        ],
        "abstract": "  To acquire noun phrases from running texts is useful for many applications, such as word grouping,terminology indexing, etc. The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem. In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism. The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically. The results of this preliminary experiment are encouraging.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9405035",
        "title": "Dual-Coding Theory and Connectionist Lexical Selection",
        "authors": [
            "Ye-Yi Wang"
        ],
        "abstract": "  We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation. This lexical selection approach has two advantages. First, it is learnable. Little human effort on knowledge engineering is required. Secondly, it is psycholinguistically well-founded.\n    ",
        "submission_date": "1994-05-31T00:00:00",
        "last_modified_date": "1994-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406001",
        "title": "Intentions and Information in Discourse",
        "authors": [
            "Nicholas Asher",
            "Alex Lascarides"
        ],
        "abstract": "  This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406002",
        "title": "Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation",
        "authors": [
            "Katashi Nagao",
            "Akikazu Takeuchi"
        ],
        "abstract": "  Human face-to-face conversation is an ideal model for human-computer dialogue. One of the major features of face-to-face communication is its multiplicity of communication channels that act on multiple modalities. To realize a natural multimodal dialogue, it is necessary to study how humans perceive information and determine the information to which humans are sensitive. A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial expressions. We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation. Our experiments have shown that facial expressions are helpful, especially upon first contact with the system. We have also discovered that featuring facial expressions at an early stage improves subsequent interaction.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406003",
        "title": "A Learning Approach to Natural Language Understanding",
        "authors": [
            "Roberto Pieraccini",
            "Esther Levin"
        ],
        "abstract": "  In this paper we propose a learning paradigm for the problem of understanding spoken language. The basis of the work is in a formalization of the understanding problem as a communication problem. This results in the definition of a stochastic model of the production of speech or text starting from the meaning of a sentence. The resulting understanding algorithm consists in a Viterbi maximization procedure, analogous to that commonly used for recognizing speech. The algorithm was implemented for building\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406004",
        "title": "Towards a Principled Representation of Discourse Plans",
        "authors": [
            "R. Michael Young",
            "Johanna D. Moore",
            "Martha E. Pollack"
        ],
        "abstract": "  We argue that discourse plans must capture the intended causal and decompositional relations between communicative actions. We present a planning algorithm, DPOCL, that builds plan structures that properly capture these relations, and show how these structures are used to solve the problems that plagued previous discourse planners, and allow a system to participate effectively and flexibly in an ongoing dialogue.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406005",
        "title": "Word-Sense Disambiguation Using Decomposable Models",
        "authors": [
            "Rebecca Bruce",
            "Janyce Wiebe"
        ],
        "abstract": "  Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun \"interest\". We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406006",
        "title": "Detecting and Correcting Speech Repairs",
        "authors": [
            "Peter Heeman",
            "James Allen"
        ],
        "abstract": "  Interactive spoken dialog provides many new challenges for spoken language systems. One of the most critical is the prevalence of speech repairs. This paper presents an algorithm that detects and corrects speech repairs based on finding the repair pattern. The repair pattern is built by finding word matches and word replacements, and identifying fragments and editing terms. Rather than using a set of prebuilt templates, we build the pattern on the fly. In a fair test, our method, when combined with a statistical model to filter possible repairs, was successful at detecting and correcting 80\\% of the repairs, without using prosodic information or a parser.\n    ",
        "submission_date": "1994-06-01T00:00:00",
        "last_modified_date": "1994-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406007",
        "title": "Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria",
        "authors": [
            "Dekai Wu"
        ],
        "abstract": "  We describe our experience with automatic alignment of sentences in parallel English-Chinese texts.  Our report concerns three related topics:\n",
        "submission_date": "1994-06-02T00:00:00",
        "last_modified_date": "1994-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406008",
        "title": "Parsing Turkish with the Lexical Functional Grammar Formalism",
        "authors": [
            "Zelal Gungordu",
            "Kemal Oflazer"
        ],
        "abstract": "  This paper describes our work on parsing Turkish using the lexical-functional grammar formalism. This work represents the first significant effort for parsing Turkish. Our implementation is based on Tomita's parser developed at Carnegie-Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer. After a discussion of key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82\\% of the sentences directly and almost all the remaining with very minor pre-editing.\n    ",
        "submission_date": "1994-06-02T00:00:00",
        "last_modified_date": "1994-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406009",
        "title": "Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on Derivations",
        "authors": [
            "Owen Rambow"
        ],
        "abstract": "  This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links. The former models certain uses of multiset-valued feature structures in unification-based formalisms, while the latter is motivated by word order variation and by ``quasi-trees'', a generalization of trees. The two formalisms are weakly equivalent, and an important subset is at most context-sensitive and polynomially parsable.\n    ",
        "submission_date": "1994-06-02T00:00:00",
        "last_modified_date": "1994-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406010",
        "title": "Some Advances in Transformation-Based Part of Speech Tagging",
        "authors": [
            "Eric Brill"
        ],
        "abstract": "  Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In [Brill92], a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that are not captured by stochastic taggers. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty.\n    ",
        "submission_date": "1994-06-02T00:00:00",
        "last_modified_date": "1994-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406011",
        "title": "Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging",
        "authors": [
            "Lance A. Ramshaw",
            "Mitchell P. Marcus"
        ],
        "abstract": "  Eric Brill has recently proposed a simple and powerful corpus-based language modeling approach that can be applied to various tasks including part-of-speech tagging and building phrase structure trees. The method learns a series of symbolic transformational rules, which can then be applied in sequence to a test corpus to produce predictions. The learning process only requires counting matches for a given set of rule templates, allowing the method to survey a very large space of possible contextual factors. This paper analyses Brill's approach as an interesting variation on existing decision tree methods, based on experiments involving part-of-speech tagging for both English and ancient Greek corpora. In particular, the analysis throws light on why the new mechanism seems surprisingly resistant to overtraining. A fast, incremental implementation and a mechanism for recording the dependencies that underlie the resulting rule sequence are also described.\n    ",
        "submission_date": "1994-06-03T00:00:00",
        "last_modified_date": "1994-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406012",
        "title": "Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions",
        "authors": [
            "Patrick Juola"
        ],
        "abstract": "  With the advent of faster computers, the notion of doing machine translation from a huge stored database of translation examples is no longer unreasonable. This paper describes an attempt to merge the Example-Based Machine Translation (EBMT) approach with psycholinguistic principles. A new formalism for context- free grammars, called *marker-normal form*, is demonstrated and used to describe language data in a way compatible with psycholinguistic theories. By embedding this formalism in a standard multivariate optimization framework, a system can be built that infers correct transfer functions for a set of bilingual sentence pairs and then uses those functions to translate novel sentences. The validity of this line of reasoning has been tested in the development of a system called METLA-1. This system has been used to infer English->French and English->Urdu transfer functions from small corpora. The results of those experiments are examined, both in engineering terms as well as in more linguistic terms. In general, the results of these experiments were psycho- logically and linguistically well-grounded while still achieving a respectable level of success when compared against a similar prototype using Hidden Markov Models.\n    ",
        "submission_date": "1994-06-03T00:00:00",
        "last_modified_date": "1994-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406013",
        "title": "Graded Unification: A Framework for Interactive Processing",
        "authors": [
            "Albert Kim"
        ],
        "abstract": "  An extension to classical unification, called {\\em graded unification} is presented. It is capable of combining contradictory information. An interactive processing paradigm and parser based on this new operator are also presented.\n    ",
        "submission_date": "1994-06-05T00:00:00",
        "last_modified_date": "1994-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406014",
        "title": "A Hybrid Reasoning Model for Indirect Answers",
        "authors": [
            "Nancy Green",
            "Sandra Carberry"
        ],
        "abstract": "  This paper presents our implemented computational model for interpreting and generating indirect answers to Yes-No questions. Its main features are 1) a discourse-plan-based approach to implicature, 2) a reversible architecture for generation and interpretation, 3) a hybrid reasoning model that employs both plan inference and logical inference, and 4) use of stimulus conditions to model a speaker's motivation for providing appropriate, unrequested information. The model handles a wider range of types of indirect answers than previous computational models and has several significant advantages.\n    ",
        "submission_date": "1994-06-07T00:00:00",
        "last_modified_date": "1994-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406015",
        "title": "Statistical Augmentation of a Chinese Machine-Readable Dictionary",
        "authors": [
            "Pascale Fung",
            "Dekai Wu"
        ],
        "abstract": "  We describe a method of using statistically-collected Chinese character groups from a corpus to augment a Chinese dictionary. The method is particularly useful for extracting domain-specific and regional words not readily available in machine-readable dictionaries. Output was evaluated both using human evaluators and against a previously available dictionary. We also evaluated performance improvement in automatic Chinese tokenization. Results show that our method outputs legitimate words, acronymic constructions, idioms, names and titles, as well as technical compounds, many of which were lacking from the original dictionary.\n    ",
        "submission_date": "1994-06-07T00:00:00",
        "last_modified_date": "1994-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406016",
        "title": "Corpus-Driven Knowledge Acquisition for Discourse Analysis",
        "authors": [
            "Stephen Soderland",
            "Wendy Lehnert"
        ],
        "abstract": "  The availability of large on-line text corpora provides a natural and promising bridge between the worlds of natural language processing (NLP) and machine learning (ML). In recent years, the NLP community has been aggressively investigating statistical techniques to drive part-of-speech taggers, but application-specific text corpora can be used to drive knowledge acquisition at much higher levels as well. In this paper we will show how ML techniques can be used to support knowledge acquisition for information extraction systems. It is often very difficult to specify an explicit domain model for many information extraction applications, and it is always labor intensive to implement hand-coded heuristics for each new domain. We have discovered that it is nevertheless possible to use ML algorithms in order to capture knowledge that is only implicitly present in a representative text corpus. Our work addresses issues traditionally associated with discourse analysis and intersentential inference generation, and demonstrates the utility of ML algorithms at this higher level of language analysis. The benefits of our work address the portability and scalability of information extraction (IE) technologies. When hand-coded heuristics are used to manage discourse analysis in an information extraction system, months of programming effort are easily needed to port a successful IE system to a new domain. We will show how ML algorithms can reduce this\n    ",
        "submission_date": "1994-06-07T00:00:00",
        "last_modified_date": "1994-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406017",
        "title": "An Automatic Method of Finding Topic Boundaries",
        "authors": [
            "Jeffrey C. Reynar"
        ],
        "abstract": "  This article outlines a new method of locating discourse boundaries based on lexical cohesion and a graphical technique called dotplotting. The application of dotplotting to discourse segmentation can be performed either manually, by examining a graph, or automatically, using an optimization algorithm. The results of two experiments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined.\n    ",
        "submission_date": "1994-06-07T00:00:00",
        "last_modified_date": "1994-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406018",
        "title": "TDL--- A Type Description Language for Constraint-Based Grammars",
        "authors": [
            "Hans-Ulrich Krieger",
            "Ulrich Sch\u00e4fer"
        ],
        "abstract": "  This paper presents \\tdl, a typed feature-based representation language and inference system. Type definitions in \\tdl\\ consist of type and feature constraints over the boolean connectives. \\tdl\\ supports open- and closed-world reasoning over types and allows for partitions and incompatible types. Working with partially as well as with fully expanded types is possible. Efficient reasoning in \\tdl\\ is accomplished through specialized modules.\n    ",
        "submission_date": "1994-06-08T00:00:00",
        "last_modified_date": "1994-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406019",
        "title": "A Complete and Recursive Feature Theory",
        "authors": [
            "Rolf Backofen",
            "Gert Smolka"
        ],
        "abstract": "  Various feature descriptions are being employed in logic programming languages and constrained-based grammar formalisms. The common notational primitive of these descriptions are functional attributes called features. The descriptions considered in this paper are the possibly quantified first-order formulae obtained from a signature of binary and unary predicates called features and sorts, respectively. We establish a first-order theory FT by means of three axiom schemes, show its completeness, and construct three elementarily equivalent models. One of the models consists of so-called feature graphs, a data structure common in computational linguistics. The other two models consist of so-called feature trees, a record-like data structure generalizing the trees corresponding to first-order terms. Our completeness proof exhibits a terminating simplification system deciding validity and satisfiability of possibly quantified feature descriptions.\n    ",
        "submission_date": "1994-06-10T00:00:00",
        "last_modified_date": "1994-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406020",
        "title": "DPOCL: A Principled Approach to Discourse Planning",
        "authors": [
            "R. Michael Young",
            "Johanna D. Moore"
        ],
        "abstract": "  Research in discourse processing has identified two representational requirements for discourse planning systems. First, discourse plans must adequately represent the intentional structure of the utterances they produce in order to enable a computational discourse agent to respond effectively to communicative failures \\cite{MooreParisCL}. Second, discourse plans must represent the informational structure of utterances. In addition to these representational requirements, we argue that discourse planners should be formally characterizable in terms of soundness and completeness.\n    ",
        "submission_date": "1994-06-10T00:00:00",
        "last_modified_date": "1994-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406021",
        "title": "A symbolic description of punning riddles and its computer implementation",
        "authors": [
            "Kim Binsted",
            "Graeme Ritchie"
        ],
        "abstract": "  Riddles based on simple puns can be classified according to the patterns of word, syllable or phrase similarity they depend upon. We have devised a formal model of the semantic and syntactic regularities underlying some of the simpler types of punning riddle. We have also implemented this preliminary theory in a computer program which can generate riddles from a lexicon containing general data about words and phrases; that is, the lexicon content is not customised to produce jokes. Informal evaluation of the program's results by a set of human judges suggest that the riddles produced by this program are of comparable quality to those in general circulation among school children.\n    ",
        "submission_date": "1994-06-13T00:00:00",
        "last_modified_date": "1994-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406022",
        "title": "An implemented model of punning riddles",
        "authors": [
            "Kim Binsted",
            "Graeme Ritchie"
        ],
        "abstract": "  In this paper, we discuss a model of simple question-answer punning, implemented in a program, JAPE, which generates riddles from humour-independent lexical entries. The model uses two main types of structure: schemata, which determine the relationships between key words in a joke, and templates, which produce the surface form of the joke. JAPE succeeds in generating pieces of text that are recognizably jokes, but some of them are not very good jokes. We mention some potential improvements and extensions, including post-production heuristics for ordering the jokes according to quality.\n    ",
        "submission_date": "1994-06-13T00:00:00",
        "last_modified_date": "1994-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406023",
        "title": "A Spanish Tagset for the CRATER Project",
        "authors": [
            "Fernando S\u00e1nchez Le\u00f3n"
        ],
        "abstract": "  This working paper describes the Spanish tagset to be used in the context of CRATER, a CEC funded project aiming at the creation of a multilingual (English, French, Spanish) aligned corpus using the International Telecommunications Union corpus. In this respect, each version of the corpus will be (or is currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to perform the tagging of the Spanish version. This tagset has been devised as the ideal one for Spanish, and has been posted to several lists in order to get feedback to it.\n    ",
        "submission_date": "1994-06-14T00:00:00",
        "last_modified_date": "1994-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406024",
        "title": "Learning Fault-tolerant Speech Parsing with SCREEN",
        "authors": [
            "Stefan Wermter",
            "Volker Weber"
        ],
        "abstract": "  This paper describes a new approach and a system SCREEN for fault-tolerant speech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for Natural language. Speech parsing describes the syntactic and semantic analysis of spontaneous spoken language. The general approach is based on incremental immediate flat analysis, learning of syntactic and semantic speech parsing, parallel integration of current hypotheses, and the consideration of various forms of speech related errors. The goal for this approach is to explore the parallel interactions between various knowledge sources for learning incremental fault-tolerant speech parsing. This approach is examined in a system SCREEN using various hybrid connectionist techniques. Hybrid connectionist techniques are examined because of their promising properties of inherent fault tolerance, learning, gradedness and parallel constraint integration. The input for SCREEN is hypotheses about recognized words of a spoken utterance potentially analyzed by a speech system, the output is hypotheses about the flat syntactic and semantic analysis of the utterance. In this paper we focus on the general approach, the overall architecture, and examples for learning flat syntactic speech parsing. Different from most other speech language architectures SCREEN emphasizes an interactive rather than an autonomous position, learning rather than encoding, flat analysis rather than in-depth analysis, and fault-tolerant processing of phonetic, syntactic and semantic knowledge.\n    ",
        "submission_date": "1994-06-16T00:00:00",
        "last_modified_date": "1994-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406025",
        "title": "Emergent Parsing and Generation with Generalized Chart",
        "authors": [
            "HASIDA Koiti"
        ],
        "abstract": "  A new, flexible inference method for Horn logic program is proposed, which is a drastic generalization of chart parsing, partial instantiations of clauses in a program roughly corresponding to arcs in a chart. Chart-like parsing and semantic-head-driven generation emerge from this method. With a parsimonious instantiation scheme for ambiguity packing, the parsing complexity reduces to that of standard chart-based algorithms.\n    ",
        "submission_date": "1994-06-16T00:00:00",
        "last_modified_date": "1994-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406026",
        "title": "The Very Idea of Dynamic Semantics",
        "authors": [
            "David Israel"
        ],
        "abstract": "  \"Natural languages are programming languages for minds.\" Can we or should we take this slogan seriously? If so, how? Can answers be found by looking at the various \"dynamic\" treatments of natural language developed over the last decade or so, mostly in response to problems associated with donkey anaphora? In Dynamic Logic of Programs, the meaning of a program is a binary relation on the set of states of some abstract machine. This relation is meant to model aspects of the effects of the execution of the program, in particular its input-output behavior. What, if anything, are the dynamic aspects of various proposed dynamic semantics for natural languages supposed to model? Is there anything dynamic to be modeled? If not, what is all the full about? We shall try to answer some, at least, of these questions and provide materials for answers to others.\n    ",
        "submission_date": "1994-06-17T00:00:00",
        "last_modified_date": "1994-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406027",
        "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
        "authors": [
            "Joerg P. Ueberla"
        ],
        "abstract": "  In many current speech recognizers, a statistical language model is used to indicate how likely it is that a certain word will be spoken next, given the words recognized so far. How can statistical language models be improved so that more complex speech recognition tasks can be tackled? Since the knowledge of the weaknesses of any theory often makes improving the theory easier, the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them. To that end, we formally define a weakness of a statistical language model in terms of the logarithm of the total probability, LTP, a term closely related to the standard perplexity measure used to evaluate statistical language models. We apply our definition of a weakness to a frequently used statistical language model, called a bi-pos model. This results, for example, in a new modeling of unknown words which improves the performance of the model by 14% to 21%. Moreover, one of the identified weaknesses has prompted the development of our generalized N-pos language model, which is also outlined in this thesis. It can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional N-pos model. This leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge. These results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general.\n    ",
        "submission_date": "1994-06-17T00:00:00",
        "last_modified_date": "1994-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406028",
        "title": "Resolution of Syntactic Ambiguity: the Case of New Subjects",
        "authors": [
            "Michael Niv"
        ],
        "abstract": "  I review evidence for the claim that syntactic ambiguities are resolved on the basis of the meaning of the competing analyses, not their structure. I identify a collection of ambiguities that do not yet have a meaning-based account and propose one which is based on the interaction of discourse and grammatical function. I provide evidence for my proposal by examining statistical properties of the Penn Treebank of syntactically annotated text.\n    ",
        "submission_date": "1994-06-20T00:00:00",
        "last_modified_date": "1994-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406029",
        "title": "A Computational Model of Syntactic Processing: Ambiguity Resolution from Interpretation",
        "authors": [
            "Michael Niv"
        ],
        "abstract": "  Syntactic ambiguity abounds in natural language, yet humans have no difficulty coping with it. In fact, the process of ambiguity resolution is almost always unconscious. But it is not infallible, however, as example 1 demonstrates.\n",
        "submission_date": "1994-06-20T00:00:00",
        "last_modified_date": "1994-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406030",
        "title": "The complexity of normal form rewrite sequences for Associativity",
        "authors": [
            "Michael Niv"
        ],
        "abstract": "  The complexity of a particular term-rewrite system is considered: the rule of associativity (x*y)*z --> x*(y*z). Algorithms and exact calculations are given for the longest and shortest sequences of applications of --> that result in normal form (NF). The shortest NF sequence for a term x is always n-drm(x), where n is the number of occurrences of * in x and drm(x) is the depth of the rightmost leaf of x. The longest NF sequence for any term is of length n(n-1)/2.\n    ",
        "submission_date": "1994-06-20T00:00:00",
        "last_modified_date": "1994-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406031",
        "title": "A Psycholinguistically Motivated Parser for CCG",
        "authors": [
            "Michael Niv"
        ],
        "abstract": "  Considering the speed in which humans resolve syntactic ambiguity, and the overwhelming evidence that syntactic ambiguity is resolved through selection of the analysis whose interpretation is the most `sensible', one comes to the conclusion that interpretation, hence parsing take place incrementally, just about every word. Considerations of parsimony in the theory of the syntactic processor lead one to explore the simplest of parsers: one which represents only analyses as defined by the grammar and no other information.\n",
        "submission_date": "1994-06-20T00:00:00",
        "last_modified_date": "1994-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406032",
        "title": "Anytime Algorithms for Speech Parsing?",
        "authors": [
            "Guenther Goerz",
            "Marcus Kesseler"
        ],
        "abstract": "  This paper discusses to which extent the concept of ``anytime algorithms'' can be applied to parsing algorithms with feature unification. We first try to give a more precise definition of what an anytime algorithm is. We arque that parsing algorithms have to be classified as contract algorithms as opposed to (truly) interruptible algorithms. With the restriction that the transaction being active at the time an interrupt is issued has to be completed before the interrupt can be executed, it is possible to provide a parser with limited anytime behavior, which is in fact being realized in our research prototype.\n    ",
        "submission_date": "1994-06-21T00:00:00",
        "last_modified_date": "1994-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406033",
        "title": "Verb Semantics and Lexical Selection",
        "authors": [
            "Zhibiao Wu",
            "Martha Palmer"
        ],
        "abstract": "  This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.\n    ",
        "submission_date": "1994-06-22T00:00:00",
        "last_modified_date": "1994-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406034",
        "title": "Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French",
        "authors": [
            "David Yarowsky"
        ],
        "abstract": "  This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text.\n    ",
        "submission_date": "1994-06-23T00:00:00",
        "last_modified_date": "1994-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406035",
        "title": "DISCO---An HPSG-based NLP System and its Application for Appointment Scheduling (Project Note)",
        "authors": [
            "Hans Uszkoreit",
            "Rolf Backofen",
            "Stephan Busemann",
            "Abdel Kader Diagne",
            "Elizabeth A. Hinkelman",
            "Walter Kasper",
            "Bernd Kiefer",
            "Hans-Ulrich Krieger",
            "Klaus Netter",
            "Guenter Neumann",
            "Stephan Oepen",
            "Stephen P. Spackman"
        ],
        "abstract": "  The natural language system DISCO is described. It combines o a powerful and flexible grammar development system; o linguistic competence for German including morphology, syntax and semantics; o new methods for linguistic performance modelling on the basis of high-level competence grammars; o new methods for modelling multi-agent dialogue competence; o an interesting sample application for appointment scheduling and calendar management.\n    ",
        "submission_date": "1994-06-23T00:00:00",
        "last_modified_date": "1994-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406036",
        "title": "Text Analysis Tools in Spoken Language Processing",
        "authors": [
            "Michael Riley",
            "Richard Sproat"
        ],
        "abstract": "  This submission contains the postscript of the final version of the slides used in our ACL-94 tutorial.\n    ",
        "submission_date": "1994-06-23T00:00:00",
        "last_modified_date": "1994-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406037",
        "title": "Multi-Paragraph Segmentation of Expository Text",
        "authors": [
            "Marti A. Hearst"
        ],
        "abstract": "  This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.\n    ",
        "submission_date": "1994-06-23T00:00:00",
        "last_modified_date": "1994-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406038",
        "title": "An Empirical Model of Acknowledgment for Spoken-Language Systems",
        "authors": [
            "David G. Novick",
            "Stephen Sutton"
        ],
        "abstract": "  We refine and extend prior views of the description, purposes, and contexts-of-use of acknowledgment acts through empirical examination of the use of acknowledgments in task-based conversation. We distinguish three broad classes of acknowledgments (other-->ackn, self-->other-->ackn, and self+ackn) and present a catalogue of 13 patterns within these classes that account for the specific uses of acknowledgment in the corpus.\n    ",
        "submission_date": "1994-06-27T00:00:00",
        "last_modified_date": "1994-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406039",
        "title": "Three studies of grammar-based surface-syntactic parsing of unrestricted English text. A summary and orientation",
        "authors": [
            "Atro Voutilainen"
        ],
        "abstract": "  The dissertation addresses the design of parsing grammars for automatic surface-syntactic analysis of unconstrained English text. It consists of a summary and three articles. {\\it Morphological disambiguation} documents a grammar for morphological (or part-of-speech) disambiguation of English, done within the Constraint Grammar framework proposed by Fred Karlsson. The disambiguator seeks to discard those of the alternative morphological analyses proposed by the lexical analyser that are contextually illegitimate. The 1,100 constraints express some 23 general, essentially syntactic statements as restrictions on the linear order of morphological tags. The error rate of the morphological disambiguator is about ten times smaller than that of another state-of-the-art probabilistic disambiguator, given that both are allowed to leave some of the hardest ambiguities unresolved. This accuracy suggests the viability of the grammar-based approach to natural language parsing, thus also contributing to the more general debate concerning the viability of probabilistic vs.\\ linguistic techniques. {\\it Experiments with heuristics} addresses the question of how to resolve those ambiguities that survive the morphological disambiguator. Two approaches are presented and empirically evaluated: (i) heuristic disambiguation constraints and (ii) techniques for learning from the fully disambiguated part of the corpus and then applying this information to resolving remaining ambiguities.\n    ",
        "submission_date": "1994-06-27T00:00:00",
        "last_modified_date": "1994-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9406040",
        "title": "Learning unification-based grammars using the Spoken English Corpus",
        "authors": [
            "Miles Osborne",
            "Derek Bridge"
        ],
        "abstract": "  This paper describes a grammar learning system that combines model-based and data-driven learning within a single framework. Our results from learning grammars using the Spoken English Corpus (SEC) suggest that combined model-based and data-driven learning can produce a more plausible grammar than is the case when using either learning style isolation.\n    ",
        "submission_date": "1994-06-28T00:00:00",
        "last_modified_date": "1994-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407001",
        "title": "Morphology with a Null-Interface",
        "authors": [
            "Harald Trost",
            "Johannes Matiasek"
        ],
        "abstract": "  We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm. The core of the system is a CLP implementation of a unification engine for feature structures supporting relational values. In this framework an HPSG-style grammar is implemented. Word-level processing uses X2MorF, a morphological component based on an extended version of two-level morphology. This component is tightly integrated with the grammar as a relation. The advantage of this approach is that morphology and syntax are kept logically autonomous while at the same time minimizing interface problems.\n    ",
        "submission_date": "1994-07-04T00:00:00",
        "last_modified_date": "1994-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407002",
        "title": "Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm",
        "authors": [
            "Mehryar Mohri"
        ],
        "abstract": "  Local grammars can be represented in a very convenient way by automata. This paper describes and illustrates an efficient algorithm for the application of local grammars put in this form to lemmatized texts.\n    ",
        "submission_date": "1994-07-04T00:00:00",
        "last_modified_date": "1994-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407003",
        "title": "Compact Representations by Finite-State Transducers",
        "authors": [
            "Mehryar Mohri"
        ],
        "abstract": "  Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation.\n    ",
        "submission_date": "1994-07-04T00:00:00",
        "last_modified_date": "1994-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407004",
        "title": "Japanese word sense disambiguation based on examples of synonyms",
        "authors": [
            "Mitsutaka Matsumoto"
        ],
        "abstract": "  (This is not the abstract): The language is Japanese. If your printer does not have fonts for Japases characters, the characters in figures will not be printed out correctly. Dissertation for Bachelor's degree at Kyoto University(Nagao lab.),March 1994.\n    ",
        "submission_date": "1994-07-05T00:00:00",
        "last_modified_date": "1994-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407005",
        "title": "A Corrective Training Algorithm for Adaptive Learning in Bag Generation",
        "authors": [
            "Hsin-Hsi Chen",
            "Yue-Shi Lee"
        ],
        "abstract": "  The sampling problem in training corpus is one of the major sources of errors in corpus-based applications. This paper proposes a corrective training algorithm to best-fit the run-time context domain in the application of bag generation. It shows which objects to be adjusted and how to adjust their probabilities. The resulting techniques are greatly simplified and the experimental results demonstrate the promising effects of the training algorithm from generic domain to specific domain. In general, these techniques can be easily extended to various language models and corpus-based applications.\n    ",
        "submission_date": "1994-07-06T00:00:00",
        "last_modified_date": "1994-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407006",
        "title": "Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser",
        "authors": [
            "John Dowding",
            "Robert Moore",
            "Francois Andry",
            "Douglas Moran"
        ],
        "abstract": "  We describe an efficient bottom-up parser that interleaves syntactic and semantic structure building. Two techniques are presented for reducing search by reducing local ambiguity: Limited left-context constraints are used to reduce local syntactic ambiguity, and deferred sortal-constraint application is used to reduce local semantic ambiguity. We experimentally evaluate these techniques, and show dramatic reductions in both number of chart-edges and total parsing time. The robust processing capabilities of the parser are demonstrated in its use in improving the accuracy of a speech recognizer.\n    ",
        "submission_date": "1994-07-05T00:00:00",
        "last_modified_date": "1994-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407007",
        "title": "GEMINI: A Natural Language System for Spoken-Language Understanding",
        "authors": [
            "John Dowding",
            "Jean Mark Gawron",
            "Doug Appelt",
            "John Bear",
            "Lynn Cherny",
            "Robert Moore",
            "Douglas Moran"
        ],
        "abstract": "  Gemini is a natural language understanding system developed for spoken language applications. The paper describes the architecture of Gemini, paying particular attention to resolving the tension between robustness and overgeneration. Gemini features a broad-coverage unification-based grammar of English, fully interleaved syntactic and semantic processing in an all-paths, bottom-up parser, and an utterance-level parser to find interpretations of sentences that might not be analyzable as complete sentences. Gemini also includes novel components for recognizing and correcting grammatical disfluencies, and for doing parse preferences. This paper presents a component-by-component view of Gemini, providing detailed relevant measurements of size, efficiency, and performance.\n    ",
        "submission_date": "1994-07-05T00:00:00",
        "last_modified_date": "1994-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407008",
        "title": "Tricolor DAGs for Machine Translation",
        "authors": [
            "Koichi Takeda"
        ],
        "abstract": "  Machine translation (MT) has recently been formulated in terms of constraint-based knowledge representation and unification theories, but it is becoming more and more evident that it is not possible to design a practical MT system without an adequate method of handling mismatches between semantic representations in the source and target languages. In this paper, we introduce the idea of ``information-based'' MT, which is considerably more flexible than interlingual MT or the conventional transfer-based MT.\n    ",
        "submission_date": "1994-07-06T00:00:00",
        "last_modified_date": "1994-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407009",
        "title": "Estimating Performance of Pipelined Spoken Language Translation Systems",
        "authors": [
            "Manny Rayner",
            "David Carter",
            "Patti Price",
            "Bertil Lyberg"
        ],
        "abstract": "  Most spoken language translation systems developed to date rely on a pipelined architecture, in which the main stages are speech recognition, linguistic analysis, transfer, generation and speech synthesis. When making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies.\n",
        "submission_date": "1994-07-12T00:00:00",
        "last_modified_date": "1994-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407010",
        "title": "Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists",
        "authors": [
            "Manny Rayner",
            "David Carter",
            "Vassilios Digalakis",
            "Patti Price"
        ],
        "abstract": "  A simple and general method is described that can combine different knowledge sources to reorder N-best lists of hypotheses produced by a speech recognizer. The method is automatically trainable, acquiring information from both positive and negative examples. Experiments are described in which it was tested on a 1000-utterance sample of unseen ATIS data.\n    ",
        "submission_date": "1994-07-12T00:00:00",
        "last_modified_date": "1994-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407011",
        "title": "Discourse Obligations in Dialogue Processing",
        "authors": [
            "David R. Traum",
            "James F. Allen"
        ],
        "abstract": "  We show that in modeling social interaction, particularly dialogue, the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief, goal, and intention and their mutual and shared counterparts. In particular, we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system.\n    ",
        "submission_date": "1994-07-14T00:00:00",
        "last_modified_date": "1994-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407012",
        "title": "Phoneme Recognition Using Acoustic Events",
        "authors": [
            "Kai Huebener",
            "Julie Carson-Berndsen"
        ],
        "abstract": "  This paper presents a new approach to phoneme recognition using nonsequential sub--phoneme units. These units are called acoustic events and are phonologically meaningful as well as recognizable from speech signals. Acoustic events form a phonologically incomplete representation as compared to distinctive features. This problem may partly be overcome by incorporating phonological constraints. Currently, 24 binary events describing manner and place of articulation, vowel quality and voicing are used to recognize all German phonemes. Phoneme recognition in this paradigm consists of two steps: After the acoustic events have been determined from the speech signal, a phonological parser is used to generate syllable and phoneme hypotheses from the event lattice. Results obtained on a speaker--dependent corpus are presented.\n    ",
        "submission_date": "1994-07-15T00:00:00",
        "last_modified_date": "1994-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407013",
        "title": "The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic Representations",
        "authors": [
            "Carl de Marcken"
        ],
        "abstract": "  We present an algorithm that acquires words (pairings of phonological forms and semantic representations) from larger utterances of unsegmented phoneme sequences and semantic representations. The algorithm maintains from utterance to utterance only a single coherent dictionary, and learns in the presence of homonymy, synonymy, and noise. Test results over a corpus of utterances generated from the Childes database of mother-child interactions are presented.\n    ",
        "submission_date": "1994-07-15T00:00:00",
        "last_modified_date": "1994-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407014",
        "title": "Abstract Machine for Typed Feature Structures",
        "authors": [
            "Shuly Wintner",
            "Nissim Francez"
        ],
        "abstract": "  This paper describes a first step towards the definition of an abstract machine for linguistic formalisms that are based on typed feature structures, such as HPSG. The core design of the abstract machine is given in detail, including the compilation process from a high-level specification language to the abstract machine language and the implementation of the abstract instructions. We thus apply methods that were proved useful in computer science to the study of natural languages: a grammar specified using the formalism is endowed with an operational semantics. Currently, our machine supports the unification of simple feature structures, unification of sequences of such structures, cyclic structures and disjunction.\n    ",
        "submission_date": "1994-07-17T00:00:00",
        "last_modified_date": "1994-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407015",
        "title": "Specifying Intonation from Context for Speech Synthesis",
        "authors": [
            "Scott Prevost",
            "Mark Steedman"
        ],
        "abstract": "  This paper presents a theory and a computational implementation for generating prosodically appropriate synthetic speech in response to database queries. Proper distinctions of contrast and emphasis are expressed in an intonation contour that is synthesized by rule under the control of a grammar, a discourse model, and a knowledge base. The theory is based on Combinatory Categorial Grammar, a formalism which easily integrates the notions of syntactic constituency, semantics, prosodic phrasing and information structure. Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities for a given sentence depending on the discourse context.\n    ",
        "submission_date": "1994-07-18T00:00:00",
        "last_modified_date": "1994-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407016",
        "title": "The Role of Cognitive Modeling in Achieving Communicative Intentions",
        "authors": [
            "Marilyn Walker",
            "Owen Rambow"
        ],
        "abstract": "  A discourse planner for (task-oriented) dialogue must be able to make choices about whether relevant, but optional information (for example, the \"satellites\" in an RST-based planner) should be communicated. We claim that effective text planners must explicitly model aspects of the Hearer's cognitive state, such as what the hearer is attending to and what inferences the hearer can draw, in order to make these choices. We argue that a mere representation of the Hearer's knowledge is inadequate. We support this claim by (1) an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task).\n    ",
        "submission_date": "1994-07-19T00:00:00",
        "last_modified_date": "1994-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407017",
        "title": "Generating Context-Appropriate Word Orders in Turkish",
        "authors": [
            "Beryl Hoffman"
        ],
        "abstract": "  Turkish has considerably freer word order than English. The interpretations of different word orders in Turkish rely on information that describes how a sentence relates to its discourse context. To capture the syntactic features of a free word order language, I present an adaptation of Combinatory Categorial Grammars called {}-CCGs (set-CCGs). In {}-CCGs, a verb's subcategorization requirements are relaxed so that it requires a set of arguments without specifying their linear order. I integrate a level of information structure, representing pragmatic functions such as topic and focus, with {}-CCGs to allow certain pragmatic distinctions in meaning to influence the word order of a sentence in a compositional way. Finally, I discuss how this strategy is used within an implemented generation system which produces Turkish sentences with context-appropriate word orders in a simple database query task.\n    ",
        "submission_date": "1994-07-20T00:00:00",
        "last_modified_date": "1994-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407018",
        "title": "Generating Multilingual Documents from a Knowledge Base: The TECHDOC Project",
        "authors": [
            "Dietmar R\u00f6sner",
            "Manfred Stede"
        ],
        "abstract": "  TECHDOC is an implemented system demonstrating the feasibility of generating multilingual technical documents on the basis of a language-independent knowledge base. Its application domain is user and maintenance instructions, which are produced from underlying plan structures representing the activities, the participating objects with their properties, relations, and so on. This paper gives a brief outline of the system architecture and discusses some recent developments in the project: the addition of actual event simulation in the KB, steps towards a document authoring tool, and a multimodal user interface. (slightly corrected version of a paper to appear in: COLING 94, Proceedings)\n    ",
        "submission_date": "1994-07-21T00:00:00",
        "last_modified_date": "1994-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407019",
        "title": "Tracking Point of View in Narrative",
        "authors": [
            "Janyce M. Wiebe"
        ],
        "abstract": "  Third-person fictional narrative text is composed not only of passages that objectively narrate events, but also of passages that present characters' thoughts, perceptions, and inner states. Such passages take a character's ``psychological point of view''. A language understander must determine the current psychological point of view in order to distinguish the beliefs of the characters from the facts of the story, to correctly attribute beliefs and other attitudes to their sources, and to understand the discourse relations among sentences. Tracking the psychological point of view is not a trivial problem, because many sentences are not explicitly marked for point of view, and whether the point of view of a sentence is objective or that of a character (and if the latter, which character it is) often depends on the context in which the sentence appears. Tracking the psychological point of view is the problem addressed in this work. The approach is to seek, by extensive examinations of naturally-occurring narrative, regularities in the ways that authors manipulate point of view, and to develop an algorithm that tracks point of view on the basis of the regularities found. This paper presents this algorithm, gives demonstrations of an implemented system, and describes the results of some preliminary empirical studies, which lend support to the algorithm.\n    ",
        "submission_date": "1994-07-22T00:00:00",
        "last_modified_date": "1994-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407020",
        "title": "A Sequential Algorithm for Training Text Classifiers",
        "authors": [
            "David D. Lewis",
            "William A. Gale"
        ],
        "abstract": "  The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.\n    ",
        "submission_date": "1994-07-24T00:00:00",
        "last_modified_date": "1994-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407021",
        "title": "K-vec: A New Approach for Aligning Parallel Texts",
        "authors": [
            "Pascale Fung",
            "Kenneth Church"
        ],
        "abstract": "  Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates(Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word \"fisheries\" is similar to the French \"pe^ches\" by noting that the distribution of \"fisheries\" in the English text is similar to the distribution of \"pe^ches\" in the French. K-vec does not depend on sentence boundaries.\n    ",
        "submission_date": "1994-07-25T00:00:00",
        "last_modified_date": "1994-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407022",
        "title": "Comparative Discourse Analysis of Parallel Texts",
        "authors": [
            "Pim van der Eijk"
        ],
        "abstract": "  A quantitative representation of discourse structure can be computed by measuring lexical cohesion relations among adjacent blocks of text. These representations have been proposed to deal with sub-topic text segmentation. In a parallel corpus, similar representations can be derived for versions of a text in various languages. These can be used for parallel segmentation and as an alternative measure of text-translation similarity.\n    ",
        "submission_date": "1994-07-26T00:00:00",
        "last_modified_date": "1994-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407023",
        "title": "Multi-Tape Two-Level Morphology: A Case Study in Semitic Non-linear Morphology",
        "authors": [
            "George Kiraz"
        ],
        "abstract": "  This paper presents an implemented multi-tape two-level model capable of describing Semitic non-linear morphology. The computational framework behind the current work is motivated by Kay (1987); the formalism presented here is an extension to the formalism reported by Pulman and Hepple (1993). The objectives of the current work are: to stay as close as possible, in spirit, to standard two-level morphology, to stay close to the linguistic description of Semitic stems, and to present a model which can be used with ease by the Semitist. The paper illustrates that if finite-state transducers (FSTs) in a standard two-level morphology model are replaced with multi-tape auxiliary versions (AFSTs), one can account for Semitic root-and-pattern morphology using high level notation.\n    ",
        "submission_date": "1994-07-26T00:00:00",
        "last_modified_date": "1994-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407024",
        "title": "PRINCIPAR---An Efficient, Broad-coverage, Principle-based Parser",
        "authors": [
            "Dekang Lin"
        ],
        "abstract": "  We present an efficient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries.\n    ",
        "submission_date": "1994-07-27T00:00:00",
        "last_modified_date": "1994-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407025",
        "title": "Recovering From Parser Failures: A Hybrid Statistical/Symbolic Approach",
        "authors": [
            "Carolyn Penstein Rose'",
            "Alex Waibel"
        ],
        "abstract": "  We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system. We describe a module which takes as input a fragmented parse and returns a repaired meaning representation. It negotiates with the speaker about what the complete meaning of the utterance is by generating hypotheses about how to fit the fragments of the partial parse together into a coherent meaning representation. By drawing upon both statistical and symbolic information, it constrains its repair hypotheses to those which are both likely and meaningful. Because it updates its statistical model during use, it improves its performance over time.\n    ",
        "submission_date": "1994-07-28T00:00:00",
        "last_modified_date": "1994-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407026",
        "title": "Tagging and Morphological Disambiguation of Turkish Text",
        "authors": [
            "Kemal Oflazer",
            "Ilker Kuruoz"
        ],
        "abstract": "  Automatic text tagging is an important component in higher level analysis of text corpora, and its output can be used in many natural language processing applications. In languages like Turkish or Finnish, with agglutinative morphology, morphological disambiguation is a very crucial process in tagging, as the structures of many lexical forms are morphologically ambiguous. This paper describes a POS tagger for Turkish text based on a full-scale two-level specification of Turkish morphology that is based on a lexicon of about 24,000 root words. This is augmented with a multi-word and idiomatic construct recognizer, and most importantly morphological disambiguator based on local neighborhood constraints, heuristics and limited amount of statistical information. The tagger also has functionality for statistics compilation and fine tuning of the morphological analyzer, such as logging erroneous morphological parses, commonly used roots, etc. Preliminary results indicate that the tagger can tag about 98-99\\% of the texts accurately with very minimal user intervention. Furthermore for sentences morphologically disambiguated with the tagger, an LFG parser developed for Turkish, generates, on the average, 50\\% less ambiguous parses and parses almost 2.5 times faster. The tagging functionality is not specific to Turkish, and can be applied to any language with a proper morphological analysis interface.\n    ",
        "submission_date": "1994-07-29T00:00:00",
        "last_modified_date": "1994-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407027",
        "title": "Parsing as Tree Traversal",
        "authors": [
            "Dale Gerdemann"
        ],
        "abstract": "  This paper presents a unified approach to parsing, in which top-down, bottom-up and left-corner parsers are related to preorder, postorder and inorder tree traversals. It is shown that the simplest bottom-up and left-corner parsers are left recursive and must be converted using an extended Greibach normal form. With further partial execution, the bottom-up and left-corner parsers collapse together as in the BUP parser of Matsumoto.\n    ",
        "submission_date": "1994-07-29T00:00:00",
        "last_modified_date": "1994-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407028",
        "title": "Automated Postediting of Documents",
        "authors": [
            "Kevin Knight",
            "Ishwar Chander"
        ],
        "abstract": "  Large amounts of low- to medium-quality English texts are now being produced by machine translation (MT) systems, optical character readers (OCR), and non-native speakers of English. Most of this text must be postedited by hand before it sees the light of day. Improving text quality is tedious work, but its automation has not received much research attention. Anyone who has postedited a technical report or thesis written by a non-native speaker of English knows the potential of an automated postediting system. For the case of MT-generated text, we argue for the construction of postediting modules that are portable across MT systems, as an alternative to hardcoding improvements inside any one system. As an example, we have built a complete self-contained postediting module for the task of article selection (a, an, the) for English noun phrases. This is a notoriously difficult problem for Japanese-English MT. Our system contains over 200,000 rules derived automatically from online text resources. We report on learning algorithms, accuracy, and comparisons with human performance.\n    ",
        "submission_date": "1994-07-29T00:00:00",
        "last_modified_date": "1994-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407029",
        "title": "Building a Large-Scale Knowledge Base for Machine Translation",
        "authors": [
            "Kevin Knight",
            "Steve K. Luk"
        ],
        "abstract": "  Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PANGLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.\n    ",
        "submission_date": "1994-07-29T00:00:00",
        "last_modified_date": "1994-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9407030",
        "title": "Computing FIRST and FOLLOW Functions for Feature-Theoretic Grammars",
        "authors": [
            "Arturo Trujillo"
        ],
        "abstract": "  This paper describes an algorithm for the computation of FIRST and FOLLOW sets for use with feature-theoretic grammars in which the value of the sets consists of pairs of feature-theoretic categories. The algorithm preserves as much information from the grammars as possible, using negative restriction to define equivalence classes. Addition of a simple data structure leads to an order of magnitude improvement in execution time over a naive implementation.\n    ",
        "submission_date": "1994-07-30T00:00:00",
        "last_modified_date": "1994-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408001",
        "title": "The Correct and Efficient Implementation of Appropriateness Specifications for Typed Feature Structures",
        "authors": [
            "Dale Gerdemann",
            "Paul John King"
        ],
        "abstract": "  In this paper, we argue that type inferencing incorrectly implements appropriateness specifications for typed feature structures, promote a combination of type resolution and unfilling as a correct and efficient alternative, and consider the expressive limits of this alternative approach. Throughout, we use feature cooccurence restrictions as illustration and linguistic motivation.\n    ",
        "submission_date": "1994-08-01T00:00:00",
        "last_modified_date": "1994-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408002",
        "title": "Computational Analyses of Arabic Morphology",
        "authors": [
            "George A. Kiraz"
        ],
        "abstract": "  This paper demonstrates how a (multi-tape) two-level formalism can be used to write two-level grammars for Arabic non-linear morphology using a high level, but computationally tractable, notation. Three illustrative grammars are provided based on CV-, moraic- and affixational analyses. These are complemented by a proposal for handling the hitherto computationally untreated problem of the broken plural. It will be shown that the best grammars for describing Arabic non-linear morphology are moraic in the case of templatic stems, and affixational in the case of a-templatic stems. The paper will demonstrate how the broken plural can be derived under two-level theory via the `implicit' derivation of the singular.\n    ",
        "submission_date": "1994-08-01T00:00:00",
        "last_modified_date": "1994-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408003",
        "title": "Typed Feature Structures as Descriptions",
        "authors": [
            "Paul John King"
        ],
        "abstract": "  A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.\n    ",
        "submission_date": "1994-08-02T00:00:00",
        "last_modified_date": "1994-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408004",
        "title": "Parsing with Principles and Probabilities",
        "authors": [
            "Andrew Fordham",
            "Matthew Crocker"
        ],
        "abstract": "  This paper is an attempt to bring together two approaches to language analysis. The possible use of probabilistic information in principle-based grammars and parsers is considered, including discussion on some theoretical and computational problems that arise. Finally a partial implementation of these ideas is presented, along with some preliminary results from testing on a small set of sentences.\n    ",
        "submission_date": "1994-08-02T00:00:00",
        "last_modified_date": "1994-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408005",
        "title": "A Modular and Flexible Architecture for an Integrated Corpus Query System",
        "authors": [
            "Oliver Christ"
        ],
        "abstract": "  The paper describes the architecture of an integrated and extensible corpus query system developed at the University of Stuttgart and gives examples of some of the modules realized within this architecture. The modules form the core of a corpus workbench. Within the proposed architecture, information required for the evaluation of queries may be derived from different knowledge sources (the corpus text, databases, on-line thesauri) and by different means: either through direct lookup in a database or by calling external tools which may infer the necessary information at the time of query evaluation. The information available and the method of information access can be stated declaratively and individually for each corpus, leading to a flexible, extensible and modular corpus workbench.\n    ",
        "submission_date": "1994-08-02T00:00:00",
        "last_modified_date": "1994-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408006",
        "title": "LHIP: Extended DCGs for Configurable Robust Parsing",
        "authors": [
            "Afzal Ballim",
            "Graham Russell"
        ],
        "abstract": "  We present LHIP, a system for incremental grammar development using an extended DCG formalism. The system uses a robust island-based parsing method controlled by user-defined performance thresholds.\n    ",
        "submission_date": "1994-08-03T00:00:00",
        "last_modified_date": "1994-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408007",
        "title": "Emergent Linguistic Rules from Inducing Decision Trees: Disambiguating Discourse Clue Words",
        "authors": [
            "Eric V. Siegel",
            "Kathleen R. McKeown"
        ],
        "abstract": "  We apply decision tree induction to the problem of discourse clue word sense disambiguation with a genetic algorithm. The automatic partitioning of the training set which is intrinsic to decision tree induction gives rise to linguistically viable rules.\n    ",
        "submission_date": "1994-08-13T00:00:00",
        "last_modified_date": "1994-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408008",
        "title": "Statistical versus symbolic parsing for captioned-information retrieval",
        "authors": [
            "Neil C. Rowe"
        ],
        "abstract": "  We discuss implementation issues of MARIE-1, a mostly symbolic parser fully implemented, and MARIE-2, a more statistical parser partially implemented. They address a corpus of 100,000 picture captions. We argue that the mixed approach of MARIE-2 should be better for this corpus because its algorithms (not data) are simpler.\n    ",
        "submission_date": "1994-08-15T00:00:00",
        "last_modified_date": "1994-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408009",
        "title": "Tagging accurately -- Don't guess if you know",
        "authors": [
            "Pasi Tapanainen",
            "Atro Voutilainen"
        ],
        "abstract": "  We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers. We use two mature taggers, ENGCG and Xerox Tagger, to independently tag the same text and combine the results to produce a fully disambiguated text. In a 27000 word test sample taken from a previously unseen corpus we achieve 98.5% accuracy. This paper presents the data in detail. We describe the problems we encountered in the course of combining the two taggers and discuss the problem of evaluating taggers.\n    ",
        "submission_date": "1994-08-16T00:00:00",
        "last_modified_date": "1994-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408010",
        "title": "On Using Selectional Restriction in Language Models for Speech Recognition",
        "authors": [
            "Joerg P. Ueberla"
        ],
        "abstract": "  In this paper, we investigate the use of selectional restriction -- the constraints a predicate imposes on its arguments -- in a language model for speech recognition. We use an un-tagged corpus, followed by a public domain tagger and a very simple finite state machine to obtain verb-object pairs from unrestricted English text. We then measure the impact the knowledge of the verb has on the prediction of the direct object in terms of the perplexity of a cluster-based language model. The results show that even though a clustered bigram is more useful than a verb-object model, the combination of the two leads to an improvement over the clustered bigram model.\n    ",
        "submission_date": "1994-08-19T00:00:00",
        "last_modified_date": "1994-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408011",
        "title": "Distributional Clustering of English Words",
        "authors": [
            "Fernando Pereira",
            "Naftali Tishby",
            "Lillian Lee"
        ],
        "abstract": "  We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ``soft'' clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.\n    ",
        "submission_date": "1994-08-22T00:00:00",
        "last_modified_date": "1994-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408012",
        "title": "Approximate N-Gram Markov Model for Natural Language Generation",
        "authors": [
            "Hsin-Hsi Chen",
            "Yue-Shi Lee"
        ],
        "abstract": "  This paper proposes an Approximate n-gram Markov Model for bag generation. Directed word association pairs with distances are used to approximate (n-1)-gram and n-gram training tables. This model has parameters of word association model, and merits of both word association model and Markov Model. The training knowledge for bag generation can be also applied to lexical selection in machine translation design.\n    ",
        "submission_date": "1994-08-24T00:00:00",
        "last_modified_date": "1994-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408013",
        "title": "Training and Scaling Preference Functions for Disambiguation",
        "authors": [
            "Hiyan Alshawi",
            "David Carter"
        ],
        "abstract": "  We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least-squares minimization problem, and improvements are then made by hill-climbing. The method is applied to disambiguating sentences in the ATIS (Air Travel Information System) corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations.\n    ",
        "submission_date": "1994-08-24T00:00:00",
        "last_modified_date": "1994-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408014",
        "title": "Qualitative and Quantitative Models of Speech Translation",
        "authors": [
            "Hiyan Alshawi"
        ],
        "abstract": "  This paper compares a qualitative reasoning model of translation with a quantitative statistical model. We consider these models within the context of two hypothetical speech translation systems, starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second, quantitative design. The quantitative language and translation models are based on relations between lexical heads of phrases. Statistical parameters for structural dependency, lexical transfer, and linear order are used to select a set of implicit relations between words in a source utterance, a corresponding set of relations between target language words, and the most likely translation of the original utterance.\n    ",
        "submission_date": "1994-08-24T00:00:00",
        "last_modified_date": "1994-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408015",
        "title": "Experimentally Evaluating Communicative Strategies: The Effect of the Task",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "  Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents' resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents' resources and communicative strategies.\n    ",
        "submission_date": "1994-08-24T00:00:00",
        "last_modified_date": "1994-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408016",
        "title": "On Implementing an HPSG theory -- Aspects of the logical architecture, the formalization, and the implementation of head-driven phrase structure grammars",
        "authors": [
            "Walt Detmar Meurers"
        ],
        "abstract": "  The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408017",
        "title": "Reaping the Benefits of Interactive Syntax and Semantics",
        "authors": [
            "Kavi Mahesh"
        ],
        "abstract": "  Semantic feedback is an important source of information that a parser could use to deal with local ambiguities in syntax. However, it is difficult to devise a systematic communication mechanism for interactive syntax and semantics. In this article, I propose a variant of left-corner parsing to define the points at which syntax and semantics should interact, an account of grammatical relations and thematic roles to define the content of the communication, and a conflict resolution strategy based on independent preferences from syntax and semantics. The resulting interactive model has been implemented in a program called COMPERE and shown to account for a wide variety of psycholinguistic data on structural and lexical ambiguities.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408018",
        "title": "Uniform Representations for Syntax-Semantics Arbitration",
        "authors": [
            "Kavi Mahesh",
            "Kurt P. Eiselt"
        ],
        "abstract": "  Psychological investigations have led to considerable insight into the working of the human language comprehension system. In this article, we look at a set of principles derived from psychological findings to argue for a particular organization of linguistic knowledge along with a particular processing strategy and present a computational model of sentence processing based on those principles. Many studies have shown that human sentence comprehension is an incremental and interactive process in which semantic and other higher-level information interacts with syntactic information to make informed commitments as early as possible at a local ambiguity. Early commitments may be made by using top-down guidance from knowledge of different types, each of which must be applicable independently of others. Further evidence from studies of error recovery and delayed decisions points toward an arbitration mechanism for combining syntactic and semantic information in resolving ambiguities. In order to account for all of the above, we propose that all types of linguistic knowledge must be represented in a common form but must be separable so that they can be applied independently of each other and integrated at processing time by the arbitrator. We present such a uniform representation and a computational model called COMPERE based on the representation and the processing strategy.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408019",
        "title": "Building a Parser That can Afford to Interact with Semantics",
        "authors": [
            "Kavi Mahesh"
        ],
        "abstract": "  Natural language understanding programs get bogged down by the multiplicity of possible syntactic structures while processing real world texts that human understanders do not have much difficulty with. In this work, I analyze the relationships between parsing strategies, the degree of local ambiguity encountered by them, and semantic feedback to syntax, and propose a parsing algorithm called {\\em Head-Signaled Left Corner Parsing} (HSLC) that minimizes local ambiguities while supporting interactive syntactic and semantic analysis. Such a parser has been implemented in a sentence understanding program called COMPERE.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408020",
        "title": "Having Your Cake and Eating It Too: Autonomy and Interaction in a Model of Sentence Processing",
        "authors": [
            "Kurt P. Eiselt",
            "Kavi Mahesh",
            "Jennifer K. Holbrook"
        ],
        "abstract": "  Is the human language understander a collection of modular processes operating with relative autonomy, or is it a single integrated process? This ongoing debate has polarized the language processing community, with two fundamentally different types of model posited, and with each camp concluding that the other is wrong. One camp puts forth a model with separate processors and distinct knowledge sources to explain one body of data, and the other proposes a model with a single processor and a homogeneous, monolithic knowledge source to explain the other body of data. In this paper we argue that a hybrid approach which combines a unified processor with separate knowledge sources provides an explanation of both bodies of data, and we demonstrate the feasibility of this approach with the computational model called COMPERE. We believe that this approach brings the language processing community significantly closer to offering human-like language processing systems.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9408021",
        "title": "A Unified Process Model of Syntactic and Semantic Error Recovery in Sentence Understanding",
        "authors": [
            "Jennifer K. Holbrook",
            "Kurt P. Eiselt",
            "Kavi Mahesh"
        ],
        "abstract": "  The development of models of human sentence processing has traditionally followed one of two paths. Either the model posited a sequence of processing modules, each with its own task-specific knowledge (e.g., syntax and semantics), or it posited a single processor utilizing different types of knowledge inextricably integrated into a monolithic knowledge base. Our previous work in modeling the sentence processor resulted in a model in which different processing modules used separate knowledge sources but operated in parallel to arrive at the interpretation of a sentence. One highlight of this model is that it offered an explanation of how the sentence processor might recover from an error in choosing the meaning of an ambiguous word. Recent experimental work by Laurie Stowe strongly suggests that the human sentence processor deals with syntactic error recovery using a mechanism very much like that proposed by our model of semantic error recovery. Another way to interpret Stowe's finding is this: the human sentence processor consists of a single unified processing module utilizing multiple independent knowledge sources in parallel. A sentence processor built upon this architecture should at times exhibit behavior associated with modular approaches, and at other times act like an integrated system. In this paper we explore some of these ideas via a prototype computational model of sentence processing called COMPERE, and propose a set of psychological experiments for testing our theories.\n    ",
        "submission_date": "1994-08-31T00:00:00",
        "last_modified_date": "1994-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409001",
        "title": "Integrating Knowledge Bases and Statistics in MT",
        "authors": [
            "Kevin Knight",
            "Ishwar Chander",
            "Matthew Haines",
            "Vasileios Hatzivassiloglou",
            "Eduard Hovy",
            "Masayo Iida",
            "Steve K. Luk",
            "Akitoshi Okumura",
            "Richard Whitney",
            "Kenji Yamada"
        ],
        "abstract": "  We summarize recent machine translation (MT) research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps.\n    ",
        "submission_date": "1994-09-05T00:00:00",
        "last_modified_date": "1994-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409002",
        "title": "Conceptual Association for Compound Noun Analysis",
        "authors": [
            "Mark Lauer"
        ],
        "abstract": "  This paper describes research toward the automatic interpretation of compound nouns using corpus statistics. An initial study aimed at syntactic disambiguation is presented. The approach presented bases associations upon thesaurus categories. Association data is gathered from unambiguous cases extracted from a corpus and is then applied to the analysis of ambiguous compound nouns. While the work presented is still in progress, a first attempt to syntactically analyse a test set of 244 examples shows 75% correctness. Future work is aimed at improving this accuracy and extending the technique to assign semantic role information, thus producing a complete interpretation.\n    ",
        "submission_date": "1994-09-06T00:00:00",
        "last_modified_date": "1996-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409003",
        "title": "A Probabilistic Model of Compound Nouns",
        "authors": [
            "Mark Lauer",
            "Mark Dras"
        ],
        "abstract": "  Compound nouns such as example noun compound are becoming more common in natural language and pose a number of difficult problems for NLP systems, notably increasing the complexity of parsing. In this paper we develop a probabilistic model for syntactically analysing such compounds. The model predicts compound noun structures based on knowledge of affinities between nouns, which can be acquired from a corpus. Problems inherent in this corpus-based approach are addressed: data sparseness is overcome by the use of semantically motivated word classes and sense ambiguity is explicitly handled in the model. An implementation based on this model is described in Lauer (1994) and correctly parses 77% of the test set.\n    ",
        "submission_date": "1994-09-06T00:00:00",
        "last_modified_date": "1994-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409004",
        "title": "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "authors": [
            "Francesc Ribas"
        ],
        "abstract": "  We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora. The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items. Some experimental results about the performance of the method are provided.\n    ",
        "submission_date": "1994-09-07T00:00:00",
        "last_modified_date": "1994-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409005",
        "title": "Focusing for Pronoun Resolution in English Discourse: An Implementation",
        "authors": [
            "Ebru Ersan",
            "Varol Akman"
        ],
        "abstract": "  Anaphora resolution is one of the most active research areas in natural language processing. This study examines focusing as a tool for the resolution of pronouns which are a kind of anaphora. Focusing is a discourse phenomenon like anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and devised several algorithms to resolve definite anaphora including pronouns. She presented her theory in a computational framework but did not generally implement the algorithms. Her algorithms related to focusing and pronoun resolution are implemented in this thesis. This implementation provides a better comprehension of the theory both from a conceptual and a computational point of view. The resulting program is tested on different discourse segments, and evaluation and analysis of the experiments are presented together with the statistical results.\n    ",
        "submission_date": "1994-09-07T00:00:00",
        "last_modified_date": "1994-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409006",
        "title": "Situated Modeling of Epistemic Puzzles",
        "authors": [
            "Murat Ersan",
            "Varol Akman"
        ],
        "abstract": "  Situation theory is a mathematical theory of meaning introduced by Jon Barwise and John Perry. It has evoked great theoretical and practical interest and motivated the framework of a few `computational' systems. PROSIT is the pioneering work in this direction. Unfortunately, there is a lack of real-life applications on these systems and this study is a preliminary attempt to remedy this deficiency. Here, we examine how much PROSIT reflects situation-theoretic concepts and solve a group of epistemic puzzles using the constructs provided by this programming language.\n    ",
        "submission_date": "1994-09-07T00:00:00",
        "last_modified_date": "1994-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409007",
        "title": "Treating `Free Word Order' in Machine Translation",
        "authors": [
            "Ralf Steinberger"
        ],
        "abstract": "  In `free word order' languages, every sentence is embedded in its specific context. Among others, the order of constituents is determined by the categories `theme', `rheme' and `contrastive focus'. This paper shows how to recognise and to translate these categories automatically on a sentential basis, so that sentence embedding can be achieved without having to refer to the context. Modifier classes, which are traditionally neglected in linguistic description, are fully covered by the proposed method. (Coling 94, Kyoto, Vol. I, pages 69-75)\n    ",
        "submission_date": "1994-09-08T00:00:00",
        "last_modified_date": "1994-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409008",
        "title": "Parsing of Spoken Language under Time Constraints",
        "authors": [
            "Wolfgang Menzel"
        ],
        "abstract": "  Spoken language applications in natural dialogue settings place serious requirements on the choice of processing architecture. Especially under adverse phonetic and acoustic conditions parsing procedures have to be developed which do not only analyse the incoming speech in a time-synchroneous and incremental manner, but which are able to schedule their resources according to the varying conditions of the recognition process. Depending on the actual degree of local ambiguity the parser has to select among the available constraints in order to narrow down the search space with as little effort as possible.\n",
        "submission_date": "1994-09-09T00:00:00",
        "last_modified_date": "1994-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409009",
        "title": "Linguistics Computation, Automatic Model Generation, and Intensions",
        "authors": [
            "Cyrus F. Nourani"
        ],
        "abstract": "  Techniques are presented for defining models of computational linguistics theories. The methods of generalized diagrams that were developed by this author for modeling artificial intelligence planning and reasoning are shown to be applicable to models of computation of linguistics theories. It is shown that for extensional and intensional interpretations, models can be generated automatically which assign meaning to computations of linguistics theories for natural languages.\n",
        "submission_date": "1994-09-09T00:00:00",
        "last_modified_date": "1994-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409010",
        "title": "Inducing Probabilistic Grammars by Bayesian Model Merging",
        "authors": [
            "Andreas Stolcke",
            "Stephen M. Omohundro"
        ],
        "abstract": "  We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are {\\em incorporated} by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are {\\em merged} to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based $n$-grams, and stochastic context-free grammars.\n    ",
        "submission_date": "1994-09-13T00:00:00",
        "last_modified_date": "1994-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409011",
        "title": "Aligning Noisy Parallel Corpora Across Language Groups : Word Pair Feature Matching by Dynamic Time Warping",
        "authors": [
            "Pascale Fung",
            "Kathleen McKeown"
        ],
        "abstract": "  We propose a new algorithm called DK-vec for aligning pairs of Asian/Indo-European noisy parallel texts without sentence boundaries. DK-vec improves on previous alignment algorithms in that it handles better the non-linear nature of noisy corpora. The algorithm uses frequency, position and recency information as features for pattern matching. Dynamic Time Warping is used as the matching technique between word pairs. This algorithm produces a small bilingual lexicon which provides anchor points for alignment.\n    ",
        "submission_date": "1994-09-22T00:00:00",
        "last_modified_date": "1994-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9409012",
        "title": "Towards an Automatic Dictation System for Translators: the TransTalk Project",
        "authors": [
            "Marc Dymetman",
            "Julie Brousseau",
            "George Foster",
            "Pierre Isabelle",
            "Yves Normandin",
            "Pierre Plamondon"
        ],
        "abstract": "  Professional translators often dictate their translations orally and have them typed afterwards. The TransTalk project aims at automating the second part of this process. Its originality as a dictation system lies in the fact that both the acoustic signal produced by the translator and the source text under translation are made available to the system. Probable translations of the source text can be predicted and these predictions used to help the speech recognition system in its lexical choices. We present the results of the first prototype, which show a marked improvement in the performance of the speech recognition task when translation predictions are taken into account.\n    ",
        "submission_date": "1994-09-28T00:00:00",
        "last_modified_date": "1994-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410001",
        "title": "Improving Language Models by Clustering Training Sentences",
        "authors": [
            "David Carter"
        ],
        "abstract": "  Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences. I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster. This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model. It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model. As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain. These results are consistent with other findings for such models, suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model.\n    ",
        "submission_date": "1994-10-04T00:00:00",
        "last_modified_date": "1994-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410002",
        "title": "Lexikoneintraege fuer deutsche Adverbien (Dictionary Entries for German Adverbs)",
        "authors": [
            "Ralf Steinberger"
        ],
        "abstract": "  Modifiers in general, and adverbs in particular, are neglected categories in linguistics, and consequently, their treatment in Natural Language Processing poses problems. In this article, we present the dictionary information for German adverbs which is necessary to deal with word order, degree modifier scope and other problems in NLP. We also give evidence for the claim that a classification according to position classes differs from any semantic classification.\n    ",
        "submission_date": "1994-10-04T00:00:00",
        "last_modified_date": "1994-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410003",
        "title": "Principle Based Semantics for HPSG",
        "authors": [
            "A. Frank",
            "U. Reyle"
        ],
        "abstract": "  The paper presents a constraint based semantic formalism for HPSG. The advantages of the formlism are shown with respect to a grammar for a fragment of German that deals with (i) quantifier scope ambiguities triggered by scrambling and/or movement and (ii) ambiguities that arise from the collective/distributive distinction of plural NPs. The syntax-semantics interface directly implements syntactic conditions on quantifier scoping and distributivity. The construction of semantic representations is guided by general principles governing the interaction between syntax and semantics. Each of these principles acts as a constraint to narrow down the set of possible interpretations of a sentence. Meanings of ambiguous sentences are represented by single partial representations (so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which further constraints can be added monotonically to gain more information about the content of a sentence. There is no need to build up a large number of alternative representations of the sentence which are then filtered by subsequent discourse and world knowledge. The advantage of UDRSs is not only that they allow for monotonic incremental interpretation but also that they are equipped with truth conditions and a proof theory that allows for inferences to be drawn directly on structures where quantifier scope is not resolved.\n    ",
        "submission_date": "1994-10-05T00:00:00",
        "last_modified_date": "1994-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410004",
        "title": "Spelling Correction in Agglutinative Languages",
        "authors": [
            "Kemal Oflazer"
        ],
        "abstract": "  This paper presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic programming based search algorithm. Spelling correction in agglutinative languages is significantly different than in languages like English. The concept of a word in such languages is much wider that the entries found in a dictionary, owing to {}~productive word formation by derivational and inflectional affixations. After an overview of certain issues and relevant mathematical preliminaries, we formally present the problem and our solution. We then present results from our experiments with spelling correction in Turkish, a Ural--Altaic agglutinative language. Our results indicate that we can find the intended correct word in 95\\% of the cases and offer it as the first candidate in 74\\% of the cases, when the edit distance is 1.\n    ",
        "submission_date": "1994-10-06T00:00:00",
        "last_modified_date": "1994-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410005",
        "title": "A Centering Approach to Pronouns",
        "authors": [
            "Susan E. Brennan",
            "Marilyn Walker Friedman",
            "Carl J. Pollard"
        ],
        "abstract": "  In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in Grosz, Joshi and Weinstein (1986), the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting. We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.\n    ",
        "submission_date": "1994-10-10T00:00:00",
        "last_modified_date": "1994-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410006",
        "title": "Evaluating Discourse Processing Algorithms",
        "authors": [
            "Marilyn A. Walker"
        ],
        "abstract": "  In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.\n    ",
        "submission_date": "1994-10-10T00:00:00",
        "last_modified_date": "1994-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410007",
        "title": "A Formal Look at Dependency Grammars and Phrase-Structure Grammars, with Special Consideration of Word-Order Phenomena",
        "authors": [
            "Owen Rambow",
            "Aravind Joshi"
        ],
        "abstract": "  The central role of the lexicon in Meaning-Text Theory (MTT) and other dependency-based linguistic theories cannot be replicated in linguistic theories based on context-free grammars (CFGs). We describe Tree Adjoining Grammar (TAG) as a system that arises naturally in the process of lexicalizing CFGs. A TAG grammar can therefore be compared directly to an Meaning-Text Model (MTM). We illustrate this point by discussing the computational complexity of certain non-projective constructions, and suggest a way of incorporating locality of word-order definitions into the Surface-Syntactic Component of MTT.\n    ",
        "submission_date": "1994-10-18T00:00:00",
        "last_modified_date": "1994-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410008",
        "title": "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "authors": [
            "Jussi Karlgren",
            "Douglass Cutting"
        ],
        "abstract": "  A simple method for categorizing texts into predetermined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus. Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream, and combine them into a small number of functions, with the parameters weighted on basis of how useful they are for discriminating text genres. An application to information retrieval is discussed.\n    ",
        "submission_date": "1994-10-20T00:00:00",
        "last_modified_date": "1994-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410009",
        "title": "Lexical Functions and Machine Translation",
        "authors": [
            "Dirk Heylen",
            "Kerry G. Maxwell",
            "Marc Verhagen"
        ],
        "abstract": "  This paper discusses the lexicographical concept of lexical functions and their potential exploitation in the development of a machine translation lexicon designed to handle collocations.\n    ",
        "submission_date": "1994-10-20T00:00:00",
        "last_modified_date": "1994-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410010",
        "title": "XTAG system - A Wide Coverage Grammar for English",
        "authors": [
            "Christy Doran",
            "Dania Egedi",
            "Beth Ann Hockey",
            "B. Srinivas",
            "Martin Zaidel"
        ],
        "abstract": "  This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of XTAG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given.\n    ",
        "submission_date": "1994-10-20T00:00:00",
        "last_modified_date": "1994-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410011",
        "title": "Dilemma - An Instant Lexicographer",
        "authors": [
            "Hans Karlgren",
            "Jussi Karlgren",
            "Magnus Nordstr\u00f6m",
            "Paul Pettersson",
            "Bengt Wahrol\u00e9n"
        ],
        "abstract": "  Dilemma is intended to enhance quality and increase productivity of expert human translators by presenting to the writer relevant lexical information mechanically extracted from comparable existing translations, thus replacing - or compensating for the absence of - a lexicographer and stand-by terminologist rather than the translator. Using statistics and crude surface analysis and a minimum of prior information, Dilemma identifies instances and suggests their counterparts in parallel source and target texts, on all levels down to individual words. Dilemma forms part of a tool kit for translation where focus is on text structure and over-all consistency in large text volumes rather than on framing sentences, on interaction between many actors in a large project rather than on retrieval of machine-stored data and on decision making rather than on application of given rules. In particular, the system has been tuned to the needs of the ongoing translation of European Community legislation into the languages of candidate member countries. The system has been demonstrated to and used by professional translators with promising results.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410012",
        "title": "Does Baum-Welch Re-estimation Help Taggers?",
        "authors": [
            "David Elworthy"
        ],
        "abstract": "  In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. More recently, Cutting {\\it et al.} (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of {\\em a priori} information about probabilities, by using Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410013",
        "title": "Automatic Error Detection in Part of Speech Tagging",
        "authors": [
            "David Elworthy"
        ],
        "abstract": "  A technique for detecting errors made by Hidden Markov Model taggers is described, based on comparing observable values of the tagging process with a threshold. The resulting approach allows the accuracy of the tagger to be improved by accepting a lower efficiency, defined as the proportion of words which are tagged. Empirical observations are presented which demonstrate the validity of the technique and suggest how to choose an appropriate threshold.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410014",
        "title": "A Freely Available Syntactic Lexicon for English",
        "authors": [
            "Dania Egedi",
            "Patrick Martin"
        ],
        "abstract": "  This paper presents a syntactic lexicon for English that was originally derived from the Oxford Advanced Learner's Dictionary and the Oxford Dictionary of Current Idiomatic English, and then modified and augmented by hand. There are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows based tool is available for maintaining the lexicon and performing searches. C and Lisp hooks are also available so that the lexicon can be easily utilized by parsers and other programs.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410015",
        "title": "Lexicalization and Grammar Development",
        "authors": [
            "B. Srinivas",
            "Dania Egedi",
            "Christy Doran",
            "Tilman Becker"
        ],
        "abstract": "  In this paper we present a fully lexicalized grammar formalism as a particularly attractive framework for the specification of natural language grammars. We discuss in detail Feature-based, Lexicalized Tree Adjoining Grammars (FB-LTAGs), a representative of the class of lexicalized grammars. We illustrate the advantages of lexicalized grammars in various contexts of natural language processing, ranging from wide-coverage grammar development to parsing and machine translation. We also present a method for compact and efficient representation of lexicalized trees.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410016",
        "title": "Dutch Cross Serial Dependencies in HPSG",
        "authors": [
            "Gerrit Rentier"
        ],
        "abstract": "  We present an analysis of Dutch cross serial dependencies in Head-driven Phrase Structure Grammar. Arguably, our analysis differs from other analyses in that we do not refer to `additional' mechanisms (e.g., sequence union, head wrapping): just standard structure sharing, an immediate dominance schema and a linear precedence rule.\n    ",
        "submission_date": "1994-10-21T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410017",
        "title": "Concurrent Lexicalized Dependency Parsing: The ParseTalk Model",
        "authors": [
            "Norbert Broeker",
            "Udo Hahn",
            "Susanne Schacht"
        ],
        "abstract": "  A grammar model for concurrent, object-oriented natural language parsing is introduced. Complete lexical distribution of grammatical knowledge is achieved building upon the head-oriented notions of valency and dependency, while inheritance mechanisms are used to capture lexical generalizations. The underlying concurrent computation model relies upon the actor paradigm. We consider message passing protocols for establishing dependency relations and ambiguity handling.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410018",
        "title": "Part-of-Speech Tagging with Neural Networks",
        "authors": [
            "Helmut Schmid"
        ],
        "abstract": "  Text corpora which are tagged with part-of-speech information are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method based on neural networks (Net- Tagger) is presented and its performance is compared to that of a HMM-tagger and a trigram-based tagger. It is shown that the Net- Tagger performs as well as the trigram-based tagger and better than the HMM-tagger.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410019",
        "title": "Concurrent Lexicalized Dependency Parsing: A Behavioral View on ParseTalk Events",
        "authors": [
            "Susanne Schacht",
            "Udo Hahn",
            "Norbert Broeker"
        ],
        "abstract": "  The behavioral specification of an object-oriented grammar model is considered. The model is based on full lexicalization, head-orientation via valency constraints and dependency relations, inheritance as a means for non-redundant lexicon specification, and concurrency of computation. The computation model relies upon the actor paradigm, with concurrency entering through asynchronous message passing between actors. In particular, we here elaborate on principles of how the global behavior of a lexically distributed grammar and its corresponding parser can be specified in terms of event type networks and event networks, resp.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410020",
        "title": "Construction of a Bilingual Dictionary Intermediated by a Third Language",
        "authors": [
            "Kumiko TANAKA",
            "Kyoji UMEMURA"
        ],
        "abstract": "  When using a third language to construct a bilingual dictionary, it is necessary to discriminate equivalencies from inappropriate words derived as a result of ambiguity in the third language. We propose a method to treat this by utilizing the structures of dictionaries to measure the nearness of the meanings of words. The resulting dictionary is a word-to-word bilingual dictionary of nouns and can be used to refine the entries and equivalencies in published bilingual dictionaries.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410021",
        "title": "Reference Resolution Using Semantic Patterns in Japanese Newspaper Articles",
        "authors": [
            "Takahiro Wakao"
        ],
        "abstract": "  Reference resolution is one of the important tasks in natural language processing. In this paper, the author first determines the referents and their locations of \"dousha\", literally meaning \"the same company\", which appear in Japanese newspaper articles. Secondly, three heuristic methods, two of which use semantic information in text such as company names and their patterns, are proposed and tested on how accurately they identify the correct referents. The proposed methods based on semantic patterns show high accuracy for reference resolution of \"dousha\" (more than 90\\%). This suggests that semantic pattern-matching methods are effective for reference resolution in newspaper articles.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410022",
        "title": "Automated tone transcription",
        "authors": [
            "Steven Bird"
        ],
        "abstract": "  In this paper I report on an investigation into the problem of assigning tones to pitch contours. The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages. Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and others, I provide a parametrised F_0 prediction function P which generates F_0 values from a tone sequence, and I explore the asymptotic behaviour of downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0) values amounts to finding a tone sequence T such that P(T) {}~= X. This is a combinatorial optimisation problem, for which two non-deterministic search techniques are provided: a genetic algorithm and a simulated annealing algorithm. Finally, two implementations---one for each technique---are described and then compared using both artificial and real data for sequences of up to 20 tones. These programs can be adapted to other tone languages by adjusting the F_0 prediction function.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410023",
        "title": "Korean to English Translation Using Synchronous TAGs",
        "authors": [
            "Dania Egedi",
            "Martha Palmer",
            "Hyun S. Park",
            "Aravind K. Joshi"
        ],
        "abstract": "  It is often argued that accurate machine translation requires reference to contextual knowledge for the correct treatment of linguistic phenomena such as dropped arguments and accurate lexical selection. One of the historical arguments in favor of the interlingua approach has been that, since it revolves around a deep semantic representation, it is better able to handle the types of linguistic phenomena that are seen as requiring a knowledge-based approach. In this paper we present an alternative approach, exemplified by a prototype system for machine translation of English and Korean which is implemented in Synchronous TAGs. This approach is essentially transfer based, and uses semantic feature unification for accurate lexical selection of polysemous verbs. The same semantic features, when combined with a discourse model which stores previously mentioned entities, can also be used for the recovery of topicalized arguments. In this paper we concentrate on the translation of Korean to English.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410024",
        "title": "A Freely Available Wide Coverage Morphological Analyzer for English",
        "authors": [
            "Daniel Karp",
            "Yves Schabes",
            "Martin Zaidel",
            "Dania Egedi"
        ],
        "abstract": "  This paper presents a morphological lexicon for English that handles more than 317000 inflected forms derived from over 90000 stems. The lexicon is available in two formats. The first can be used by an implementation of a two-level processor for morphological analysis. The second, derived from the first one for efficiency reasons, consists of a disk-based database using a UNIX hash table facility. We also built an X Window tool to facilitate the maintenance and browsing of the lexicon. The package is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C.\n    ",
        "submission_date": "1994-10-24T00:00:00",
        "last_modified_date": "1994-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410025",
        "title": "Syntactic Analysis Of Natural Language Using Linguistic Rules And Corpus-based Patterns",
        "authors": [
            "Pasi Tapanainen",
            "Timo J\u00e4rvinen"
        ],
        "abstract": "  We are concerned with the syntactic annotation of unrestricted text. We combine a rule-based analysis with subsequent exploitation of empirical data. The rule-based surface syntactic analyser leaves some amount of ambiguity in the output that is resolved using empirical patterns. We have implemented a system for generating and applying corpus-based patterns. Some patterns describe the main constituents in the sentence and some the local context of the each syntactic function. There are several (partly) reduntant patterns, and the ``pattern'' parser selects analysis of the sentence that matches the strictest possible pattern(s). The system is applied to an experimental corpus. We present the results and discuss possible refinements of the method from a linguistic point of view.\n    ",
        "submission_date": "1994-10-25T00:00:00",
        "last_modified_date": "1994-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410026",
        "title": "A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation",
        "authors": [
            "Eric Brill",
            "Philip Resnik"
        ],
        "abstract": "  In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem.\n    ",
        "submission_date": "1994-10-25T00:00:00",
        "last_modified_date": "1994-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410027",
        "title": "Probabilistic Tagging with Feature Structures",
        "authors": [
            "Andre Kempe"
        ],
        "abstract": "  The described tagger is based on a hidden Markov model and uses tags composed of features such as part-of-speech, gender, etc. The contextual probability of a tag (state transition probability) is deduced from the contextual probabilities of its feature-value-pairs. This approach is advantageous when the available training corpus is small and the tag set large, which can be the case with morphologically rich languages.\n    ",
        "submission_date": "1994-10-25T00:00:00",
        "last_modified_date": "1994-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410028",
        "title": "Minimal Change and Bounded Incremental Parsing",
        "authors": [
            "Mats Wir\u00e9n"
        ],
        "abstract": "  Ideally, the time that an incremental algorithm uses to process a change should be a function of the size of the change rather than, say, the size of the entire current input. Based on a formalization of ``the set of things changed'' by an incremental modification, this paper investigates how and to what extent it is possible to give such a guarantee for a chart-based parsing framework and discusses the general utility of a minimality notion in incremental processing.\n    ",
        "submission_date": "1994-10-25T00:00:00",
        "last_modified_date": "1994-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410029",
        "title": "Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing",
        "authors": [
            "Aravind K. Joshi",
            "B. Srinivas"
        ],
        "abstract": "  In a lexicalized grammar formalism such as Lexicalized Tree-Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (supertag) that localizes syntactic and semantic dependencies. Thus a parser for a lexicalized grammar must search a large set of supertags to choose the right ones to combine for the parse of the sentence. We present techniques for disambiguating supertags using local information such as lexical preference and local lexical dependencies. The similarity between LTAG and Dependency grammars is exploited in the dependency model of supertag disambiguation. The performance results for various models of supertag disambiguation such as unigram, trigram and dependency-based models are presented.\n    ",
        "submission_date": "1994-10-26T00:00:00",
        "last_modified_date": "1994-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410030",
        "title": "Feature-Based TAG in place of multi-component adjunction: Computational Implications",
        "authors": [
            "B.A. Hockey",
            "B. Srinivas"
        ],
        "abstract": "  Using feature-based Tree Adjoining Grammar (TAG), this paper presents linguistically motivated analyses of constructions claimed to require multi-component adjunction. These feature-based TAG analyses permit parsing of these constructions using an existing unification-based Earley-style TAG parser, thus obviating the need for a multi-component TAG parser without sacrificing linguistic coverage for English.\n    ",
        "submission_date": "1994-10-26T00:00:00",
        "last_modified_date": "1994-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410031",
        "title": "Towards a More User-friendly Correction",
        "authors": [
            "Damien Genthial",
            "Jacques Courtin",
            "Jacques Menezo Equipe Trilan"
        ],
        "abstract": "  We first present our view of detection and correction of syntactic errors. We then introduce a new correction method, based on heuristic criteria used to decide which correction should be preferred. Weighting of these criteria leads to a flexible and parametrable system, which can adapt itself to the user. A partitioning of the trees based on linguistic criteria: agreement rules, rather than computational criteria is then necessary. We end by proposing extensions to lexical correction and to some syntactic errors. Our aim is an adaptable and user-friendly system capable of automatic correction for some applications.\n    ",
        "submission_date": "1994-10-27T00:00:00",
        "last_modified_date": "1994-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410032",
        "title": "Planning Argumentative Texts",
        "authors": [
            "Xiaorong Huang"
        ],
        "abstract": "  This paper presents \\proverb\\, a text planner for argumentative texts. \\proverb\u015b main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way. The former splits the task of presenting a particular proof into subtasks of presenting subproofs. The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus.\n    ",
        "submission_date": "1994-10-28T00:00:00",
        "last_modified_date": "1994-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410033",
        "title": "Default Handling in Incremental Generation",
        "authors": [
            "Karin Harbusch",
            "Gen-ichiro Kikui",
            "Anne Kilger"
        ],
        "abstract": "  Natural language generation must work with insufficient input. Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input. The paper aims to escape from such dead-end situations by making assumptions. We discuss global aspects of default handling. Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion.\n    ",
        "submission_date": "1994-10-30T00:00:00",
        "last_modified_date": "1994-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9410034",
        "title": "A Comparison of Two Smoothing Methods for Word Bigram Models",
        "authors": [
            "Linda Bauman Peto"
        ],
        "abstract": "  A COMPARISON OF TWO SMOOTHING METHODS FOR WORD BIGRAM MODELS\n",
        "submission_date": "1994-10-31T00:00:00",
        "last_modified_date": "1994-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411001",
        "title": "Sublanguage Terms: Dictionaries, Usage, and Automatic Classification",
        "authors": [
            "Robert M. Losee",
            "Stephanie W. Haas"
        ],
        "abstract": "  The use of terms from natural and social scientific titles and abstracts is studied from the perspective of sublanguages and their specialized dictionaries. Different notions of sublanguage distinctiveness are explored. Objective methods for separating hard and soft sciences are suggested based on measures of sublanguage use, dictionary characteristics, and sublanguage distinctiveness. Abstracts were automatically classified with a high degree of accuracy by using a formula that considers the degree of uniqueness of terms in each sublanguage. This may prove useful for text filtering or information retrieval systems.\n    ",
        "submission_date": "1994-11-01T00:00:00",
        "last_modified_date": "1994-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411002",
        "title": "CLARE: A Contextual Reasoning and Cooperative Response Framework for the Core Language Engine",
        "authors": [
            "Hiyan Alshawi",
            "David Carter",
            "Richard Crouch",
            "Steve Pulman",
            "Manny Rayner",
            "Arnold Smith"
        ],
        "abstract": "  This report describes the research, design and implementation work carried out in building the CLARE system at SRI International, Cambridge, England. CLARE was designed as a natural language processing system with facilities for reasoning and understanding in context and for generating cooperative responses. The project involved both further development of SRI's Core Language Engine (Alshawi, 1992, MIT Press) natural language processor and the design and implementation of new components for reasoning and response generation. The CLARE system has advanced the state of the art in a wide variety of areas, both through the use of novel techniques developed on the project, and by extending the coverage or scale of known techniques. The language components are application-independent and provide interfaces for the development of new types of application.\n    ",
        "submission_date": "1994-11-01T00:00:00",
        "last_modified_date": "1994-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411003",
        "title": "Adnominal adjectives, code-switching and lexicalized TAG",
        "authors": [
            "Shahrzad Mahootian",
            "Beatrice Santorini"
        ],
        "abstract": "  In codeswitching contexts, the language of a syntactic head determines the distribution of its complements. Mahootian 1993 derives this generalization by representing heads as the anchors of elementary trees in a lexicalized TAG. However, not all codeswitching sequences are amenable to a head-complement analysis. For instance, adnominal adjectives can occupy positions not available to them in their own language, and the TAG derivation of such sequences must use unanchored auxiliary trees. palabras heavy-duty `heavy-duty words' (Spanish-English; Poplack 1980:584) taste lousy sana `very lousy taste' (English-Swahili; Myers-Scotton 1993:29, (10)) Given the null hypothesis that codeswitching and monolingual sequences are derived in an identical manner, sequences like those above provide evidence that pure lexicalized TAGs are inadequate for the description of natural language.\n    ",
        "submission_date": "1994-11-02T00:00:00",
        "last_modified_date": "1994-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411004",
        "title": "Determining Determiner Sequencing: A Syntactic Analysis for English",
        "authors": [
            "Beth Ann Hockey",
            "Dania Egedi"
        ],
        "abstract": "  Previous work on English determiners has primarily concentrated on their semantics or scoping properties rather than their complex ordering behavior. The little work that has been done on determiner ordering generally splits determiners into three subcategories. However, this small number of categories does not capture the finer distinctions necessary to correctly order determiners. This paper presents a syntactic account of determiner sequencing based on eight independently identified semantic features. Complex determiners, such as genitives, partitives, and determiner modifying adverbials, are also presented. This work has been implemented as part of XTAG, a wide-coverage grammar for English based in the Feature-Based, Lexicalized Tree Adjoining Grammar (FB-LTAG) formalism.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411005",
        "title": "Constraining Lexical Selection Across Languages Using TAGs",
        "authors": [
            "Dania Egedi",
            "Martha Palmer"
        ],
        "abstract": "  Lexical selection in Machine Translation consists of several related components. Two that have received a lot of attention are lexical mapping from an underlying concept or lexical item, and choosing the correct subcategorization frame based on argument structure. Because most MT applications are small or relatively domain specific, a third component of lexical selection is generally overlooked - distinguishing between lexical items that are closely related conceptually. While some MT systems have proposed using a 'world knowledge' module to decide which word is more appropriate based on various pragmatic or stylistic constraints, we are interested in seeing how much we can accomplish using a combination of syntax and lexical semantics. By using separate ontologies for each language implemented in FB-LTAGs, we are able to elegantly model the more specific and language dependent syntactic and semantic distinctions necessary to further filter the choice of the lexical item.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411006",
        "title": "Status of the XTAG System",
        "authors": [
            "Christy Doran",
            "Dania Egedi",
            "Beth Ann Hockey",
            "B. Srinivas"
        ],
        "abstract": "  XTAG is an ongoing project to develop a wide-coverage grammar for English, based on the Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) formalism. The XTAG system integrates a morphological analyzer, an N-best part-of-speech tagger, an Early-style parser and an X-window interface, along with a wide-coverage grammar for English developed using the system. This system serves as a linguist's workbench for developing FB-LTAG specifications. This paper presents a description of and recent improvements to the various components of the XTAG system. It also presents the recent performance of the wide-coverage grammar on various corpora and compares it against the performance of other wide-coverage and domain-specific grammars.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411007",
        "title": "The Linguistic Relevance of Quasi-Trees",
        "authors": [
            "Anthony Kroch",
            "Owen Rambow"
        ],
        "abstract": "  We discuss two constructions (long scrambling and ECM verbs) which challenge most syntactic theories (including traditional TAG approaches) since they seem to require exceptional mechanisms and postulates. We argue that these constructions should in fact be analyzed in a similar manner, namely as involving a verb which selects for a ``defective'' complement. These complements are defective in that they lack certain Case-assigning abilities (represented as functional heads). The constructions differ in how many such abilities are lacking. Following the previous analysis of scrambling of Rambow (1994), we propose a TAG analysis based on quasi-trees.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411008",
        "title": "Parsing Free Word-Order Languages in Polynomial Time",
        "authors": [
            "Tilman Becker",
            "Owen Rambow"
        ],
        "abstract": "  We present a parsing algorithm with polynomial time complexity for a large subset of V-TAG languages. V-TAG, a variant of multi-component TAG, can handle free-word order phenomena which are beyond the class LCFRS (which includes regular TAG). Our algorithm is based on a CYK-style parser for TAGs.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411009",
        "title": "Bootstrapping A Wide-Coverage CCG from FB-LTAG",
        "authors": [
            "Christine Doran",
            "B. Srinivas"
        ],
        "abstract": "  A number of researchers have noted the similarities between LTAGs and CCGs. Observing this resemblance, we felt that we could make use of the wide-coverage grammar developed in the XTAG project to build a wide-coverage CCG. To our knowledge there have been no attempts to construct a large-scale CCG parser with the lexicon to support it. In this paper, we describe such a system, built by adapting various XTAG components to CCG. We find that, despite the similarities between the formalisms, certain parts of the grammatical workload are distributed differently. In addition, the flexibility of CCG derivations allows the translated grammar to handle a number of ``non-constituent'' constructions which the XTAG grammar cannot.\n    ",
        "submission_date": "1994-11-03T00:00:00",
        "last_modified_date": "1994-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411010",
        "title": "The \"Whiteboard\" Architecture: a way to integrate heterogeneous components of NLP systems",
        "authors": [
            "Christian Boitet",
            "Mark Seligman"
        ],
        "abstract": "  We present a new software architecture for NLP systems made of heterogeneous components, and demonstrate an architectural prototype we have built at ATR in the context of Speech Translation.\n    ",
        "submission_date": "1994-11-04T00:00:00",
        "last_modified_date": "1994-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411011",
        "title": "Acquiring Knowledge from Encyclopedic Texts",
        "authors": [
            "Fernando Gomez",
            "Richard Hull",
            "Carlos Segami"
        ],
        "abstract": "  A computational model for the acquisition of knowledge from encyclopedic texts is described. The model has been implemented in a program, called SNOWY, that reads unedited texts from {\\em The World Book Encyclopedia}, and acquires new concepts and conceptual relations about topics dealing with the dietary habits of animals, their classifications and habitats. The program is also able to answer an ample set of questions about the knowledge that it has acquired. This paper describes the essential components of this model, namely semantic interpretation, inferences and representation, and ends with an evaluation of the performance of the program, a sample of the questions that it is able to answer, and its relation to other programs of similar nature.\n    ",
        "submission_date": "1994-11-04T00:00:00",
        "last_modified_date": "1994-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411012",
        "title": "From Regular to Context Free to Mildly Context Sensitive Tree Rewriting Systems: The Path of Child Language Acquisition",
        "authors": [
            "Robert Frank"
        ],
        "abstract": "  Current syntactic theory limits the range of grammatical variation so severely that the logical problem of grammar learning is trivial. Yet, children exhibit characteristic stages in syntactic development at least through their sixth year. Rather than positing maturational delays, I suggest that acquisition difficulties are the result of limitations in manipulating grammatical representations. I argue that the genesis of complex sentences reflects increasing generative capacity in the systems generating structural descriptions: conjoined clauses demand only a regular tree rewriting system; sentential embedding uses a context-free tree substitution grammar; modification requires TAG, a mildly context-sensitive system.\n    ",
        "submission_date": "1994-11-04T00:00:00",
        "last_modified_date": "1994-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411013",
        "title": "Phoneme-level speech and natural language intergration for agglutinative languages",
        "authors": [
            "Geunbae Lee Jong-Hyeok Lee Kyunghee Kim"
        ],
        "abstract": "  A new tightly coupled speech and natural language integration model is presented for a TDNN-based large vocabulary continuous speech recognition system. Unlike the popular n-best techniques developed for integrating mainly HMM-based speech and natural language systems in word level, which is obviously inadequate for the morphologically complex agglutinative languages, our model constructs a spoken language system based on the phoneme-level integration. The TDNN-CYK spoken language architecture is designed and implemented using the TDNN-based diphone recognition module integrated with the table-driven phonological/morphological co-analysis. Our integration model provides a seamless integration of speech and natural language for connectionist speech recognition systems especially for morphologically complex languages such as Korean. Our experiment results show that the speaker-dependent continuous Eojeol (word) recognition can be integrated with the morphological analysis with over 80\\% morphological analysis success rate directly from the speech input for the middle-level vocabularies.\n    ",
        "submission_date": "1994-11-05T00:00:00",
        "last_modified_date": "1994-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411014",
        "title": "Automatically Identifying Morphological Relations in = Machine-Readable Dictionaries",
        "authors": [
            "Joseph Pentheroudakis",
            "Lucy Vanderwende",
            "Microsoft Corporation"
        ],
        "abstract": "  We describe an automated method for identifying classes of morphologically related words in an on-line dictionary, and for linking individual senses in the derived form to one or more senses in the base form by means of morphological relation attributes. We also present an algorithm for computing a score reflecting the system=92s certainty in these derivational links; this computation relies on the content of semantic relations associated with each sense, which are extracted automatically by parsing each sense definition and subjecting the parse structure to automated semantic analysis. By processing the entire set of headwords in the dictionary in this fashion we create a large set of directed derivational graphs, which can then be accessed by other components in our broad-coverage NLP system. Spurious or unlikely derivations are not discarded, but are rather added to the dictionary and assigned a negative score; this allows the system to handle non-standard uses of these forms.\n    ",
        "submission_date": "1994-11-08T00:00:00",
        "last_modified_date": "1994-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411015",
        "title": "Parsing Using Linearly Ordered Phonological Rules",
        "authors": [
            "Michael Maxwell"
        ],
        "abstract": "  A generate and test algorithm is described which parses a surface form into one or more lexical entries using linearly ordered phonological rules. This algorithm avoids the exponential expansion of search space which a naive parsing algorithm would face by encoding into the form being parsed the ambiguities which arise during parsing. The algorithm has been implemented and tested on real language data, and its speed compares favorably with that of a KIMMO-type parser.\n    ",
        "submission_date": "1994-11-08T00:00:00",
        "last_modified_date": "1994-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411016",
        "title": "Extending DRT with a Focusing Mechanism for Pronominal Anaphora and Ellipsis Resolution",
        "authors": [
            "Jose Abracos",
            "Jose Gabriel Lopes"
        ],
        "abstract": "  Cormack (1992) proposed a framework for pronominal anaphora resolution. Her proposal integrates focusing theory (Sidner et al.) and DRT (Kamp and Reyle). We analyzed this methodology and adjusted it to the processing of Portuguese texts. The scope of the framework was widened to cover sentences containing restrictive relative clauses and subject ellipsis. Tests were conceived and applied to probe the adequacy of proposed modifications when dealing with processing of current texts.\n    ",
        "submission_date": "1994-11-09T00:00:00",
        "last_modified_date": "1994-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411017",
        "title": "Comlex Syntax: Building a Computational Lexicon",
        "authors": [
            "Ralph Grishman",
            "Catherine Macleod",
            "Adam Meyers"
        ],
        "abstract": "  We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.\n    ",
        "submission_date": "1994-11-10T00:00:00",
        "last_modified_date": "1994-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411018",
        "title": "Interlanguage Signs and Lexical Transfer Errors",
        "authors": [
            "Atle Ro"
        ],
        "abstract": "  A theory of interlanguage (IL) lexicons is outlined, with emphasis on IL lexical entries, based on the HPSG notion of lexical sign. This theory accounts for idiosyncratic or lexical transfer of syntactic subcategorisation and idioms from the first language to the IL. It also accounts for developmental stages in IL lexical grammar, and grammatical variation in the use of the same lexical item. The theory offers a tool for robust parsing of lexical transfer errors and diagnosis of such errors.\n    ",
        "submission_date": "1994-11-11T00:00:00",
        "last_modified_date": "1994-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411019",
        "title": "Focus on ``only\" and ``Not\"",
        "authors": [
            "Allan Ramsay"
        ],
        "abstract": "  Krifka [1993] has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on, rather than as a specific semantic or pragmatic function itself. The current paper describes an implementation of this general idea, and applies it to the interpretation of {\\em only} and {\\em not}.\n    ",
        "submission_date": "1994-11-11T00:00:00",
        "last_modified_date": "1994-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411020",
        "title": "Extraction in Dutch with Lexical Rules",
        "authors": [
            "Gerrit Rentier"
        ],
        "abstract": "  Unbounded dependencies are often modelled by ``traces'' (and ``gap threading'') in unification-based grammars. Pollard and Sag, however, suggest an analysis of extraction based on lexical rules, which excludes the notion of traces (P&S 1994, Chapter 9). In parsing, it suggests a trade of indeterminism for lexical ambiguity. This paper provides a short introduction to this approach to extraction with lexical rules, and illustrates the linguistic power of the approach by applying it to particularly idiosyncratic Dutch extraction data.\n    ",
        "submission_date": "1994-11-14T00:00:00",
        "last_modified_date": "1994-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411021",
        "title": "Free-ordered CUG on Chemical Abstract Machine",
        "authors": [
            "Satoshi Tojo"
        ],
        "abstract": "  We propose a paradigm for concurrent natural language generation. In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional type. We augment typed lambda calculus with several new combinators, to make the order of lambda-conversions free for partial / local processing. The concurrent calculus is modeled with Chemical Abstract Machine. We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination.\n    ",
        "submission_date": "1994-11-16T00:00:00",
        "last_modified_date": "1994-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411022",
        "title": "Adaptive Sentence Boundary Disambiguation",
        "authors": [
            "David D. Palmer",
            "Marti A. Hearst"
        ],
        "abstract": "  Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98.5\\% of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts.\n    ",
        "submission_date": "1994-11-16T00:00:00",
        "last_modified_date": "1994-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411023",
        "title": "Abstract Generation based on Rhetorical Structure Extraction",
        "authors": [
            "Kenji Ono",
            "Kazuo Sumita",
            "Seiji Miike Research",
            "Development Center",
            "Toshiba Corporation Komukai-Toshiba-cho 1",
            "Saiwai-ku",
            "Kawasaki",
            "Japan"
        ],
        "abstract": "  We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction. The system first extracts the rhetorical structure, the compound of the rhetorical relations between sentences, and then cuts out less important parts in the extracted structure to generate an abstract of the desired length.\n",
        "submission_date": "1994-11-17T00:00:00",
        "last_modified_date": "1994-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411024",
        "title": "Reverse Queries in DATR",
        "authors": [
            "Hagen Langer"
        ],
        "abstract": "  DATR is a declarative representation language for lexical information and as such, in principle, neutral with respect to particular processing strategies. Previous DATR compiler/interpreter systems support only one access strategy that closely resembles the set of inference rules of the procedural semantics of DATR (Evans & Gazdar 1989a). In this paper we present an alternative access strategy (reverse query strategy) for a non-trivial subset of DATR.\n    ",
        "submission_date": "1994-11-17T00:00:00",
        "last_modified_date": "1994-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411025",
        "title": "Multi-Dimensional Inheritance",
        "authors": [
            "Gregor Erbach"
        ],
        "abstract": "  In this paper, we present an alternative approach to multiple inheritance for typed feature structures. In our approach, a feature structure can be associated with several types coming from different hierarchies (dimensions). In case of multiple inheritance, a type has supertypes from different hierarchies. We contrast this approach with approaches based on a single type hierarchy where a feature structure has only one unique most general type, and multiple inheritance involves computation of greatest lower bounds in the hierarchy. The proposed approach supports current linguistic analyses in constraint-based formalisms like HPSG, inheritance in the lexicon, and knowledge representation for NLP systems. Finally, we show that multi-dimensional inheritance hierarchies can be compiled into a Prolog term representation, which allows to compute the conjunction of two types efficiently by Prolog term unification.\n    ",
        "submission_date": "1994-11-17T00:00:00",
        "last_modified_date": "1994-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411026",
        "title": "Manipulating Human-oriented Dictionaries with very simple tools",
        "authors": [
            "Jean Gaschler",
            "Mathieu Lafourcade"
        ],
        "abstract": "  This paper presents a methodology for building and manipulating human-oriented dictionaries. This methodology has been applied in the construction of a French-English-Malay dictionary which has been obtained by \"crossing\" semi-automatically two bilingual dictionaries. We use only Microsoft Word, a specialized language for writing transcriptors and a small but powerful dictionary tool.\n    ",
        "submission_date": "1994-11-18T00:00:00",
        "last_modified_date": "1994-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411027",
        "title": "Classifier Assignment by Corpus-based Approach",
        "authors": [
            "Virach Sornlertlamvanich",
            "Wantanee Pantachat",
            "Surapant Meknavin"
        ],
        "abstract": "  This paper presents an algorithm for selecting an appropriate classifier word for a noun. In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole spe ech community and individual speakers. Basically, there is no exect rule for classifier selection. As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun. Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation. We propose a corpus-based method (Biber, 1993; Nagao, 1993; Smadja, 1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase. The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences.\n    ",
        "submission_date": "1994-11-21T00:00:00",
        "last_modified_date": "1995-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411028",
        "title": "The Speech-Language Interface in the Spoken Language Translator",
        "authors": [
            "David Carter",
            "Manny Rayner"
        ],
        "abstract": "  The Spoken Language Translator is a prototype for practically useful systems capable of translating continuous spoken language within restricted domains. The prototype system translates air travel (ATIS) queries from spoken English to spoken Swedish and to French. It is constructed, with as few modifications as possible, from existing pieces of speech and language processing software. The speech recognizer and language understander are connected by a fairly conventional pipelined N-best interface. This paper focuses on the ways in which the language processor makes intelligent use of the sentence hypotheses delivered by the recognizer. These ways include (1) producing modified hypotheses to reflect the possible presence of repairs in the uttered word sequence; (2) fast parsing with a version of the grammar automatically specialized to the more frequent constructions in the training corpus; and (3) allowing syntactic and semantic factors to interact with acoustic ones in the choice of a meaning structure for translation, so that the acoustically preferred hypothesis is not always selected even if it is within linguistic coverage.\n    ",
        "submission_date": "1994-11-23T00:00:00",
        "last_modified_date": "1994-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411029",
        "title": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities",
        "authors": [
            "Andreas Stolcke"
        ],
        "abstract": "  We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.\n    ",
        "submission_date": "1994-11-28T00:00:00",
        "last_modified_date": "1994-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411030",
        "title": "Complexity of Scrambling: A New Twist to the Competence - Performance Distinction",
        "authors": [
            "Aravind K Joshi"
        ],
        "abstract": "  In this paper we discuss the following issue: How do we decide whether a certain property of language is a competence property or a performance property? Our claim is that the answer to this question is not given a-priori. The answer depends on the formal devices (formal grammars and machines) available to us for describing language. We discuss this issue in the context of the complexity of processing of center embedding (of relative clauses in English) and scrambling (in German, for example) from arbitrary depths of embedding.\n    ",
        "submission_date": "1994-11-29T00:00:00",
        "last_modified_date": "1994-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411031",
        "title": "Automatic Generation of Technical Documentation",
        "authors": [
            "Ehud Reiter",
            "Chris Mellish",
            "John Levine"
        ],
        "abstract": "  Natural-language generation (NLG) techniques can be used to automatically produce technical documentation from a domain knowledge base and linguistic and contextual models. We discuss this application of NLG technology from both a technical and a usefulness (costs and benefits) perspective. This discussion is based largely on our experiences with the IDAS documentation-generation project, and the reactions various interested people from industry have had to IDAS. We hope that this summary of our experiences with IDAS and the lessons we have learned from it will be beneficial for other researchers who wish to build technical-documentation generation systems.\n    ",
        "submission_date": "1994-11-29T00:00:00",
        "last_modified_date": "1994-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9411032",
        "title": "Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?",
        "authors": [
            "Ehud Reiter"
        ],
        "abstract": "  I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this `consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.\n    ",
        "submission_date": "1994-11-30T00:00:00",
        "last_modified_date": "1994-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412001",
        "title": "Dependency Grammar and the Parsing of Chinese Sentences",
        "authors": [
            "Bong Yeung Tom Lai",
            "Changning Huang"
        ],
        "abstract": "  Dependency Grammar has been used by linguists as the basis of the syntactic components of their grammar formalisms. It has also been used in natural language parsing. In China, attempts have been made to use this grammar formalism to parse Chinese sentences using corpus-based techniques. This paper reviews the properties of Dependency Grammar as embodied in four axioms for the well-formedness conditions for dependency structures. It is shown that allowing multiple governors as done by some followers of this formalism is unnecessary. The practice of augmenting Dependency Grammar with functional labels is also discussed in the light of building functional structures when the sentence is parsed. This will also facilitate semantic interpretation.\n    ",
        "submission_date": "1994-12-01T00:00:00",
        "last_modified_date": "1994-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412002",
        "title": "N-Gram Cluster Identification During Empirical Knowledge Representation Generation",
        "authors": [
            "Robin Collier"
        ],
        "abstract": "  This paper presents an overview of current research concerning knowledge extraction from technical texts. In particular, the use of empirical techniques during the identification and generation of a semantic representation is considered. A key step is the discovery of useful n-grams and correlations between clusters of these n-grams.\n    ",
        "submission_date": "1994-12-05T00:00:00",
        "last_modified_date": "1994-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412003",
        "title": "An Extended Clustering Algorithm for Statistical Language Models",
        "authors": [
            "Joerg P. Ueberla"
        ],
        "abstract": "  Statistical language models frequently suffer from a lack of training data. This problem can be alleviated by clustering, because it reduces the number of free parameters that need to be trained. However, clustered models have the following drawback: if there is ``enough'' data to train an unclustered model, then the clustered variant may perform worse. On currently used language modeling corpora, e.g. the Wall Street Journal corpus, how do the performances of a clustered and an unclustered model compare? While trying to address this question, we develop the following two ideas. First, to get a clustering algorithm with potentially high performance, an existing algorithm is extended to deal with higher order N-grams. Second, to make it possible to cluster large amounts of training data more efficiently, a heuristic to speed up the algorithm is presented. The resulting clustering algorithm can be used to cluster trigrams on the Wall Street Journal corpus and the language models it produces can compete with existing back-off models. Especially when there is only little training data available, the clustered models clearly outperform the back-off models.\n    ",
        "submission_date": "1994-12-06T00:00:00",
        "last_modified_date": "1994-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412004",
        "title": "Knowledge Representation for Lexical Semantics: Is Standard First Order Logic Enough?",
        "authors": [
            "Marc Light",
            "Lenhart Schubert"
        ],
        "abstract": "  Natural language understanding applications such as interactive planning and face-to-face translation require extensive inferencing. Many of these inferences are based on the meaning of particular open class words. Providing a representation that can support such lexically-based inferences is a primary concern of lexical semantics. The representation language of first order logic has well-understood semantics and a multitude of inferencing systems have been implemented for it. Thus it is a prime candidate to serve as a lexical semantics representation. However, we argue that FOL, although a good starting point, needs to be extended before it can efficiently and concisely support all the lexically-based inferences needed.\n    ",
        "submission_date": "1994-12-10T00:00:00",
        "last_modified_date": "1994-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412005",
        "title": "Segmenting speech without a lexicon: The roles of phonotactics and speech source",
        "authors": [
            "Timothy Andrew Cartwright",
            "Michael R. Brent"
        ],
        "abstract": "  Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon. Several sources of information in speech might help infants solve this problem, including prosody, semantic correlations and phonotactics. Research to date has focused on determining to which of these sources infants might be sensitive, but little work has been done to determine the potential usefulness of each source. The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences. The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle. Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules, the combination of both sources is most useful.\n    ",
        "submission_date": "1994-12-15T00:00:00",
        "last_modified_date": "1994-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412006",
        "title": "Robust stochastic parsing using the inside-outside algorithm",
        "authors": [
            "Briscoe",
            "Waegner",
            "Nick"
        ],
        "abstract": "  The paper describes a parser of sequences of (English) part-of-speech labels which utilises a probabilistic grammar trained using the inside-outside algorithm. The initial (meta)grammar is defined by a linguist and further rules compatible with metagrammatical constraints are automatically generated. During training, rules with very low probability are rejected yielding a wide-coverage parser capable of ranking alternative analyses. A series of corpus-based experiments describe the parser's performance.\n    ",
        "submission_date": "1994-12-19T00:00:00",
        "last_modified_date": "1994-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412007",
        "title": "Coupling Phonology and Phonetics in a Constraint-Based Gestural Model",
        "authors": [
            "Markus Walther",
            "Bernd J. Kroeger"
        ],
        "abstract": "  An implemented approach which couples a constraint-based phonology component with an articulatory speech synthesizer is proposed. Articulatory gestures ensure a tight connection between both components, as they comprise both physical-phonetic and phonological aspects. The phonological modelling of e.g. syllabification and phonological processes such as German final devoicing is expressed in the constraint logic programming language CUF. Extending CUF by arithmetic constraints allows the simultaneous description of both phonology and phonetics. Thus declarative lexicalist theories of grammar such as HPSG may be enriched up to the level of detailed phonetic realisation. Initial acoustic demonstrations show that our approach is in principle capable of synthesizing full utterances in a linguistically motivated fashion.\n    ",
        "submission_date": "1994-12-23T00:00:00",
        "last_modified_date": "1994-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cmp-lg/9412008",
        "title": "Analysis of Japanese Compound Nouns using Collocational Information",
        "authors": [
            "Kobayasi Yosiyuki",
            "Takunaga Takenobu",
            "Tanaka Hozumi"
        ],
        "abstract": "  Analyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus. An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters. The accuracy of this method is about 80%.\n    ",
        "submission_date": "1994-12-25T00:00:00",
        "last_modified_date": "1994-12-25T00:00:00"
    }
]