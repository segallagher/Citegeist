[
    {
        "url": "https://arxiv.org/abs/1301.0722",
        "title": "Good parts first - a new algorithm for approximate search in lexica and string databases",
        "authors": [
            "Stefan Gerdjikov",
            "Stoyan Mihov",
            "Petar Mitankin",
            "Klaus U. Schulz"
        ],
        "abstract": "We present a new efficient method for approximate search in electronic lexica. Given an input string (the pattern) and a similarity threshold, the algorithm retrieves all entries of the lexicon that are sufficiently similar to the pattern. Search is organized in subsearches that always start with an exact partial match where a substring of the input pattern is aligned with a substring of a lexicon word. Afterwards this partial match is extended stepwise to larger substrings. For aligning further parts of the pattern with corresponding parts of lexicon entries, more errors are tolerated at each subsequent step. For supporting this alignment order, which may start at any part of the pattern, the lexicon is represented as a structure that enables immediate access to any substring of a lexicon word and permits the extension of such substrings in both directions. Experimental evaluations of the approximate search procedure are given that show significant efficiency improvements compared to existing techniques. Since the technique can be used for large error bounds it offers interesting possibilities for approximate search in special collections of \"long\" strings, such as phrases, sentences, or book ti\n    ",
        "submission_date": "2013-01-04T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1950",
        "title": "Syntactic Analysis Based on Morphological Characteristic Features of the Romanian Language",
        "authors": [
            "Bogdan Patrut"
        ],
        "abstract": "This paper refers to the syntactic analysis of phrases in Romanian, as an important process of natural language processing. We will suggest a real-time solution, based on the idea of using some words or groups of words that indicate grammatical category; and some specific endings of some parts of sentence. Our idea is based on some characteristics of the Romanian language, where some prepositions, adverbs or some specific endings can provide a lot of information about the structure of a complex sentence. Such characteristics can be found in other languages, too, such as French. Using a special grammar, we developed a system (DIASEXP) that can perform a dialogue in natural language with assertive and interogative sentences about a \"story\" (a set of sentences describing some events from the real life).\n    ",
        "submission_date": "2013-01-09T00:00:00",
        "last_modified_date": "2013-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2444",
        "title": "TEI and LMF crosswalks",
        "authors": [
            "Laurent Romary"
        ],
        "abstract": "The present paper explores various arguments in favour of making the Text Encoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO standard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the issues that would have to be resolved in order to reach an appropriate implementation of these ideas, in particular in terms of infor-mational coverage. We show how the customisation facilities offered by the TEI guidelines can provide an adequate background, not only to cover missing components within the current Dictionary chapter of the TEI guidelines, but also to allow specific lexical projects to deal with local constraints. We expect this proposal to be a basis for a future ISO project in the context of the on going revision of LMF.\n    ",
        "submission_date": "2013-01-11T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2466",
        "title": "Determining token sequence mistakes in responses to questions with open text answer",
        "authors": [
            "Oleg Sychev",
            "Dmitry Mamontov"
        ],
        "abstract": "When learning grammar of the new language, a teacher should routinely check student's exercises for grammatical correctness. The paper describes a method of automatically detecting and reporting grammar mistakes, regarding an order of tokens in the response. It could report extra tokens, missing tokens and misplaced tokens. The method is useful when teaching language, where order of tokens is important, which includes most formal languages and some natural ones (like English). The method was implemented in a question type plug-in CorrectWriting for the widely used learning manage system Moodle.\n    ",
        "submission_date": "2013-01-11T00:00:00",
        "last_modified_date": "2013-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2811",
        "title": "Cutting Recursive Autoencoder Trees",
        "authors": [
            "Christian Scheible",
            "Hinrich Schuetze"
        ],
        "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.\n    ",
        "submission_date": "2013-01-13T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2857",
        "title": "SpeedRead: A Fast Named Entity Recognition Pipeline",
        "authors": [
            "Rami Al-Rfou'",
            "Steven Skiena"
        ],
        "abstract": "Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer.\n    ",
        "submission_date": "2013-01-14T00:00:00",
        "last_modified_date": "2013-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3214",
        "title": "The Manifold of Human Emotions",
        "authors": [
            "Seungyeon Kim",
            "Fuxin Li",
            "Guy Lebanon",
            "Irfan Essa"
        ],
        "abstract": "Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper, we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities.\n    ",
        "submission_date": "2013-01-15T00:00:00",
        "last_modified_date": "2013-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3547",
        "title": "A Rhetorical Analysis Approach to Natural Language Processing",
        "authors": [
            "Benjamin Englard"
        ],
        "abstract": "The goal of this research was to find a way to extend the capabilities of computers through the processing of language in a more human way, and present applications which demonstrate the power of this method. This research presents a novel approach, Rhetorical Analysis, to solving problems in Natural Language Processing (NLP). The main benefit of Rhetorical Analysis, as opposed to previous approaches, is that it does not require the accumulation of large sets of training data, but can be used to solve a multitude of problems within the field of NLP. The NLP problems investigated with Rhetorical Analysis were the Author Identification problem - predicting the author of a piece of text based on its rhetorical strategies, Election Prediction - predicting the winner of a presidential candidate's re-election campaign based on rhetorical strategies within that president's inaugural address, Natural Language Generation - having a computer produce text containing rhetorical strategies, and Document Summarization. The results of this research indicate that an Author Identification system based on Rhetorical Analysis could predict the correct author 100% of the time, that a re-election predictor based on Rhetorical Analysis could predict the correct winner of a re-election campaign 55% of the time, that a Natural Language Generation system based on Rhetorical Analysis could output text with up to 87.3% similarity to Shakespeare in style, and that a Document Summarization system based on Rhetorical Analysis could extract highly relevant sentences. Overall, this study demonstrated that Rhetorical Analysis could be a useful approach to solving problems in NLP.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3614",
        "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine Translation",
        "authors": [
            "Tsuyoshi Okita"
        ],
        "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2017-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3618",
        "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors",
        "authors": [
            "Danqi Chen",
            "Richard Socher",
            "Christopher D. Manning",
            "Andrew Y. Ng"
        ],
        "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3627",
        "title": "Two SVDs produce more focal deep learning representations",
        "authors": [
            "Hinrich Schuetze",
            "Christian Scheible"
        ],
        "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3781",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "authors": [
            "Tomas Mikolov",
            "Kai Chen",
            "Greg Corrado",
            "Jeffrey Dean"
        ],
        "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4432",
        "title": "Language learning from positive evidence, reconsidered: A simplicity-based approach",
        "authors": [
            "Anne S. Hsu",
            "Nick Chater",
            "Paul M.B. Vit\u00e1nyi"
        ],
        "abstract": "Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from positive evidence has been viewed as raising logical problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this logical problem can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the ability to learn a linguistic prediction, grammaticality judgements, language production, and form-meaning mappings. The simplicity approach can also be scaled-down to analyse the ability to learn a specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition.\n    ",
        "submission_date": "2013-01-18T00:00:00",
        "last_modified_date": "2013-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.5686",
        "title": "Transfer Topic Modeling with Ease and Scalability",
        "authors": [
            "Jeon-Hyung Kang",
            "Jun Ma",
            "Yan Liu"
        ],
        "abstract": "The increasing volume of short texts generated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standard text collections including AP and RCV1 datasets.\n    ",
        "submission_date": "2013-01-24T00:00:00",
        "last_modified_date": "2013-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.6939",
        "title": "Multi-Step Regression Learning for Compositional Distributional Semantics",
        "authors": [
            "Edward Grefenstette",
            "Georgiana Dinu",
            "Yao-Zhong Zhang",
            "Mehrnoosh Sadrzadeh",
            "Marco Baroni"
        ],
        "abstract": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face.\n    ",
        "submission_date": "2013-01-29T00:00:00",
        "last_modified_date": "2013-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7738",
        "title": "PyPLN: a Distributed Platform for Natural Language Processing",
        "authors": [
            "Fl\u00e1vio Code\u00e7o Coelho",
            "Renato Rocha Souza",
            "\u00c1lvaro Justen",
            "Fl\u00e1vio Amieiro",
            "Heliana Mello"
        ],
        "abstract": "This paper presents a distributed platform for Natural Language Processing called PyPLN. PyPLN leverages a vast array of NLP and text processing open source tools, managing the distribution of the workload on a variety of configurations: from a single server to a cluster of linux servers. PyPLN is developed using Python 2.7.3 but makes it very easy to incorporate other softwares for specific tasks as long as a linux version is available. PyPLN facilitates analyses both at document and corpus level, simplifying management and publication of corpora and analytical results through an easy to use web interface. In the current (beta) release, it supports English and Portuguese languages with support to other languages planned for future releases. To support the Portuguese language PyPLN uses the PALAVRAS parser\\citep{Bick2000}. Currently PyPLN offers the following features: Text extraction with encoding normalization (to UTF-8), part-of-speech tagging, token frequency, semantic annotation, n-gram extraction, word and sentence repertoire, and full-text search across corpora. The platform is licensed as GPL-v3.\n    ",
        "submission_date": "2013-01-31T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1123",
        "title": "Large Scale Distributed Acoustic Modeling With Back-off N-grams",
        "authors": [
            "Ciprian Chelba",
            "Peng Xu",
            "Fernando Pereira",
            "Thomas Richardson"
        ],
        "abstract": "The paper revives an older approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data and model size (as measured by the number of parameters in the model), to approximately 100 times larger than current sizes used in automatic speech recognition. In such a data-rich setting, we can expand the phonetic context significantly beyond triphones, as well as increase the number of Gaussian mixture components for the context-dependent states that allow it. We have experimented with contexts that span seven or more context-independent phones, and up to 620 mixture components per state. Dealing with unseen phonetic contexts is accomplished using the familiar back-off technique used in language modeling due to implementation simplicity. The back-off acoustic model is estimated, stored and served using MapReduce distributed computing infrastructure.\n",
        "submission_date": "2013-02-05T00:00:00",
        "last_modified_date": "2013-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1380",
        "title": "Towards the Rapid Development of a Natural Language Understanding Module",
        "authors": [
            "Catarina Moreira",
            "Ana Cristina Mendes",
            "Lu\u00edsa Coheur",
            "Bruno Martins"
        ],
        "abstract": "When developing a conversational agent, there is often an urgent need to have a prototype available in order to test the application with real users. A Wizard of Oz is a possibility, but sometimes the agent should be simply deployed in the environment where it will be used. Here, the agent should be able to capture as many interactions as possible and to understand how people react to failure. In this paper, we focus on the rapid development of a natural language understanding module by non experts. Our approach follows the learning paradigm and sees the process of understanding natural language as a classification problem. We test our module with a conversational agent that answers questions in the art domain. Moreover, we show how our approach can be used by a natural language interface to a cinema database.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1422",
        "title": "S\u00e9mantique des d\u00e9terminants dans un cadre richement typ\u00e9",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "The variation of word meaning according to the context leads us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae. But in our rich type system the usual semantic terms do not work. We propose a solution ins- pired by the tau and epsilon operators of Hilbert, kinds of generic elements and choice functions. This approach unifies the treatment of the different determi- ners and quantifiers as well as the dynamic binding of pronouns. Above all, this fully computational view fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1572",
        "title": "Lexical Access for Speech Understanding using Minimum Message Length Encoding",
        "authors": [
            "Ian Thomas",
            "Ingrid Zukerman",
            "Jonathan Oliver",
            "David Albrecht",
            "Bhavani Raskutti"
        ],
        "abstract": "The Lexical Access Problem consists of determining the intended sequence of words corresponding to an input sequence of phonemes (basic speech sounds) that come from a low-level phoneme recognizer.  In this paper we present an information-theoretic approach  based on the Minimum Message Length Criterion for solving the Lexical Access Problem.  We model sentences using phoneme realizations seen in training, and word and part-of-speech information obtained from text corpora. We show results on multiple-speaker, continuous, read speech and discuss a heuristic using equivalence classes of similar sounding words which speeds up the recognition process without significant deterioration in recognition accuracy.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3057",
        "title": "Building a reordering system using tree-to-string hierarchical model",
        "authors": [
            "Jacob Dlougach",
            "Irina Galinskaya"
        ],
        "abstract": "This paper describes our submission to the First Workshop on Reordering for Statistical Machine Translation. We have decided to build a reordering system based on tree-to-string model, using only publicly available tools to accomplish this task. With the provided training data we have built a translation model using Moses toolkit, and then we applied a chart decoder, implemented in Moses, to reorder the sentences. Even though our submission only covered English-Farsi language pair, we believe that the approach itself should work regardless of the choice of the languages, so we have also carried out the experiments for English-Italian and English-Urdu. For these language pairs we have noticed a significant improvement over the baseline in BLEU, Kendall-Tau and Hamming metrics. A detailed description is given, so that everyone can reproduce our results. Also, some possible directions for further improvements are discussed.\n    ",
        "submission_date": "2013-02-13T00:00:00",
        "last_modified_date": "2013-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4489",
        "title": "Termhood-based Comparability Metrics of Comparable Corpus in Special Domain",
        "authors": [
            "Sa Liu",
            "Chengzhi Zhang"
        ],
        "abstract": "Cross-Language Information Retrieval (CLIR) and machine translation (MT) resources, such as dictionaries and parallel corpora, are scarce and hard to come by for special domains. Besides, these resources are just limited to a few languages, such as English, French, and Spanish and so on. So, obtaining comparable corpora automatically for such domains could be an answer to this problem effectively. Comparable corpora, that the subcorpora are not translations of each other, can be easily obtained from web. Therefore, building and using comparable corpora is often a more feasible option in multilingual information processing. Comparability metrics is one of key issues in the field of building and using comparable corpus. Currently, there is no widely accepted definition or metrics method of corpus comparability. In fact, Different definitions or metrics methods of comparability might be given to suit various tasks about natural language processing. A new comparability, namely, termhood-based metrics, oriented to the task of bilingual terminology extraction, is proposed in this paper. In this method, words are ranked by termhood not frequency, and then the cosine similarities, calculated based on the ranking lists of word termhood, is used as comparability. Experiments results show that termhood-based metrics performs better than traditional frequency-based metrics.\n    ",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4492",
        "title": "Bilingual Terminology Extraction Using Multi-level Termhood",
        "authors": [
            "Chengzhi Zhang",
            "Dan Wu"
        ],
        "abstract": "Purpose: Terminology is the set of technical words or expressions used in specific contexts, which denotes the core concept in a formal discipline and is usually applied in the fields of machine translation, information retrieval, information extraction and text categorization, etc. Bilingual terminology extraction plays an important role in the application of bilingual dictionary compilation, bilingual Ontology construction, machine translation and cross-language information retrieval etc. This paper addresses the issues of monolingual terminology extraction and bilingual term alignment based on multi-level termhood.\n",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4619",
        "title": "Compactified Horizontal Visibility Graph for the Language Network",
        "authors": [
            "D.V. Lande",
            "A.A.Snarskii"
        ],
        "abstract": "A compactified horizontal visibility graph for the language network is proposed. It was found that the networks constructed in such way are scale free, and have a property that among the nodes with largest degrees there are words that determine not only a text structure communication, but also its informational structure.\n    ",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4811",
        "title": "Towards a Semantic-based Approach for Modeling Regulatory Documents in Building Industry",
        "authors": [
            "Khalil Riad Bouzidi",
            "Catherine Faron-Zucker",
            "Bruno Fies",
            "Olivier Corby",
            "Le-Thanh Nhan"
        ],
        "abstract": "Regulations in the Building Industry are becoming increasingly complex and involve more than one technical area. They cover products, components and project implementation. They also play an important role to ensure the quality of a building, and to minimize its environmental impact. In this paper, we are particularly interested in the modeling of the regulatory constraints derived from the Technical Guides issued by CSTB and used to validate Technical Assessments. We first describe our approach for modeling regulatory constraints in the SBVR language, and formalizing them in the SPARQL language. Second, we describe how we model the processes of compliance checking described in the CSTB Technical Guides. Third, we show how we implement these processes to assist industrials in drafting Technical Documents in order to acquire a Technical Assessment; a compliance report is automatically generated to explain the compliance or noncompliance of this Technical Documents.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4813",
        "title": "Probabilistic Frame Induction",
        "authors": [
            "Jackie Chi Kit Cheung",
            "Hoifung Poon",
            "Lucy Vanderwende"
        ],
        "abstract": "In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, participants as latent topics and learns those frame and event transitions that best explain the text. The number of frames is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produced state-of-the-art results while substantially reducing engineering effort.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4814",
        "title": "NLP and CALL: integration is working",
        "authors": [
            "Georges Antoniadis",
            "Sylviane Granger",
            "Olivier Kraif",
            "Claude Ponton",
            "Virginie Zampa"
        ],
        "abstract": "In the first part of this article, we explore the background of computer-assisted learning from its beginnings in the early XIXth century and the first teaching machines, founded on theories of learning, at the start of the XXth century. With the arrival of the computer, it became possible to offer language learners different types of language activities such as comprehension tasks, simulations, etc. However, these have limits that cannot be overcome without some contribution from the field of natural language processing (NLP). In what follows, we examine the challenges faced and the issues raised by integrating NLP into CALL. We hope to demonstrate that the key to success in integrating NLP into CALL is to be found in multidisciplinary work between computer experts, linguists, language teachers, didacticians and NLP specialists.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4874",
        "title": "A Labeled Graph Kernel for Relationship Extraction",
        "authors": [
            "Gon\u00e7alo Sim\u00f5es",
            "Helena Galhardas",
            "David Matos"
        ],
        "abstract": "In this paper, we propose an approach for Relationship Extraction (RE) based on labeled graph kernels. The kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the RE literature: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions. We performed experiments on a dataset of protein-protein interactions and the results show that our approach obtains effectiveness values that are comparable with the state-of-the art kernel methods. Moreover, our approach is able to outperform the state-of-the-art kernels when combined with other kernel methods.\n    ",
        "submission_date": "2013-02-20T00:00:00",
        "last_modified_date": "2013-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5645",
        "title": "Role of temporal inference in the recognition of textual inference",
        "authors": [
            "Djallel Bouneffouf"
        ],
        "abstract": "This project is a part of nature language processing and its aims to develop a system of recognition inference text-appointed TIMINF. This type of system can detect, given two portions of text, if a text is semantically deducted from the other. We focused on making the inference time in this type of system. For that we have built and analyzed a body built from questions collected through the web. This study has enabled us to classify different types of times inferences and for designing the architecture of TIMINF which seeks to integrate a module inference time in a detection system inference text. We also assess the performance of sorties TIMINF system on a test corpus with the same strategy adopted in the challenge RTE.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5675",
        "title": "Development of Yes/No Arabic Question Answering System",
        "authors": [
            "Wafa N. Bdour",
            "Natheer K. Gharaibeh"
        ],
        "abstract": "Developing Question Answering systems has been one of the important research issues because it requires insights from a variety of disciplines,including,Artificial Intelligence,Information Retrieval, Information Extraction,Natural Language Processing, and ",
        "submission_date": "2013-02-22T00:00:00",
        "last_modified_date": "2013-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6334",
        "title": "Non-simplifying Graph Rewriting Termination",
        "authors": [
            "Guillaume Bonfante",
            "Bruno Guillaume"
        ],
        "abstract": "So far, a very large amount of work in Natural Language Processing (NLP) rely on trees as the core mathematical structure to represent linguistic informations (e.g. in Chomsky's work). However, some linguistic phenomena do not cope properly with trees. In a former paper, we showed the benefit of encoding linguistic structures by graphs and of using graph rewriting rules to compute on those structures. Justified by some linguistic considerations, graph rewriting is characterized by two features: first, there is no node creation along computations and second, there are non-local edge modifications. Under these hypotheses, we show that uniform termination is undecidable and that non-uniform termination is decidable. We describe two termination techniques based on weights and we give complexity bound on the derivation length for these rewriting system.\n    ",
        "submission_date": "2013-02-26T00:00:00",
        "last_modified_date": "2013-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.6777",
        "title": "Ending-based Strategies for Part-of-speech Tagging",
        "authors": [
            "Greg Adams",
            "Beth Millar",
            "Eric Neufeld",
            "Tim Philip"
        ],
        "abstract": "Probabilistic approaches to part-of-speech tagging rely primarily on whole-word statistics about word/tag combinations as well as contextual information.  But experience shows about 4 per cent of tokens encountered in test sets are unknown even when the training set is as large as a million words.  Unseen words are tagged using secondary strategies that exploit word features such as endings, capitalizations and punctuation marks.  In this work, word-ending statistics are primary and whole-word statistics are secondary.  First, a tagger was trained and tested on word endings only.  Subsequent experiments added back whole-word statistics for the words occurring most frequently in the training set.  As grew larger, performance was expected to improve, in the limit performing the same as word-based taggers.  Surprisingly, the ending-based tagger initially performed nearly as well as the word-based tagger; in the best case, its performance significantly exceeded that of the word-based tagger.  Lastly, and unexpectedly, an effect of negative returns was observed - as grew larger, performance generally improved and then declined.  By varying factors such as ending length and tag-list strategy, we achieved a success rate of 97.5 percent.\n    ",
        "submission_date": "2013-02-27T00:00:00",
        "last_modified_date": "2013-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.7056",
        "title": "KSU KDD: Word Sense Induction by Clustering in Topic Space",
        "authors": [
            "Wesam Elshamy",
            "Doina Caragea",
            "William Hsu"
        ],
        "abstract": "We describe our language-independent unsupervised word sense induction system. This system only uses topic features to cluster different word senses in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then uses it to infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems.\n    ",
        "submission_date": "2013-02-28T00:00:00",
        "last_modified_date": "2013-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0350",
        "title": "Structure-semantics interplay in complex networks and its effects on the predictability of similarity in texts",
        "authors": [
            "Diego R. Amancio",
            "Osvaldo N. Oliveira Jr.",
            "Luciano da F. Costa"
        ],
        "abstract": "There are different ways to define similarity for grouping similar texts into clusters, as the concept of similarity may depend on the purpose of the task. For instance, in topic extraction similar texts mean those within the same semantic field, whereas in author recognition stylistic features should be considered. In this study, we introduce ways to classify texts employing concepts of complex networks, which may be able to capture syntactic, semantic and even pragmatic features. The interplay between the various metrics of the complex networks is analyzed with three applications, namely identification of machine translation (MT) systems, evaluation of quality of machine translated texts and authorship recognition. We shall show that topological features of the networks representing texts can enhance the ability to identify MT systems in particular cases. For evaluating the quality of MT texts, on the other hand, high correlation was obtained with methods capable of capturing the semantics. This was expected because the golden standards used are themselves based on word co-occurrence. Notwithstanding, the Katz similarity, which involves semantic and structure in the comparison of texts, achieved the highest correlation with the NIST measurement, indicating that in some cases the combination of both approaches can improve the ability to quantify quality in MT. In authorship recognition, again the topological features were relevant in some contexts, though for the books and authors analyzed good results were obtained with semantic features as well. Because hybrid approaches encompassing semantic and topological features have not been extensively used, we believe that the methodology proposed here may be useful to enhance text classification considerably, as it combines well-established strategies.\n    ",
        "submission_date": "2013-03-02T00:00:00",
        "last_modified_date": "2013-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0446",
        "title": "Statistical sentiment analysis performance in Opinum",
        "authors": [
            "Boyan Bonev",
            "Gema Ram\u00edrez-S\u00e1nchez",
            "Sergio Ortiz Rojas"
        ],
        "abstract": "The classification of opinion texts in positive and negative is becoming a subject of great interest in sentiment analysis. The existence of many labeled opinions motivates the use of statistical and machine-learning methods. First-order statistics have proven to be very limited in this field. The Opinum approach is based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most named-entities with wildcards. Opinum presents an accuracy above 81% for Spanish opinions in the financial products domain. In this work we discuss which are the most important factors that have impact on the classification performance.\n    ",
        "submission_date": "2013-03-03T00:00:00",
        "last_modified_date": "2013-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0489",
        "title": "A Semantic approach for effective document clustering using WordNet",
        "authors": [
            "Leena H. Patil",
            "Mohammed Atique"
        ],
        "abstract": "Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc.\n    ",
        "submission_date": "2013-03-03T00:00:00",
        "last_modified_date": "2013-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1232",
        "title": "Japanese-Spanish Thesaurus Construction Using English as a Pivot",
        "authors": [
            "Jessica Ram\u00edrez",
            "Masayuki Asahara",
            "Yuji Matsumoto"
        ],
        "abstract": "We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual Japanese-Spanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2013-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1929",
        "title": "Towards the Fully Automatic Merging of Lexical Resources: A Step Forward",
        "authors": [
            "Muntsa Padr\u00f3",
            "N\u00faria Bel",
            "Silvia Necsulescu"
        ],
        "abstract": "This article reports on the results of the research done towards the fully automatically merging of lexical resources. Our main goal is to show the generality of the proposed approach, which have been previously applied to merge Spanish Subcategorization Frames lexica. In this work we extend and apply the same technique to perform the merging of morphosyntactic lexica encoded in LMF. The experiments showed that the technique is general enough to obtain good results in these two different tasks which is an important step towards performing the merging of lexical resources fully automatically.\n    ",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1930",
        "title": "Automatic lexical semantic classification of nouns",
        "authors": [
            "N\u00faria Bel",
            "Lauren Romeo",
            "Muntsa Padr\u00f3"
        ],
        "abstract": "The work we present here addresses cue-based noun classification in English and Spanish. Its main objective is to automatically acquire lexical semantic information by classifying nouns into previously known noun lexical classes. This is achieved by using particular aspects of linguistic contexts as cues that identify a specific lexical class. Here we concentrate on the task of identifying such cues and the theoretical background that allows for an assessment of the complexity of the task. The results show that, despite of the a-priori complexity of the task, cue-based classification is a useful tool in the automatic acquisition of lexical semantic classes.\n    ",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1931",
        "title": "A Classification of Adjectives for Polarity Lexicons Enhancement",
        "authors": [
            "Silvia V\u00e1zquez",
            "N\u00faria Bel"
        ],
        "abstract": "Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis.\n    ",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1932",
        "title": "Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform",
        "authors": [
            "N\u00faria Bel",
            "Vassilis Papavasiliou",
            "Prokopis Prokopidis",
            "Antonio Toral",
            "Victoria Arranz"
        ],
        "abstract": "The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform that automates the stages involved in the acquisition, production, updating and maintenance of the large language resources required by, among others, MT systems. The development of a Corpus Acquisition Component (CAC) for extracting monolingual and bilingual data from the web is one of the most innovative building blocks of PANACEA. The CAC, which is the first stage in the PANACEA pipeline for building Language Resources, adopts an efficient and distributed methodology to crawl for web documents with rich textual content in specific languages and predefined domains. The CAC includes modules that can acquire parallel data from sites with in-domain content available in more than one language. In order to extrinsically evaluate the CAC methodology, we have conducted several experiments that used crawled parallel corpora for the identification and extraction of parallel sentences using sentence alignment. The corpora were then successfully used for domain adaptation of Machine Translation Systems.\n    ",
        "submission_date": "2013-03-08T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2448",
        "title": "Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon Production",
        "authors": [
            "N\u00faria Bel",
            "Maria Coll",
            "Gabriela Resnik"
        ],
        "abstract": "In this work we present the results of our experimental work on the develop-ment of lexical class-based lexica by automatic means. The objective is to as-sess the use of linguistic lexical-class based information as a feature selection methodology for the use of classifiers in quick lexical development. The results show that the approach can help in re-ducing the human effort required in the development of language resources sig-nificantly.\n    ",
        "submission_date": "2013-03-11T00:00:00",
        "last_modified_date": "2013-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2449",
        "title": "Using qualia information to identify lexical semantic classes in an unsupervised clustering task",
        "authors": [
            "Lauren Romeo",
            "Sara Mendes",
            "N\u00faria Bel"
        ],
        "abstract": "Acquiring lexical information is a complex problem, typically approached by relying on a number of contexts to contribute information for classification. One of the first issues to address in this domain is the determination of such contexts. The work presented here proposes the use of automatically obtained FORMAL role descriptors as features used to draw nouns from the same lexical semantic class together in an unsupervised clustering task. We have dealt with three lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The results obtained show that it is possible to discriminate between elements from different lexical semantic classes using only FORMAL role information, hence validating our initial hypothesis. Also, iterating our method accurately accounts for fine-grained distinctions within lexical classes, namely distinctions involving ambiguous expressions. Moreover, a filtering and bootstrapping strategy employed in extracting FORMAL role descriptors proved to minimize effects of sparse data and noise in our task.\n    ",
        "submission_date": "2013-03-11T00:00:00",
        "last_modified_date": "2013-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2826",
        "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA",
        "authors": [
            "William M. Darling",
            "Fei Song"
        ],
        "abstract": "This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as \"nouns about weather\" or \"verbs about law\". We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised part-of-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art.\n    ",
        "submission_date": "2013-03-12T00:00:00",
        "last_modified_date": "2013-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3170",
        "title": "Types and forgetfulness in categorical linguistics and quantum mechanics",
        "authors": [
            "Peter Hines"
        ],
        "abstract": "The role of types in categorical models of meaning is investigated. A general scheme for how typed models of meaning may be used to compare sentences, regardless of their grammatical structure is described, and a toy example is used as an illustration. Taking as a starting point the question of whether the evaluation of such a type system 'loses information', we consider the parametrized typing associated with connectives from this viewpoint.\n",
        "submission_date": "2013-03-13T00:00:00",
        "last_modified_date": "2013-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3592",
        "title": "Expressing Ethnicity through Behaviors of a Robot Character",
        "authors": [
            "Maxim Makatchev",
            "Reid Simmons",
            "Majd Sakr",
            "Micheline Ziadee"
        ],
        "abstract": "Achieving homophily, or association based on similarity, between a human user and a robot holds a promise of improved perception and task performance. However, no previous studies that address homophily via ethnic similarity with robots exist. In this paper, we discuss the difficulties of evoking ethnic cues in a robot, as opposed to a virtual agent, and an approach to overcome those difficulties based on using ethnically salient behaviors. We outline our methodology for selecting and evaluating such behaviors, and culminate with a study that evaluates our hypotheses of the possibility of ethnic attribution of a robot character through verbal and nonverbal behaviors and of achieving the homophily effect.\n    ",
        "submission_date": "2013-03-14T00:00:00",
        "last_modified_date": "2013-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3948",
        "title": "An Adaptive Methodology for Ubiquitous ASR System",
        "authors": [
            "Urmila Shrawankar",
            "Vilas Thakare"
        ],
        "abstract": "Achieving and maintaining the performance of ubiquitous (Automatic Speech Recognition) ASR system is a real challenge. The main objective of this work is to develop a method that will improve and show the consistency in performance of ubiquitous ASR system for real world noisy environment. An adaptive methodology has been developed to achieve an objective with the help of implementing followings, -Cleaning speech signal as much as possible while preserving originality / intangibility using various modified filters and enhancement techniques. -Extracting features from speech signals using various sizes of parameter. -Train the system for ubiquitous environment using multi-environmental adaptation training methods. -Optimize the word recognition rate with appropriate variable size of parameters using fuzzy technique. The consistency in performance is tested using standard noise databases as well as in real world environment. A good improvement is noticed. This work will be helpful to give discriminative training of ubiquitous ASR system for better Human Computer Interaction (HCI) using Speech User Interface (SUI).\n    ",
        "submission_date": "2013-03-16T00:00:00",
        "last_modified_date": "2013-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.4293",
        "title": "A Multilingual Semantic Wiki Based on Attempto Controlled English and Grammatical Framework",
        "authors": [
            "Kaarel Kaljurand",
            "Tobias Kuhn"
        ],
        "abstract": "We describe a semantic wiki system with an underlying controlled natural language grammar implemented in Grammatical Framework (GF). The grammar restricts the wiki content to a well-defined subset of Attempto Controlled English (ACE), and facilitates a precise bidirectional automatic translation between ACE and language fragments of a number of other natural languages, making the wiki content accessible multilingually. Additionally, our approach allows for automatic translation into the Web Ontology Language (OWL), which enables automatic reasoning over the wiki content. The developed wiki environment thus allows users to build, query and view OWL knowledge bases via a user-friendly multilingual natural language interface. As a further feature, the underlying multilingual grammar is integrated into the wiki and can be collaboratively edited to extend the vocabulary of the wiki or even customize its sentence structures. This work demonstrates the combination of the existing technologies of Attempto Controlled English and Grammatical Framework, and is implemented as an extension of the existing semantic wiki engine AceWiki.\n    ",
        "submission_date": "2013-03-11T00:00:00",
        "last_modified_date": "2013-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5148",
        "title": "Estimating Confusions in the ASR Channel for Improved Topic-based Language Model Adaptation",
        "authors": [
            "Damianos Karakos",
            "Mark Dredze",
            "Sanjeev Khudanpur"
        ],
        "abstract": "Human language is a combination of elemental languages/domains/styles that change across and sometimes within discourses. Language models, which play a crucial role in speech recognizers and machine translation systems, are particularly sensitive to such changes, unless some form of adaptation takes place. One approach to speech language model adaptation is self-training, in which a language model's parameters are tuned based on automatically transcribed audio. However, transcription errors can misguide self-training, particularly in challenging settings such as conversational speech. In this work, we propose a model that considers the confusions (errors) of the ASR channel. By modeling the likely confusions in the ASR output instead of using just the 1-best, we improve self-training efficacy by obtaining a more reliable reference transcription estimate. We demonstrate improved topic-based language modeling adaptation results over both 1-best and lattice self-training using our ASR channel confusion estimates on telephone conversations.\n    ",
        "submission_date": "2013-03-21T00:00:00",
        "last_modified_date": "2013-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5513",
        "title": "Parameters Optimization for Improving ASR Performance in Adverse Real World Noisy Environmental Conditions",
        "authors": [
            "Urmila Shrawankar",
            "Vilas Thakare"
        ],
        "abstract": "From the existing research it has been observed that many techniques and methodologies are available for performing every step of Automatic Speech Recognition (ASR) system, but the performance (Minimization of Word Error Recognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology is not dependent on the only technique applied in that method. The research work indicates that, performance mainly depends on the category of the noise, the level of the noise and the variable size of the window, frame, frame overlap etc is considered in the existing methods. The main aim of the work presented in this paper is to use variable size of parameters like window size, frame size and frame overlap percentage to observe the performance of algorithms for various categories of noise with different levels and also train the system for all size of parameters and category of real world noisy environment to improve the performance of the speech recognition system. This paper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by applying variable size of parameters. It is observed that, it is really very hard to evaluate test results and decide parameter size for ASR performance improvement for its resultant optimization. Hence, this study further suggests the feasible and optimum parameter size using Fuzzy Inference System (FIS) for enhancing resultant accuracy in adverse real world noisy environmental conditions. This work will be helpful to give discriminative training of ubiquitous ASR system for better Human Computer Interaction (HCI).\n    ",
        "submission_date": "2013-03-22T00:00:00",
        "last_modified_date": "2013-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5515",
        "title": "Adverse Conditions and ASR Techniques for Robust Speech User Interface",
        "authors": [
            "Urmila Shrawankar",
            "VM Thakare"
        ],
        "abstract": "The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system.\n    ",
        "submission_date": "2013-03-22T00:00:00",
        "last_modified_date": "2013-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5960",
        "title": "SYNTAGMA. A Linguistic Approach to Parsing",
        "authors": [
            "Daniel Christen"
        ],
        "abstract": "SYNTAGMA is a rule-based parsing system, structured on two levels: a general parsing engine and a language specific grammar. The parsing engine is a language independent program, while grammar and language specific rules and resources are given as text files, consisting in a list of constituent structuresand a lexical database with word sense related features and constraints. Since its theoretical background is principally Tesniere's Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument structure (valency) in constraint satisfaction, and allows also horizontal bounds, for instance treating coordination. Notions such as Pro, traces, empty categories are derived from Generative Grammar and some solutions are close to Government&Binding Theory, although they are the result of an autonomous research. These properties allow SYNTAGMA to manage complex syntactic configurations and well known weak points in parsing engineering. An important resource is the semantic network, which is used in disambiguation tasks. Parsing process follows a bottom-up, rule driven strategy. Its behavior can be controlled and fine-tuned.\n    ",
        "submission_date": "2013-03-24T00:00:00",
        "last_modified_date": "2016-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.7310",
        "title": "Exploring the Role of Logically Related Non-Question Phrases for Answering Why-Questions",
        "authors": [
            "Niraj Kumar",
            "Rashmi Gangadharaiah",
            "Kannan Srinathan",
            "Vasudeva Varma"
        ],
        "abstract": "In this paper, we show that certain phrases although not present in a given question/query, play a very important role in answering the question. Exploring the role of such phrases in answering questions not only reduces the dependency on matching question phrases for extracting answers, but also improves the quality of the extracted answers. Here matching question phrases means phrases which co-occur in given question and candidate answers. To achieve the above discussed goal, we introduce a bigram-based word graph model populated with semantic and topical relatedness of terms in the given document. Next, we apply an improved version of ranking with a prior-based approach, which ranks all words in the candidate document with respect to a set of root words (i.e. non-stopwords present in the question and in the candidate document). As a result, terms logically related to the root words are scored higher than terms that are not related to the root words. Experimental results show that our devised system performs better than state-of-the-art for the task of answering Why-questions.\n    ",
        "submission_date": "2013-03-29T00:00:00",
        "last_modified_date": "2013-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3265",
        "title": "Extension of hidden markov model for recognizing large vocabulary of sign language",
        "authors": [
            "Maher Jebali",
            "Patrice Dalle",
            "Mohamed Jemni"
        ],
        "abstract": "Computers still have a long way to go before they can interact with users in a truly natural fashion. From a users perspective, the most natural way to interact with a computer would be through a speech and gesture interface. Although speech recognition has made significant advances in the past ten years, gesture recognition has been lagging behind. Sign Languages (SL) are the most accomplished forms of gestural communication. Therefore, their automatic analysis is a real challenge, which is interestingly implied to their lexical and syntactic organization levels. Statements dealing with sign language occupy a significant interest in the Automatic Natural Language Processing (ANLP) domain. In this work, we are dealing with sign language recognition, in particular of French Sign Language (FSL). FSL has its own specificities, such as the simultaneity of several parameters, the important role of the facial expression or movement and the use of space for the proper utterance organization. Unlike speech recognition, Frensh sign language (FSL) events occur both sequentially and simultaneously. Thus, the computational processing of FSL is too complex than the spoken languages. We present a novel approach based on HMM to reduce the recognition complexity.\n    ",
        "submission_date": "2013-04-11T00:00:00",
        "last_modified_date": "2013-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3841",
        "title": "The risks of mixing dependency lengths from sequences of different length",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Haitao Liu"
        ],
        "abstract": "Mixing dependency lengths from sequences of different length is a common practice in language research. However, the empirical distribution of dependency lengths of sentences of the same length differs from that of sentences of varying length and the distribution of dependency lengths depends on sentence length for real sentences and also under the null hypothesis that dependencies connect vertices located in random positions of the sequence. This suggests that certain results, such as the distribution of syntactic dependency lengths mixing dependencies from sentences of varying length, could be a mere consequence of that mixing. Furthermore, differences in the global averages of dependency length (mixing lengths from sentences of varying length) for two different languages do not simply imply a priori that one language optimizes dependency lengths better than the other because those differences could be due to differences in the distribution of sentence lengths and other factors.\n    ",
        "submission_date": "2013-04-13T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4086",
        "title": "Hubiness, length, crossings and their relationships in dependency trees",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Here tree dependency structures are studied from three different perspectives: their degree variance (hubiness), the mean dependency length and the number of dependency crossings. Bounds that reveal pairwise dependencies among these three metrics are derived. Hubiness (the variance of degrees) plays a central role: the mean dependency length is bounded below by hubiness while the number of crossings is bounded above by hubiness. Our findings suggest that the online memory cost of a sentence might be determined not just by the ordering of words but also by the hubiness of the underlying structure. The 2nd moment of degree plays a crucial role that is reminiscent of its role in large complex networks.\n    ",
        "submission_date": "2013-04-15T00:00:00",
        "last_modified_date": "2013-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.4520",
        "title": "Sentiment Analysis : A Literature Survey",
        "authors": [
            "Subhabrata Mukherjee",
            "Pushpak Bhattacharyya"
        ],
        "abstract": "Our day-to-day life has always been influenced by what people think. Ideas and opinions of others have always affected our own opinions. The explosion of Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging, Contributing to RSS, Social Bookmarking, and Social Networking. As a result there has been an eruption of interest in people to mine these vast resources of data for opinions. Sentiment Analysis or Opinion Mining is the computational treatment of opinions, sentiments and subjectivity of text. In this report, we take a look at the various challenges and applications of Sentiment Analysis. We will discuss in details various approaches to perform a computational treatment of sentiments and opinions. Various supervised or data-driven techniques to SA like Na\u00efve Byes, Maximum Entropy, SVM, and Voted Perceptrons will be discussed and their strengths and drawbacks will be touched upon. We will also see a new dimension of analyzing sentiments by Cognitive Psychology mainly through the work of Janyce Wiebe, where we will see ways to detect subjectivity, perspective in narrative and understanding the discourse structure. We will also study some specific topics in Sentiment Analysis and the contemporary works in those areas.\n    ",
        "submission_date": "2013-04-16T00:00:00",
        "last_modified_date": "2013-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5880",
        "title": "Dealing with natural language interfaces in a geolocation context",
        "authors": [
            "M.-A. Abchir",
            "Isis Truck",
            "Anna Pappa"
        ],
        "abstract": "In the geolocation field where high-level programs and low-level devices coexist, it is often difficult to find a friendly user inter- face to configure all the parameters. The challenge addressed in this paper is to propose intuitive and simple, thus natural lan- guage interfaces to interact with low-level devices. Such inter- faces contain natural language processing and fuzzy represen- tations of words that facilitate the elicitation of business-level objectives in our context.\n    ",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7157",
        "title": "Question Answering Against Very-Large Text Collections",
        "authors": [
            "Leon Derczynski",
            "Richard Shaw",
            "Ben Solway",
            "Jun Wang"
        ],
        "abstract": "Question answering involves developing methods to extract useful information from large collections of documents. This is done with specialised search engines such as Answer Finder. The aim of Answer Finder is to provide an answer to a question rather than a page listing related documents that may contain the correct answer. So, a question such as \"How tall is the Eiffel Tower\" would simply return \"325m\" or \"1,063ft\". Our task was to build on the current version of Answer Finder by improving information retrieval, and also improving the pre-processing involved in question series analysis.\n    ",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7282",
        "title": "An Improved Approach for Word Ambiguity Removal",
        "authors": [
            "Priti Saktel",
            "Urmila Shrawankar"
        ],
        "abstract": "Word ambiguity removal is a task of removing ambiguity from a word, i.e. correct sense of word is identified from ambiguous sentences. This paper describes a model that uses Part of Speech tagger and three categories for word sense disambiguation (WSD). Human Computer Interaction is very needful to improve interactions between users and computers. For this, the Supervised and Unsupervised methods are combined. The WSD algorithm is used to find the efficient and accurate sense of a word based on domain information. The accuracy of this work is evaluated with the aim of finding best suitable domain of word.\n    ",
        "submission_date": "2013-04-25T00:00:00",
        "last_modified_date": "2013-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7289",
        "title": "TimeML-strict: clarifying temporal annotation",
        "authors": [
            "Leon Derczynski",
            "Hector Llorens",
            "Naushad UzZaman"
        ],
        "abstract": "TimeML is an XML-based schema for annotating temporal information over discourse. The standard has been used to annotate a variety of resources and is followed by a number of tools, the creation of which constitute hundreds of thousands of man-hours of research work. However, the current state of resources is such that many are not valid, or do not produce valid output, or contain ambiguous or custom additions and removals. Difficulties arising from these variances were highlighted in the TempEval-3 exercise, which included its own extra stipulations over conventional TimeML as a response.\n",
        "submission_date": "2013-04-26T00:00:00",
        "last_modified_date": "2013-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7507",
        "title": "Measuring Cultural Relativity of Emotional Valence and Arousal using Semantic Clustering and Twitter",
        "authors": [
            "Eugene Yuta Bann",
            "Joanna J. Bryson"
        ],
        "abstract": "Researchers since at least Darwin have debated whether and to what extent emotions are universal or culture-dependent. However, previous studies have primarily focused on facial expressions and on a limited set of emotions. Given that emotions have a substantial impact on human lives, evidence for cultural emotional relativity might be derived by applying distributional semantics techniques to a text corpus of self-reported behaviour. Here, we explore this idea by measuring the valence and arousal of the twelve most popular emotion keywords expressed on the micro-blogging site Twitter. We do this in three geographical regions: Europe, Asia and North America. We demonstrate that in our sample, the valence and arousal levels of the same emotion keywords differ significantly with respect to these geographical regions --- Europeans are, or at least present themselves as more positive and aroused, North Americans are more negative and Asians appear to be more positive but less aroused when compared to global valence and arousal levels of the same emotion keywords. Our work is the first in kind to programatically map large text corpora to a dimensional model of affect.\n    ",
        "submission_date": "2013-04-28T00:00:00",
        "last_modified_date": "2013-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7728",
        "title": "Machine Translation Systems in India",
        "authors": [
            "Sugata Sanyal",
            "Rajdeep Borgohain"
        ],
        "abstract": "Machine Translation is the translation of one natural language into another using automated and computerized means. For a multilingual country like India, with the huge amount of information exchanged between various regions and in different languages in digitized format, it has become necessary to find an automated process from one language to another. In this paper, we take a look at the various Machine Translation System in India which is specifically built for the purpose of translation between the Indian languages. We discuss the various approaches taken for building the machine translation system and then discuss some of the Machine Translation Systems in India along with their features.\n    ",
        "submission_date": "2013-04-29T00:00:00",
        "last_modified_date": "2013-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7942",
        "title": "ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge",
        "authors": [
            "Michele Filannino",
            "Gavin Brown",
            "Goran Nenadic"
        ],
        "abstract": "This paper describes a temporal expression identification and normalization system, ManTIME, developed for the TempEval-3 challenge. The identification phase combines the use of conditional random fields along with a post-processing identification pipeline, whereas the normalization phase is carried out using NorMA, an open-source rule-based temporal normalizer. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance.\n    ",
        "submission_date": "2013-04-30T00:00:00",
        "last_modified_date": "2013-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0556",
        "title": "A quantum teleportation inspired algorithm produces sentence meaning from word meaning and grammatical structure",
        "authors": [
            "Stephen Clark",
            "Bob Coecke",
            "Edward Grefenstette",
            "Stephen Pulman",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "We discuss an algorithm which produces the meaning of a sentence given meanings of its words, and its resemblance to quantum teleportation. In fact, this protocol was the main source of inspiration for this algorithm which has many applications in the area of Natural Language Processing.\n    ",
        "submission_date": "2013-05-02T00:00:00",
        "last_modified_date": "2013-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1319",
        "title": "New Alignment Methods for Discriminative Book Summarization",
        "authors": [
            "David Bamman",
            "Noah A. Smith"
        ],
        "abstract": "We consider the unsupervised alignment of the full text of a book with a human-written summary. This presents challenges not seen in other text alignment problems, including a disparity in length and, consequent to this, a violation of the expectation that individual words and phrases should align, since large passages and chapters can be distilled into a single summary phrase. We present two new methods, based on hidden Markov models, specifically targeted to this problem, and demonstrate gains on an extractive book summarization task. While there is still much room for improvement, unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization.\n    ",
        "submission_date": "2013-05-06T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2680",
        "title": "A study for the effect of the Emphaticness and language and dialect for Voice Onset Time (VOT) in Modern Standard Arabic (MSA)",
        "authors": [
            "Sulaiman S. AlDahri"
        ],
        "abstract": "The signal sound contains many different features, including Voice Onset Time (VOT), which is a very important feature of stop sounds in many languages. The only application of VOT values is stopping phoneme subsets. This subset of consonant sounds is stop phonemes exist in the Arabic language, and in fact, all languages. The pronunciation of these sounds is hard and unique especially for less-educated Arabs and non-native Arabic speakers. VOT can be utilized by the human auditory system to distinguish between voiced and unvoiced stops such as /p/ and /b/ in ",
        "submission_date": "2013-05-13T00:00:00",
        "last_modified_date": "2013-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2846",
        "title": "Opportunities & Challenges In Automatic Speech Recognition",
        "authors": [
            "Rashmi Makhijani",
            "Urmila Shrawankar",
            "V M Thakare"
        ],
        "abstract": "Automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2847",
        "title": "An Overview of Hindi Speech Recognition",
        "authors": [
            "Neema Mishra",
            "Urmila Shrawankar",
            "V M Thakare"
        ],
        "abstract": "In this age of information technology, information access in a convenient manner has gained importance. Since speech is a primary mode of communication among human beings, it is natural for people to expect to be able to carry out spoken dialogue with computer. Speech recognition system permits ordinary people to speak to the computer to retrieve information. It is desirable to have a human computer dialogue in local language. Hindi being the most widely spoken Language in India is the natural primary human language candidate for human machine interaction. There are five pairs of vowels in Hindi languages; one member is longer than the other one. This paper describes an overview of speech recognition system that includes how speech is produced and the properties and characteristics of Hindi Phoneme.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.3882",
        "title": "Rule-Based Semantic Tagging. An Application Undergoing Dictionary Glosses",
        "authors": [
            "Daniel Christen"
        ],
        "abstract": "The project presented in this article aims to formalize criteria and procedures in order to extract semantic information from parsed dictionary glosses. The actual purpose of the project is the generation of a semantic network (nearly an ontology) issued from a monolingual Italian dictionary, through unsupervised procedures. Since the project involves rule-based Parsing, Semantic Tagging and Word Sense Disambiguation techniques, its outcomes may find an interest also beyond this immediate intent. The cooperation of both syntactic and semantic features in meaning construction are investigated, and procedures which allows a translation of syntactic dependencies in semantic relations are discussed. The procedures that rise from this project can be applied also to other text types than dictionary glosses, as they convert the output of a parsing process into a semantic representation. In addition some mechanism are sketched that may lead to a kind of procedural semantics, through which multiple paraphrases of an given expression can be generated. Which means that these techniques may find an application also in 'query expansion' strategies, interesting Information Retrieval, Search Engines and Question Answering Systems.\n    ",
        "submission_date": "2013-05-16T00:00:00",
        "last_modified_date": "2013-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.3981",
        "title": "Binary Tree based Chinese Word Segmentation",
        "authors": [
            "Kaixu Zhang",
            "Can Wang",
            "Maosong Sun"
        ],
        "abstract": "Chinese word segmentation is a fundamental task for Chinese language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%.\n    ",
        "submission_date": "2013-05-17T00:00:00",
        "last_modified_date": "2013-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.4561",
        "title": "Random crossings in dependency trees",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "It has been hypothesized that the rather small number of crossings in real syntactic dependency trees is a side-effect of pressure for dependency length minimization. Here we answer a related important research question: what would be the expected number of crossings if the natural order of a sentence was lost and replaced by a random ordering? We show that this number depends only on the number of vertices of the dependency tree (the sentence length) and the second moment about zero of vertex degrees. The expected number of crossings is minimum for a star tree (crossings are impossible) and maximum for a linear tree (the number of crossings is of the order of the square of the sequence length).\n    ",
        "submission_date": "2013-05-20T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5753",
        "title": "A probabilistic framework for analysing the compositionality of conceptual combinations",
        "authors": [
            "Peter D. Bruza",
            "Kirsty Kitto",
            "Brentyn J. Ramm",
            "Laurianne Sitbon"
        ],
        "abstract": "Conceptual combination performs a fundamental role in creating the broad range of compound phrases utilized in everyday language. This article provides a novel probabilistic framework for assessing whether the semantics of conceptual combinations are compositional, and so can be considered as a function of the semantics of the constituent concepts, or not. While the systematicity and productivity of language provide a strong argument in favor of assuming compositionality, this very assumption is still regularly questioned in both cognitive science and philosophy. Additionally, the principle of semantic compositionality is underspecified, which means that notions of both \"strong\" and \"weak\" compositionality appear in the literature. Rather than adjudicating between different grades of compositionality, the framework presented here contributes formal methods for determining a clear dividing line between compositional and non-compositional semantics. In addition, we suggest that the distinction between these is contextually sensitive. Utilizing formal frameworks developed for analyzing composite systems in quantum theory, we present two methods that allow the semantics of conceptual combinations to be classified as \"compositional\" or \"non-compositional\". Compositionality is first formalised by factorising the joint probability distribution modeling the combination, where the terms in the factorisation correspond to individual concepts. This leads to the necessary and sufficient condition for the joint probability distribution to exist. A failure to meet this condition implies that the underlying concepts cannot be modeled in a single probability space when considering their combination, and the combination is thus deemed \"non-compositional\". The formal analysis methods are demonstrated by applying them to an empirical study of twenty-four non-lexicalised conceptual combinations.\n    ",
        "submission_date": "2013-05-23T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5785",
        "title": "An Inventory of Preposition Relations",
        "authors": [
            "Vivek Srikumar",
            "Dan Roth"
        ],
        "abstract": "We describe an inventory of semantic relations that are expressed by prepositions. We define these relations by building on the word sense disambiguation task for prepositions and propose a mapping from preposition senses to the relation labels by collapsing semantically related senses across prepositions.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5918",
        "title": "Reduce Meaningless Words for Joint Chinese Word Segmentation and Part-of-speech Tagging",
        "authors": [
            "Kaixu Zhang",
            "Maosong Sun"
        ],
        "abstract": "Conventional statistics-based methods for joint Chinese word segmentation and part-of-speech tagging (S&T) have generalization ability to recognize new words that do not appear in the training data. An undesirable side effect is that a number of meaningless words will be incorrectly created. We propose an effective and efficient framework for S&T that introduces features to significantly reduce meaningless words generation. A general lexicon, Wikepedia and a large-scale raw corpus of 200 billion characters are used to generate word-based features for the wordhood. The word-lattice based framework consists of a character-based model and a word-based model in order to employ our word-based features. Experiments on Penn Chinese treebank 5 show that this method has a 62.9% reduction of meaningless word generation in comparison with the baseline. As a result, the F1 measure for segmentation is increased to 0.984.\n    ",
        "submission_date": "2013-05-25T00:00:00",
        "last_modified_date": "2013-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6143",
        "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes model",
        "authors": [
            "Vivek Narayanan",
            "Ishan Arora",
            "Arjun Bhatia"
        ],
        "abstract": "We have explored different methods of improving the accuracy of a Naive Bayes classifier for sentiment analysis. We observed that a combination of methods like negation handling, word n-grams and feature selection by mutual information results in a significant improvement in accuracy. This implies that a highly accurate and fast sentiment classifier can be built using a simple Naive Bayes model that has linear training and testing time complexities. We achieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.\n    ",
        "submission_date": "2013-05-27T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6211",
        "title": "Development of a Hindi Lemmatizer",
        "authors": [
            "Snigdha Paul",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "We live in a translingual society, in order to communicate with people from different parts of the world we need to have an expertise in their respective languages. Learning all these languages is not at all possible; therefore we need a mechanism which can do this task for us. Machine translators have emerged as a tool which can perform this task. In order to develop a machine translator we need to develop several different rules. The very first module that comes in machine translation pipeline is morphological analysis. Stemming and lemmatization comes under morphological analysis. In this paper we have created a lemmatizer which generates rules for removing the affixes along with the addition of rules for creating a proper root word.\n    ",
        "submission_date": "2013-05-24T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.6238",
        "title": "Extended Lambek calculi and first-order linear logic",
        "authors": [
            "Richard Moot"
        ],
        "abstract": "First-order multiplicative intuitionistic linear logic (MILL1) can be seen as an extension of the Lambek calculus. In addition to the fragment of MILL1 which corresponds to the Lambek calculus (of Moot & Piazza 2001), I will show fragments of MILL1 which generate the multiple context-free languages and which correspond to the Displacement calculus of Morrilll e.a.\n    ",
        "submission_date": "2013-05-27T00:00:00",
        "last_modified_date": "2013-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1343",
        "title": "The User Feedback on SentiWordNet",
        "authors": [
            "Andrea Esuli"
        ],
        "abstract": "With the release of SentiWordNet 3.0 the related Web interface has been restyled and improved in order to allow users to submit feedback on the SentiWordNet entries, in the form of the suggestion of alternative triplets of values for an entry. This paper reports on the release of the user feedback collected so far and on the plans for the future.\n    ",
        "submission_date": "2013-06-06T00:00:00",
        "last_modified_date": "2013-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2091",
        "title": "A framework for (under)specifying dependency syntax without overloading annotators",
        "authors": [
            "Nathan Schneider",
            "Brendan O'Connor",
            "Naomi Saphra",
            "David Bamman",
            "Manaal Faruqui",
            "Noah A. Smith",
            "Chris Dyer",
            "Jason Baldridge"
        ],
        "abstract": "We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations.\n    ",
        "submission_date": "2013-06-10T00:00:00",
        "last_modified_date": "2013-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2158",
        "title": "\"Not not bad\" is not \"bad\": A distributional account of negation",
        "authors": [
            "Karl Moritz Hermann",
            "Edward Grefenstette",
            "Phil Blunsom"
        ],
        "abstract": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.\n    ",
        "submission_date": "2013-06-10T00:00:00",
        "last_modified_date": "2013-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2838",
        "title": "The Quantum Challenge in Concept Theory and Natural Language Processing",
        "authors": [
            "Diederik Aerts",
            "Jan Broekaert",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "The mathematical formalism of quantum theory has been successfully used in human cognition to model decision processes and to deliver representations of human knowledge. As such, quantum cognition inspired tools have improved technologies for Natural Language Processing and Information Retrieval. In this paper, we overview the quantum cognition approach developed in our Brussels team during the last two decades, specifically our identification of quantum structures in human concepts and language, and the modeling of data from psychological and corpus-text-based experiments. We discuss our quantum-theoretic framework for concepts and their conjunctions/disjunctions in a Fock-Hilbert space structure, adequately modeling a large amount of data collected on concept combinations. Inspired by this modeling, we put forward elements for a quantum contextual and meaning-based approach to information technologies in which 'entities of meaning' are inversely reconstructed from texts, which are considered as traces of these entities' states.\n    ",
        "submission_date": "2013-06-12T00:00:00",
        "last_modified_date": "2013-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3584",
        "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality",
        "authors": [
            "Nal Kalchbrenner",
            "Phil Blunsom"
        ],
        "abstract": "The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment.\n    ",
        "submission_date": "2013-06-15T00:00:00",
        "last_modified_date": "2013-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3692",
        "title": "An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling",
        "authors": [
            "Felipe S\u00e1nchez-Mart\u00ednez",
            "Isabel Mart\u00ednez-Sempere",
            "Xavier Ivars-Ribes",
            "Rafael C. Carrasco"
        ],
        "abstract": "The IMPACT-es diachronic corpus of historical Spanish compiles over one hundred books --containing approximately 8 million words-- in addition to a complementary lexicon which links more than 10 thousand lemmas with attestations of the different variants found in the documents. This textual corpus and the accompanying lexicon have been released under an open license (Creative Commons by-nc-sa) in order to permit their intensive exploitation in linguistic research. Approximately 7% of the words in the corpus (a selection aimed at enhancing the coverage of the most frequent word forms) have been annotated with their lemma, part of speech, and modern equivalent. This paper describes the annotation criteria followed and the standards, based on the Text Encoding Initiative recommendations, used to the represent the texts in digital form. As an illustration of the possible synergies between diachronic textual resources and linguistic research, we describe the application of statistical machine translation techniques to infer probabilistic context-sensitive rules for the automatic modernisation of spelling. The automatic modernisation with this type of statistical methods leads to very low character error rates when the output is compared with the supervised modern version of the text.\n    ",
        "submission_date": "2013-06-16T00:00:00",
        "last_modified_date": "2013-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.3920",
        "title": "Discriminating word senses with tourist walks in complex networks",
        "authors": [
            "Thiago C. Silva",
            "Diego R. Amancio"
        ],
        "abstract": "Patterns of topological arrangement are widely used for both animal and human brains in the learning process. Nevertheless, automatic learning techniques frequently overlook these patterns. In this paper, we apply a learning technique based on the structural organization of the data in the attribute space to the problem of discriminating the senses of 10 polysemous words. Using two types of characterization of meanings, namely semantical and topological approaches, we have observed significative accuracy rates in identifying the suitable meanings in both techniques. Most importantly, we have found that the characterization based on the deterministic tourist walk improves the disambiguation process when one compares with the discrimination achieved with traditional complex networks measurements such as assortativity and clustering coefficient. To our knowledge, this is the first time that such deterministic walk has been applied to such a kind of problem. Therefore, our finding suggests that the tourist walk characterization may be useful in other related applications.\n    ",
        "submission_date": "2013-06-17T00:00:00",
        "last_modified_date": "2013-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4134",
        "title": "Dialogue System: A Brief Review",
        "authors": [
            "Suket Arora",
            "Kamaljeet Batra",
            "Sarabjit Singh"
        ],
        "abstract": "A Dialogue System is a system which interacts with human in natural language. At present many universities are developing the dialogue system in their regional language. This paper will discuss about dialogue system, its components, challenges and its evaluation. This paper helps the researchers for getting info regarding dialogues system.\n    ",
        "submission_date": "2013-06-18T00:00:00",
        "last_modified_date": "2013-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4139",
        "title": "Punjabi Language Interface to Database: a brief review",
        "authors": [
            "Preeti Verma",
            "Suket Arora",
            "Kamaljit Batra"
        ],
        "abstract": "Unlike most user-computer interfaces, a natural language interface allows users to communicate fluently with a computer system with very little preparation. Databases are often hard to use in cooperating with the users because of their rigid interface. A good NLIDB allows a user to enter commands and ask questions in native language and then after interpreting respond to the user in native language. For a large number of applications requiring interaction between humans and the computer systems, it would be convenient to provide the end-user friendly interface. Punjabi language interface to database would proof fruitful to native people of Punjab, as it provides ease to them to use various e-governance applications like Punjab Sewa, Suwidha, Online Public Utility Forms, Online Grievance Cell, Land Records Management System,legacy matters, e-District, agriculture, etc. Punjabi is the mother tongue of more than 110 million people all around the world. According to available information, Punjabi ranks 10th from top out of a total of 6,900 languages recognized internationally by the United Nations. This paper covers a brief overview of the Natural language interface to database, its different components, its advantages, disadvantages, approaches and techniques used. The paper ends with the work done on Punjabi language interface to database and future enhancements that can be done.\n    ",
        "submission_date": "2013-06-18T00:00:00",
        "last_modified_date": "2013-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4886",
        "title": "Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization",
        "authors": [
            "Luis Marujo",
            "Anatole Gershman",
            "Jaime Carbonell",
            "Robert Frederking",
            "Jo\u00e3o P. Neto"
        ],
        "abstract": "Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a \"Gold Standard\" - a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true \"Gold Standard\", we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores.\n    ",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4890",
        "title": "Key Phrase Extraction of Lightly Filtered Broadcast News",
        "authors": [
            "Luis Marujo",
            "Ricardo Ribeiro",
            "David Martins de Matos",
            "Jo\u00e3o P. Neto",
            "Anatole Gershman",
            "Jaime Carbonell"
        ],
        "abstract": "This paper explores the impact of light filtering on automatic key phrase extraction (AKE) applied to Broadcast News (BN). Key phrases are words and expressions that best characterize the content of a document. Key phrases are often used to index the document or as features in further processing. This makes improvements in AKE accuracy particularly important. We hypothesized that filtering out marginally relevant sentences from a document would improve AKE accuracy. Our experiments confirmed this hypothesis. Elimination of as little as 10% of the document sentences lead to a 2% improvement in AKE precision and recall. AKE is built over MAUI toolkit that follows a supervised learning approach. We trained and tested our AKE method on a gold standard made of 8 BN programs containing 110 manually annotated news stories. The experiments were conducted within a Multimedia Monitoring Solution (MMS) system for TV and radio news/programs, running daily, and monitoring 12 TV and 4 radio channels.\n    ",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.4908",
        "title": "Recognition of Named-Event Passages in News Articles",
        "authors": [
            "Luis Marujo",
            "Wang Ling",
            "Anatole Gershman",
            "Jaime Carbonell",
            "Jo\u00e3o P. Neto",
            "David Matos"
        ],
        "abstract": "We extend the concept of Named Entities to Named Events - commonly occurring events such as battles and earthquakes. We propose a method for finding specific passages in news articles that contain information about such events and report our preliminary evaluation results. Collecting \"Gold Standard\" data presents many problems, both practical and conceptual. We present a method for obtaining such data using the Amazon Mechanical Turk service.\n    ",
        "submission_date": "2013-06-20T00:00:00",
        "last_modified_date": "2013-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6078",
        "title": "A Computational Approach to Politeness with Application to Social Factors",
        "authors": [
            "Cristian Danescu-Niculescu-Mizil",
            "Moritz Sudhof",
            "Dan Jurafsky",
            "Jure Leskovec",
            "Christopher Potts"
        ],
        "abstract": "We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.\n    ",
        "submission_date": "2013-06-25T00:00:00",
        "last_modified_date": "2013-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6130",
        "title": "Competency Tracking for English as a Second or Foreign Language Learners",
        "authors": [
            "Robert Bishop Jr"
        ],
        "abstract": "My system utilizes the outcomes feature found in Moodle and other learning content management systems (LCMSs) to keep track of where students are in terms of what language competencies they have mastered and the competencies they need to get where they want to go. These competencies are based on the Common European Framework for (English) Language Learning. This data can be available for everyone involved with a given student's progress (e.g. educators, parents, supervisors and the students themselves). A given student's record of past accomplishments can also be meshed with those of his classmates. Not only are a student's competencies easily seen and tracked, educators can view competencies of a group of students that were achieved prior to enrollment in the class. This should make curriculum decision making easier and more efficient for educators.\n    ",
        "submission_date": "2013-06-26T00:00:00",
        "last_modified_date": "2013-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6755",
        "title": "Arabizi Detection and Conversion to Arabic",
        "authors": [
            "Kareem Darwish"
        ],
        "abstract": "Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.\n    ",
        "submission_date": "2013-06-28T00:00:00",
        "last_modified_date": "2013-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.6944",
        "title": "The DeLiVerMATH project - Text analysis in mathematics",
        "authors": [
            "Ulf Sch\u00f6neberg",
            "Wolfram Sperber"
        ],
        "abstract": "A high-quality content analysis is essential for retrieval functionalities but the manual extraction of key phrases and classification is expensive. Natural language processing provides a framework to automatize the process. Here, a machine-based approach for the content analysis of mathematical texts is described. A prototype for key phrase extraction and classification of mathematical texts is presented.\n    ",
        "submission_date": "2013-06-07T00:00:00",
        "last_modified_date": "2013-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0596",
        "title": "Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence",
        "authors": [
            "Om P. Damani"
        ],
        "abstract": "We design a new co-occurrence based word association measure by incorporating the concept of significant cooccurrence in the popular word association measure Pointwise Mutual Information (PMI). By extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light, compares well with the best known resource-heavy distributional similarity and knowledge based word association measures. We investigate the source of this performance improvement and find that of the two types of significant co-occurrence - corpus-level and document-level, the concept of corpus level significance combined with the use of document counts in place of word counts is responsible for all the performance gains observed. The concept of document level significance is not helpful for PMI adaptation.\n    ",
        "submission_date": "2013-07-02T00:00:00",
        "last_modified_date": "2013-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1662",
        "title": "Polyglot: Distributed Word Representations for Multilingual NLP",
        "authors": [
            "Rami Al-Rfou",
            "Bryan Perozzi",
            "Steven Skiena"
        ],
        "abstract": "Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.\n    ",
        "submission_date": "2013-07-05T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.1872",
        "title": "Intelligent Hybrid Man-Machine Translation Quality Estimation",
        "authors": [
            "Ibrahim Sabek",
            "Noha A. Yousri",
            "Nagwa Elmakky",
            "Mona Habib"
        ],
        "abstract": "Inferring evaluation scores based on human judgments is invaluable compared to using current evaluation metrics which are not suitable for real-time applications e.g. post-editing. However, these judgments are much more expensive to collect especially from expert translators, compared to evaluation based on indicators contrasting source and translation texts. This work introduces a novel approach for quality estimation by combining learnt confidence scores from a probabilistic inference model based on human judgments, with selective linguistic features-based scores, where the proposed inference model infers the credibility of given human ranks to solve the scarcity and inconsistency issues of human judgments. Experimental results, using challenging language-pairs, demonstrate improvement in correlation with human judgments over traditional evaluation metrics.\n    ",
        "submission_date": "2013-07-07T00:00:00",
        "last_modified_date": "2013-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3310",
        "title": "Improving the quality of Gujarati-Hindi Machine Translation through part-of-speech tagging and stemmer-assisted transliteration",
        "authors": [
            "Juhi Ameta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Machine Translation for Indian languages is an emerging research area. Transliteration is one such module that we design while designing a translation system. Transliteration means mapping of source language text into the target language. Simple mapping decreases the efficiency of overall translation system. We propose the use of stemming and part-of-speech tagging for transliteration. The effectiveness of translation can be improved if we use part-of-speech tagging and stemming assisted ",
        "submission_date": "2013-07-12T00:00:00",
        "last_modified_date": "2013-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3336",
        "title": "Opinion Mining and Analysis: A survey",
        "authors": [
            "Arti Buche",
            "Dr. M. B. Chandak",
            "Akshay Zadgaonkar"
        ],
        "abstract": "The current research is focusing on the area of Opinion Mining also called as sentiment analysis due to sheer volume of opinion rich web resources such as discussion forums, review sites and blogs are available in digital form. One important problem in sentiment analysis of product reviews is to produce summary of opinions based on product features. We have surveyed and analyzed in this paper, various techniques that have been developed for the key tasks of opinion mining. We have provided an overall picture of what is involved in developing a software system for opinion mining on the basis of our survey and analysis.\n    ",
        "submission_date": "2013-07-12T00:00:00",
        "last_modified_date": "2013-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3489",
        "title": "Genetic approach for arabic part of speech tagging",
        "authors": [
            "Bilel Ben Ali",
            "Fethi Jarray"
        ],
        "abstract": "With the growing number of textual resources available, the ability to understand them becomes critical. An essential first step in understanding these sources is the ability to identify the part of speech in each sentence. Arabic is a morphologically rich language, wich presents a challenge for part of speech tagging. In this paper, our goal is to propose, improve and implement a part of speech tagger based on a genetic alorithm. The accuracy obtained with this method is comparable to that of other probabilistic approaches.\n    ",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2013-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4299",
        "title": "Part of Speech Tagging of Marathi Text Using Trigram Method",
        "authors": [
            "Jyoti Singh",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "In this paper we present a Marathi part of speech tagger. It is a morphologically rich language. It is spoken by the native people of Maharashtra. The general approach used for development of tagger is statistical using trigram Method. The main concept of trigram is to explore the most likely POS for a token based on given information of previous two tags by calculating probabilities to determine which is the best sequence of a tag. In this paper we show the development of the tagger. Moreover we have also shown the evaluation done.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4300",
        "title": "Rule Based Transliteration Scheme for English to Punjabi",
        "authors": [
            "Deepti Bhalla",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Machine Transliteration has come out to be an emerging and a very important research area in the field of machine translation. Transliteration basically aims to preserve the phonological structure of words. Proper transliteration of name entities plays a very significant role in improving the quality of machine translation. In this paper we are doing machine transliteration for English-Punjabi language pair using rule based approach. We have constructed some rules for syllabification. Syllabification is the process to extract or separate the syllable from the words. In this we are calculating the probabilities for name entities (Proper names and location). For those words which do not come under the category of name entities, separate probabilities are being calculated by using relative frequency through a statistical machine translation toolkit known as MOSES. Using these probabilities we are transliterating our input text from English to Punjabi.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4879",
        "title": "Says who? Automatic Text-Based Content Analysis of Television News",
        "authors": [
            "Carlos Castillo",
            "Gianmarco De Francisci Morales",
            "Marcelo Mendoza",
            "Nasir Khan"
        ],
        "abstract": "We perform an automatic analysis of television news programs, based on the closed captions that accompany them. Specifically, we collect all the news broadcasted in over 140 television channels in the US during a period of six months. We start by segmenting, processing, and annotating the closed captions automatically. Next, we focus on the analysis of their linguistic style and on mentions of people using NLP methods. We present a series of key insights about news providers, people in the news, and we discuss the biases that can be uncovered by automatic means. These insights are contrasted by looking at the data from multiple points of view, including qualitative assessment.\n    ",
        "submission_date": "2013-07-18T00:00:00",
        "last_modified_date": "2013-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5336",
        "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts",
        "authors": [
            "Pekka Malo",
            "Ankur Sinha",
            "Pyry Takala",
            "Pekka Korhonen",
            "Jyrki Wallenius"
        ],
        "abstract": "The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to investigate how financial sentiments relate to future company performance. However, based on experience from other fields, where sentiment analysis is commonly applied, it is well-known that the overall semantic orientation of a sentence may differ from the prior polarity of individual words. The objective of this article is to investigate how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase-structure information and domain-specific use of language. Our three main contributions are: (1) establishment of a human-annotated finance phrase-bank, which can be used as benchmark for training and evaluating alternative models; (2) presentation of a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect overall sentiment; (3) development of a linearized phrase-structure model for detecting contextual semantic orientations in financial and economic news texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning-algorithm are demonstrated in a comparative study against previously used general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature-space caused by the use of conventional n-gram features.\n    ",
        "submission_date": "2013-07-19T00:00:00",
        "last_modified_date": "2013-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5393",
        "title": "Clustering Algorithm for Gujarati Language",
        "authors": [
            "Miral Patel",
            "Prem Balani"
        ],
        "abstract": "Natural language processing area is still under research. But now a day it is on platform for worldwide researchers. Natural language processing includes analyzing the language based on its structure and then tagging of each word appropriately with its grammar base. Here we have 50,000 tagged words set and we try to cluster those Gujarati words based on proposed algorithm, we have defined our own algorithm for processing. Many clustering techniques are available Ex. Single linkage, complete, linkage,average linkage, Hear no of clusters to be formed are not known, so it is all depends on the type of data set provided . Clustering is preprocess for stemming . Stemming is the process where root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and plural form.\n    ",
        "submission_date": "2013-07-20T00:00:00",
        "last_modified_date": "2013-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.5736",
        "title": "Speaker Independent Continuous Speech to Text Converter for Mobile Application",
        "authors": [
            "R. Sandanalakshmi",
            "P. Abinaya Viji",
            "M. Kiruthiga",
            "M. Manjari",
            "M. Sharina"
        ],
        "abstract": "An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system, VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.\n    ",
        "submission_date": "2013-07-19T00:00:00",
        "last_modified_date": "2013-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6163",
        "title": "Human and Automatic Evaluation of English-Hindi Machine Translation",
        "authors": [
            "Nisheeth Joshi",
            "Hemant Darbari",
            "Iti Mathur"
        ],
        "abstract": "For the past 60 years, Research in machine translation is going on. For the development in this field, a lot of new techniques are being developed each day. As a result, we have witnessed development of many automatic machine translators. A manager of machine translation development project needs to know the performance increase/decrease, after changes have been done in his system. Due to this reason, a need for evaluation of machine translation systems was felt. In this article, we shall present the evaluation of some machine translators. This evaluation will be done by a human evaluator and by some automatic evaluation metrics, which will be done at sentence, document and system level. In the end we shall also discuss the comparison between the evaluations.\n    ",
        "submission_date": "2013-07-23T00:00:00",
        "last_modified_date": "2013-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6726",
        "title": "Information content versus word length in natural language: A reply to Ferrer-i-Cancho and Moscoso del Prado Martin [",
        "authors": [
            "Steven T. Piantadosi",
            "Harry Tily",
            "Edward Gibson"
        ],
        "abstract": "Recently, Ferrer i Cancho and Moscoso del Prado Martin [",
        "submission_date": "2013-07-25T00:00:00",
        "last_modified_date": "2013-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7382",
        "title": "Learning Frames from Text with an Unsupervised Latent Variable Model",
        "authors": [
            "Brendan O'Connor"
        ],
        "abstract": "We develop a probabilistic latent-variable model to discover semantic frames---types of events and their participants---from corpora. We present a Dirichlet-multinomial model in which frames are latent categories that explain the linking of verb-subject-object triples, given document-level sparsity. We analyze what the model learns, and compare it to FrameNet, noting it learns some novel and interesting frames. This document also contains a discussion of inference issues, including concentration parameter learning; and a small-scale error analysis of syntactic parsing accuracy.\n    ",
        "submission_date": "2013-07-28T00:00:00",
        "last_modified_date": "2013-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.7973",
        "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction",
        "authors": [
            "Jason Weston",
            "Antoine Bordes",
            "Oksana Yakhnenko",
            "Nicolas Usunier"
        ],
        "abstract": "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8057",
        "title": "Extracting Connected Concepts from Biomedical Texts using Fog Index",
        "authors": [
            "Rushdi Shams",
            "Robert E. Mercer"
        ],
        "abstract": "In this paper, we establish Fog Index (FI) as a text filter to locate the sentences in texts that contain connected biomedical concepts of interest. To do so, we have used 24 random papers each containing four pairs of connected concepts. For each pair, we categorize sentences based on whether they contain both, any or none of the concepts. We then use FI to measure difficulty of the sentences of each category and find that sentences containing both of the concepts have low readability. We rank sentences of a text according to their FI and select 30 percent of the most difficult sentences. We use an association matrix to track the most frequent pairs of concepts in them. This matrix reports that the first filter produces some pairs that hold almost no connections. To remove these unwanted pairs, we use the Equally Weighted Harmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a second filter. Experimental results demonstrate the effectiveness of our method.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0658",
        "title": "Exploring The Contribution of Unlabeled Data in Financial Sentiment Analysis",
        "authors": [
            "Jimmy SJ. Ren",
            "Wei Wang",
            "Jiawei Wang",
            "Stephen Shaoyi Liao"
        ],
        "abstract": "With the proliferation of its applications in various industries, sentiment analysis by using publicly available web data has become an active research area in text classification during these years. It is argued by researchers that semi-supervised learning is an effective approach to this problem since it is capable to mitigate the manual labeling effort which is usually expensive and time-consuming. However, there was a long-term debate on the effectiveness of unlabeled data in text classification. This was partially caused by the fact that many assumptions in theoretic analysis often do not hold in practice. We argue that this problem may be further understood by adding an additional dimension in the experiment. This allows us to address this problem in the perspective of bias and variance in a broader view. We show that the well-known performance degradation issue caused by unlabeled data can be reproduced as a subset of the whole scenario. We argue that if the bias-variance trade-off is to be better balanced by a more effective feature selection method unlabeled data is very likely to boost the classification performance. We then propose a feature selection framework in which labeled and unlabeled training samples are both considered. We discuss its potential in achieving such a balance. Besides, the application in financial sentiment analysis is chosen because it not only exemplifies an important application, the data possesses better illustrative power as well. The implications of this study in text classification and financial sentiment analysis are both discussed.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0897",
        "title": "Context Specific Event Model For News Articles",
        "authors": [
            "Kowcika A",
            "Uma Maheswari",
            "Geetha T V"
        ],
        "abstract": "We present a new context based event indexing and event ranking model for News Articles. The context event clusters formed from the UNL Graphs uses the modified scoring scheme for segmenting events which is followed by clustering of events. From the context clusters obtained three models are developed- Identification of Main and Sub events; Event Indexing and Event Ranking. Based on the properties considered from the UNL Graphs for the modified scoring main events and sub events associated with main-events are identified. The temporal details obtained from the context cluster are stored using hashmap data structure. The temporal details are place-where the event took; person-who involved in that event; time-when the event took place. Based on the information collected from the context clusters three indices are generated- Time index, Person index, and Place index. This index gives complete details about every event obtained from context clusters. A new scoring scheme is introduced for ranking the events. The scoring scheme for event ranking gives weight-age based on the priority level of the events. The priority level includes the occurrence of the event in the title of the document, event frequency, and inverse document frequency of the events.\n    ",
        "submission_date": "2013-08-05T00:00:00",
        "last_modified_date": "2013-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1004",
        "title": "Boundary identification of events in clinical named entity recognition",
        "authors": [
            "Azad Dehghan"
        ],
        "abstract": "The problem of named entity recognition in the medical/clinical domain has gained increasing attention do to its vital role in a wide range of clinical decision support applications. The identification of complete and correct term span is vital for further knowledge synthesis (e.g., coding/mapping concepts thesauruses and classification standards). This paper investigates boundary adjustment by sequence labeling representations models and post-processing techniques in the problem of clinical named entity recognition (recognition of clinical events). Using current state-of-the-art sequence labeling algorithm (conditional random fields), we show experimentally that sequence labeling representation and post-processing can be significantly helpful in strict boundary identification of clinical events.\n    ",
        "submission_date": "2013-08-05T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1507",
        "title": "Logical analysis of natural language semantics to solve the problem of computer understanding",
        "authors": [
            "Yuriy Ostapov"
        ],
        "abstract": "An object--oriented approach to create a natural language understanding system is considered. The understanding program is a formal system built on the base of predicative calculus. Horn's clauses are used as well--formed formulas. An inference is based on the principle of resolution. Sentences of natural language are represented in the view of typical predicate set. These predicates describe physical objects and processes, abstract objects, categories and semantic relations between objects. Predicates for concrete assertions are saved in a database. To describe the semantics of classes for physical objects, abstract concepts and processes, a knowledge base is applied. The proposed representation of natural language sentences is a semantic net. Nodes of such net are typical predicates. This approach is perspective as, firstly, such typification of nodes facilitates essentially forming of processing algorithms and object descriptions, secondly, the effectiveness of algorithms is increased (particularly for the great number of nodes), thirdly, to describe the semantics of words, encyclopedic knowledge is used, and this permits essentially to extend the class of solved problems.\n    ",
        "submission_date": "2013-08-07T00:00:00",
        "last_modified_date": "2013-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1847",
        "title": "The Royal Birth of 2013: Analysing and Visualising Public Sentiment in the UK Using Twitter",
        "authors": [
            "Vu Dung Nguyen",
            "Blesson Varghese",
            "Adam Barker"
        ],
        "abstract": "Analysis of information retrieved from microblogging services such as Twitter can provide valuable insight into public sentiment in a geographic region. This insight can be enriched by visualising information in its geographic context. Two underlying approaches for sentiment analysis are dictionary-based and machine learning. The former is popular for public sentiment analysis, and the latter has found limited use for aggregating public sentiment from Twitter data. The research presented in this paper aims to extend the machine learning approach for aggregating public sentiment. To this end, a framework for analysing and visualising public sentiment from a Twitter corpus is developed. A dictionary-based approach and a machine learning approach are implemented within the framework and compared using one UK case study, namely the royal birth of 2013. The case study validates the feasibility of the framework for analysis and rapid visualisation. One observation is that there is good correlation between the results produced by the popular dictionary-based approach and the machine learning approach when large volumes of tweets are analysed. However, for rapid analysis to be possible faster methods need to be developed using big data techniques and parallel methods.\n    ",
        "submission_date": "2013-08-08T00:00:00",
        "last_modified_date": "2013-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2359",
        "title": "Exploratory Analysis of Highly Heterogeneous Document Collections",
        "authors": [
            "Arun S. Maiya",
            "John P. Thompson",
            "Francisco Loaiza-Lemos",
            "Robert M. Rolfe"
        ],
        "abstract": "We present an effective multifaceted system for exploratory analysis of highly heterogeneous document collections. Our system is based on intelligently tagging individual documents in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework. Tagging strategies employed include both unsupervised and supervised approaches based on machine learning and natural language processing. As one of our key tagging strategies, we introduce the KERA algorithm (Keyword Extraction for Reports and Articles). KERA extracts topic-representative terms from individual documents in a purely unsupervised fashion and is revealed to be significantly more effective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a large heterogeneous sea of information.\n    ",
        "submission_date": "2013-08-11T00:00:00",
        "last_modified_date": "2013-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2428",
        "title": "Hidden Structure and Function in the Lexicon",
        "authors": [
            "Olivier Picard",
            "M\u00e9lanie Lord",
            "Alexandre Blondin-Mass\u00e9",
            "Odile Marcotte",
            "Marcos Lopes",
            "Stevan Harnad"
        ],
        "abstract": "How many words are needed to define all the words in a dictionary? Graph-theoretic analysis reveals that about 10% of a dictionary is a unique Kernel of words that define one another and all the rest, but this is not the smallest such subset. The Kernel consists of one huge strongly connected component (SCC), about half its size, the Core, surrounded by many small SCCs, the Satellites. Core words can define one another but not the rest of the dictionary. The Kernel also contains many overlapping Minimal Grounding Sets (MGSs), each about the same size as the Core, each part-Core, part-Satellite. MGS words can define all the rest of the dictionary. They are learned earlier, more concrete and more frequent than the rest of the dictionary. Satellite words, not correlated with age or frequency, are less concrete (more abstract) words that are also needed for full lexical power.\n    ",
        "submission_date": "2013-08-11T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.2696",
        "title": "B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language",
        "authors": [
            "A. Paxton",
            "R. Dale"
        ],
        "abstract": "Discourse analysis may seek to characterize not only the overall composition of a given text but also the dynamic patterns within the data. This technical report introduces a data format intended to facilitate multi-level investigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by the long-form data format required for mixed-effects modeling, B(eo)W(u)LF structures linguistic data into an expanded matrix encoding any number of researchers-specified markers, making it ideal for recurrence-based analyses. While we do not necessarily claim to be the first to use methods along these lines, we have created a series of tools utilizing Python and MATLAB to enable such discourse analyses and demonstrate them using 319 lines of the Old English epic poem, Beowulf, translated into modern English.\n    ",
        "submission_date": "2013-08-12T00:00:00",
        "last_modified_date": "2013-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3106",
        "title": "System and Methods for Converting Speech to SQL",
        "authors": [
            "Sachin Kumar",
            "Ashish Kumar",
            "Pinaki Mitra",
            "Girish Sundaram"
        ],
        "abstract": "This paper concerns with the conversion of a Spoken English Language Query into SQL for retrieving data from RDBMS. A User submits a query as speech signal through the user interface and gets the result of the query in the text format. We have developed the acoustic and language models using which a speech utterance can be converted into English text query and thus natural language processing techniques can be applied on this English text query to generate an equivalent SQL query. For conversion of speech into English text HTK and Julius tools have been used and for conversion of English text query into SQL query we have implemented a System which uses rule based translation to translate English Language Query into SQL Query. The translation uses lexical analyzer, parser and syntax directed translation techniques like in compilers. JFLex and BYACC tools have been used to build lexical analyzer and parser respectively. System is domain independent i.e. system can run on different database as it generates lex files from the underlying database.\n    ",
        "submission_date": "2013-08-14T00:00:00",
        "last_modified_date": "2013-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3785",
        "title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla Speech Recognition",
        "authors": [
            "Md. Ali Hossain",
            "Md. Mijanur Rahman",
            "Uzzal Kumar Prodhan",
            "Md. Farukuzzaman Khan"
        ],
        "abstract": "This paper is concerned with the development of Back-propagation Neural Network for Bangla Speech Recognition. In this paper, ten bangla digits were recorded from ten speakers and have been recognized. The features of these speech digits were extracted by the method of Mel Frequency Cepstral Coefficient (MFCC) analysis. The mfcc features of five speakers were used to train the network with Back propagation algorithm. The mfcc features of ten bangla digit speeches, from 0 to 9, of another five speakers were used to test the system. All the methods and algorithms used in this research were implemented using the features of Turbo C and C++ languages. From our investigation it is seen that the developed system can successfully encode and analyze the mfcc features of the speech signal to recognition. The developed system achieved recognition rate about 96.332% for known speakers (i.e., speaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n    ",
        "submission_date": "2013-08-17T00:00:00",
        "last_modified_date": "2013-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3830",
        "title": "Natural Language Web Interface for Database (NLWIDB)",
        "authors": [
            "Rukshan Alexander",
            "Prashanthi Rukshan",
            "Sinnathamby Mahesan"
        ],
        "abstract": "It is a long term desire of the computer users to minimize the communication gap between the computer and a human. On the other hand, almost all ICT applications store information in to databases and retrieve from them. Retrieving information from the database requires knowledge of technical languages such as Structured Query Language. However majority of the computer users who interact with the databases do not have a technical background and are intimidated by the idea of using languages such as SQL. For above reasons, a Natural Language Web Interface for Database (NLWIDB) has been developed. The NLWIDB allows the user to query the database in a language more like English, through a convenient interface over the Internet.\n    ",
        "submission_date": "2013-08-18T00:00:00",
        "last_modified_date": "2013-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3839",
        "title": "Consensus Sequence Segmentation",
        "authors": [
            "Tamal Chowdhury",
            "Rabindra Rakshit",
            "Arko Banerjee"
        ],
        "abstract": "In this paper we introduce a method to detect words or phrases in a given sequence of alphabets without knowing the lexicon. Our linear time unsupervised algorithm relies entirely on statistical relationships among alphabets in the input sequence to detect location of word boundaries. We compare our algorithm to previous approaches from unsupervised sequence segmentation literature and provide superior segmentation over number of benchmarks.\n    ",
        "submission_date": "2013-08-18T00:00:00",
        "last_modified_date": "2013-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4479",
        "title": "An Investigation of the Sampling-Based Alignment Method and Its Contributions",
        "authors": [
            "Juan Luo",
            "Yves Lepage"
        ],
        "abstract": "By investigating the distribution of phrase pairs in phrase translation tables, the work in this paper describes an approach to increase the number of n-gram alignments in phrase translation tables output by a sampling-based alignment method. This approach consists in enforcing the alignment of n-grams in distinct translation subtables so as to increase the number of n-grams. Standard normal distribution is used to allot alignment time among translation subtables, which results in adjustment of the distribution of n- grams. This leads to better evaluation results on statistical machine translation tasks than the original sampling-based alignment approach. Furthermore, the translation quality obtained by merging phrase translation tables computed from the sampling-based alignment method and from MGIZA++ is examined.\n    ",
        "submission_date": "2013-08-21T00:00:00",
        "last_modified_date": "2013-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4618",
        "title": "Can inferred provenance and its visualisation be used to detect erroneous annotation? A case study using UniProtKB",
        "authors": [
            "Michael J. Bell",
            "Matthew Collison",
            "Phillip Lord"
        ],
        "abstract": "A constant influx of new data poses a challenge in keeping the annotation in biological databases current. Most biological databases contain significant quantities of textual annotation, which often contains the richest source of knowledge. Many databases reuse existing knowledge, during the curation process annotations are often propagated between entries. However, this is often not made explicit. Therefore, it can be hard, potentially impossible, for a reader to identify where an annotation originated from. Within this work we attempt to identify annotation provenance and track its subsequent propagation. Specifically, we exploit annotation reuse within the UniProt Knowledgebase (UniProtKB), at the level of individual sentences. We describe a visualisation approach for the provenance and propagation of sentences in UniProtKB which enables a large-scale statistical analysis. Initially levels of sentence reuse within UniProtKB were analysed, showing that reuse is heavily prevalent, which enables the tracking of provenance and propagation. By analysing sentences throughout UniProtKB, a number of interesting propagation patterns were identified, covering over 100, 000 sentences. Over 8000 sentences remain in the database after they have been removed from the entries where they originally occurred. Analysing a subset of these sentences suggest that approximately 30% are erroneous, whilst 35% appear to be inconsistent. These results suggest that being able to visualise sentence propagation and provenance can aid in the determination of the accuracy and quality of textual annotation. Source code and supplementary data are available from the authors website.\n    ",
        "submission_date": "2013-08-21T00:00:00",
        "last_modified_date": "2013-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5423",
        "title": "A Literature Review: Stemming Algorithms for Indian Languages",
        "authors": [
            "M.Thangarasu",
            "R.Manavalan"
        ],
        "abstract": "Stemming is the process of extracting root word from the given inflection word. It also plays significant role in numerous application of Natural Language Processing (NLP). The stemming problem has addressed in many contexts and by researchers in many disciplines. This expository paper presents survey of some of the latest developments on stemming algorithms in data mining and also presents with some of the solutions for various Indian language stemming algorithms along with the results.\n    ",
        "submission_date": "2013-08-25T00:00:00",
        "last_modified_date": "2013-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5499",
        "title": "Linear models and linear mixed effects models in R with linguistic applications",
        "authors": [
            "Bodo Winter"
        ],
        "abstract": "This text is a conceptual introduction to mixed effects modeling with linguistic applications, using the R programming environment. The reader is introduced to linear modeling and assumptions, as well as to mixed effects/multilevel modeling, including a discussion of random intercepts, random slopes and likelihood ratio tests. The example used throughout the text focuses on the phonetic analysis of voice pitch data.\n    ",
        "submission_date": "2013-08-26T00:00:00",
        "last_modified_date": "2013-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6242",
        "title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets",
        "authors": [
            "Saif M. Mohammad",
            "Svetlana Kiritchenko",
            "Xiaodan Zhu"
        ],
        "abstract": "In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources.\n    ",
        "submission_date": "2013-08-28T00:00:00",
        "last_modified_date": "2013-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6297",
        "title": "Crowdsourcing a Word-Emotion Association Lexicon",
        "authors": [
            "Saif M. Mohammad",
            "Peter D. Turney"
        ],
        "abstract": "Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word-emotion and word-polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion-annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher inter-annotator agreement than that obtained by asking if a term evokes an emotion.\n    ",
        "submission_date": "2013-08-28T00:00:00",
        "last_modified_date": "2013-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6300",
        "title": "Computing Lexical Contrast",
        "authors": [
            "Saif M. Mohammad",
            "Bonnie J. Dorr",
            "Graeme Hirst",
            "Peter D. Turney"
        ],
        "abstract": "Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually-created lexicons focus on opposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such as antipodals, complementaries, and gradable. However, existing lexicons often do not classify opposites into the different kinds. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm freezing}. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, $A$ and $B$, are contrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and $C$ are strongly related and $B$ and $D$ are strongly related. (For example, there exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm tropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm cold}.) We will call this the contrast hypothesis. We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we flesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods.\n    ",
        "submission_date": "2013-08-28T00:00:00",
        "last_modified_date": "2013-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.0326",
        "title": "Tagging Scientific Publications using Wikipedia and Natural Language Processing Tools. Comparison on the ArXiv Dataset",
        "authors": [
            "Micha\u0142 \u0141opuszy\u0144ski",
            "\u0141ukasz Bolikowski"
        ],
        "abstract": "In this work, we compare two simple methods of tagging scientific publications with labels reflecting their content. As a first source of labels Wikipedia is employed, second label set is constructed from the noun phrases occurring in the analyzed corpus. We examine the statistical properties and the effectiveness of both approaches on the dataset consisting of abstracts from 0.7 million of scientific documents deposited in the ArXiv preprint collection. We believe that obtained tags can be later on applied as useful document features in various machine learning tasks (document similarity, clustering, topic modelling, etc.).\n    ",
        "submission_date": "2013-09-02T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1014",
        "title": "Advances in the Logical Representation of Lexical Semantics",
        "authors": [
            "Bruno Mery",
            "Christian Retor\u00e9"
        ],
        "abstract": "The integration of lexical semantics and pragmatics in the analysis of the meaning of natural lan- guage has prompted changes to the global framework derived from Montague. In those works, the original lexicon, in which words were assigned an atomic type of a single-sorted logic, has been re- placed by a set of many-facetted lexical items that can compose their meaning with salient contextual properties using a rich typing system as a guide. Having related our proposal for such an expanded framework \\LambdaTYn, we present some recent advances in the logical formalisms associated, including constraints on lexical transformations and polymorphic quantifiers, and ongoing discussions and research on the granularity of the type system and the limits of transitivity.\n    ",
        "submission_date": "2013-09-04T00:00:00",
        "last_modified_date": "2013-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1125",
        "title": "Learning to answer questions",
        "authors": [
            "Ana Cristina Mendes",
            "Lu\u00edsa Coheur",
            "S\u00e9rgio Curto"
        ],
        "abstract": "We present an open-domain Question-Answering system that learns to answer questions based on successful past interactions. We follow a pattern-based approach to Answer-Extraction, where (lexico-syntactic) patterns that relate a question to its answer are automatically learned and used to answer future questions. Results show that our approach contributes to the system's best performance when it is conjugated with typical Answer-Extraction strategies. Moreover, it allows the system to learn with the answered questions and to rectify wrong or unsolved past questions.\n    ",
        "submission_date": "2013-09-04T00:00:00",
        "last_modified_date": "2013-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1129",
        "title": "Analysing Quality of English-Hindi Machine Translation Engine Outputs Using Bayesian Classification",
        "authors": [
            "Rashmi Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "This paper considers the problem for estimating the quality of machine translation outputs which are independent of human intervention and are generally addressed using machine learning ",
        "submission_date": "2013-09-04T00:00:00",
        "last_modified_date": "2013-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1536",
        "title": "Rank-frequency relation for Chinese characters",
        "authors": [
            "W.B. Deng",
            "A.E. Allahverdyan",
            "B. Li",
            "Q.A. Wang"
        ],
        "abstract": "We show that the Zipf's law for Chinese characters perfectly holds for sufficiently short texts (few thousand different characters). The scenario of its validity is similar to the Zipf's law for words in short English texts. For long Chinese texts (or for mixtures of short Chinese texts), rank-frequency relations for Chinese characters display a two-layer, hierarchic structure that combines a Zipfian power-law regime for frequent characters (first layer) with an exponential-like regime for less frequent characters (second layer). For these two layers we provide different (though related) theoretical descriptions that include the range of low-frequency characters (hapax legomena). The comparative analysis of rank-frequency relations for Chinese characters versus English words illustrates the extent to which the characters play for Chinese writers the same role as the words for those writing within alphabetical systems.\n    ",
        "submission_date": "2013-09-06T00:00:00",
        "last_modified_date": "2014-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1649",
        "title": "Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages",
        "authors": [
            "Jinho D. Choi"
        ],
        "abstract": "This document gives a brief description of Korean data prepared for the SPMRL 2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for the shared task. All constituent trees are collected from the KAIST Treebank and transformed to the Penn Treebank style. All dependency trees are converted from the transformed constituent trees using heuristics and labeling rules de- signed specifically for the KAIST Treebank. In addition to the gold-standard morphological analysis provided by the KAIST Treebank, two sets of automatic morphological analysis are provided for the shared task, one is generated by the HanNanum morphological analyzer, and the other is generated by the Sejong morphological analyzer.\n    ",
        "submission_date": "2013-09-06T00:00:00",
        "last_modified_date": "2013-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1939",
        "title": "The placement of the head that minimizes online memory: a complex systems approach",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "It is well known that the length of a syntactic dependency determines its online memory cost. Thus, the problem of the placement of a head and its dependents (complements or modifiers) that minimizes online memory is equivalent to the problem of the minimum linear arrangement of a star tree. However, how that length is translated into cognitive cost is not known. This study shows that the online memory cost is minimized when the head is placed at the center, regardless of the function that transforms length into cost, provided only that this function is strictly monotonically increasing. Online memory defines a quasi-convex adaptive landscape with a single central minimum if the number of elements is odd and two central minima if that number is even. We discuss various aspects of the dynamics of word order of subject (S), verb (V) and object (O) from a complex systems perspective and suggest that word orders tend to evolve by swapping adjacent constituents from an initial or early SOV configuration that is attracted towards a central word order by online memory minimization. We also suggest that the stability of SVO is due to at least two factors, the quasi-convex shape of the adaptive landscape in the online memory dimension and online memory adaptations that avoid regression to SOV. Although OVS is also optimal for placing the verb at the center, its low frequency is explained by its long distance to the seminal SOV in the permutation space.\n    ",
        "submission_date": "2013-09-08T00:00:00",
        "last_modified_date": "2015-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2471",
        "title": "Implementation of nlization framework for verbs, pronouns and determiners with eugene",
        "authors": [
            "Harinder Singh",
            "Parteek Kumar"
        ],
        "abstract": "UNL system is designed and implemented by a nonprofit organization, UNDL Foundation at Geneva in 1999. UNL applications are application softwares that allow end users to accomplish natural language tasks, such as translating, summarizing, retrieving or extracting information, etc. Two major web based application softwares are Interactive ANalyzer (IAN), which is a natural language analysis system. It represents natural language sentences as semantic networks in the UNL format. Other application software is dEep-to-sUrface GENErator (EUGENE), which is an open-source interactive NLizer. It generates natural language sentences out of semantic networks represented in the UNL format. In this paper, NLization framework with EUGENE is focused, while using UNL system for accomplishing the task of machine translation. In whole NLization process, EUGENE takes a UNL input and delivers an output in natural language without any human intervention. It is language-independent and has to be parametrized to the natural language input through a dictionary and a grammar, provided as separate interpretable files. In this paper, it is explained that how UNL input is syntactically and semantically analyzed with the UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns and determiners for Punjabi natural language.\n    ",
        "submission_date": "2013-09-10T00:00:00",
        "last_modified_date": "2013-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.2853",
        "title": "General Purpose Textual Sentiment Analysis and Emotion Detection Tools",
        "authors": [
            "Alexandre Denis",
            "Samuel Cruz-Lara",
            "Nadia Bellalem"
        ],
        "abstract": "Textual sentiment analysis and emotion detection consists in retrieving the sentiment or emotion carried by a text or document. This task can be useful in many domains: opinion mining, prediction, feedbacks, etc. However, building a general purpose tool for doing sentiment analysis and emotion detection raises a number of issues, theoretical issues like the dependence to the domain or to the language but also pratical issues like the emotion representation for interoperability. In this paper we present our sentiment/emotion analysis tools, the way we propose to circumvent the di culties and the applications they are used for.\n    ",
        "submission_date": "2013-09-11T00:00:00",
        "last_modified_date": "2013-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3323",
        "title": "Mapping Mutable Genres in Structurally Complex Volumes",
        "authors": [
            "Ted Underwood",
            "Michael L. Black",
            "Loretta Auvil",
            "Boris Capitanu"
        ],
        "abstract": "To mine large digital libraries in humanistically meaningful ways, scholars need to divide them by genre. This is a task that classification algorithms are well suited to assist, but they need adjustment to address the specific challenges of this domain. Digital libraries pose two problems of scale not usually found in the article datasets used to test these algorithms. 1) Because libraries span several centuries, the genres being identified may change gradually across the time axis. 2) Because volumes are much longer than articles, they tend to be internally heterogeneous, and the classification task needs to begin with segmentation. We describe a multi-layered solution that trains hidden Markov models to segment volumes, and uses ensembles of overlapping classifiers to address historical change. We test this approach on a collection of 469,200 volumes drawn from HathiTrust Digital Library. To demonstrate the humanistic value of these methods, we extract 32,209 volumes of fiction from the digital library, and trace the changing proportions of first- and third-person narration in the corpus. We note that narrative points of view seem to have strong associations with particular themes and genres.\n    ",
        "submission_date": "2013-09-12T00:00:00",
        "last_modified_date": "2013-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4035",
        "title": "Domain and Function: A Dual-Space Model of Semantic Relations and Compositions",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4058",
        "title": "Why SOV might be initially preferred and then lost or recovered? A theoretical framework",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Little is known about why SOV order is initially preferred and then discarded or recovered. Here we present a framework for understanding these and many related word order phenomena: the diversity of dominant orders, the existence of free words orders, the need of alternative word orders and word order reversions and cycles in evolution. Under that framework, word order is regarded as a multiconstraint satisfaction problem in which at least two constraints are in conflict: online memory minimization and maximum predictability.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4168",
        "title": "Exploiting Similarities among Languages for Machine Translation",
        "authors": [
            "Tomas Mikolov",
            "Quoc V. Le",
            "Ilya Sutskever"
        ],
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.\n    ",
        "submission_date": "2013-09-17T00:00:00",
        "last_modified_date": "2013-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.4628",
        "title": "Text segmentation with character-level text embeddings",
        "authors": [
            "Grzegorz Chrupa\u0142a"
        ],
        "abstract": "Learning word representations has recently seen much success in computational linguistics. However, assuming sequences of word tokens as input to linguistic analysis is often unjustified. For many languages word segmentation is a non-trivial task and naturally occurring text is sometimes a mixture of natural language strings and other character data. We propose to learn text representations directly from raw character sequences by training a Simple recurrent Network to predict the next character in text. The network uses its hidden layer to evolve abstract representations of the character sequences it sees. To demonstrate the usefulness of the learned text embeddings, we use them as features in a supervised character level text segmentation and labeling task: recognizing spans of text containing programming language code. By using the embeddings as features we are able to substantially improve over a baseline which uses only surface character n-grams.\n    ",
        "submission_date": "2013-09-18T00:00:00",
        "last_modified_date": "2013-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5223",
        "title": "JRC EuroVoc Indexer JEX - A freely available multi-label categorisation tool",
        "authors": [
            "Ralf Steinberger",
            "Mohamed Ebrahim",
            "Marco Turchi"
        ],
        "abstract": "EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700 hierarchically organised subject domains used by European Institutions and many authorities in Member States of the European Union (EU) for the classification and retrieval of official documents. JEX is JRC-developed multi-label classification software that learns from manually labelled data to automatically assign EuroVoc descriptors to new documents in a profile-based category-ranking task. The JEX release consists of trained classifiers for 22 official EU languages, of parallel training data in the same languages, of an interface that allows viewing and amending the assignment results, and of a module that allows users to re-train the tool on their own document collections. JEX allows advanced users to change the document representation so as to possibly improve the categorisation result through linguistic pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor assignment to increase speed and consistency of the human categorisation process, or it can be used fully automatically. The output of JEX is a language-independent EuroVoc feature vector lending itself also as input to various other Language Technology tasks, including cross-lingual clustering and classification, cross-lingual plagiarism detection, sentence selection and ranking, and more.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5226",
        "title": "DGT-TM: A freely Available Translation Memory in 22 Languages",
        "authors": [
            "Ralf Steinberger",
            "Andreas Eisele",
            "Szymon Klocek",
            "Spyridon Pilos",
            "Patrick Schl\u00fcter"
        ],
        "abstract": "The European Commission's (EC) Directorate General for Translation, together with the EC's Joint Research Centre, is making available a large translation memory (TM; i.e. sentences and their professionally produced translations) covering twenty-two official European Union (EU) languages and their 231 language pairs. Such a resource is typically used by translation professionals in combination with TM software to improve speed and consistency of their translations. However, this resource has also many uses for translation studies and for language technology applications, including Statistical Machine Translation (SMT), terminology extraction, Named Entity Recognition (NER), multilingual classification and clustering, and many more. In this reference paper for DGT-TM, we introduce this new resource, provide statistics regarding its size, and explain how it was produced and how to use it.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5290",
        "title": "An introduction to the Europe Media Monitor family of applications",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Erik van der Goot"
        ],
        "abstract": "Most large organizations have dedicated departments that monitor the media to keep up-to-date with relevant developments and to keep an eye on how they are represented in the news. Part of this media monitoring work can be automated. In the European Union with its 23 official languages, it is particularly important to cover media reports in many languages in order to capture the complementary news content published in the different countries. It is also important to be able to access the news content across languages and to merge the extracted information. We present here the four publicly accessible systems of the Europe Media Monitor (EMM) family of applications, which cover between 19 and 50 languages (see ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5319",
        "title": "Recognizing Speech in a Novel Accent: The Motor Theory of Speech Perception Reframed",
        "authors": [
            "Cl\u00e9ment Moulin-Frier",
            "M. A. Arbib"
        ],
        "abstract": "The motor theory of speech perception holds that we perceive the speech of another in terms of a motor representation of that speech. However, when we have learned to recognize a foreign accent, it seems plausible that recognition of a word rarely involves reconstruction of the speech gestures of the speaker rather than the listener. To better assess the motor theory and this observation, we proceed in three stages. Part 1 places the motor theory of speech perception in a larger framework based on our earlier models of the adaptive formation of mirror neurons for grasping, and for viewing extensions of that mirror system as part of a larger system for neuro-linguistic processing, augmented by the present consideration of recognizing speech in a novel accent. Part 2 then offers a novel computational model of how a listener comes to understand the speech of someone speaking the listener's native language with a foreign accent. The core tenet of the model is that the listener uses hypotheses about the word the speaker is currently uttering to update probabilities linking the sound produced by the speaker to phonemes in the native language repertoire of the listener. This, on average, improves the recognition of later words. This model is neutral regarding the nature of the representations it uses (motor vs. auditory). It serve as a reference point for the discussion in Part 3, which proposes a dual-stream neuro-linguistic architecture to revisits claims for and against the motor theory of speech perception and the relevance of mirror neurons, and extracts some implications for the reframing of the motor theory.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5391",
        "title": "Even the Abstract have Colour: Consensus in Word-Colour Associations",
        "authors": [
            "Saif M. Mohammad"
        ],
        "abstract": "Colour is a key component in the successful dissemination of information. Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept-colour associations. We present a method to create a large word-colour association lexicon by crowdsourcing. A word-choice question was used to obtain sense-level annotations and to ensure data quality. We focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. Thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5652",
        "title": "LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual",
        "authors": [
            "Mona Diab",
            "Nizar Habash",
            "Owen Rambow",
            "Ryan Roth"
        ],
        "abstract": "The Linguistic Data Consortium (LDC) has developed hundreds of data corpora for natural language processing (NLP) research. Among these are a number of annotated treebank corpora for Arabic. Typically, these corpora consist of a single collection of annotated documents. NLP research, however, usually requires multiple data sets for the purposes of training models, developing techniques, and final evaluation. Therefore it becomes necessary to divide the corpora used into the required data sets (divisions). This document details a set of rules that have been defined to enable consistent divisions for old and new Arabic treebanks (ATB) and related corpora.\n    ",
        "submission_date": "2013-09-22T00:00:00",
        "last_modified_date": "2013-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5657",
        "title": "A Hybrid Algorithm for Matching Arabic Names",
        "authors": [
            "T.El-Shishtawy"
        ],
        "abstract": "In this paper, a new hybrid algorithm which combines both of token-based and character-based approaches is presented. The basic Levenshtein approach has been extended to token-based distance metric. The distance metric is enhanced to set the proper granularity level behavior of the algorithm. It smoothly maps a threshold of misspellings differences at the character level, and the importance of token level errors in terms of token's position and frequency. Using a large Arabic dataset, the experimental results show that the proposed algorithm overcomes successfully many types of errors such as: typographical errors, omission or insertion of middle name components, omission of non-significant popular name components, and different writing styles character variations. When compared the results with other classical algorithms, using the same dataset, the proposed algorithm was found to increase the minimum success level of best tested algorithms, while achieving higher upper limits .\n    ",
        "submission_date": "2013-09-22T00:00:00",
        "last_modified_date": "2013-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5843",
        "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet",
        "authors": [
            "Marco Guerini",
            "Lorenzo Gatti",
            "Marco Turchi"
        ],
        "abstract": "Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-of-the-art approach in computing words' prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.\n    ",
        "submission_date": "2013-09-23T00:00:00",
        "last_modified_date": "2013-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5909",
        "title": "From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales",
        "authors": [
            "Saif Mohammad"
        ],
        "abstract": "Today we have access to unprecedented amounts of literary texts. However, search still relies heavily on key words. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in both individual books and across very large collections. We introduce the concept of emotion word density, and using the Brothers Grimm fairy tales as example, we show how collections of text can be organized for better search. Using the Google Books Corpus we show how to determine an entity's emotion associations from co-occurring words. Finally, we compare emotion words in fairy tales and novels, to show that fairy tales have a much wider range of emotion word densities than novels.\n    ",
        "submission_date": "2013-09-23T00:00:00",
        "last_modified_date": "2013-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5942",
        "title": "Colourful Language: Measuring Word-Colour Associations",
        "authors": [
            "Saif Mohammad"
        ],
        "abstract": "Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept-colour associations. We present a method to create a large word-colour association lexicon by crowdsourcing. We focus especially on abstract concepts and emotions to show that even though they cannot be physically visualized, they too tend to have strong colour associations. Finally, we show how word-colour associations manifest themselves in language, and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6162",
        "title": "JRC-Names: A freely available, highly multilingual named entity resource",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Mijail Kabadjov",
            "Erik van der Goot"
        ],
        "abstract": "This paper describes a new, freely available, highly multilingual named entity resource for person and organisation names that has been compiled over seven years of large-scale multilingual news analysis combined with Wikipedia mining, resulting in 205,000 per-son and organisation names plus about the same number of spelling variants written in over 20 different scripts and in many more languages. This resource, produced as part of the Europe Media Monitor activity (EMM, ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6176",
        "title": "Feature Learning with Gaussian Restricted Boltzmann Machine for Robust Speech Recognition",
        "authors": [
            "Xin Zheng",
            "Zhiyong Wu",
            "Helen Meng",
            "Weifeng Li",
            "Lianhong Cai"
        ],
        "abstract": "In this paper, we first present a new variant of Gaussian restricted Boltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann machine (MGRBM), with its definition and learning algorithm. Then we propose using a learned GRBM or MGRBM to extract better features for robust speech recognition. Our experiments on Aurora2 show that both GRBM-extracted and MGRBM-extracted feature performs much better than Mel-frequency cepstral coefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN) acoustic model, and MGRBM-extracted feature is slightly better.\n    ",
        "submission_date": "2013-09-23T00:00:00",
        "last_modified_date": "2013-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6185",
        "title": "Acronym recognition and processing in 22 languages",
        "authors": [
            "Maud Ehrmann",
            "Leonida della Rocca",
            "Ralf Steinberger",
            "Hristo Tanev"
        ],
        "abstract": "We are presenting work on recognising acronyms of the form Long-Form (Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news articles in twenty-two languages, as part of our more general effort to recognise entities and their variants in news text and to use them for the automatic analysis of the news, including the linking of related news across languages. We show how the acronym recognition patterns, initially developed for medical terms, needed to be adapted to the more general news domain and we present evaluation results. We describe our effort to automatically merge the numerous long-form variants referring to the same short-form, while keeping non-related long-forms separate. Finally, we provide extensive statistics on the frequency and the distribution of short-form/long-form pairs across languages.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6202",
        "title": "Sentiment Analysis in the News",
        "authors": [
            "Alexandra Balahur",
            "Ralf Steinberger",
            "Mijail Kabadjov",
            "Vanni Zavarella",
            "Erik van der Goot",
            "Matina Halkia",
            "Bruno Pouliquen",
            "Jenya Belyaeva"
        ],
        "abstract": "Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles - author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which (a) we test the relative suitability of various sentiment dictionaries and (b) we attempt to separate positive or negative opinion from good or bad news. In the experiments described here, we tested whether or not subject domain-defining vocabulary should be ignored. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6347",
        "title": "Tracking Sentiment in Mail: How Genders Differ on Emotional Axes",
        "authors": [
            "Saif M. Mohammad",
            "Tony",
            "Yang"
        ],
        "abstract": "With the widespread use of email, we now have access to unprecedented amounts of text that we ourselves have written. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail. We create a large word--emotion association lexicon by crowdsourcing, and use it to compare emotions in love letters, hate mail, and suicide notes. We show that there are marked differences across genders in how they use emotion words in work-place email. For example, women use many words from the joy--sadness axis, whereas men prefer terms from the fear--trust axis. Finally, we show visualizations that can help people track emotions in their emails.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6352",
        "title": "Using Nuances of Emotion to Identify Personality",
        "authors": [
            "Saif M. Mohammad",
            "Svetlana Kiritchenko"
        ],
        "abstract": "Past work on personality detection has shown that frequency of lexical categories such as first person pronouns, past tense verbs, and sentiment words have significant correlations with personality traits. In this paper, for the first time, we show that fine affect (emotion) categories such as that of excitement, guilt, yearning, and admiration are significant indicators of personality. Additionally, we perform experiments to show that the gains provided by the fine affect categories are not obtained by using coarse affect categories alone or with specificity features alone. We employ these features in five SVM classifiers for detecting five personality traits through essays. We find that the use of fine emotion features leads to statistically significant improvement over a competitive baseline, whereas the use of coarse affect and specificity features does not.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6650",
        "title": "An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching",
        "authors": [
            "Haytham Al-Feel",
            "Ralph Schafermeier",
            "Adrian Paschke"
        ],
        "abstract": "Ontologies are considered as the backbone of the Semantic Web. With the rising success of the Semantic Web, the number of participating communities from different countries is constantly increasing. The growing number of ontologies available in different natural languages leads to an interoperability problem. In this paper, we discuss several approaches for ontology matching; examine similarities and differences, identify weaknesses, and compare the existing automated approaches with the manual approaches for integrating multilingual ontologies. In addition to that, we propose a new architecture for a multilingual ontology matching service. As a case study we used an example of two multilingual enterprise ontologies - the university ontology of Freie Universitaet Berlin and the ontology for Fayoum University in Egypt.\n    ",
        "submission_date": "2013-09-25T00:00:00",
        "last_modified_date": "2013-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6722",
        "title": "Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern Generation",
        "authors": [
            "Tang Duyu",
            "Qin Bing",
            "Zhou LanJun",
            "Wong KamFai",
            "Zhao Yanyan",
            "Liu Ting"
        ],
        "abstract": "This paper focuses on the automatic extraction of domain-specific sentiment word (DSSW), which is a fundamental subtask of sentiment analysis. Most previous work utilizes manual patterns for this task. However, the performance of those methods highly relies on the labelled patterns or selected seeds. In order to overcome the above problem, this paper presents an automatic framework to detect large-scale domain-specific patterns for DSSW extraction. To this end, sentiment seeds are extracted from massive dataset of user comments. Subsequently, these sentiment seeds are expanded by synonyms using a bootstrapping mechanism. Simultaneously, a synonymy graph is built and the graph propagation algorithm is applied on the built synonymy graph. Afterwards, syntactic and sequential relations between target words and high-ranked sentiment words are extracted automatically to construct large-scale patterns, which are further used to extracte DSSWs. The experimental results in three domains reveal the effectiveness of our method.\n    ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7312",
        "title": "Development and Transcription of Assamese Speech Corpus",
        "authors": [
            "Himangshu Sarma",
            "Navanath Saharia",
            "Utpal Sharma",
            "Smriti Kumar Sinha",
            "Mancha Jyoti Malakar"
        ],
        "abstract": "A balanced speech corpus is the basic need for any speech processing task. In this report we describe our effort on development of Assamese speech corpus. We mainly focused on some issues and challenges faced during development of the corpus. Being a less computationally aware language, this is the first effort to develop speech corpus for Assamese. As corpus development is an ongoing process, in this paper we report only the initial task.\n    ",
        "submission_date": "2013-09-27T00:00:00",
        "last_modified_date": "2013-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0201",
        "title": "Cross-Recurrence Quantification Analysis of Categorical and Continuous Time Series: an R package",
        "authors": [
            "Moreno I. Coco",
            "Rick Dale"
        ],
        "abstract": "This paper describes the R package crqa to perform cross-recurrence quantification analysis of two time series of either a categorical or continuous nature. Streams of behavioral information, from eye movements to linguistic elements, unfold over time. When two people interact, such as in conversation, they often adapt to each other, leading these behavioral levels to exhibit recurrent states. In dialogue, for example, interlocutors adapt to each other by exchanging interactive cues: smiles, nods, gestures, choice of words, and so on. In order for us to capture closely the goings-on of dynamic interaction, and uncover the extent of coupling between two individuals, we need to quantify how much recurrence is taking place at these levels. Methods available in crqa would allow researchers in cognitive science to pose such questions as how much are two people recurrent at some level of analysis, what is the characteristic lag time for one person to maximally match another, or whether one person is leading another. First, we set the theoretical ground to understand the difference between 'correlation' and 'co-visitation' when comparing two time series, using an aggregative or cross-recurrence approach. Then, we describe more formally the principles of cross-recurrence, and show with the current package how to carry out analyses applying them. We end the paper by comparing computational efficiency, and results' consistency, of crqa R package, with the benchmark MATLAB toolbox crptoolbox. We show perfect comparability between the two libraries on both levels.\n    ",
        "submission_date": "2013-10-01T00:00:00",
        "last_modified_date": "2013-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0573",
        "title": "Improving the Quality of MT Output using Novel Name Entity Translation Scheme",
        "authors": [
            "Deepti Bhalla",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "This paper presents a novel approach to machine translation by combining the state of art name entity translation scheme. Improper translation of name entities lapse the quality of machine translated output. In this work, name entities are transliterated by using statistical rule based approach. This paper describes the translation and transliteration of name entities from English to Punjabi. We have experimented on four types of name entities which are: Proper names, Location names, Organization names and miscellaneous. Various rules for the purpose of syllabification have been constructed. Transliteration of name entities is accomplished with the help of Probability calculation. N-Gram probabilities for the extracted syllables have been calculated using statistical machine translation toolkit MOSES.\n    ",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0575",
        "title": "Development of Marathi Part of Speech Tagger Using Statistical Approach",
        "authors": [
            "Jyoti Singh",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Part-of-speech (POS) tagging is a process of assigning the words in a text corresponding to a particular part of speech. A fundamental version of POS tagging is the identification of words as nouns, verbs, adjectives etc. For processing natural languages, Part of Speech tagging is a prominent tool. It is one of the simplest as well as most constant and statistical model for many NLP applications. POS Tagging is an initial stage of linguistics, text analysis like information retrieval, machine translator, text to speech synthesis, information extraction etc. In POS Tagging we assign a Part of Speech tag to each word in a sentence and literature. Various approaches have been proposed to implement POS taggers. In this paper we present a Marathi part of speech tagger. It is morphologically rich language. Marathi is spoken by the native people of Maharashtra. The general approach used for development of tagger is statistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear idea about all the algorithms with suitable examples. It also introduces a tag set for Marathi which can be used for tagging Marathi text. In this paper we have shown the development of the tagger as well as compared to check the accuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram, Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82% respectively.\n    ",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0578",
        "title": "Subjective and Objective Evaluation of English to Urdu Machine Translation",
        "authors": [
            "Vaishali Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Machine translation is research based area where evaluation is very important phenomenon for checking the quality of MT output. The work is based on the evaluation of English to Urdu Machine translation. In this research work we have evaluated the translation quality of Urdu language which has been translated by using different Machine Translation systems like Google, Babylon and Ijunoon. The evaluation process is done by using two approaches - Human evaluation and Automatic evaluation. We have worked for both the approaches where in human evaluation emphasis is given to scales and parameters while in automatic evaluation emphasis is given to some automatic metric such as BLEU, GTM, METEOR and ATEC.\n    ",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0581",
        "title": "Rule Based Stemmer in Urdu",
        "authors": [
            "Vaishali Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Urdu is a combination of several languages like Arabic, Hindi, English, Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reason why not much work has been done in Urdu language processing. Stemming is used to convert a word into its respective root form. In stemming, we separate the suffix and prefix from the word. It is useful in search engines, natural language processing and word processing, spell checkers, word parsing, word frequency and count studies. This paper presents a rule based stemmer for Urdu. The stemmer that we have discussed here is used in information retrieval. We have also evaluated our results by verifying it with a human expert.\n    ",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.0754",
        "title": "Stemmers for Tamil Language: Performance Analysis",
        "authors": [
            "M.Thangarasu",
            "R.Manavalan"
        ],
        "abstract": "Stemming is the process of extracting root word from the given inflection word and also plays significant role in numerous application of Natural Language Processing (NLP). Tamil Language raises several challenges to NLP, since it has rich morphological patterns than other languages. The rule based approach light-stemmer is proposed in this paper, to find stem word for given inflection Tamil word. The performance of proposed approach is compared to a rule based suffix removal stemmer based on correctly and incorrectly predicted. The experimental result clearly show that the proposed approach light stemmer for Tamil language perform better than suffix removal stemmer and also more effective in Information Retrieval System (IRS).\n    ",
        "submission_date": "2013-10-02T00:00:00",
        "last_modified_date": "2013-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1285",
        "title": "Semantic Measures for the Comparison of Units of Language, Concepts or Instances from Text and Knowledge Base Analysis",
        "authors": [
            "S\u00e9bastien Harispe",
            "Sylvie Ranwez",
            "Stefan Janaqi",
            "Jacky Montmain"
        ],
        "abstract": "Semantic measures are widely used today to estimate the strength of the semantic relationship between elements of various types: units of language (e.g., words, sentences, documents), concepts or even instances semantically characterized (e.g., diseases, genes, geographical locations). Semantic measures play an important role to compare such elements according to semantic proxies: texts and knowledge representations, which support their meaning or describe their nature. Semantic measures are therefore essential for designing intelligent agents which will for example take advantage of semantic analysis to mimic human ability to compare abstract or concrete objects. This paper proposes a comprehensive survey of the broad notion of semantic measure for the comparison of units of language, concepts or instances based on semantic proxy analyses. Semantic measures generalize the well-known notions of semantic similarity, semantic relatedness and semantic distance, which have been extensively studied by various communities over the last decades (e.g., Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).\n    ",
        "submission_date": "2013-10-04T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1425",
        "title": "A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages",
        "authors": [
            "Mohammad Nasiruddin"
        ],
        "abstract": "Word Sense Disambiguation (WSD), the process of automatically identifying the meaning of a polysemous word in a sentence, is a fundamental task in Natural Language Processing (NLP). Progress in this approach to WSD opens up many promising developments in the field of NLP and its applications. Indeed, improvement over current performance levels could allow us to take a first step towards natural language understanding. Due to the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced languages. This paper is an investigation on how to initiate research in WSD for under-resourced languages by applying Word Sense Induction (WSI) and suggests some interesting topics to focus on.\n    ",
        "submission_date": "2013-10-05T00:00:00",
        "last_modified_date": "2013-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1426",
        "title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is Better for MLN-Based Bangla Speech Recognition?",
        "authors": [
            "Foyzul Hassan",
            "Mohammed Rokibul Alam Kotwal",
            "Md. Mostafizur Rahman",
            "Mohammad Nasiruddin",
            "Md. Abdul Latif",
            "Mohammad Nurul Huda"
        ],
        "abstract": "This paper discusses the dominancy of local features (LFs), as input to the multilayer neural network (MLN), extracted from a Bangla input speech over mel frequency cepstral coefficients (MFCCs). Here, LF-based method comprises three stages: (i) LF extraction from input speech, (ii) phoneme probabilities extraction using MLN from LF and (iii) the hidden Markov model (HMM) based classifier to obtain more accurate phoneme strings. In the experiments on Bangla speech corpus prepared by us, it is observed that the LFbased automatic speech recognition (ASR) system provides higher phoneme correct rate than the MFCC-based system. Moreover, the proposed system requires fewer mixture components in the HMMs.\n    ",
        "submission_date": "2013-10-05T00:00:00",
        "last_modified_date": "2013-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1590",
        "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study",
        "authors": [
            "Paheli Bhattacharya",
            "Arnab Bhattacharya"
        ],
        "abstract": "Active languages such as Bangla (or Bengali) evolve over time due to a variety of social, cultural, economic, and political issues. In this paper, we analyze the change in the written form of the modern phase of Bangla quantitatively in terms of character-level, syllable-level, morpheme-level and word-level features. We collect three different types of corpora---classical, newspapers and blogs---and test whether the differences in their features are statistically significant. Results suggest that there are significant changes in the length of a word when measured in terms of characters, but there is not much difference in usage of different characters, syllables and morphemes in a word or of different words in a sentence. To the best of our knowledge, this is the first work on Bangla of this kind.\n    ",
        "submission_date": "2013-10-06T00:00:00",
        "last_modified_date": "2013-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1597",
        "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning",
        "authors": [
            "Mengqiu Wang",
            "Christopher D. Manning"
        ],
        "abstract": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.\n    ",
        "submission_date": "2013-10-06T00:00:00",
        "last_modified_date": "2013-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1964",
        "title": "Named entity recognition using conditional random fields with non-local relational constraints",
        "authors": [
            "Flavio Massimiliano Cecchini",
            "Elisabetta Fersini"
        ],
        "abstract": "We begin by introducing the Computer Science branch of Natural Language Processing, then narrowing the attention on its subbranch of Information Extraction and particularly on Named Entity Recognition, discussing briefly its main methodological approaches. It follows an introduction to state-of-the-art Conditional Random Fields under the form of linear chains. Subsequently, the idea of constrained inference as a way to model long-distance relationships in a text is presented, based on an Integer Linear Programming representation of the problem. Adding such relationships to the problem as automatically inferred logical formulas, translatable into linear conditions, we propose to solve the resulting more complex problem with the aid of Lagrangian relaxation, of which some technical details are explained. Lastly, we give some experimental results.\n    ",
        "submission_date": "2013-10-07T00:00:00",
        "last_modified_date": "2013-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1975",
        "title": "ARKref: a rule-based coreference resolution system",
        "authors": [
            "Brendan O'Connor",
            "Michael Heilman"
        ],
        "abstract": "ARKref is a tool for noun phrase coreference. It is a deterministic, rule-based system that uses syntactic information from a constituent parser, and semantic information from an entity recognition component. Its architecture is based on the work of Haghighi and Klein (2009). ARKref was originally written in 2009. At the time of writing, the last released version was in March 2011. This document describes that version, which is open-source and publicly available at: ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2527",
        "title": "Treating clitics with minimalist grammars",
        "authors": [
            "Maxime Amblard"
        ],
        "abstract": "We propose an extension of Stabler's version of clitics treatment for a wider coverage of the French language. For this, we present the lexical entries needed in the lexicon. Then, we show the recognition of complex syntactic phenomena as (left and right) dislo- cation, clitic climbing over modal and extraction from determiner phrase. The aim of this presentation is the syntax-semantic interface for clitics analyses in which we will stress on clitic climbing over verb and raising verb.\n    ",
        "submission_date": "2013-10-08T00:00:00",
        "last_modified_date": "2013-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4546",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "authors": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg Corrado",
            "Jeffrey Dean"
        ],
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n    ",
        "submission_date": "2013-10-16T00:00:00",
        "last_modified_date": "2013-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4938",
        "title": "A Logic-based Approach for Recognizing Textual Entailment Supported by Ontological Background Knowledge",
        "authors": [
            "Andreas Wotzlaw",
            "Ravi Coote"
        ],
        "abstract": "We present the architecture and the evaluation of a new system for recognizing textual entailment (RTE). In RTE we want to identify automatically the type of a logical relation between two input texts. In particular, we are interested in proving the existence of an entailment between them. We conceive our system as a modular environment allowing for a high-coverage syntactic and semantic text analysis combined with logical inference. For the syntactic and semantic analysis we combine a deep semantic analysis with a shallow one supported by statistical models in order to increase the quality and the accuracy of results. For RTE we use logical inference of first-order employing model-theoretic techniques and automated reasoning tools. The inference is supported with problem-relevant background knowledge extracted automatically and on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or other, more experimental sources with, e.g., manually defined presupposition resolutions, or with axiomatized general and common sense knowledge. The results show that fine-grained and consistent knowledge coming from diverse sources is a necessary condition determining the correctness and traceability of results.\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5884",
        "title": "The optimality of attaching unlinked labels to unlinked meanings",
        "authors": [
            "Ramon Ferrer-i-Cancho"
        ],
        "abstract": "Vocabulary learning by children can be characterized by many biases. When encountering a new word, children as well as adults, are biased towards assuming that it means something totally different from the words that they already know. To the best of our knowledge, the 1st mathematical proof of the optimality of this bias is presented here. First, it is shown that this bias is a particular case of the maximization of mutual information between words and meanings. Second, the optimality is proven within a more general information theoretic framework where mutual information maximization competes with other information theoretic principles. The bias is a prediction from modern information theory. The relationship between information theoretic principles and the principles of contrast and mutual exclusivity is also shown.\n    ",
        "submission_date": "2013-10-22T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6772",
        "title": "Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities",
        "authors": [
            "Thamar Solorio",
            "Ragib Hasan",
            "Mainul Mizan"
        ],
        "abstract": "This paper describes the corpus of sockpuppet cases we gathered from Wikipedia. A sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process. We used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases. To the best of our knowledge, this is the first corpus available on real-world deceptive writing. We describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research. The dataset will be released under a Creative Commons license from our project website: ",
        "submission_date": "2013-10-24T00:00:00",
        "last_modified_date": "2013-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8059",
        "title": "Description and Evaluation of Semantic Similarity Measures Approaches",
        "authors": [
            "Thabet Slimani"
        ],
        "abstract": "In recent years, semantic similarity measure has a great interest in Semantic Web and Natural Language Processing (NLP). Several similarity measures have been developed, being given the existence of a structured knowledge representation offered by ontologies and corpus which enable semantic interpretation of terms. Semantic similarity measures compute the similarity between concepts/terms included in knowledge sources in order to perform estimations. This paper discusses the existing semantic similarity methods based on structure, information content and feature approaches. Additionally, we present a critical evaluation of several categories of semantic similarity approaches based on two standard benchmarks. The aim of this paper is to give an efficient evaluation of all these measures which help researcher and practitioners to select the measure that best fit for their requirements.\n    ",
        "submission_date": "2013-10-30T00:00:00",
        "last_modified_date": "2013-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.0833",
        "title": "A Comparative Study on Linguistic Feature Selection in Sentiment Polarity Classification",
        "authors": [
            "Zitao Liu"
        ],
        "abstract": "Sentiment polarity classification is perhaps the most widely studied topic. It classifies an opinionated document as expressing a positive or negative opinion. In this paper, using movie review dataset, we perform a comparative study with different single kind linguistic features and the combinations of these features. We find that the classic topic-based classifier(Naive Bayes and Support Vector Machine) do not perform as well on sentiment polarity classification. And we find that with some combination of different linguistic features, the classification accuracy can be boosted a lot. We give some reasonable explanations about these boosting outcomes.\n    ",
        "submission_date": "2013-11-04T00:00:00",
        "last_modified_date": "2013-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1169",
        "title": "Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages",
        "authors": [
            "D\u00e1niel Kondor",
            "Istv\u00e1n Csabai",
            "L\u00e1szl\u00f3 Dobos",
            "J\u00e1nos Sz\u00fcle",
            "Norbert Barankai",
            "Tam\u00e1s Hanyecz",
            "Tam\u00e1s Seb\u0151k",
            "Zs\u00f3fia Kallus",
            "G\u00e1bor Vattay"
        ],
        "abstract": "Principal component analysis (PCA) and related techniques have been successfully employed in natural language processing. Text mining applications in the age of the online social media (OSM) face new challenges due to properties specific to these use cases (e.g. spelling issues specific to texts posted by users, the presence of spammers and bots, service announcements, etc.). In this paper, we employ a Robust PCA technique to separate typical outliers and highly localized topics from the low-dimensional structure present in language use in online social networks. Our focus is on identifying geospatial features among the messages posted by the users of the Twitter microblogging service. Using a dataset which consists of over 200 million geolocated tweets collected over the course of a year, we investigate whether the information present in word usage frequencies can be used to identify regional features of language use and topics of interest. Using the PCA pursuit method, we are able to identify important low-dimensional features, which constitute smoothly varying functions of the geographic location.\n    ",
        "submission_date": "2013-11-05T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1194",
        "title": "Identifying Purpose Behind Electoral Tweets",
        "authors": [
            "Saif M. Mohammad",
            "Svetlana Kiritchenko",
            "Joel Martin"
        ],
        "abstract": "Tweets pertaining to a single event, such as a national election, can number in the hundreds of millions. Automatically analyzing them is beneficial in many downstream natural language applications such as question answering and summarization. In this paper, we propose a new task: identifying the purpose behind electoral tweets--why do people post election-oriented tweets? We show that identifying purpose is correlated with the related phenomenon of sentiment and emotion detection, but yet significantly different. Detecting purpose has a number of applications including detecting the mood of the electorate, estimating the popularity of policies, identifying key issues of contention, and predicting the course of events. We create a large dataset of electoral tweets and annotate a few thousand tweets for purpose. We develop a system that automatically classifies electoral tweets as per their purpose, obtaining an accuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class task (both accuracies well above the most-frequent-class baseline). Finally, we show that resources developed for emotion detection are also helpful for detecting purpose.\n    ",
        "submission_date": "2013-11-05T00:00:00",
        "last_modified_date": "2013-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1539",
        "title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics",
        "authors": [
            "Edward Grefenstette"
        ],
        "abstract": "This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text.\n",
        "submission_date": "2013-11-06T00:00:00",
        "last_modified_date": "2013-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2252",
        "title": "Semantic Sort: A Supervised Approach to Personalized Semantic Relatedness",
        "authors": [
            "Ran El-Yaniv",
            "David Yanay"
        ],
        "abstract": "We propose and study a novel supervised approach to learning statistical semantic relatedness models from subjectively annotated training examples. The proposed semantic model consists of parameterized co-occurrence statistics associated with textual units of a large background knowledge corpus. We present an efficient algorithm for learning such semantic models from a training sample of relatedness preferences. Our method is corpus independent and can essentially rely on any sufficiently large (unstructured) collection of coherent texts. Moreover, the approach facilitates the fitting of semantic models for specific users or groups of users. We present the results of extensive range of experiments from small to large scale, indicating that the proposed method is effective and competitive with the state-of-the-art.\n    ",
        "submission_date": "2013-11-10T00:00:00",
        "last_modified_date": "2013-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2978",
        "title": "Authorship Attribution Using Word Network Features",
        "authors": [
            "Shibamouli Lahiri",
            "Rada Mihalcea"
        ],
        "abstract": "In this paper, we explore a set of novel features for authorship attribution of documents. These features are derived from a word network representation of natural language text. As has been noted in previous studies, natural language tends to show complex network structure at word level, with low degrees of separation and scale-free (power law) degree distribution. There has also been work on authorship attribution that incorporates ideas from complex networks. The goal of our paper is to explore properties of these complex networks that are suitable as features for machine-learning-based authorship attribution of documents. We performed experiments on three different datasets, and obtained promising results.\n    ",
        "submission_date": "2013-11-12T00:00:00",
        "last_modified_date": "2013-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3011",
        "title": "Cornell SPF: Cornell Semantic Parsing Framework",
        "authors": [
            "Yoav Artzi"
        ],
        "abstract": "The Cornell Semantic Parsing Framework (SPF) is a learning and inference framework for mapping natural language to formal representation of its meaning.\n    ",
        "submission_date": "2013-11-13T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3175",
        "title": "Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System",
        "authors": [
            "Athira P. M.",
            "Sreeja M.",
            "P. C. Reghu Raj"
        ],
        "abstract": "Question answering (QA) system aims at retrieving precise information from a large collection of documents against a query. This paper describes the architecture of a Natural Language Question Answering (NLQA) system for a specific domain based on the ontological information, a step towards semantic web question answering. The proposed architecture defines four basic modules suitable for enhancing current QA capabilities with the ability of processing complex questions. The first module was the question processing, which analyses and classifies the question and also reformulates the user query. The second module allows the process of retrieving the relevant documents. The next module processes the retrieved documents, and the last module performs the extraction and generation of a response. Natural language processing techniques are used for processing the question and documents and also for answer extraction. Ontology and domain knowledge are used for reformulating queries and identifying the relations. The aim of the system is to generate short and specific answer to the question that is asked in the natural language in a specific domain. We have achieved 94 % accuracy of natural language question answering in our implementation.\n    ",
        "submission_date": "2013-11-13T00:00:00",
        "last_modified_date": "2013-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3961",
        "title": "HEVAL: Yet Another Human Evaluation Metric",
        "authors": [
            "Nisheeth Joshi",
            "Iti Mathur",
            "Hemant Darbari",
            "Ajai Kumar"
        ],
        "abstract": "Machine translation evaluation is a very important activity in machine translation development. Automatic evaluation metrics proposed in literature are inadequate as they require one or more human reference translations to compare them with output produced by machine translation. This does not always give accurate results as a text can have several different translations. Human evaluation metrics, on the other hand, lacks inter-annotator agreement and repeatability. In this paper we have proposed a new human evaluation metric which addresses these issues. Moreover this metric also provides solid grounds for making sound assumptions on the quality of the text produced by a machine translation.\n    ",
        "submission_date": "2013-11-15T00:00:00",
        "last_modified_date": "2013-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.3987",
        "title": "Big Data and Cross-Document Coreference Resolution: Current State and Future Opportunities",
        "authors": [
            "Seyed-Mehdi-Reza Beheshti",
            "Srikumar Venugopal",
            "Seung Hwan Ryu",
            "Boualem Benatallah",
            "Wei Wang"
        ],
        "abstract": "Information Extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. Among various IE tasks, extracting actionable intelligence from ever-increasing amount of data depends critically upon Cross-Document Coreference Resolution (CDCR) - the task of identifying entity mentions across multiple documents that refer to the same underlying entity. Recently, document datasets of the order of peta-/tera-bytes has raised many challenges for performing effective CDCR such as scaling to large numbers of mentions and limited representational power. The problem of analysing such datasets is called \"big data\". The aim of this paper is to provide readers with an understanding of the central concepts, subtasks, and the current state-of-the-art in CDCR process. We provide assessment of existing tools/techniques for CDCR subtasks and highlight big data challenges in each of them to help readers identify important and outstanding issues for further investigation. Finally, we provide concluding remarks and discuss possible directions for future work.\n    ",
        "submission_date": "2013-11-14T00:00:00",
        "last_modified_date": "2013-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.5401",
        "title": "Clustering and Relational Ambiguity: from Text Data to Natural Data",
        "authors": [
            "Nicolas Turenne"
        ],
        "abstract": "Text data is often seen as \"take-away\" materials with little noise and easy to process information. Main questions are how to get data and transform them into a good document format. But data can be sensitive to noise oftenly called ambiguities. Ambiguities are aware from a long time, mainly because polysemy is obvious in language and context is required to remove uncertainty. I claim in this paper that syntactic context is not suffisant to improve interpretation. In this paper I try to explain that firstly noise can come from natural data themselves, even involving high technology, secondly texts, seen as verified but meaningless, can spoil content of a corpus; it may lead to contradictions and background noise.\n    ",
        "submission_date": "2013-11-21T00:00:00",
        "last_modified_date": "2014-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.5427",
        "title": "Complexity measurement of natural and artificial languages",
        "authors": [
            "Gerardo Febres",
            "Klaus Jaffe",
            "Carlos Gershenson"
        ],
        "abstract": "We compared entropy for texts written in natural languages (English, Spanish) and artificial languages (computer software) based on a simple expression for the entropy as a function of message length and specific word diversity. Code text written in artificial languages showed higher entropy than text of similar length expressed in natural languages. Spanish texts exhibit more symbolic diversity than English ones. Results showed that algorithms based on complexity measures differentiate artificial from natural languages, and that text analysis based on complexity measures allows the unveiling of important aspects of their nature. We propose specific expressions to examine entropy related aspects of tests and estimate the values of entropy, emergence, self-organization and complexity based on specific diversity and message length.\n    ",
        "submission_date": "2013-11-20T00:00:00",
        "last_modified_date": "2013-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.5836",
        "title": "Automatic Ranking of MT Outputs using Approximations",
        "authors": [
            "Pooja Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "Since long, research on machine translation has been ongoing. Still, we do not get good translations from MT engines so developed. Manual ranking of these outputs tends to be very time consuming and expensive. Identifying which one is better or worse than the others is a very taxing task. In this paper, we show an approach which can provide automatic ranks to MT outputs (translations) taken from different MT Engines and which is based on N-gram approximations. We provide a solution where no human intervention is required for ranking systems. Further we also show the evaluations of our results which show equivalent results as that of human ranking.\n    ",
        "submission_date": "2013-11-22T00:00:00",
        "last_modified_date": "2013-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6045",
        "title": "Build Electronic Arabic Lexicon",
        "authors": [
            "Nidhal El-Abbadi",
            "Ahmed Nidhal Khdhair",
            "Adel Al-Nasrawi"
        ],
        "abstract": "There are many known Arabic lexicons organized on different ways, each of them has a different number of Arabic words according to its organization way. This paper has used mathematical relations to count a number of Arabic words, which proofs the number of Arabic words presented by Al Farahidy. The paper also presents new way to build an electronic Arabic lexicon by using a hash function that converts each word (as input) to correspond a unique integer number (as output), these integer numbers will be used as an index to a lexicon entry.\n    ",
        "submission_date": "2013-11-23T00:00:00",
        "last_modified_date": "2013-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6063",
        "title": "NILE: Fast Natural Language Processing for Electronic Health Records",
        "authors": [
            "Sheng Yu",
            "Tianrun Cai",
            "Tianxi Cai"
        ],
        "abstract": "Objective: Narrative text in Electronic health records (EHR) contain rich information for medical and data science studies. This paper introduces the design and performance of Narrative Information Linear Extraction (NILE), a natural language processing (NLP) package for EHR analysis that we share with the medical informatics community. Methods: NILE uses a modified prefix-tree search algorithm for named entity recognition, which can detect prefix and suffix sharing. The semantic analyses are implemented as rule-based finite state machines. Analyses include negation, location, modification, family history, and ignoring. Result: The processing speed of NILE is hundreds to thousands times faster than existing NLP software for medical text. The accuracy of presence analysis of NILE is on par with the best performing models on the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and being able to operate via API make NILE a valuable addition to the NLP software for medical informatics and data science.\n    ",
        "submission_date": "2013-11-23T00:00:00",
        "last_modified_date": "2019-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0482",
        "title": "Learning Semantic Representations for the Phrase Translation Model",
        "authors": [
            "Jianfeng Gao",
            "Xiaodong He",
            "Wen-tau Yih",
            "Li Deng"
        ],
        "abstract": "This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results. Experimental evaluation has been performed on two Europarl translation tasks, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation sys-tem, leading to a gain of 0.7-1.0 BLEU points.\n    ",
        "submission_date": "2013-11-28T00:00:00",
        "last_modified_date": "2013-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2087",
        "title": "Towards Structural Natural Language Formalization: Mapping Discourse to Controlled Natural Language",
        "authors": [
            "Nicholas H. Kirk"
        ],
        "abstract": "The author describes a conceptual study towards mapping grounded natural language discourse representation structures to instances of controlled language statements. This can be achieved via a pipeline of preexisting state of the art technologies, namely natural language syntax to semantic discourse mapping, and a reduction of the latter to controlled language discourse, given a set of previously learnt reduction rules. Concludingly a description on evaluation, potential and limitations for ontology-based reasoning is presented.\n    ",
        "submission_date": "2013-12-07T00:00:00",
        "last_modified_date": "2013-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2244",
        "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation",
        "authors": [
            "Rumeng Li",
            "Tao Wang",
            "Xun Wang"
        ],
        "abstract": "Timeline Generation aims at summarizing news from different epochs and telling readers how an event evolves. It is a new challenge that combines salience ranking with novelty detection. For long-term public events, the main topic usually includes various aspects across different epochs and each aspect has its own evolving pattern. Existing approaches neglect such hierarchical topic structure involved in the news corpus in timeline generation. In this paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our model can aptly detect different levels of topic information across corpus and such structure is further used for sentence selection. Based on the topic mined fro HDM, sentences are selected by considering different aspects such as relevance, coherence and coverage. We develop experimental systems to evaluate 8 long-term events that public concern. Performance comparison between different systems demonstrates the effectiveness of our model in terms of ROUGE metrics.\n    ",
        "submission_date": "2013-12-08T00:00:00",
        "last_modified_date": "2017-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3005",
        "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
        "authors": [
            "Ciprian Chelba",
            "Tomas Mikolov",
            "Mike Schuster",
            "Qi Ge",
            "Thorsten Brants",
            "Phillipp Koehn",
            "Tony Robinson"
        ],
        "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3168",
        "title": "Semantic Types, Lexical Sorts and Classifiers",
        "authors": [
            "Bruno Mery",
            "Christian Retor\u00e9"
        ],
        "abstract": "We propose a cognitively and linguistically motivated set of sorts for lexical semantics in a compositional setting: the classifiers in languages that do have such pronouns. These sorts are needed to include lexical considerations in a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical extensions of usual Montague semantics to model restriction of selection, felicitous and infelicitous copredication require a rich and refined type system whose base types are the lexical sorts, the basis of the many-sorted logic in which semantical representations of sentences are stated. However, none of those approaches define precisely the actual base types or sorts to be used in the lexicon. In this article, we shall discuss some of the options commonly adopted by researchers in formal lexical semantics, and defend the view that classifiers in the languages which have such pronouns are an appealing solution, both linguistically and cognitively motivated.\n    ",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2013-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3251",
        "title": "Towards The Development of a Bishnupriya Manipuri Corpus",
        "authors": [
            "Nayan Jyoti Kalita",
            "Navanath Saharia",
            "Smriti Kumar Sinha"
        ],
        "abstract": "For any deep computational processing of language we need evidences, and one such set of evidences is corpus. This paper describes the development of a text-based corpus for the Bishnupriya Manipuri language. A Corpus is considered as a building block for any language processing tasks. Due to the lack of awareness like other Indian languages, it is also studied less frequently. As a result the language still lacks a good corpus and basic language processing tools. As per our knowledge this is the first effort to develop a corpus for Bishnupriya Manipuri language.\n    ",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2013-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.3258",
        "title": "Implicit Sensitive Text Summarization based on Data Conveyed by Connectives",
        "authors": [
            "Henda Chorfi Ouertani"
        ],
        "abstract": "So far and trying to reach human capabilities, research in automatic summarization has been based on hypothesis that are both enabling and limiting. Some of these limitations are: how to take into account and reflect (in the generated summary) the implicit information conveyed in the text, the author intention, the reader intention, the context influence, the general world knowledge. Thus, if we want machines to mimic human abilities, then they will need access to this same large variety of knowledge. The implicit is affecting the orientation and the argumentation of the text and consequently its summary. Most of Text Summarizers (TS) are processing as compressing the initial data and they necessarily suffer from information loss. TS are focusing on features of the text only, not on what the author intended or why the reader is reading the text. In this paper, we address this problem and we present a system focusing on acquiring knowledge that is implicit. We principally spotlight the implicit information conveyed by the argumentative connectives such as: but, even, yet and their effect on the summary.\n    ",
        "submission_date": "2013-12-11T00:00:00",
        "last_modified_date": "2013-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4092",
        "title": "Domain adaptation for sequence labeling using hidden Markov models",
        "authors": [
            "Edouard Grave",
            "Guillaume Obozinski",
            "Francis Bach"
        ],
        "abstract": "Most natural language processing systems based on machine learning are not robust to domain shift. For example, a state-of-the-art syntactic dependency parser trained on Wall Street Journal sentences has an absolute drop in performance of more than ten points when tested on textual data from the Web. An efficient solution to make these methods more robust to domain shift is to first learn a word representation using large amounts of unlabeled data from both domains, and then use this representation as features in a supervised learning algorithm. In this paper, we propose to use hidden Markov models to learn word representations for part-of-speech tagging. In particular, we study the influence of using data from the source, the target or both domains to learn the representation and the different ways to represent words using an HMM.\n    ",
        "submission_date": "2013-12-14T00:00:00",
        "last_modified_date": "2013-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5129",
        "title": "Deep Learning Embeddings for Discontinuous Linguistic Units",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.\n    ",
        "submission_date": "2013-12-18T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5542",
        "title": "Word Emdeddings through Hellinger PCA",
        "authors": [
            "R\u00e9mi Lebret",
            "Ronan Collobert"
        ],
        "abstract": "Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2017-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5559",
        "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds",
        "authors": [
            "Irina Sergienya",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5985",
        "title": "Learning Type-Driven Tensor-Based Meaning Representations",
        "authors": [
            "Tamara Polajnar",
            "Luana Fagarasan",
            "Stephen Clark"
        ],
        "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6173",
        "title": "Multilingual Distributed Representations without Word Alignment",
        "authors": [
            "Karl Moritz Hermann",
            "Phil Blunsom"
        ],
        "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2014-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6192",
        "title": "Can recursive neural tensor networks learn logical reasoning?",
        "authors": [
            "Samuel R. Bowman"
        ],
        "abstract": "Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of \"some animal walks\" from \"some dog walks\" or \"some cat walks,\" given that dogs and cats are animals. This model learns representations that generalize well to new types of reasoning pattern in all but a few cases, a result which is promising for the ability of learned representation models to capture logical reasoning.\n    ",
        "submission_date": "2013-12-21T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6849",
        "title": "Speech Recognition Front End Without Information Loss",
        "authors": [
            "Matthew Ager",
            "Zoran Cvetkovic",
            "Peter Sollich"
        ],
        "abstract": "Speech representation and modelling in high-dimensional spaces of acoustic waveforms, or a linear transformation thereof, is investigated with the aim of improving the robustness of automatic speech recognition to additive noise. The motivation behind this approach is twofold: (i) the information in acoustic waveforms that is usually removed in the process of extracting low-dimensional features might aid robust recognition by virtue of structured redundancy analogous to channel coding, (ii) linear feature domains allow for exact noise adaptation, as opposed to representations that involve non-linear processing which makes noise adaptation challenging. Thus, we develop a generative framework for phoneme modelling in high-dimensional linear feature domains, and use it in phoneme classification and recognition tasks. Results show that classification and recognition in this framework perform better than analogous PLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional and MFCC features at the likelihood level performs uniformly better than either of the individual representations across all noise levels.\n    ",
        "submission_date": "2013-12-24T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6947",
        "title": "Formal Ontology Learning on Factual IS-A Corpus in English using Description Logics",
        "authors": [
            "Sourish Dasgupta",
            "Ankur Padia",
            "Kushal Shah",
            "Prasenjit Majumder"
        ],
        "abstract": "Ontology Learning (OL) is the computational task of generating a knowledge base in the form of an ontology given an unstructured corpus whose content is in natural language (NL). Several works can be found in this area most of which are limited to statistical and lexico-syntactic pattern matching based techniques Light-Weight OL. These techniques do not lead to very accurate learning mostly because of several linguistic nuances in NL. Formal OL is an alternative (less explored) methodology were deep linguistics analysis is made using theory and tools found in computational linguistics to generate formal axioms and definitions instead simply inducing a taxonomy. In this paper we propose \"Description Logic (DL)\" based formal OL framework for learning factual IS-A type sentences in English. We claim that semantic construction of IS-A sentences is non trivial. Hence, we also claim that such sentences requires special studies in the context of OL before any truly formal OL can be proposed. We introduce a learner tool, called DLOL_IS-A, that generated such ontologies in the owl format. We have adopted \"Gold Standard\" based OL evaluation on IS-A rich WCL v.1.1 dataset and our own Community representative IS-A dataset. We observed significant improvement of DLOL_IS-A when compared to the light-weight OL tool Text2Onto and formal OL tool FRED.\n    ",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2016-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6948",
        "title": "Description Logics based Formalization of Wh-Queries",
        "authors": [
            "Sourish Dasgupta",
            "Rupali KaPatel",
            "Ankur Padia",
            "Kushal Shah"
        ],
        "abstract": "The problem of Natural Language Query Formalization (NLQF) is to translate a given user query in natural language (NL) into a formal language so that the semantic interpretation has equivalence with the NL interpretation. Formalization of NL queries enables logic based reasoning during information retrieval, database query, question-answering, etc. Formalization also helps in Web query normalization and indexing, query intent analysis, etc. In this paper we are proposing a Description Logics based formal methodology for wh-query intent (also called desire) identification and corresponding formal translation. We evaluated the scalability of our proposed formalism using Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\n    ",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7077",
        "title": "Language Modeling with Power Low Rank Ensembles",
        "authors": [
            "Ankur P. Parikh",
            "Avneesh Saluja",
            "Chris Dyer",
            "Eric P. Xing"
        ],
        "abstract": "We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.\n    ",
        "submission_date": "2013-12-26T00:00:00",
        "last_modified_date": "2014-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7223",
        "title": "Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier",
        "authors": [
            "Rashmi Gupta",
            "Nisheeth Joshi",
            "Iti Mathur"
        ],
        "abstract": "In this paper we present an approach for estimating the quality of machine translation system. There are various methods for estimating the quality of output sentences, but in this paper we focus on Na\u00efve Bayes classifier to build model using features which are extracted from the input sentences. These features are used for finding the likelihood of each of the sentences of the training data which are then further used for determining the scores of the test data. On the basis of these scores we determine the class labels of the test data.\n    ",
        "submission_date": "2013-12-27T00:00:00",
        "last_modified_date": "2013-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.0570",
        "title": "Reduction of Maximum Entropy Models to Hidden Markov Models",
        "authors": [
            "Joshua Goodman"
        ],
        "abstract": "We show that maximum entropy (maxent) models can be modeled with certain kinds of HMMs, allowing us to construct maxent models with hidden variables, hidden state sequences, or other characteristics. The models can be trained using the forward-backward algorithm.  While the results are primarily of theoretical interest, unifying apparently unrelated concepts, we also give experimental results for a maxent model with a hidden variable on a word disambiguation task; the model outperforms standard  techniques. \n    ",
        "submission_date": "2012-12-12T00:00:00",
        "last_modified_date": "2012-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.1429",
        "title": "Adaptation of fictional and online conversations to communication media",
        "authors": [
            "Christian M. Alis",
            "May T. Lim"
        ],
        "abstract": "Conversations allow the quick transfer of short bits of information and it is reasonable to expect that changes in communication medium affect how we converse. Using conversations in works of fiction and in an online social networking platform, we show that the utterance length of conversations is slowly shortening with time but adapts more strongly to the constraints of the communication medium. This indicates that the introduction of any new medium of communication can affect the way natural language evolves.\n    ",
        "submission_date": "2013-01-08T00:00:00",
        "last_modified_date": "2013-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.2405",
        "title": "Dating medieval English charters",
        "authors": [
            "Gelila Tilahun",
            "Andrey Feuerverger",
            "Michael Gervers"
        ],
        "abstract": "Deeds, or charters, dealing with property rights, provide a continuous documentation which can be used by historians to study the evolution of social, economic and political changes. This study is concerned with charters (written in Latin) dating from the tenth through early fourteenth centuries in England. Of these, at least one million were left undated, largely due to administrative changes introduced by William the Conqueror in 1066. Correctly dating such charters is of vital importance in the study of English medieval history. This paper is concerned with computer-automated statistical methods for dating such document collections, with the goal of reducing the considerable efforts required to date them manually and of improving the accuracy of assigned dates. Proposed methods are based on such data as the variation over time of word and phrase usage, and on measures of distance between documents. The extensive (and dated) Documents of Early England Data Set (DEEDS) maintained at the University of Toronto was used for this purpose.\n    ",
        "submission_date": "2013-01-11T00:00:00",
        "last_modified_date": "2013-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3226",
        "title": "The Expressive Power of Word Embeddings",
        "authors": [
            "Yanqing Chen",
            "Bryan Perozzi",
            "Rami Al-Rfou",
            "Steven Skiena"
        ],
        "abstract": "We seek to better understand the difference in quality of the several publicly released embeddings. We propose several tasks that help to distinguish the characteristics of different embeddings. Our evaluation of sentiment polarity and synonym/antonym relations shows that embeddings are able to capture surprisingly nuanced semantics even in the absence of sentence structure. Moreover, benchmarking the embeddings shows great variance in quality and characteristics of the semantics captured by the tested embeddings. Finally, we show the impact of varying the number of dimensions and the resolution of each dimension on the effective useful features captured by the embedding space. Our contributions highlight the importance of embeddings for NLP tasks and the effect of their quality on the final results.\n    ",
        "submission_date": "2013-01-15T00:00:00",
        "last_modified_date": "2013-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.3605",
        "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks",
        "authors": [
            "Dong Yu",
            "Michael L. Seltzer",
            "Jinyu Li",
            "Jui-Ting Huang",
            "Frank Seide"
        ],
        "abstract": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.\n    ",
        "submission_date": "2013-01-16T00:00:00",
        "last_modified_date": "2013-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.4938",
        "title": "A type theoretical framework for natural language semantics: the Montagovian generative lexicon",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "We present a framework, named the Montagovian generative lexicon, for computing the semantics of natural language sentences, expressed in many sorted higher order logic. Word meaning is depicted by lambda terms of second order lambda calculus (Girard's system F) with base types including a type for propositions and many types for sorts of a many sorted logic. This framework is able to integrate a proper treatment of lexical phenomena into a Montagovian compositional semantics, including the restriction of selection which imposes the nature of the arguments of a predicate, and the possible adaptation of a word meaning to some contexts. Among these adaptations of a word's sense to the context, ontological inclusions are handled by an extension of system F with coercive subtyping that is introduced in the present paper. The benefits of this framework for lexical pragmatics are illustrated on meaning transfers and coercions, on possible and impossible copredication over different senses, on deverbal ambiguities, and on \"fictive motion\". Next we show that the compositional treatment of determiners, quantifiers, plurals,... are finer grained in our framework. We then conclude with the linguistic, logical and computational perspectives opened by the Montagovian generative lexicon.\n    ",
        "submission_date": "2013-01-21T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1301.7382",
        "title": "Inferring Informational Goals from Free-Text Queries: A Bayesian Approach",
        "authors": [
            "David Heckerman",
            "Eric J. Horvitz"
        ],
        "abstract": "People using consumer software applications typically do not use technical jargon when querying an online database of help topics. Rather, they attempt to communicate their goals with common words and phrases that describe software functionality in terms of structure and objects they understand. We describe a Bayesian approach to modeling the relationship between words in a user's query for assistance and the informational goals of the user. After reviewing the general method, we describe several extensions that center on integrating additional distinctions and structure about language usage and user goals into the Bayesian models.\n    ",
        "submission_date": "2013-01-30T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.0393",
        "title": "Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams for Lambek Calculus",
        "authors": [
            "Bob Coecke",
            "Edward Grefenstette",
            "Mehrnoosh Sadrzadeh"
        ],
        "abstract": "The Distributional Compositional Categorical (DisCoCat) model is a mathematical framework that provides compositional semantics for meanings of natural language sentences. It consists of a computational procedure for constructing meanings of sentences, given their grammatical structure in terms of compositional type-logic, and given the empirically derived meanings of their words. For the particular case that the meaning of words is modelled within a distributional vector space model, its experimental predictions, derived from real large scale data, have outperformed other empirically validated methods that could build vectors for a full sentence. This success can be attributed to a conceptually motivated mathematical underpinning, by integrating qualitative compositional type-logic and quantitative modelling of meaning within a category-theoretic mathematical framework.\n",
        "submission_date": "2013-02-02T00:00:00",
        "last_modified_date": "2013-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.1612",
        "title": "Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering",
        "authors": [
            "Hanane Froud",
            "Abdelmonaime Lachkar",
            "Said Alaoui Ouatik"
        ],
        "abstract": "Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.\n    ",
        "submission_date": "2013-02-06T00:00:00",
        "last_modified_date": "2013-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2131",
        "title": "Data Mining of the Concept \"End of the World\" in Twitter Microblogs",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "This paper describes the analysis of quantitative characteristics of frequent sets and association rules in the posts of Twitter microblogs, related to the discussion of \"end of the world\", which was allegedly predicted on December 21, 2012 due to the Mayan calendar. Discovered frequent sets and association rules characterize semantic relations between the concepts of analyzed ",
        "submission_date": "2013-02-08T00:00:00",
        "last_modified_date": "2013-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.2569",
        "title": "Toric grammars: a new statistical approach to natural language modeling",
        "authors": [
            "Olivier Catoni",
            "Thomas Mainguy"
        ],
        "abstract": "We propose a new statistical model for computational linguistics. Rather than trying to estimate directly the probability distribution of a random sentence of the language, we define a Markov chain on finite sets of sentences with many finite recurrent communicating classes and define our language model as the invariant probability measures of the chain on each recurrent communicating class. This Markov chain, that we call a communication model, recombines at each step randomly the set of sentences forming its current state, using some grammar rules. When the grammar rules are fixed and known in advance instead of being estimated on the fly, we can prove supplementary mathematical properties. In particular, we can prove in this case that all states are recurrent states, so that the chain defines a partition of its state space into finite recurrent communicating classes. We show that our approach is a decisive departure from Markov models at the sentence level and discuss its relationships with Context Free Grammars. Although the toric grammars we use are closely related to Context Free Grammars, the way we generate the language from the grammar is qualitatively different. Our communication model has two purposes. On the one hand, it is used to define indirectly the probability distribution of a random sentence of the language. On the other hand it can serve as a (crude) model of language transmission from one speaker to another speaker through the communication of a (large) set of sentences.\n    ",
        "submission_date": "2013-02-11T00:00:00",
        "last_modified_date": "2013-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3831",
        "title": "Quantum Entanglement in Concept Combinations",
        "authors": [
            "Diederik Aerts",
            "Sandro Sozzo"
        ],
        "abstract": "Research in the application of quantum structures to cognitive science confirms that these structures quite systematically appear in the dynamics of concepts and their combinations and quantum-based models faithfully represent experimental data of situations where classical approaches are problematical. In this paper, we analyze the data we collected in an experiment on a specific conceptual combination, showing that Bell's inequalities are violated in the experiment. We present a new refined entanglement scheme to model these data within standard quantum theory rules, where 'entangled measurements and entangled evolutions' occur, in addition to the expected 'entangled states', and present a full quantum representation in complex Hilbert space of the data. This stronger form of entanglement in measurements and evolutions might have relevant applications in the foundations of quantum theory, as well as in the interpretation of nonlocality tests. It could indeed explain some non-negligible 'anomalies' identified in EPR-Bell experiments.\n    ",
        "submission_date": "2013-02-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.3892",
        "title": "Identifying trends in word frequency dynamics",
        "authors": [
            "Eduardo G. Altmann",
            "Zakary L. Whichard",
            "Adilson E. Motter"
        ],
        "abstract": "The word-stock of a language is a complex dynamical system in which words can be created, evolve, and become extinct. Even more dynamic are the short-term fluctuations in word usage by individuals in a population. Building on the recent demonstration that word niche is a strong determinant of future rise or fall in word frequency, here we introduce a model that allows us to distinguish persistent from temporary increases in frequency. Our model is illustrated using a 10^8-word database from an online discussion group and a 10^11-word collection of digitized books. The model reveals a strong relation between changes in word dissemination and changes in frequency. Aside from their implications for short-term word frequency dynamics, these observations are potentially important for language evolution as new words must survive in the short term in order to survive in the long term.\n    ",
        "submission_date": "2013-02-15T00:00:00",
        "last_modified_date": "2013-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4383",
        "title": "Explaining Zipf's Law via Mental Lexicon",
        "authors": [
            "Armen E. Allahverdyan",
            "Weibing Deng",
            "Q. A. Wang"
        ],
        "abstract": "The Zipf's law is the major regularity of statistical linguistics that served as a prototype for rank-frequency relations and scaling laws in natural sciences. Here we show that the Zipf's law -- together with its applicability for a single text and its generalizations to high and low frequencies including hapax legomena -- can be derived from assuming that the words are drawn into the text with random probabilities. Their apriori density relates, via the Bayesian statistics, to general features of the mental lexicon of the author who produced the text.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4465",
        "title": "Unveiling the relationship between complex networks metrics and word senses",
        "authors": [
            "Diego R. Amancio",
            "Osvaldo N. Oliveira Jr.",
            "Luciano da F. Costa"
        ],
        "abstract": "The automatic disambiguation of word senses (i.e., the identification of which of the meanings is used in a given context for a word that has multiple meanings) is essential for such applications as machine translation and information retrieval, and represents a key step for developing the so-called Semantic Web. Humans disambiguate words in a straightforward fashion, but this does not apply to computers. In this paper we address the problem of Word Sense Disambiguation (WSD) by treating texts as complex networks, and show that word senses can be distinguished upon characterizing the local structure around ambiguous words. Our goal was not to obtain the best possible disambiguation system, but we nevertheless found that in half of the cases our approach outperforms traditional shallow methods. We show that the hierarchical connectivity and clustering of words are usually the most relevant features for WSD. The results reported here shine light on the relationship between semantic and structural parameters of complex networks. They also indicate that when combined with traditional techniques the complex network approach may be useful to enhance the discrimination of senses in large texts\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4471",
        "title": "Word sense disambiguation via high order of learning in complex networks",
        "authors": [
            "Thiago C. Silva",
            "Diego R. Amancio"
        ],
        "abstract": "Complex networks have been employed to model many real systems and as a modeling tool in a myriad of applications. In this paper, we use the framework of complex networks to the problem of supervised classification in the word disambiguation task, which consists in deriving a function from the supervised (or labeled) training data of ambiguous words. Traditional supervised data classification takes into account only topological or physical features of the input data. On the other hand, the human (animal) brain performs both low- and high-level orders of learning and it has facility to identify patterns according to the semantic meaning of the input data. In this paper, we apply a hybrid technique which encompasses both types of learning in the field of word sense disambiguation and show that the high-level order of learning can really improve the accuracy rate of the model. This evidence serves to demonstrate that the internal structures formed by the words do present patterns that, generally, cannot be correctly unveiled by only traditional techniques. Finally, we exhibit the behavior of the model for different weights of the low- and high-level classifiers by plotting decision boundaries. This study helps one to better understand the effectiveness of the model.\n    ",
        "submission_date": "2013-02-18T00:00:00",
        "last_modified_date": "2013-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4490",
        "title": "Complex networks analysis of language complexity",
        "authors": [
            "Diego R. Amancio",
            "Sandra M. Aluisio",
            "Osvaldo N. Oliveira Jr.",
            "Luciano da F. Costa"
        ],
        "abstract": "Methods from statistical physics, such as those involving complex networks, have been increasingly used in quantitative analysis of linguistic phenomena. In this paper, we represented pieces of text with different levels of simplification in co-occurrence networks and found that topological regularity correlated negatively with textual complexity. Furthermore, in less complex texts the distance between concepts, represented as nodes, tended to decrease. The complex networks metrics were treated with multivariate pattern recognition techniques, which allowed us to distinguish between original texts and their simplified versions. For each original text, two simplified versions were generated manually with increasing number of simplification operations. As expected, distinction was easier for the strongly simplified versions, where the most relevant metrics were node strength, shortest paths and diversity. Also, the discrimination of complex texts was improved with higher hierarchical network metrics, thus pointing to the usefulness of considering wider contexts around the concepts. Though the accuracy rate in the distinction was not as high as in methods using deep linguistic knowledge, the complex network approach is still useful for a rapid screening of texts whenever assessing complexity is essential to guarantee accessibility to readers with limited reading ability\n    ",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.4726",
        "title": "An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments",
        "authors": [
            "Khalil Riad Bouzidi",
            "Bruno Fies",
            "Marc Bourdeau",
            "Catherine Faron-Zucker",
            "Nhan Le-Thanh"
        ],
        "abstract": "In this paper, we present a semantic web approach for modelling the process of creating new technical and regulatory documents related to the Building sector. This industry, among other industries, is currently experiencing a phenomenal growth in its technical and regulatory texts. Therefore, it is urgent and crucial to improve the process of creating regulations by automating it as much as possible. We focus on the creation of particular technical documents issued by the French Scientific and Technical Centre for Building (CSTB), called Technical Assessments, and we propose services based on Semantic Web models and techniques for modelling the process of their creation.\n    ",
        "submission_date": "2013-02-19T00:00:00",
        "last_modified_date": "2013-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5181",
        "title": "Basic Classes of Grammars with Prohibition",
        "authors": [
            "Mark Burgin"
        ],
        "abstract": "A practical tool for natural language modeling and development of human-machine interaction is developed in the context of formal grammars and languages. A new type of formal grammars, called grammars with prohibition, is introduced. Grammars with prohibition provide more powerful tools for natural language generation and better describe processes of language learning than the conventional formal grammars. Here we study relations between languages generated by different grammars with prohibition based on conventional types of formal grammars such as context-free or context sensitive grammars. Besides, we compare languages generated by different grammars with prohibition and languages generated by conventional formal grammars. In particular, it is demonstrated that they have essentially higher computational power and expressive possibilities in comparison with the conventional formal grammars. Thus, while conventional formal grammars are recursive and subrecursive algorithms, many classes of grammars with prohibition are superrecursive algorithms. Results presented in this work are aimed at the development of human-machine interaction, modeling natural languages, empowerment of programming languages, computer simulation, better software systems, and theory of recursion.\n    ",
        "submission_date": "2013-02-21T00:00:00",
        "last_modified_date": "2013-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1302.5526",
        "title": "Stochastic dynamics of lexicon learning in an uncertain and nonuniform world",
        "authors": [
            "Rainer Reisenauer",
            "Kenny Smith",
            "Richard A. Blythe"
        ],
        "abstract": "We study the time taken by a language learner to correctly identify the meaning of all words in a lexicon under conditions where many plausible meanings can be inferred whenever a word is uttered. We show that the most basic form of cross-situational learning - whereby information from multiple episodes is combined to eliminate incorrect meanings - can perform badly when words are learned independently and meanings are drawn from a nonuniform distribution. If learners further assume that no two words share a common meaning, we find a phase transition between a maximally-efficient learning regime, where the learning time is reduced to the shortest it can possibly be, and a partially-efficient regime where incorrect candidate meanings for words persist at late times. We obtain exact results for the word-learning process through an equivalence to a statistical mechanical problem of enumerating loops in the space of word-meaning mappings.\n    ",
        "submission_date": "2013-02-22T00:00:00",
        "last_modified_date": "2013-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0347",
        "title": "Probing the statistical properties of unknown texts: application to the Voynich Manuscript",
        "authors": [
            "Diego R. Amancio",
            "Eduardo G. Altmann",
            "Diego Rybski",
            "Osvaldo N. Oliveira Jr.",
            "Luciano da F. Costa"
        ],
        "abstract": "While the use of statistical physics methods to analyze large corpora has been useful to unveil many patterns in texts, no comprehensive investigation has been performed investigating the properties of statistical measurements across different languages and texts. In this study we propose a framework that aims at determining if a text is compatible with a natural language and which languages are closest to it, without any knowledge of the meaning of the words. The approach is based on three types of statistical measurements, i.e. obtained from first-order statistics of word properties in a text, from the topology of complex networks representing text, and from intermittency concepts where text is treated as a time series. Comparative experiments were performed with the New Testament in 15 different languages and with distinct books in English and Portuguese in order to quantify the dependency of the different measurements on the language and on the story being told in the book. The metrics found to be informative in distinguishing real texts from their shuffled versions include assortativity, degree and selectivity of words. As an illustration, we analyze an undeciphered medieval manuscript known as the Voynich Manuscript. We show that it is mostly compatible with natural languages and incompatible with random texts. We also obtain candidates for key-words of the Voynich Manuscript which could be helpful in the effort of deciphering it. Because we were able to identify statistical measurements that are more dependent on the syntax than on the semantics, the framework may also serve for text analysis in language-dependent applications.\n    ",
        "submission_date": "2013-03-02T00:00:00",
        "last_modified_date": "2013-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.0445",
        "title": "Detecting and resolving spatial ambiguity in text using named entity extraction and self learning fuzzy logic techniques",
        "authors": [
            "Kanagavalli V R",
            "Raja. K"
        ],
        "abstract": "Information extraction identifies useful and relevant text in a document and converts unstructured text into a form that can be loaded into a database table. Named entity extraction is a main task in the process of information extraction and is a classification problem in which words are assigned to one or more semantic classes or to a default non-entity class. A word which can belong to one or more classes and which has a level of uncertainty in it can be best handled by a self learning Fuzzy Logic Technique. This paper proposes a method for detecting the presence of spatial uncertainty in the text and dealing with spatial ambiguity using named entity extraction techniques coupled with self learning fuzzy logic techniques\n    ",
        "submission_date": "2013-03-03T00:00:00",
        "last_modified_date": "2013-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1441",
        "title": "A Hybrid Approach to Extract Keyphrases from Medical Documents",
        "authors": [
            "Kamal Sarkar"
        ],
        "abstract": "Keyphrases are the phrases, consisting of one or more words, representing the important concepts in the articles. Keyphrases are useful for a variety of tasks such as text summarization, automatic indexing, clustering/classification, text mining etc. This paper presents a hybrid approach to keyphrase extraction from medical documents. The keyphrase extraction approach presented in this paper is an amalgamation of two methods: the first one assigns weights to candidate keyphrases based on an effective combination of features such as position, term frequency, inverse document frequency and the second one assign weights to candidate keyphrases using some knowledge about their similarities to the structure and characteristics of keyphrases available in the memory (stored list of keyphrases). An efficient candidate keyphrase identification method as the first component of the proposed keyphrase extraction system has also been introduced in this paper. The experimental results show that the proposed hybrid approach performs better than some state-of-the art keyphrase extraction approaches.\n    ",
        "submission_date": "2013-03-06T00:00:00",
        "last_modified_date": "2014-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1599",
        "title": "Efficient learning strategy of Chinese characters based on network approach",
        "authors": [
            "Xiao-Yong Yan",
            "Ying Fan",
            "Zengru Di",
            "Shlomo Havlin",
            "Jinshan Wu"
        ],
        "abstract": "Based on network analysis of hierarchical structural relations among Chinese characters, we develop an efficient learning strategy of Chinese characters. We regard a more efficient learning method if one learns the same number of useful Chinese characters in less effort or time. We construct a node-weighted network of Chinese characters, where character usage frequencies are used as node weights. Using this hierarchical node-weighted network, we propose a new learning method, the distributed node weight (DNW) strategy, which is based on a new measure of nodes' importance that takes into account both the weight of the nodes and the hierarchical structure of the network. Chinese character learning strategies, particularly their learning order, are analyzed as dynamical processes over the network. We compare the efficiency of three theoretical learning methods and two commonly used methods from mainstream Chinese textbooks, one for Chinese elementary school students and the other for students learning Chinese as a second language. We find that the DNW method significantly outperforms the others, implying that the efficiency of current learning methods of major textbooks can be greatly improved.\n    ",
        "submission_date": "2013-03-07T00:00:00",
        "last_modified_date": "2013-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.1703",
        "title": "Concept-based indexing in text information retrieval",
        "authors": [
            "Fatiha Boubekeur",
            "Wassila Azzoug"
        ],
        "abstract": "Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexical-focused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.\n    ",
        "submission_date": "2013-03-07T00:00:00",
        "last_modified_date": "2013-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.2430",
        "title": "Quantum and Concept Combination, Entangled Measurements and Prototype Theory",
        "authors": [
            "Diederik Aerts"
        ],
        "abstract": "We analyze the meaning of the violation of the marginal probability law for situations of correlation measurements where entanglement is identified. We show that for quantum theory applied to the cognitive realm such a violation does not lead to the type of problems commonly believed to occur in situations of quantum theory applied to the physical realm. We briefly situate our quantum approach for modeling concepts and their combinations with respect to the notions of 'extension' and 'intension' in theories of meaning, and in existing concept theories.\n    ",
        "submission_date": "2013-03-11T00:00:00",
        "last_modified_date": "2013-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.3036",
        "title": "Type-theoretical natural language semantics: on the system F for meaning assembly",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "This paper presents and extends our type theoretical framework for a compositional treatment of natural language semantics with some lexical features like coercions (e.g. of a town into a football club) and copredication (e.g. on a town as a set of people and as a location). The second order typed lambda calculus was shown to be a good framework, and here we discuss how to introduced predefined types and coercive subtyping which are much more natural than internally coded similar constructs. Linguistic applications of these new features are also exemplified.\n    ",
        "submission_date": "2013-03-12T00:00:00",
        "last_modified_date": "2013-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.4959",
        "title": "Analytic solution of a model of language competition with bilingualism and interlinguistic similarity",
        "authors": [
            "Victoria Otero-Espinar",
            "Lu\u00eds F. Seoane",
            "Juan J. Nieto",
            "Jorge Mira"
        ],
        "abstract": "An in-depth analytic study of a model of language dynamics is presented: a model which tackles the problem of the coexistence of two languages within a closed community of speakers taking into account bilingualism and incorporating a parameter to measure the distance between languages. After previous numerical simulations, the model yielded that coexistence might lead to survival of both languages within monolingual speakers along with a bilingual community or to extinction of the weakest tongue depending on different parameters. In this paper, such study is closed with thorough analytical calculations to settle the results in a robust way and previous results are refined with some modifications. From the present analysis it is possible to almost completely assay the number and nature of the equilibrium points of the model, which depend on its parameters, as well as to build a phase space based on them. Also, we obtain conclusions on the way the languages evolve with time. Our rigorous considerations also suggest ways to further improve the model and facilitate the comparison of its consequences with those from other approaches or with real data.\n    ",
        "submission_date": "2013-03-20T00:00:00",
        "last_modified_date": "2013-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.5778",
        "title": "Speech Recognition with Deep Recurrent Neural Networks",
        "authors": [
            "Alex Graves",
            "Abdel-rahman Mohamed",
            "Geoffrey Hinton"
        ],
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.\n    ",
        "submission_date": "2013-03-22T00:00:00",
        "last_modified_date": "2013-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1303.6175",
        "title": "Compression as a universal principle of animal behavior",
        "authors": [
            "R. Ferrer-i-Cancho",
            "A. Hern\u00e1ndez-Fern\u00e1ndez",
            "D. Lusseau",
            "G. Agoramoorthy",
            "M. J. Hsu",
            "S. Semple"
        ],
        "abstract": "A key aim in biology and psychology is to identify fundamental principles underpinning the behavior of animals, including humans. Analyses of human language and the behavior of a range of non-human animal species have provided evidence for a common pattern underlying diverse behavioral phenomena: words follow Zipf's law of brevity (the tendency of more frequently used words to be shorter), and conformity to this general pattern has been seen in the behavior of a number of other animals. It has been argued that the presence of this law is a sign of efficient coding in the information theoretic sense. However, no strong direct connection has been demonstrated between the law and compression, the information theoretic principle of minimizing the expected length of a code. Here we show that minimizing the expected code length implies that the length of a word cannot increase as its frequency increases. Furthermore, we show that the mean code length or duration is significantly small in human language, and also in the behavior of other species in all cases where agreement with the law of brevity has been found. We argue that compression is a general principle of animal behavior, that reflects selection for efficiency of coding.\n    ",
        "submission_date": "2013-03-25T00:00:00",
        "last_modified_date": "2013-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0104",
        "title": "Meaning-focused and Quantum-inspired Information Retrieval",
        "authors": [
            "Diederik Aerts",
            "Jan Broekaert",
            "Sandro Sozzo",
            "Tomas Veloz"
        ],
        "abstract": "In recent years, quantum-based methods have promisingly integrated the traditional procedures in information retrieval (IR) and natural language processing (NLP). Inspired by our research on the identification and application of quantum structures in cognition, more specifically our work on the representation of concepts and their combinations, we put forward a 'quantum meaning based' framework for structured query retrieval in text corpora and standardized testing corpora. This scheme for IR rests on considering as basic notions, (i) 'entities of meaning', e.g., concepts and their combinations and (ii) traces of such entities of meaning, which is how documents are considered in this approach. The meaning content of these 'entities of meaning' is reconstructed by solving an 'inverse problem' in the quantum formalism, consisting of reconstructing the full states of the entities of meaning from their collapsed states identified as traces in relevant documents. The advantages with respect to traditional approaches, such as Latent Semantic Analysis (LSA), are discussed by means of concrete examples.\n    ",
        "submission_date": "2013-03-30T00:00:00",
        "last_modified_date": "2013-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.0715",
        "title": "A cookbook of translating English to Xapi",
        "authors": [
            "Ladislau B\u00f6l\u00f6ni"
        ],
        "abstract": "The Xapagy cognitive architecture had been designed to perform narrative reasoning: to model and mimic the activities performed by humans when witnessing, reading, recalling, narrating and talking about stories. Xapagy communicates with the outside world using Xapi, a simplified, \"pidgin\" language which is strongly tied to the internal representation model (instances, scenes and verb instances) and reasoning techniques (shadows and headless shadows). While not fully a semantic equivalent of natural language, Xapi can represent a wide range of complex stories. We illustrate the representation technique used in Xapi through examples taken from folk physics, folk psychology as well as some more unusual literary examples. We argue that while the Xapi model represents a conceptual shift from the English representation, the mapping is logical and consistent, and a trained knowledge engineer can translate between English and Xapi at near-native speed.\n    ",
        "submission_date": "2013-03-31T00:00:00",
        "last_modified_date": "2013-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.1018",
        "title": "Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks",
        "authors": [
            "Dimitri Palaz",
            "Ronan Collobert",
            "Mathew Magimai.-Doss"
        ],
        "abstract": "In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text processing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal.\n    ",
        "submission_date": "2013-04-03T00:00:00",
        "last_modified_date": "2013-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.2476",
        "title": "Corpus-based Web Document Summarization using Statistical and Linguistic Approach",
        "authors": [
            "Rushdi Shams",
            "M.M.A. Hashem",
            "Afrina Hossain",
            "Suraiya Rumana Akter",
            "Monika Gope"
        ],
        "abstract": "Single document summarization generates summary by extracting the representative sentences from the document. In this paper, we presented a novel technique for summarization of domain-specific text from a single web document that uses statistical and linguistic analysis on the text in a reference corpus and the web document. The proposed summarizer uses the combinational function of Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of a sentence, where SW is the function of number of terms (t_n) and number of words (w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is the function of t_n and w_n in a subject, and t_f in the corpus. 30 percent of the ranked sentences are considered to be the summary of the web document. We generated three web document summaries using our technique and compared each of them with the summaries developed manually from 16 different human subjects. Results showed that 68 percent of the summaries produced by our approach satisfy the manual summaries.\n    ",
        "submission_date": "2013-04-09T00:00:00",
        "last_modified_date": "2013-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3092",
        "title": "Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based Systems",
        "authors": [
            "Steven J. Henkind"
        ],
        "abstract": "There has been a considerable amount of work on uncertainty in knowledge-based systems. This work has generally been concerned with uncertainty arising from the strength of inferences and the weight of evidence. In this paper we discuss another type of uncertainty: that which is due to imprecision in the underlying primitives used to represent the knowledge of the system. In particular, a given word may denote many similar but not identical entities. Such words are said to be lexically imprecise. Lexical imprecision has caused widespread problems in many areas. Unless this phenomenon is recognized and appropriately handled, it can degrade the performance of knowledge-based systems. In particular, it can lead to difficulties with the user interface, and with the inferencing processes of these systems. Some techniques are suggested for coping with this phenomenon.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3432",
        "title": "Machine Learning, Clustering, and Polymorphy",
        "authors": [
            "Stephen Jose Hanson",
            "Malcolm Bauer"
        ],
        "abstract": "This paper describes a machine induction program (WITT) that attempts to model human categorization. Properties of categories to which human subjects are sensitive includes best or prototypical members, relative contrasts between putative categories, and polymorphy (neither necessary or sufficient features). This approach represents an alternative to usual Artificial Intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules, equivalence classes, and simple search and match schemes. WITT is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes. Applications of this approach in the domains of expert systems and information retrieval are also discussed.\n    ",
        "submission_date": "2013-03-27T00:00:00",
        "last_modified_date": "2013-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.3879",
        "title": "Automatic case acquisition from texts for process-oriented case-based reasoning",
        "authors": [
            "Valmi Dufour-Lussier",
            "Florence Le Ber",
            "Jean Lieber",
            "Emmanuel Nauer"
        ],
        "abstract": "This paper introduces a method for the automatic acquisition of a rich case representation from free text for process-oriented case-based reasoning. Case engineering is among the most complicated and costly tasks in implementing a case-based reasoning system. This is especially so for process-oriented case-based reasoning, where more expressive case representations are generally used and, in our opinion, actually required for satisfactory case adaptation. In this context, the ability to acquire cases automatically from procedural texts is a major step forward in order to reason on processes. We therefore detail a methodology that makes case acquisition from processes described as free text possible, with special attention given to assembly instruction texts. This methodology extends the techniques we used to extract actions from cooking recipes. We argue that techniques taken from natural language processing are required for this task, and that they give satisfactory results. An evaluation based on our implemented prototype extracting workflows from recipe texts is provided.\n    ",
        "submission_date": "2013-04-14T00:00:00",
        "last_modified_date": "2013-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.5823",
        "title": "Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors",
        "authors": [
            "Edward Grefenstette"
        ],
        "abstract": "The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate different aspects of predicate logic.\n",
        "submission_date": "2013-04-22T00:00:00",
        "last_modified_date": "2013-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.7359",
        "title": "Constant conditional entropy and related hypotheses",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "\u0141ukasz D\u0119bowski",
            "Ferm\u00edn Moscoso del Prado Mart\u00edn"
        ],
        "abstract": "Constant entropy rate (conditional entropies must remain constant as the sequence length increases) and uniform information density (conditional probabilities must remain constant as the sequence length increases) are two information theoretic principles that are argued to underlie a wide range of linguistic phenomena. Here we revise the predictions of these principles to the light of Hilberg's law on the scaling of conditional entropy in language and related laws. We show that constant entropy rate (CER) and two interpretations for uniform information density (UID), full UID and strong UID, are inconsistent with these laws. Strong UID implies CER but the reverse is not true. Full UID, a particular case of UID, leads to costly uncorrelated sequences that are totally unrealistic. We conclude that CER and its particular cases are incomplete hypotheses about the scaling of conditional entropies.\n    ",
        "submission_date": "2013-04-27T00:00:00",
        "last_modified_date": "2013-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1304.8016",
        "title": "On Semantic Word Cloud Representation",
        "authors": [
            "Lukas Barth",
            "Stephen Kobourov",
            "Sergey Pupyrev",
            "Torsten Ueckerdt"
        ],
        "abstract": "We study the problem of computing semantic-preserving word clouds in which semantically related words are close to each other. While several heuristic approaches have been described in the literature, we formalize the underlying geometric algorithm problem: Word Rectangle Adjacency Contact (WRAC). In this model each word is associated with rectangle with fixed dimensions, and the goal is to represent semantically related words by ensuring that the two corresponding rectangles touch. We design and analyze efficient polynomial-time algorithms for some variants of the WRAC problem, show that several general variants are NP-hard, and describe a number of approximation algorithms. Finally, we experimentally demonstrate that our theoretically-sound algorithms outperform the early heuristics.\n    ",
        "submission_date": "2013-04-23T00:00:00",
        "last_modified_date": "2013-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0194",
        "title": "MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation",
        "authors": [
            "Cihan Aksoy",
            "Vincent Labatut",
            "Chantal Cherifi",
            "Jean-Fran\u00e7ois Santucci"
        ],
        "abstract": "Many recent works aim at developing methods and tools for the processing of semantic Web services. In order to be properly tested, these tools must be applied to an appropriate benchmark, taking the form of a collection of semantic WS descriptions. However, all of the existing publicly available collections are limited by their size or their realism (use of randomly generated or resampled descriptions). Larger and realistic syntactic (WSDL) collections exist, but their semantic annotation requires a certain level of automation, due to the number of operations to be processed. In this article, we propose a fully automatic method to semantically annotate such large WS collections. Our approach is multimodal, in the sense it takes advantage of the latent semantics present not only in the parameter names, but also in the type names and structures. Concept-to-word association is performed by using Sigma, a mapping of WordNet to the SUMO ontology. After having described in details our annotation method, we apply it to the larger collection of real-world syntactic WS descriptions we could find, and assess its efficiency.\n    ",
        "submission_date": "2013-05-01T00:00:00",
        "last_modified_date": "2013-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.0625",
        "title": "CONATION: English Command Input/Output System for Computers",
        "authors": [
            "Kamlesh Sharma",
            "Dr. T. V. Prasad"
        ],
        "abstract": "In this information technology age, a convenient and user friendly interface is required to operate the computer system on very fast rate. In the human being, speech being a natural mode of communication has potential to being a fast and convenient mode of interaction with computer. Speech recognition will play an important role in taking technology to them. It is the need of this era to access the information within seconds. This paper describes the design and development of speaker independent and English command interpreted system for computers. HMM model is used to represent the phoneme like speech commands. Experiments have been done on real world data and system has been trained in normal condition for real world subject.\n    ",
        "submission_date": "2013-05-03T00:00:00",
        "last_modified_date": "2013-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1145",
        "title": "Techniques for Feature Extraction In Speech Recognition System : A Comparative Study",
        "authors": [
            "Urmila Shrawankar",
            "V M Thakare"
        ],
        "abstract": "The time domain waveform of a speech signal carries all of the auditory information. From the phonological point of view, it little can be said on the basis of the waveform itself. However, past research in mathematics, acoustics, and speech technology have provided many methods for converting data that can be considered as information if interpreted correctly. In order to find some statistically relevant information from incoming data, it is important to have mechanisms for reducing the information of each segment in the audio signal into a relatively small number of parameters, or features. These features should describe each segment in such a characteristic way that other similar segments can be grouped together by comparing their features. There are enormous interesting and exceptional ways to describe the speech signal in terms of parameters. Though, they all have their strengths and weaknesses, we have presented some of the most used methods with their importance.\n    ",
        "submission_date": "2013-05-06T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1343",
        "title": "Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings",
        "authors": [
            "Arnim Bleier",
            "Andreas Strotmann"
        ],
        "abstract": "Author co-citation studies employ factor analysis to reduce high-dimensional co-citation matrices to low-dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allow well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace's (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce a new data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis.\n    ",
        "submission_date": "2013-05-06T00:00:00",
        "last_modified_date": "2013-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1426",
        "title": "Speech Enhancement Modeling Towards Robust Speech Recognition System",
        "authors": [
            "Urmila Shrawankar",
            "V. M. Thakare"
        ],
        "abstract": "Form about four decades human beings have been dreaming of an intelligent machine which can master the natural speech. In its simplest form, this machine should consist of two subsystems, namely automatic speech recognition (ASR) and speech understanding (SU). The goal of ASR is to transcribe natural speech while SU is to understand the meaning of the transcription. Recognizing and understanding a spoken sentence is obviously a knowledge-intensive process, which must take into account all variable information about the speech communication process, from acoustics to semantics and pragmatics. While developing an Automatic Speech Recognition System, it is observed that some adverse conditions degrade the performance of the Speech Recognition System. In this contribution, speech enhancement system is introduced for enhancing speech signals corrupted by additive noise and improving the performance of Automatic Speech Recognizers in noisy conditions. Automatic speech recognition experiments show that replacing noisy speech signals by the corresponding enhanced speech signals leads to an improvement in the recognition accuracies. The amount of improvement varies with the type of the corrupting noise.\n    ",
        "submission_date": "2013-05-07T00:00:00",
        "last_modified_date": "2013-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.1925",
        "title": "Speech: A Challenge to Digital Signal Processing Technology for Human-to-Computer Interaction",
        "authors": [
            "Urmila Shrawankar",
            "Anjali Mahajan"
        ],
        "abstract": "This software project based paper is for a vision of the near future in which computer interaction is characterized by natural face-to-face conversations with lifelike characters that speak, emote, and gesture. The first step is speech. The dream of a true virtual reality, a complete human-computer interaction system will not come true unless we try to give some perception to machine and make it perceive the outside world as humans communicate with each other. This software project is under development for listening and replying machine (Computer) through speech. The Speech interface is developed to convert speech input into some parametric form (Speech-to-Text) for further processing and the results, text output to speech synthesis (Text-to-Speech)\n    ",
        "submission_date": "2013-05-08T00:00:00",
        "last_modified_date": "2013-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2352",
        "title": "Speech Enhancement Using Pitch Detection Approach For Noisy Environment",
        "authors": [
            "Rashmi Makhijani",
            "Urmila Shrawankar",
            "V M Thakare"
        ],
        "abstract": "Acoustical mismatch among training and testing phases degrades outstandingly speech recognition results. This problem has limited the development of real-world nonspecific applications, as testing conditions are highly variant or even unpredictable during the training process. Therefore the background noise has to be removed from the noisy speech signal to increase the signal intelligibility and to reduce the listener fatigue. Enhancement techniques applied, as pre-processing stages; to the systems remarkably improve recognition results. In this paper, a novel approach is used to enhance the perceived quality of the speech signal when the additive noise cannot be directly controlled. Instead of controlling the background noise, we propose to reinforce the speech signal so that it can be heard more clearly in noisy environments. The subjective evaluation shows that the proposed method improves perceptual quality of speech in various noisy environments. As in some cases speaking may be more convenient than typing, even for rapid typists: many mathematical symbols are missing from the keyboard but can be easily spoken and recognized. Therefore, the proposed system can be used in an application designed for mathematical symbol recognition (especially symbols not available on the keyboard) in schools.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.2959",
        "title": "Automatic Speech Recognition Using Template Model for Man-Machine Interface",
        "authors": [
            "Neema Mishra",
            "Urmila Shrawankar",
            "V M Thakare"
        ],
        "abstract": "Speech is a natural form of communication for human beings, and computers with the ability to understand speech and speak with a human voice are expected to contribute to the development of more natural man-machine interfaces. Computers with this kind of ability are gradually becoming a reality, through the evolution of speech recognition technologies. Speech is being an important mode of interaction with computers. In this paper Feature extraction is implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern matching is done using Dynamic time warping (DTW) algorithm.\n    ",
        "submission_date": "2013-05-09T00:00:00",
        "last_modified_date": "2013-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.3107",
        "title": "I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in Twitter",
        "authors": [
            "Sasa Petrovic",
            "Miles Osborne",
            "Victor Lavrenko"
        ],
        "abstract": "Twitter has become a major source of data for social media researchers. One important aspect of Twitter not previously considered are {\\em deletions} -- removal of tweets from the stream. Deletions can be due to a multitude of reasons such as privacy concerns, rashness or attempts to undo public statements. We show how deletions can be automatically predicted ahead of time and analyse which tweets are likely to be deleted and how.\n    ",
        "submission_date": "2013-05-14T00:00:00",
        "last_modified_date": "2013-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.5566",
        "title": "The most controversial topics in Wikipedia: A multilingual and geographical analysis",
        "authors": [
            "Taha Yasseri",
            "Anselm Spoerri",
            "Mark Graham",
            "J\u00e1nos Kert\u00e9sz"
        ],
        "abstract": "We present, visualize and analyse the similarities and differences between the controversial topics related to \"edit wars\" identified in 10 different language versions of Wikipedia. After a brief review of the related work we describe the methods developed to locate, measure, and categorize the controversial topics in the different languages. Visualizations of the degree of overlap between the top 100 lists of most controversial articles in different languages and the content related to geographical locations will be presented. We discuss what the presented analysis and visualizations can tell us about the multicultural aspects of Wikipedia and practices of peer-production. Our results indicate that Wikipedia is more than just an encyclopaedia; it is also a window into convergent and divergent social-spatial priorities, interests and preferences.\n    ",
        "submission_date": "2013-05-23T00:00:00",
        "last_modified_date": "2013-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1305.7014",
        "title": "Tweets Miner for Stock Market Analysis",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "In this paper, we present a software package for the data mining of Twitter microblogs for the purpose of using them for the stock market analysis. The package is written in R langauge using apropriate R packages. The model of tweets has been considered. We have also compared stock market charts with frequent sets of keywords in Twitter microblogs messages.\n    ",
        "submission_date": "2013-05-30T00:00:00",
        "last_modified_date": "2013-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.0963",
        "title": "Inferring Robot Task Plans from Human Team Meetings: A Generative Modeling Approach with Logic-Based Prior",
        "authors": [
            "Been Kim",
            "Caleb M. Chacha",
            "Julie Shah"
        ],
        "abstract": "We aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains, such as military field operations and disaster response. Deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. A human operator then translates the agreed upon plan into machine instructions for the robots. We present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. Our approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans. This hybrid approach enables us to overcome the challenge of performing inference over the large solution space with only a small amount of noisy data from the team planning session. We validate the algorithm through human subject experimentation and show we are able to infer a human team's final plan with 83% accuracy on average. We also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a PR2 robot. To the best of our knowledge, this is the first work that integrates a logical planning technique within a generative model to perform plan inference.\n    ",
        "submission_date": "2013-06-05T00:00:00",
        "last_modified_date": "2013-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.1927",
        "title": "Learning About Meetings",
        "authors": [
            "Been Kim",
            "Cynthia Rudin"
        ],
        "abstract": "Most people participate in meetings almost every day, multiple times a day. The study of meetings is important, but also challenging, as it requires an understanding of social signals and complex interpersonal dynamics. Our aim this work is to use a data-driven approach to the science of meetings. We provide tentative evidence that: i) it is possible to automatically detect when during the meeting a key decision is taking place, from analyzing only the local dialogue acts, ii) there are common patterns in the way social dialogue acts are interspersed throughout a meeting, iii) at the time key decisions are made, the amount of time left in the meeting can be predicted from the amount of time that has passed, iv) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language (the set of persuasive words) used by the speaker.\n    ",
        "submission_date": "2013-06-08T00:00:00",
        "last_modified_date": "2013-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2268",
        "title": "Accomplishable Tasks in Knowledge Representation",
        "authors": [
            "Keehang Kwon",
            "Mi-Young Park"
        ],
        "abstract": "Knowledge Representation (KR) is traditionally based on the logic of facts, expressed in boolean logic. However, facts about an agent can also be seen as a set of accomplished tasks by the agent. This paper proposes a new approach to KR: the notion of task logical KR based on Computability Logic. This notion allows the user to represent both accomplished tasks and accomplishable tasks by the agent. This notion allows us to build sophisticated KRs about many interesting agents, which have not been supported by previous logical languages.\n    ",
        "submission_date": "2013-06-07T00:00:00",
        "last_modified_date": "2013-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2499",
        "title": "Using Arabic Wordnet for semantic indexation in information retrieval system",
        "authors": [
            "Mohammed Alaeddine Abderrahim",
            "Mohammed El Amine Abderrahim",
            "Mohammed Amine Chikh"
        ],
        "abstract": "In the context of arabic Information Retrieval Systems (IRS) guided by arabic ontology and to enable those systems to better respond to user requirements, this paper aims to representing documents and queries by the best concepts extracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNet synsets are extracted from documents and queries, and those having a single sense are expanded. The expanded query is then used by the IRS to retrieve the relevant documents searched. Our experiments are based primarily on a medium size corpus of arabic text. The results obtained shown us that there are a global improvement in the performance of the arabic IRS.\n    ",
        "submission_date": "2013-06-11T00:00:00",
        "last_modified_date": "2013-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.2593",
        "title": "A Perceptual Alphabet for the 10-dimensional Phonetic-prosodic Space",
        "authors": [
            "Elaine Y L Tsiang"
        ],
        "abstract": "We define an alphabet, the IHA, of the 10-D phonetic-prosodic space. The dimensions of this space are perceptual observables, rather than articulatory specifications. Speech is defined as a random chain in time of the 4-D phonetic subspace, that is, a symbolic sequence, augmented with diacritics of the remaining 6-D prosodic subspace. The definitions here are based on the model of speech of oral billiards, and supersedes an earlier version. This paper only enumerates the IHA in detail as a supplement to the exposition of oral billiards in a separate paper. The IHA has been implemented as the target random variable in a speech recognizer.\n    ",
        "submission_date": "2013-06-11T00:00:00",
        "last_modified_date": "2020-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5170",
        "title": "Clinical Relationships Extraction Techniques from Patient Narratives",
        "authors": [
            "Wafaa Tawfik Abdel-moneim",
            "Mohamed Hashem Abdel-Aziz",
            "Mohamed Monier Hassan"
        ],
        "abstract": "The Clinical E-Science Framework (CLEF) project was used to extract important information from medical texts by building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in the text. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from these texts that are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords: Text mining; information extraction; NLP; entities; and relations.\n    ",
        "submission_date": "2013-06-21T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1306.5263",
        "title": "Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences",
        "authors": [
            "Haonan Yu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a method for learning word meanings from complex and realistic video clips by discriminatively training (DT) positive sentential labels against negative ones, and then use the trained word models to generate sentential descriptions for new video. This new work is inspired by recent work which adopts a maximum likelihood (ML) framework to address the same problem using only positive sentential labels. The new method, like the ML-based one, is able to automatically determine which words in the sentence correspond to which concepts in the video (i.e., ground words to meanings) in a weakly supervised fashion. While both DT and ML yield comparable results with sufficient training data, DT outperforms ML significantly with smaller training sets because it can exploit negative training labels to better constrain the learning problem.\n    ",
        "submission_date": "2013-06-21T00:00:00",
        "last_modified_date": "2013-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0087",
        "title": "Semantics and pragmatics in actual software applications and in web search engines: exploring innovations",
        "authors": [
            "Fabrizio M.A. Lolli"
        ],
        "abstract": "While new ways to use the Semantic Web are developed every week, which allow the user to find information on web more accurately - for example in search engines - some sophisticated pragmatic tools are becoming more important - for example in web interfaces known as Social Intelligence, or in the most famous Siri by Apple. The work aims to analyze whether and where we can identify the boundary between semantics and pragmatics in the software used by analyzed systems. examining how the linguistic disciplines are fundamental in their progress. Is it possible to assume that the tools of social intelligence have a pragmatic approach to the questions of the user, or it is just a use of a very rich vocabulary, with the use of semantic tools?\n    ",
        "submission_date": "2013-06-29T00:00:00",
        "last_modified_date": "2013-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.0261",
        "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction",
        "authors": [
            "Bhavana Dalvi",
            "William W. Cohen",
            "Jamie Callan"
        ],
        "abstract": "We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.\n    ",
        "submission_date": "2013-07-01T00:00:00",
        "last_modified_date": "2013-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.3040",
        "title": "Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems",
        "authors": [
            "Mehul Bhatt"
        ],
        "abstract": "What lies between `\\emph{sensing}' and `\\emph{sensibility}'? In other words, what kind of cognitive processes mediate sensing capability, and the formation of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory formation, beliefs and their revision, argument formation--- in domain-specific problem solving, or in regular activities of everyday living, working and simply going around in the environment? How can knowledge and reasoning about such capabilities, as exhibited by humans in particular problem contexts, be used as a model and benchmark for the development of collaborative cognitive (interaction) systems concerned with human assistance, assurance, and empowerment?\n",
        "submission_date": "2013-07-11T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4038",
        "title": "An alternative Gospel of structure: order, composition, processes",
        "authors": [
            "Bob Coecke"
        ],
        "abstract": "We survey some basic mathematical structures, which arguably are more primitive than the structures taught at school. These structures are orders, with or without composition, and (symmetric) monoidal categories. We list several `real life' incarnations of each of these. This paper also serves as an introduction to these structures and their current and potentially future uses in linguistics, physics and knowledge representation.\n    ",
        "submission_date": "2013-07-15T00:00:00",
        "last_modified_date": "2013-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.4986",
        "title": "On the Necessity of Mixed Models: Dynamical Frustrations in the Mind",
        "authors": [
            "Diego Gabriel Krivochen"
        ],
        "abstract": "In the present work we will present and analyze some basic processes at the local and global level in linguistic derivations that seem to go beyond the limits of Markovian or Turing-like computation, and require, in our opinion, a quantum processor. We will first present briefly the working hypothesis and then focus on the empirical domain. At the same time, we will argue that a model appealing to only one kind of computation (be it quantum or not) is necessarily insufficient, and thus both linear and non-linear formal models are to be invoked in order to pursue a fuller understanding of mental computations within a unified framework.\n    ",
        "submission_date": "2013-07-18T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6235",
        "title": "Graphical law beneath each written natural language",
        "authors": [
            "Anindya Kumar Biswas"
        ],
        "abstract": "We study twenty four written natural languages. We draw in the log scale, number of words starting with a letter vs rank of the letter, both normalised. We find that all the graphs are of the similar type. The graphs are tantalisingly closer to the curves of reduced magnetisation vs reduced temperature for magnetic materials. We make a weak conjecture that a curve of magnetisation underlies a written natural language.\n    ",
        "submission_date": "2013-07-18T00:00:00",
        "last_modified_date": "2020-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.6937",
        "title": "A Novel Architecture For Question Classification Based Indexing Scheme For Efficient Question Answering",
        "authors": [
            "Renu Mudgal",
            "Rosy Madaan",
            "A.K.Sharma",
            "Ashutosh Dixit"
        ],
        "abstract": "Question answering system can be seen as the next step in information retrieval, allowing users to pose question in natural language and receive compact answers. For the Question answering system to be successful, research has shown that the correct classification of question with respect to the expected answer type is requisite. We propose a novel architecture for question classification and searching in the index, maintained on the basis of expected answer types, for efficient question answering. The system uses the criteria for Answer Relevance Score for finding the relevance of each answer returned by the system. On analysis of the proposed system, it has been found that the system has shown promising results than the existing systems based on question classification.\n    ",
        "submission_date": "2013-07-26T00:00:00",
        "last_modified_date": "2013-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8060",
        "title": "Extracting Information-rich Part of Texts using Text Denoising",
        "authors": [
            "Rushdi Shams"
        ],
        "abstract": "The aim of this paper is to report on a novel text reduction technique, called Text Denoising, that highlights information-rich content when processing a large volume of text data, especially from the biomedical domain. The core feature of the technique, the text readability index, embodies the hypothesis that complex text is more information-rich than the rest. When applied on tasks like biomedical relation bearing text extraction, keyphrase indexing and extracting sentences describing protein interactions, it is evident that the reduced set of text produced by text denoising is more information-rich than the rest.\n    ",
        "submission_date": "2013-07-30T00:00:00",
        "last_modified_date": "2013-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1307.8225",
        "title": "A Novel Architecture for Relevant Blog Page Identifcation",
        "authors": [
            "Deepti Kapri",
            "Rosy Madaan",
            "A. K Sharma",
            "Ashutosh Dixit"
        ],
        "abstract": "Blogs are undoubtedly the richest source of information available in cyberspace. Blogs can be of various natures i.e. personal blogs which contain posts on mixed issues or blogs can be domain specific which contains posts on particular topics, this is the reason, they offer wide variety of relevant information which is often focused. A general search engine gives back a huge collection of web pages which may or may not give correct answers, as web is the repository of information of all kinds and a user has to go through various documents before he gets what he was originally looking for, which is a very time consuming process. So, the search can be made more focused and accurate if it is limited to blogosphere instead of web pages. The reason being that the blogs are more focused in terms of information. So, User will only get related blogs in response to his query. These results will be then ranked according to our proposed method and are finally presented in front of user in descending order\n    ",
        "submission_date": "2013-07-31T00:00:00",
        "last_modified_date": "2013-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0661",
        "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical Texts",
        "authors": [
            "Samet Atda\u011f",
            "Vincent Labatut"
        ],
        "abstract": "Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating Wikipedia articles. We then select publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0701",
        "title": "Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text",
        "authors": [
            "Meisam Booshehri",
            "Abbas Malekpour",
            "Peter Luksch",
            "Kamran Zamanifar",
            "Shahdad Shariatmadari"
        ],
        "abstract": "In this position paper we present a new approach for discovering some special classes of assertional knowledge in the text by using large RDF repositories, resulting in the extraction of new non-taxonomic ontological relations. Also we use inductive reasoning beside our approach to make it outperform. Then, we prepare a case study by applying our approach on sample data and illustrate the soundness of our proposed approach. Moreover in our point of view current LOD cloud is not a suitable base for our proposal in all informational domains. Therefore we figure out some directions based on prior works to enrich datasets of Linked Data by using web mining. The result of such enrichment can be reused for further relation extraction and ontology enrichment from unstructured free text documents.\n    ",
        "submission_date": "2013-08-03T00:00:00",
        "last_modified_date": "2013-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.0850",
        "title": "Generating Sequences With Recurrent Neural Networks",
        "authors": [
            "Alex Graves"
        ],
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.\n    ",
        "submission_date": "2013-08-04T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.1292",
        "title": "Science Fiction as a Worldwide Phenomenon: A Study of International Creation, Consumption and Dissemination",
        "authors": [
            "Elysia Wells"
        ],
        "abstract": "This paper examines the international nature of science fiction. The focus of this research is to determine whether science fiction is primarily English speaking and Western or global; being created and consumed by people in non-Western, non-English speaking countries? Science fiction's international presence was found in three ways, by network analysis, by examining a online retailer and with a survey. Condor, a program developed by GalaxyAdvisors was used to determine if science fiction is being talked about by non-English speakers. An analysis of the international ",
        "submission_date": "2013-08-06T00:00:00",
        "last_modified_date": "2013-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3243",
        "title": "Arabic Text Recognition in Video Sequences",
        "authors": [
            "M. Ben Halima",
            "H. Karray",
            "A. M. Alimi"
        ],
        "abstract": "In this paper, we propose a robust approach for text extraction and recognition from Arabic news video sequence. The text included in video sequences is an important needful for indexing and searching system. However, this text is difficult to detect and recognize because of the variability of its size, their low resolution characters and the complexity of the backgrounds. To solve these problems, we propose a system performing in two main tasks: extraction and recognition of text. Our system is tested on a varied database composed of different Arabic news programs and the obtained results are encouraging and show the merits of our approach.\n    ",
        "submission_date": "2013-08-14T00:00:00",
        "last_modified_date": "2013-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.3294",
        "title": "A Secure and Comparable Text Encryption Algorithm",
        "authors": [
            "Nicholas Kersting"
        ],
        "abstract": "This paper discloses a simple algorithm for encrypting text messages, based on the NP-completeness of the subset sum problem, such that the similarity between encryptions is roughly proportional to the semantic similarity between their generating messages. This allows parties to compare encrypted messages for semantic overlap without trusting an intermediary and might be applied, for example, as a means of finding scientific collaborators over the Internet.\n    ",
        "submission_date": "2013-08-15T00:00:00",
        "last_modified_date": "2013-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4189",
        "title": "Seeing What You're Told: Sentence-Guided Activity Recognition In Video",
        "authors": [
            "N. Siddharth",
            "Andrei Barbu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present a system that demonstrates how the compositional structure of events, in concert with the compositional structure of language, can interplay with the underlying focusing mechanisms in video action recognition, thereby providing a medium, not only for top-down and bottom-up integration, but also for multi-modal integration between vision and language. We show how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) in the form of whole sentential descriptions mediated by a grammar, guides the activity-recognition process. Further, the utility and expressiveness of our framework is demonstrated by performing three separate tasks in the domain of multi-activity videos: sentence-guided focus of attention, generation of sentential descriptions of video, and query-based video search, simply by leveraging the framework in different manners.\n    ",
        "submission_date": "2013-08-19T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4648",
        "title": "PACE: Pattern Accurate Computationally Efficient Bootstrapping for Timely Discovery of Cyber-Security Concepts",
        "authors": [
            "Nikki McNeil",
            "Robert A. Bridges",
            "Michael D. Iannacone",
            "Bogdan Czejdo",
            "Nicolas Perez",
            "John R. Goodall"
        ],
        "abstract": "Public disclosure of important security information, such as knowledge of vulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and other online sources months before proper classification into structured databases. In order to facilitate timely discovery of such knowledge, we propose a novel semi-supervised learning algorithm, PACE, for identifying and classifying relevant entities in text sources. The main contribution of this paper is an enhancement of the traditional bootstrapping method for entity extraction by employing a time-memory trade-off that simultaneously circumvents a costly corpus search while strengthening pattern nomination, which should increase accuracy. An implementation in the cyber-security domain is discussed as well as challenges to Natural Language Processing imposed by the security domain.\n    ",
        "submission_date": "2013-08-21T00:00:00",
        "last_modified_date": "2013-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4941",
        "title": "Automatic Labeling for Entity Extraction in Cyber Security",
        "authors": [
            "Robert A. Bridges",
            "Corinne L. Jones",
            "Michael D. Iannacone",
            "Kelly M. Testa",
            "John R. Goodall"
        ],
        "abstract": "Timely analysis of cyber-security information necessitates automated information extraction from unstructured text. While state-of-the-art extraction methods produce extremely accurate results, they require ample training data, which is generally unavailable for specialized applications, such as detecting security related entities; moreover, manual annotation of corpora is very costly and often not a viable solution. In response, we develop a very precise method to automatically label text from several data sources by leveraging related, domain-specific, structured data and provide public access to a corpus annotated with cyber-security entities. Next, we implement a Maximum Entropy Model trained with the average perceptron on a portion of our corpus ($\\sim$750,000 words) and achieve near perfect precision, recall, and accuracy, with training times under 17 seconds.\n    ",
        "submission_date": "2013-08-22T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.4965",
        "title": "A proposal for a Chinese keyboard for cellphones, smartphones, ipads and tablets",
        "authors": [
            "Maurice Margenstern",
            "Lan Wu"
        ],
        "abstract": "In this paper, we investigate the possibility to use two tilings of the hyperbolic plane as basic frame for devising a way to input texts in Chinese characters into messages of cellphones, smartphones, ipads and tablets.\n    ",
        "submission_date": "2013-08-21T00:00:00",
        "last_modified_date": "2013-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.5010",
        "title": "Sentiment in New York City: A High Resolution Spatial and Temporal View",
        "authors": [
            "Karla Z. Bertrand",
            "Maya Bialik",
            "Kawandeep Virdee",
            "Andreas Gros",
            "Yaneer Bar-Yam"
        ],
        "abstract": "Measuring public sentiment is a key task for researchers and policymakers alike. The explosion of available social media data allows for a more time-sensitive and geographically specific analysis than ever before. In this paper we analyze data from the micro-blogging site Twitter and generate a sentiment map of New York City. We develop a classifier specifically tuned for 140-character Twitter messages, or tweets, using key words, phrases and emoticons to determine the mood of each tweet. This method, combined with geotagging provided by users, enables us to gauge public sentiment on extremely fine-grained spatial and temporal scales. We find that public mood is generally highest in public parks and lowest at transportation hubs, and locate other areas of strong sentiment such as cemeteries, medical centers, a jail, and a sewage facility. Sentiment progressively improves with proximity to Times Square. Periodic patterns of sentiment fluctuate on both a daily and a weekly scale: more positive tweets are posted on weekends than on weekdays, with a daily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon.\n    ",
        "submission_date": "2013-08-22T00:00:00",
        "last_modified_date": "2013-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1308.6628",
        "title": "Joint Video and Text Parsing for Understanding Events and Answering Queries",
        "authors": [
            "Kewei Tu",
            "Meng Meng",
            "Mun Wai Lee",
            "Tae Eun Choe",
            "Song-Chun Zhu"
        ],
        "abstract": "We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.\n    ",
        "submission_date": "2013-08-29T00:00:00",
        "last_modified_date": "2014-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1501",
        "title": "Improvements to deep convolutional neural networks for LVCSR",
        "authors": [
            "Tara N. Sainath",
            "Brian Kingsbury",
            "Abdel-rahman Mohamed",
            "George E. Dahl",
            "George Saon",
            "Hagen Soltau",
            "Tomas Beran",
            "Aleksandr Y. Aravkin",
            "Bhuvana Ramabhadran"
        ],
        "abstract": "Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12% relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep analysis comparing limited weight sharing and full weight sharing with state-of-the-art features. Second, we apply various pooling strategies that have shown improvements in computer vision to an LVCSR speech task. Third, we introduce a method to effectively incorporate speaker adaptation, namely fMLLR, into log-mel features. Fourth, we introduce an effective strategy to use dropout during Hessian-free sequence training. We find that with these improvements, particularly with fMLLR and dropout, we are able to achieve an additional 2-3% relative improvement in WER on a 50-hour Broadcast News task over our previous best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5% relative improvement over our previous best CNN baseline.\n    ",
        "submission_date": "2013-09-05T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.1508",
        "title": "Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and sampling",
        "authors": [
            "Tara N. Sainath",
            "Lior Horesh",
            "Brian Kingsbury",
            "Aleksandr Y. Aravkin",
            "Bhuvana Ramabhadran"
        ],
        "abstract": "Hessian-free training has become a popular parallel second or- der optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction of the number of Krylov subspace solver iterations used for implicit estimation of the Hessian. In this paper, we develop an L-BFGS based preconditioning scheme that avoids the need to access the Hessian explicitly. Since L-BFGS cannot be regarded as a fixed-point iteration, we further propose the employment of flexible Krylov subspace solvers that retain the desired theoretical convergence guarantees of their conventional counterparts. Second, we propose a new sampling algorithm, which geometrically increases the amount of data utilized for gradient and Krylov subspace iteration calculations. On a 50-hr English Broadcast News task, we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a 300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no loss in WER. These results suggest that even further speed-up is expected, as problems scale and complexity grows.\n    ",
        "submission_date": "2013-09-05T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3946",
        "title": "Using Self-Organizing Maps for Sentiment Analysis",
        "authors": [
            "Anuj Sharma",
            "Shubhamoy Dey"
        ],
        "abstract": "Web 2.0 services have enabled people to express their opinions, experience and feelings in the form of user-generated content. Sentiment analysis or opinion mining involves identifying, classifying and aggregating opinions as per their positive or negative polarity. This paper investigates the efficacy of different implementations of Self-Organizing Maps (SOM) for sentiment based visualization and classification of online reviews. Specifically, this paper implements the SOM algorithm for both supervised and unsupervised learning from text documents. The unsupervised SOM algorithm is implemented for sentiment based visualization and classification tasks. For supervised sentiment analysis, a competitive learning algorithm known as Learning Vector Quantization is used. Both algorithms are also compared with their respective multi-pass implementations where a quick rough ordering pass is followed by a fine tuning pass. The experimental results on the online movie review data set show that SOMs are well suited for sentiment based classification and sentiment polarity visualization.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.3949",
        "title": "Performance Investigation of Feature Selection Methods",
        "authors": [
            "Anuj sharma",
            "Shubhamoy Dey"
        ],
        "abstract": "Sentiment analysis or opinion mining has become an open research domain after proliferation of Internet and Web 2.0 social media. People express their attitudes and opinions on social media including blogs, discussion forums, tweets, etc. and, sentiment analysis concerns about detecting and extracting sentiment or opinion from online text. Sentiment based text classification is different from topical text classification since it involves discrimination based on expressed opinion on a topic. Feature selection is significant for sentiment analysis as the opinionated text may have high dimensions, which can adversely affect the performance of sentiment analysis classifier. This paper explores applicability of feature selection methods for sentiment analysis and investigates their performance for classification in term of recall, precision and accuracy. Five feature selection methods (Document Frequency, Information Gain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment feature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews corpus with a size of 2000 documents. The experimental results show that Information Gain gave consistent results and Gain Ratio performs overall best for sentimental feature selection while sentiment lexicons gave poor performance. Furthermore, we found that performance of the classifier depends on appropriate number of representative feature selected from text.\n    ",
        "submission_date": "2013-09-16T00:00:00",
        "last_modified_date": "2013-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.5174",
        "title": "Saying What You're Looking For: Linguistics Meets Video Search",
        "authors": [
            "Andrei Barbu",
            "N. Siddharth",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We present an approach to searching large video corpora for video clips which depict a natural-language query in the form of a sentence. This approach uses compositional semantics to encode subtle meaning that is lost in other systems, such as the difference between two sentences which have identical words but entirely different meaning: \"The person rode the horse} vs. \\emph{The horse rode the person\". Given a video-sentence pair and a natural-language parser, along with a grammar that describes the space of sentential queries, we produce a score which indicates how well the video depicts the sentence. We produce such a score for each video clip in a corpus and return a ranked list of clips. Furthermore, this approach addresses two fundamental problems simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, this uses knowledge about the intended sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While earlier work was limited to single-word queries which correspond to either verbs or nouns, we show how one can search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 141 queries involving people and horses interacting with each other in 10 full-length Hollywood movies.\n    ",
        "submission_date": "2013-09-20T00:00:00",
        "last_modified_date": "2013-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6047",
        "title": "Non-negative Matrix Factorization with Linear Constraints for Single-Channel Speech Enhancement",
        "authors": [
            "Nikolay Lyubimov",
            "Mikhail Kotov"
        ],
        "abstract": "This paper investigates a non-negative matrix factorization (NMF)-based approach to the semi-supervised single-channel speech enhancement problem where only non-stationary additive noise signals are given. The proposed method relies on sinusoidal model of speech production which is integrated inside NMF framework using linear constraints on dictionary atoms. This method is further developed to regularize harmonic amplitudes. Simple multiplicative algorithms are presented. The experimental evaluation was made on TIMIT corpus mixed with various types of noise. It has been shown that the proposed method outperforms some of the state-of-the-art noise suppression techniques in terms of signal-to-noise ratio.\n    ",
        "submission_date": "2013-09-24T00:00:00",
        "last_modified_date": "2013-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.6874",
        "title": "Integrating Document Clustering and Topic Modeling",
        "authors": [
            "Pengtao Xie",
            "Eric P. Xing"
        ],
        "abstract": "Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across ",
        "submission_date": "2013-09-26T00:00:00",
        "last_modified_date": "2013-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7270",
        "title": "Evaluating the Usefulness of Sentiment Information for Focused Crawlers",
        "authors": [
            "Tianjun Fu",
            "Ahmed Abbasi",
            "Daniel Zeng",
            "Hsinchun Chen"
        ],
        "abstract": "Despite the prevalence of sentiment-related content on the Web, there has been limited work on focused crawlers capable of effectively collecting such content. In this study, we evaluated the efficacy of using sentiment-related information for enhanced focused crawling of opinion-rich web content regarding a particular topic. We also assessed the impact of using sentiment-labeled web graphs to further improve collection accuracy. Experimental results on a large test bed encompassing over half a million web pages revealed that focused crawlers utilizing sentiment information as well as sentiment-labeled web graphs are capable of gathering more holistic collections of opinion-related content regarding a particular topic. The results have important implications for business and marketing intelligence gathering efforts in the Web 2.0 era.\n    ",
        "submission_date": "2013-09-27T00:00:00",
        "last_modified_date": "2013-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1309.7340",
        "title": "Early Stage Influenza Detection from Twitter",
        "authors": [
            "Jiwei Li",
            "Claire Cardie"
        ],
        "abstract": "Influenza is an acute respiratory illness that occurs virtually every year and results in substantial disease, death and expense. Detection of Influenza in its earliest stage would facilitate timely action that could reduce the spread of the illness. Existing systems such as CDC and EISS which try to collect diagnosis data, are almost entirely manual, resulting in about two-week delays for clinical data acquisition. Twitter, a popular microblogging service, provides us with a perfect source for early-stage flu detection due to its real- time nature. For example, when a flu breaks out, people that get the flu may post related tweets which enables the detection of the flu breakout promptly. In this paper, we investigate the real-time flu detection problem on Twitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal unsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to identify the flu breakout at the earliest stage. We test our model on real Twitter datasets from the United States along with baselines in multiple applications, such as real-time flu breakout detection, future epidemic phase prediction, or Influenza-like illness (ILI) physician visits. Experimental results show the robustness and effectiveness of our approach. We build up a real time flu reporting system based on the proposed approach, and we are hopeful that it would help government or health organizations in identifying flu outbreaks and facilitating timely actions to decrease unnecessary mortality.\n    ",
        "submission_date": "2013-09-27T00:00:00",
        "last_modified_date": "2013-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.1249",
        "title": "Reading Stockholm Riots 2013 in social media by text-mining",
        "authors": [
            "Andrzej Jarynowski",
            "Amir Rostami"
        ],
        "abstract": "The riots in Stockholm in May 2013 were an event that reverberated in the world media for its dimension of violence that had spread through the Swedish capital. In this study we have investigated the role of social media in creating media phenomena via text mining and natural language processing. We have focused on two channels of communication for our analysis: Twitter and ",
        "submission_date": "2013-10-04T00:00:00",
        "last_modified_date": "2013-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2408",
        "title": "Improved Bayesian Logistic Supervised Topic Models with Data Augmentation",
        "authors": [
            "Jun Zhu",
            "Xun Zheng",
            "Bo Zhang"
        ],
        "abstract": "Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.2479",
        "title": "Spatio-temporal variation of conversational utterances on Twitter",
        "authors": [
            "Christian M. Alis",
            "May T. Lim"
        ],
        "abstract": "Conversations reflect the existing norms of a language. Previously, we found that utterance lengths in English fictional conversations in books and movies have shortened over a period of 200 years. In this work, we show that this shortening occurs even for a brief period of 3 years (September 2009-December 2012) using 229 million utterances from Twitter. Furthermore, the subset of geographically-tagged tweets from the United States show an inverse proportion between utterance lengths and the state-level percentage of the Black population. We argue that shortening of utterances can be explained by the increasing usage of jargon including coined words.\n    ",
        "submission_date": "2013-10-09T00:00:00",
        "last_modified_date": "2013-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3099",
        "title": "A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition",
        "authors": [
            "Roland Maas",
            "Christian Huemmer",
            "Armin Sehr",
            "Walter Kellermann"
        ],
        "abstract": "This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation, missing feature, and uncertainty decoding that are well-known in the literature of robust automatic speech recognition. The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition. These extensions, in turn, can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors. By converting the observation models into a Bayesian network representation, we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches. The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches.\n    ",
        "submission_date": "2013-10-11T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3333",
        "title": "Visualizing Bags of Vectors",
        "authors": [
            "Sriramkumar Balasubramanian",
            "Raghuram Reddy Nagireddy"
        ],
        "abstract": "The motivation of this work is two-fold - a) to compare between two different modes of visualizing data that exists in a bag of vectors format b) to propose a theoretical model that supports a new mode of visualizing data. Visualizing high dimensional data can be achieved using Minimum Volume Embedding, but the data has to exist in a format suitable for computing similarities while preserving local distances. This paper compares the visualization between two methods of representing data and also proposes a new method providing sample visualizations for that method.\n    ",
        "submission_date": "2013-10-12T00:00:00",
        "last_modified_date": "2013-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3499",
        "title": "Forecasting of Events by Tweet Data Mining",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "This paper describes the analysis of quantitative characteristics of frequent sets and association rules in the posts of Twitter microblogs related to different event discussions. For the analysis, we used a theory of frequent sets, association rules and a theory of formal concept analysis. We revealed the frequent sets and association rules which characterize the semantic relations between the concepts of analyzed subjects. The support of some frequent sets reaches its global maximum before the expected event but with some time delay. Such frequent sets may be considered as predictive markers that characterize the significance of expected events for blogosphere users. We showed that the time dynamics of confidence in some revealed association rules can also have predictive characteristics. Exceeding a certain threshold may be a signal for corresponding reaction in the society within the time interval between the maximum and the probable coming of an event. In this paper, we considered two types of events: the Olympic tennis tournament final in London, 2012 and the prediction of Eurovision 2013 winner.\n    ",
        "submission_date": "2013-10-13T00:00:00",
        "last_modified_date": "2013-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.3500",
        "title": "Can Twitter Predict Royal Baby's Name ?",
        "authors": [
            "Bohdan Pavlyshenko"
        ],
        "abstract": "In this paper, we analyze the existence of possible correlation between public opinion of twitter users and the decision-making of persons who are influential in the society. We carry out this analysis on the example of the discussion of probable name of the British crown baby, born in July, 2013. In our study, we use the methods of quantitative processing of natural language, the theory of frequent sets, the algorithms of visual displaying of users' communities. We also analyzed the time dynamics of keyword frequencies. The analysis showed that the main predictable name was dominating in the spectrum of names before the official announcement. Using the theories of frequent sets, we showed that the full name consisting of three component names was the part of top 5 by the value of support. It was revealed that the structure of dynamically formed users' communities participating in the discussion is determined by only a few leaders who influence significantly the viewpoints of other users.\n    ",
        "submission_date": "2013-10-13T00:00:00",
        "last_modified_date": "2013-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.4909",
        "title": "Text Classification For Authorship Attribution Analysis",
        "authors": [
            "M. Sudheep Elayidom",
            "Chinchu Jose",
            "Anitta Puthussery",
            "Neenu K Sasi"
        ],
        "abstract": "Authorship attribution mainly deals with undecided authorship of literary texts. Authorship attribution is useful in resolving issues like uncertain authorship, recognize authorship of unknown texts, spot plagiarism so on. Statistical methods can be used to set apart the approach of an author numerically. The basic methodologies that are made use in computational stylometry are word length, sentence length, vocabulary affluence, frequencies etc. Each author has an inborn style of writing, which is particular to himself. Statistical quantitative techniques can be used to differentiate the approach of an author in a numerical way. The problem can be broken down into three sub problems as author identification, author characterization and similarity detection. The steps involved are pre-processing, extracting features, classification and author identification. For this different classifiers can be used. Here fuzzy learning classifier and SVM are used. After author identification the SVM was found to have more accuracy than Fuzzy classifier. Later combined the classifiers to obtain a better accuracy when compared to individual SVM and fuzzy classifier.\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5042",
        "title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).\n    ",
        "submission_date": "2013-10-18T00:00:00",
        "last_modified_date": "2013-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.5963",
        "title": "Improving the methods of email classification based on words ontology",
        "authors": [
            "Foruzan Kiamarzpour",
            "Rouhollah Dianat",
            "Mohammad bahrani",
            "Mehdi Sadeghzadeh"
        ],
        "abstract": "The Internet has dramatically changed the relationship among people and their relationships with others people and made the valuable information available for the users. Email is the service, which the Internet provides today for its own users; this service has attracted most of the users' attention due to the low cost. Along with the numerous benefits of Email, one of the weaknesses of this service is that the number of received emails is continually being enhanced, thus the ways are needed to automatically filter these disturbing letters. Most of these filters utilize a combination of several techniques such as the Black or white List, using the keywords and so on in order to identify the spam more accurately In this paper, we introduce a new method to classify the spam. We are seeking to increase the accuracy of Email classification by combining the output of several decision trees and the concept of ontology.\n    ",
        "submission_date": "2013-10-22T00:00:00",
        "last_modified_date": "2013-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.6775",
        "title": "Durkheim Project Data Analysis Report",
        "authors": [
            "Linas Vepstas"
        ],
        "abstract": "This report describes the suicidality prediction models created under the DARPA DCAPS program in association with the Durkheim Project [",
        "submission_date": "2013-10-24T00:00:00",
        "last_modified_date": "2013-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.7782",
        "title": "Individual Biases, Cultural Evolution, and the Statistical Nature of Language Universals: The Case of Colour Naming Systems",
        "authors": [
            "Andrea Baronchelli",
            "Vittorio Loreto",
            "Andrea Puglisi"
        ],
        "abstract": "Language universals have long been attributed to an innate Universal Grammar. An alternative explanation states that linguistic universals emerged independently in every language in response to shared cognitive or perceptual biases. A computational model has recently shown how this could be the case, focusing on the paradigmatic example of the universal properties of colour naming patterns, and producing results in quantitative agreement with the experimental data. Here we investigate the role of an individual perceptual bias in the framework of the model. We study how, and to what extent, the structure of the bias influences the corresponding linguistic universal patterns. We show that the cultural history of a group of speakers introduces population-specific constraints that act against the pressure for uniformity arising from the individual bias, and we clarify the interplay between these two forces.\n    ",
        "submission_date": "2013-10-29T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1310.8511",
        "title": "A Preadapted Universal Switch Distribution for Testing Hilberg's Conjecture",
        "authors": [
            "\u0141ukasz D\u0119bowski"
        ],
        "abstract": "  Hilberg's conjecture about natural language states that the mutual information between two adjacent long blocks of text grows like a power of the block length. The exponent in this statement can be upper bounded using the pointwise mutual information estimate computed for a carefully chosen code. The bound is the better, the lower the compression rate is but there is a requirement that the code be universal. So as to improve a received upper bound for Hilberg's exponent, in this paper, we introduce two novel universal codes, called the plain switch distribution and the preadapted switch distribution. Generally speaking, switch distributions are certain mixtures of adaptive Markov chains of varying orders with some additional communication to avoid so called catch-up phenomenon. The advantage of these distributions is that they both achieve a low compression rate and are guaranteed to be universal. Using the switch distributions we obtain that a sample of a text in English is non-Markovian with Hilberg's exponent being $\\le 0.83$, which improves over the previous bound $\\le 0.94$ obtained using the Lempel-Ziv code.\n    ",
        "submission_date": "2013-10-31T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.1897",
        "title": "Logique math\u00e9matique et linguistique formelle",
        "authors": [
            "Christian Retor\u00e9"
        ],
        "abstract": "As the etymology of the word shows, logic is intimately related to language, as exemplified by the work of philosophers from Antiquity and from the Middle-Age. At the beginning of the XX century, the crisis of the foundations of mathematics invented mathematical logic and imposed logic as a language-based foundation for mathematics. How did the relations between logic and language evolved in this newly defined mathematical framework? After a survey of the history of the relation between logic and linguistics, traditionally focused on semantics, we focus on some present issues: 1) grammar as a deductive system 2) the transformation of the syntactic structure of a sentence to a logical formula representing its meaning 3) taking into account the context when interpreting words. This lecture shows that type theory provides a convenient framework both for natural language syntax and for the interpretation of any of tis level (words, sentences, discourse).\n    ",
        "submission_date": "2013-11-08T00:00:00",
        "last_modified_date": "2013-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.2702",
        "title": "Verifiable Source Code Documentation in Controlled Natural Language",
        "authors": [
            "Tobias Kuhn",
            "Alexandre Bergel"
        ],
        "abstract": "Writing documentation about software internals is rarely considered a rewarding activity. It is highly time-consuming and the resulting documentation is fragile when the software is continuously evolving in a multi-developer setting. Unfortunately, traditional programming environments poorly support the writing and maintenance of documentation. Consequences are severe as the lack of documentation on software structure negatively impacts the overall quality of the software product. We show that using a controlled natural language with a reasoner and a query engine is a viable technique for verifying the consistency and accuracy of documentation and source code. Using ACE, a state-of-the-art controlled natural language, we present positive results on the comprehensibility and the general feasibility of creating and verifying documentation. As a case study, we used automatic documentation verification to identify and fix severe flaws in the architecture of a non-trivial piece of software. Moreover, a user experiment shows that our language is faster and easier to learn and understand than other formal languages for software documentation.\n    ",
        "submission_date": "2013-11-12T00:00:00",
        "last_modified_date": "2013-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1311.6421",
        "title": "Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies",
        "authors": [
            "Pierluigi Crescenzi",
            "Daniel Gildea",
            "Andrea Marino",
            "Gianluca Rossi",
            "Giorgio Satta"
        ],
        "abstract": "Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed translation schemata, are unlike context-free grammars in that they do not have a binary normal form. In general, parsing with SCFGs takes space and time polynomial in the length of the input strings, but with the degree of the polynomial depending on the permutations of the SCFG rules. We consider linear parsing strategies, which add one nonterminal at a time. We show that for a given input permutation, the problems of finding the linear parsing strategy with the minimum space and time complexity are both NP-hard.\n    ",
        "submission_date": "2013-11-25T00:00:00",
        "last_modified_date": "2013-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0493",
        "title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure",
        "authors": [
            "Ozan \u0130rsoy",
            "Claire Cardie"
        ],
        "abstract": "Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach.\n    ",
        "submission_date": "2013-12-02T00:00:00",
        "last_modified_date": "2013-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.0976",
        "title": "Multilinguals and Wikipedia Editing",
        "authors": [
            "Scott A. Hale"
        ],
        "abstract": "This article analyzes one month of edits to Wikipedia in order to examine the role of users editing multiple language editions (referred to as multilingual users). Such multilingual users may serve an important function in diffusing information across different language editions of the encyclopedia, and prior work has suggested this could reduce the level of self-focus bias in each edition. This study finds multilingual users are much more active than their single-edition (monolingual) counterparts. They are found in all language editions, but smaller-sized editions with fewer users have a higher percentage of multilingual users than larger-sized editions. About a quarter of multilingual users always edit the same articles in multiple languages, while just over 40% of multilingual users edit different articles in different languages. When non-English users do edit a second language edition, that edition is most frequently English. Nonetheless, several regional and linguistic cross-editing patterns are also present.\n    ",
        "submission_date": "2013-12-03T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2137",
        "title": "End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks",
        "authors": [
            "Dimitri Palaz",
            "Ronan Collobert",
            "Mathew Magimai.-Doss"
        ],
        "abstract": "Most phoneme recognition state-of-the-art systems rely on a classical neural network classifiers, fed with highly tuned features, such as MFCC or PLP features. Recent advances in ``deep learning'' approaches questioned such systems, but while some attempts were made with simpler features such as spectrograms, state-of-the-art systems still rely on MFCCs. This might be viewed as a kind of failure from deep learning approaches, which are often claimed to have the ability to train with raw signals, alleviating the need of hand-crafted features. In this paper, we investigate a convolutional neural network approach for raw speech signals. While convolutional architectures got tremendous success in computer vision or text processing, they seem to have been let down in the past recent years in the speech processing field. We show that it is possible to learn an end-to-end phoneme sequence classifier system directly from raw signal, with similar performance on the TIMIT and WSJ datasets than existing systems based on MFCC, questioning the need of complex hand-crafted features on large datasets.\n    ",
        "submission_date": "2013-12-07T00:00:00",
        "last_modified_date": "2013-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.2844",
        "title": "mARC: Memory by Association and Reinforcement of Contexts",
        "authors": [
            "Norbert Rimoux",
            "Patrice Descourt"
        ],
        "abstract": "This paper introduces the memory by Association and Reinforcement of Contexts (mARC). mARC is a novel data modeling technology rooted in the second quantization formulation of quantum mechanics. It is an all-purpose incremental and unsupervised data storage and retrieval system which can be applied to all types of signal or data, structured or unstructured, textual or not. mARC can be applied to a wide range of information clas-sification and retrieval problems like e-Discovery or contextual navigation. It can also for-mulated in the artificial life framework a.k.a Conway \"Game Of Life\" Theory. In contrast to Conway approach, the objects evolve in a massively multidimensional space. In order to start evaluating the potential of mARC we have built a mARC-based Internet search en-gine demonstrator with contextual functionality. We compare the behavior of the mARC demonstrator with Google search both in terms of performance and relevance. In the study we find that the mARC search engine demonstrator outperforms Google search by an order of magnitude in response time while providing more relevant results for some classes of queries.\n    ",
        "submission_date": "2013-12-10T00:00:00",
        "last_modified_date": "2013-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4617",
        "title": "A Survey of Data Mining Techniques for Social Media Analysis",
        "authors": [
            "Mariam Adedoyin-Olowe",
            "Mohamed Medhat Gaber",
            "Frederic Stahl"
        ],
        "abstract": "Social network has gained remarkable attention in the last decade. Accessing social network sites such as Twitter, Facebook LinkedIn and Google+ through the internet and the web 2.0 technologies has become more affordable. People are becoming more interested in and relying on social network for information, news and opinion of other users on diverse subject matters. The heavy reliance on social network sites causes them to generate massive data characterised by three computational issues namely; size, noise and dynamism. These issues often make social network data very complex to analyse manually, resulting in the pertinent use of computational means of analysing them. Data mining provides a wide range of techniques for detecting useful knowledge from massive datasets like trends, patterns and rules [44]. Data mining techniques are used for information retrieval, statistical modelling and machine learning. These techniques employ data pre-processing, data analysis, and data interpretation processes in the course of data analysis. This survey discusses different data mining techniques used in mining diverse aspects of the social network over decades going from the historical techniques to the up-to-date models, including our novel technique named TRCM. All the techniques covered in this survey are listed in the Table.1 including the tools employed as well as names of their authors.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4706",
        "title": "Designing Spontaneous Speech Search Interface for Historical Archives",
        "authors": [
            "Donna Vakharia",
            "Rachel Gibbs"
        ],
        "abstract": "Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.4824",
        "title": "Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm",
        "authors": [
            "B. P. Pande",
            "Pawan Tamta",
            "H. S. Dhami"
        ],
        "abstract": "A language independent stemmer has always been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N gram stemming one step ahead and compare our method with an established algorithm in the field, Porter's Stemmer. Results indicate that our N gram stemmer is not inferior to Porter's linguistic stemmer.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.5198",
        "title": "Learning Semantic Script Knowledge with Event Embeddings",
        "authors": [
            "Ashutosh Modi",
            "Ivan Titov"
        ],
        "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.\n    ",
        "submission_date": "2013-12-18T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6168",
        "title": "Factorial Hidden Markov Models for Learning Representations of Natural Language",
        "authors": [
            "Anjan Nepal",
            "Alexander Yates"
        ],
        "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.\n    ",
        "submission_date": "2013-12-20T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6802",
        "title": "Suffix Stripping Problem as an Optimization Problem",
        "authors": [
            "B. P. Pande",
            "Pawan Tamta",
            "H. S. Dhami"
        ],
        "abstract": "Stemming or suffix stripping, an important part of the modern Information Retrieval systems, is to find the root word (stem) out of a given cluster of words. Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given.\n    ",
        "submission_date": "2013-12-24T00:00:00",
        "last_modified_date": "2013-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.6962",
        "title": "Subjectivity Classification using Machine Learning Techniques for Mining Feature-Opinion Pairs from Web Opinion Sources",
        "authors": [
            "Ahmad Kamal"
        ],
        "abstract": "Due to flourish of the Web 2.0, web opinion sources are rapidly emerging containing precious information useful for both customers and manufactures. Recently, feature based opinion mining techniques are gaining momentum in which customer reviews are processed automatically for mining product features and user opinions expressed over them. However, customer reviews may contain both opinionated and factual sentences. Distillations of factual contents improve mining performance by preventing noisy and irrelevant extraction. In this paper, combination of both supervised machine learning and rule-based approaches are proposed for mining feasible feature-opinion pairs from subjective review sentences. In the first phase of the proposed approach, a supervised machine learning technique is applied for classifying subjective and objective sentences from customer reviews. In the next phase, a rule based method is implemented which applies linguistic and semantic analysis of texts to mine feasible feature-opinion pairs from subjective sentences retained after the first phase. The effectiveness of the proposed methods is established through experimentation over customer reviews on different electronic products.\n    ",
        "submission_date": "2013-12-25T00:00:00",
        "last_modified_date": "2013-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1312.7832",
        "title": "Defining implication relation for classical logic",
        "authors": [
            "Li Fu"
        ],
        "abstract": "In classical logic, \"P implies Q\" is equivalent to \"not-P or Q\". It is well known that the equivalence is problematic. Actually, from \"P implies Q\", \"not-P or Q\" can be inferred (\"Implication-to-Disjunction\" is valid), whereas from \"not-P or Q\", \"P implies Q\" cannot be inferred in general (\"Disjunction-to-Implication\" is not generally valid), so the equivalence between them is invalid in general. This work aims to remove the incorrect Disjunction-to-Implication from classical logic (CL). The logical system (the logic IRL) this paper proposes has the expected properties: (a) CL is obtained by adding Disjunction-to-Implication to IRL, and (b) Disjunction-to-Implication is not derivable in IRL; while (c) fundamental laws in classical logic, including law of excluded middle (LEM) and principle of double negation, law of non-contradiction (LNC) and ex contradictione quodlibet (ECQ), conjunction elimination and disjunction introduction, and hypothetical syllogism and disjunctive syllogism, are all retained in IRL.\n    ",
        "submission_date": "2013-12-30T00:00:00",
        "last_modified_date": "2025-05-02T00:00:00"
    }
]