[
    {
        "url": "https://arxiv.org/abs/cs/0201019",
        "title": "Structure from Motion: Theoretical Foundations of a Novel Approach Using Custom Built Invariants",
        "authors": [
            "Pierre-Louis Bazin",
            "Mireille Boutin"
        ],
        "abstract": "  We rephrase the problem of 3D reconstruction from images in terms of intersections of projections of orbits of custom built Lie groups actions. We then use an algorithmic method based on moving frames \"a la Fels-Olver\" to obtain a fundamental set of invariants of these groups actions. The invariants are used to define a set of equations to be solved by the points of the 3D object, providing a new technique for recovering 3D structure from motion.\n    ",
        "submission_date": "2002-01-22T00:00:00",
        "last_modified_date": "2002-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0202020",
        "title": "The Mysterious Optimality of Naive Bayes: Estimation of the Probability in the System of \"Classifiers\"",
        "authors": [
            "Oleg Kupervasser",
            "Alexsander Vardy"
        ],
        "abstract": "Bayes Classifiers are widely used currently for recognition, identification and knowledge discovery. The fields of application are, for example, image processing, medicine, chemistry (QSAR). But by mysterious way the Naive Bayes Classifier usually gives a very nice and good presentation of a recognition. It can not be improved considerably by more complex models of Bayes Classifier. We demonstrate here a very nice and simple proof of the Naive Bayes Classifier optimality, that can explain this interesting ",
        "submission_date": "2002-02-17T00:00:00",
        "last_modified_date": "2012-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0208005",
        "title": "Probabilistic Search for Object Segmentation and Recognition",
        "authors": [
            "Ulrich Hillenbrand",
            "Gerd Hirzinger"
        ],
        "abstract": "  The problem of searching for a model-based scene interpretation is analyzed within a probabilistic framework. Object models are formulated as generative models for range data of the scene. A new statistical criterion, the truncated object probability, is introduced to infer an optimal sequence of object hypotheses to be evaluated for their match to the data. The truncated probability is partly determined by prior knowledge of the objects and partly learned from data. Some experiments on sequence quality and object segmentation and recognition from stereo data are presented. The article recovers classic concepts from object recognition (grouping, geometric hashing, alignment) from the probabilistic perspective and adds insight into the optimal ordering of object hypotheses for evaluation. Moreover, it introduces point-relation densities, a key component of the truncated probability, as statistical models of local surface shape.\n    ",
        "submission_date": "2002-08-05T00:00:00",
        "last_modified_date": "2002-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0211005",
        "title": "Prosody Based Co-analysis for Continuous Recognition of Coverbal Gestures",
        "authors": [
            "Sanshzar Kettebekov",
            "Mohammed Yeasin",
            "Rajeev Sharma"
        ],
        "abstract": "  Although speech and gesture recognition has been studied extensively, all the successful attempts of combining them in the unified framework were semantically motivated, e.g., keyword-gesture cooccurrence. Such formulations inherited the complexity of natural language processing. This paper presents a Bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures. The prosodic features from the speech signal were coanalyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures. It was found that the above co-analysis helps in detecting and disambiguating visually small gestures, which subsequently improves the rate of continuous gesture recognition. The efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast. This formulation opens new avenues for bottom-up frameworks of multimodal integration.\n    ",
        "submission_date": "2002-11-05T00:00:00",
        "last_modified_date": "2002-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0202009",
        "title": "Non-negative sparse coding",
        "authors": [
            "Patrik O. Hoyer"
        ],
        "abstract": "  Non-negative sparse coding is a method for decomposing multivariate data into non-negative sparse components. In this paper we briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hidden components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.\n    ",
        "submission_date": "2002-02-11T00:00:00",
        "last_modified_date": "2002-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0210009",
        "title": "On the Cell-based Complexity of Recognition of Bounded Configurations by Finite Dynamic Cellular Automata",
        "authors": [
            "Maxim Makatchev"
        ],
        "abstract": "  This paper studies complexity of recognition of classes of bounded configurations by a generalization of conventional cellular automata (CA) -- finite dynamic cellular automata (FDCA). Inspired by the CA-based models of biological and computer vision, this study attempts to derive the properties of a complexity measure and of the classes of input configurations that make it beneficial to realize the recognition via a two-layered automaton as compared to a one-layered automaton. A formalized model of an image pattern recognition task is utilized to demonstrate that the derived conditions can be satisfied for a non-empty set of practical problems.\n    ",
        "submission_date": "2002-10-11T00:00:00",
        "last_modified_date": "2002-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212028",
        "title": "Technical Note: Bias and the Quantification of Stability",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  Research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy. We believe that there are other factors that should also play a role in the evaluation of bias. One such factor is the stability of the algorithm; in other words, the repeatability of the results. If we obtain two sets of data from the same phenomenon, with the same underlying probability distribution, then we would like our learning algorithm to induce approximately the same concepts from both sets of data. This paper introduces a method for quantifying stability, based on a measure of the agreement between concepts. We also discuss the relationships among stability, predictive accuracy, and bias.\n    ",
        "submission_date": "2002-12-11T00:00:00",
        "last_modified_date": "2002-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212029",
        "title": "A Theory of Cross-Validation Error",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  This paper presents a theory of error in cross-validation testing of algorithms for predicting real-valued attributes. The theory justifies the claim that predicting real-valued attributes requires balancing the conflicting demands of simplicity and accuracy. Furthermore, the theory indicates precisely how these conflicting demands must be balanced, in order to minimize cross-validation error. A general theory is presented, then it is developed in detail for linear regression and instance-based learning.\n    ",
        "submission_date": "2002-12-11T00:00:00",
        "last_modified_date": "2002-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212030",
        "title": "Theoretical Analyses of Cross-Validation Error and Voting in Instance-Based Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  This paper begins with a general theory of error in cross-validation testing of algorithms for supervised learning from examples. It is assumed that the examples are described by attribute-value pairs, where the values are symbolic. Cross-validation requires a set of training examples and a set of testing examples. The value of the attribute that is to be predicted is known to the learner in the training set, but unknown in the testing set. The theory demonstrates that cross-validation error has two components: error on the training set (inaccuracy) and sensitivity to noise (instability). This general theory is then applied to voting in instance-based learning. Given an example in the testing set, a typical instance-based learning algorithm predicts the designated attribute by voting among the k nearest neighbors (the k most similar examples) to the testing example in the training set. Voting is intended to increase the stability (resistance to noise) of instance-based learning, but a theoretical analysis shows that there are circumstances in which voting can be destabilizing. The theory suggests ways to minimize cross-validation error, by insuring that voting is stable and does not adversely affect accuracy.\n    ",
        "submission_date": "2002-12-11T00:00:00",
        "last_modified_date": "2002-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212031",
        "title": "Contextual Normalization Applied to Aircraft Gas Turbine Engine Diagnosis",
        "authors": [
            "Peter D. Turney",
            "Michael Halasz"
        ],
        "abstract": "  Diagnosing faults in aircraft gas turbine engines is a complex problem. It involves several tasks, including rapid and accurate interpretation of patterns in engine sensor data. We have investigated contextual normalization for the development of a software tool to help engine repair technicians with interpretation of sensor data. Contextual normalization is a new strategy for employing machine learning. It handles variation in data that is due to contextual factors, rather than the health of the engine. It does this by normalizing the data in a context-sensitive manner. This learning strategy was developed and tested using 242 observations of an aircraft gas turbine engine in a test cell, where each observation consists of roughly 12,000 numbers, gathered over a 12 second interval. There were eight classes of observations: seven deliberately implanted classes of faults and a healthy class. We compared two approaches to implementing our learning strategy: linear regression and instance-based learning. We have three main results. (1) For the given problem, instance-based learning works better than linear regression. (2) For this problem, contextual normalization works better than other common forms of normalization. (3) The algorithms described here can be the basis for a useful software tool for assisting technicians with the interpretation of sensor data.\n    ",
        "submission_date": "2002-12-11T00:00:00",
        "last_modified_date": "2002-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212034",
        "title": "Types of Cost in Inductive Concept Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  Inductive concept learning is the task of learning to assign cases to a discrete set of classes. In real-world applications of concept learning, there are many different types of cost involved. The majority of the machine learning literature ignores all types of cost (unless accuracy is interpreted as a type of cost measure). A few papers have investigated the cost of misclassification errors. Very few papers have examined the many other types of cost. In this paper, we attempt to create a taxonomy of the different types of cost that are involved in inductive concept learning. This taxonomy may help to organize the literature on cost-sensitive learning. We hope that it will inspire researchers to investigate all types of cost in inductive concept learning in more depth.\n    ",
        "submission_date": "2002-12-11T00:00:00",
        "last_modified_date": "2002-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212035",
        "title": "Exploiting Context When Learning to Classify",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  This paper addresses the problem of classifying observations when features are context-sensitive, specifically when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on two domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The problem is to recognize words spoken by a new speaker, not represented in the training set. For both domains, exploiting context results in substantially more accurate classification.\n    ",
        "submission_date": "2002-12-12T00:00:00",
        "last_modified_date": "2002-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212037",
        "title": "The Management of Context-Sensitive Features: A Review of Strategies",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  In this paper, we review five heuristic strategies for handling context-sensitive features in supervised machine learning from examples. We discuss two methods for recovering lost (implicit) contextual information. We mention some evidence that hybrid strategies can have a synergetic effect. We then show how the work of several machine learning researchers fits into this framework. While we do not claim that these strategies exhaust the possibilities, it appears that the framework includes all of the techniques that can be found in the published literature on contextsensitive learning.\n    ",
        "submission_date": "2002-12-12T00:00:00",
        "last_modified_date": "2002-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212038",
        "title": "The Identification of Context-Sensitive Features: A Formal Definition of Context for Concept Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  A large body of research in machine learning is concerned with supervised learning from examples. The examples are typically represented as vectors in a multi-dimensional feature space (also known as attribute-value descriptions). A teacher partitions a set of training examples into a finite number of classes. The task of the learning algorithm is to induce a concept from the training examples. In this paper, we formally distinguish three types of features: primary, contextual, and irrelevant features. We also formally define what it means for one feature to be context-sensitive to another feature. Context-sensitive features complicate the task of the learner and potentially impair the learner's performance. Our formal definitions make it possible for a learner to automatically identify context-sensitive features. After context-sensitive features have been identified, there are several strategies that the learner can employ for managing the features; however, a discussion of these strategies is outside of the scope of this paper. The formal definitions presented here correct a flaw in previously proposed definitions. We discuss the relationship between our work and a formal definition of relevance.\n    ",
        "submission_date": "2002-12-12T00:00:00",
        "last_modified_date": "2002-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212040",
        "title": "Data Engineering for the Analysis of Semiconductor Manufacturing Data",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering -- preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.\n    ",
        "submission_date": "2002-12-12T00:00:00",
        "last_modified_date": "2002-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0212041",
        "title": "Robust Classification with Context-Sensitive Features",
        "authors": [
            "Peter D. Turney"
        ],
        "abstract": "  This paper addresses the problem of classifying observations when features are context-sensitive, especially when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on three domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The context is given by the identity of the speaker. The problem is to recognize words spoken by a new speaker, not represented in the training set. The third domain is medical prognosis. The problem is to predict whether a patient with hepatitis will live or die. The context is the age of the patient. For all three domains, exploiting context results in substantially more accurate classification.\n    ",
        "submission_date": "2002-12-12T00:00:00",
        "last_modified_date": "2002-12-12T00:00:00"
    }
]