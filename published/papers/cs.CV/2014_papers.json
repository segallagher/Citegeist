[
    {
        "url": "https://arxiv.org/abs/1401.0092",
        "title": "A Novel Approach For Generating Face Template Using Bda",
        "authors": [
            "Shraddha S. Shinde",
            "Prof. Anagha P. Khedkar"
        ],
        "abstract": "In identity management system, commonly used biometric recognition system needs attention towards issue of biometric template protection as far as more reliable solution is concerned. In view of this biometric template protection algorithm should satisfy security, discriminability and cancelability. As no single template protection method is capable of satisfying the basic requirements, a novel technique for face template generation and protection is proposed. The novel approach is proposed to provide security and accuracy in new user enrollment as well as authentication process. This novel technique takes advantage of both the hybrid approach and the binary discriminant analysis algorithm. This algorithm is designed on the basis of random projection, binary discriminant analysis and fuzzy commitment scheme. Three publicly available benchmark face databases are used for evaluation. The proposed novel technique enhances the discriminability and recognition accuracy by 80% in terms of matching score of the face images and provides high security.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0166",
        "title": "Medical Image Fusion: A survey of the state of the art",
        "authors": [
            "A.P. James",
            "B. V. Dasarathy"
        ],
        "abstract": "Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years.\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0395",
        "title": "Hybrid Approach to Face Recognition System using Principle component and Independent component with score based fusion process",
        "authors": [
            "Trupti M. Kodinariya"
        ],
        "abstract": "Hybrid approach has a special status among Face Recognition Systems as they combine different recognition approaches in an either serial or parallel to overcome the shortcomings of individual methods. This paper explores the area of Hybrid Face Recognition using score based strategy as a combiner/fusion process. In proposed approach, the recognition system operates in two modes: training and classification. Training mode involves normalization of the face images (training set), extracting appropriate features using Principle Component Analysis (PCA) and Independent Component Analysis (ICA). The extracted features are then trained in parallel using Back-propagation neural networks (BPNNs) to partition the feature space in to different face classes. In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face image(s). The score based strategy which works as a combiner is applied to the results of both PCA BPNN and ICA BPNN to classify given new face image(s) according to face classes obtained during the training mode. The proposed approach has been tested on ORL and other face databases; the experimented results show that the proposed system has higher accuracy than face recognition systems using single feature extractor.\n    ",
        "submission_date": "2014-01-02T00:00:00",
        "last_modified_date": "2014-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0486",
        "title": "A Hybrid NN/HMM Modeling Technique for Online Arabic Handwriting Recognition",
        "authors": [
            "Najiba Tagougui",
            "Houcine Boubaker",
            "Monji Kherallah",
            "Adel M. ALIMI"
        ],
        "abstract": "In this work we propose a hybrid NN/HMM model for online Arabic handwriting recognition. The proposed system is based on Hidden Markov Models (HMMs) and Multi Layer Perceptron Neural Networks (MLPNNs). The input signal is segmented to continuous strokes called segments based on the Beta-Elliptical strategy by inspecting the extremum points of the curvilinear velocity profile. A neural network trained with segment level contextual information is used to extract class character probabilities. The output of this network is decoded by HMMs to provide character level recognition. In evaluations on the ADAB database, we achieved 96.4% character recognition accuracy that is statistically significantly important in comparison with character recognition accuracies obtained from state-of-the-art online Arabic systems.8\n    ",
        "submission_date": "2014-01-02T00:00:00",
        "last_modified_date": "2014-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0583",
        "title": "Adaptive-Rate Compressive Sensing Using Side Information",
        "authors": [
            "Garrett Warnell",
            "Sourabh Bhattacharya",
            "Rama Chellappa",
            "Tamer Basar"
        ],
        "abstract": "We provide two novel adaptive-rate compressive sensing (CS) strategies for sparse, time-varying signals using side information. Our first method utilizes extra cross-validation measurements, and the second one exploits extra low-resolution measurements. Unlike the majority of current CS techniques, we do not assume that we know an upper bound on the number of significant coefficients that comprise the images in the video sequence. Instead, we use the side information to predict the number of significant coefficients in the signal at the next time instant. For each image in the video sequence, our techniques specify a fixed number of spatially-multiplexed CS measurements to acquire, and adjust this quantity from image to image. Our strategies are developed in the specific context of background subtraction for surveillance video, and we experimentally validate the proposed methods on real video sequences.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0689",
        "title": "Machine Assisted Authentication of Paper Currency: an Experiment on Indian Banknotes",
        "authors": [
            "Ankush Roy",
            "Biswajit Halder",
            "Utpal Garain",
            "David S. Doermann"
        ],
        "abstract": "Automatic authentication of paper money has been targeted. Indian bank notes are taken as reference to show how a system can be developed for discriminating fake notes from genuine ones. Image processing and pattern recognition techniques are used to design the overall approach. The ability of the embedded security aspects is thoroughly analysed for detecting fake currencies. Real forensic samples are involved in the experiment that shows a high precision machine can be developed for authentication of paper money. The system performance is reported in terms of both accuracy and processing speed. Comparison with human subjects namely forensic experts and bank staffs clearly shows its applicability for mass checking of currency notes in the real world. The analysis of security features to protect counterfeiting highlights some facts that should be taken care of in future designing of currency notes.\n    ",
        "submission_date": "2014-01-02T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0730",
        "title": "What is usual in unusual videos? Trajectory snippet histograms for discovering unusualness",
        "authors": [
            "Ahmet Iscen",
            "Anil Armagan",
            "Pinar Duygulu"
        ],
        "abstract": "Unusual events are important as being possible indicators of undesired consequences. Moreover, unusualness in everyday life activities may also be amusing to watch as proven by the popularity of such videos shared in social media. Discovery of unusual events in videos is generally attacked as a problem of finding usual patterns, and then separating the ones that do not resemble to those. In this study, we address the problem from the other side, and try to answer what type of patterns are shared among unusual videos that make them resemble to each other regardless of the ongoing event. With this challenging problem at hand, we propose a novel descriptor to encode the rapid motions in videos utilizing densely extracted trajectories. The proposed descriptor, which is referred to as trajectory snipped histograms, is used to distinguish unusual videos from usual videos, and further exploited to discover snapshots in which unusualness happen. Experiments on domain specific people falling videos and unrestricted funny videos show the effectiveness of our method in capturing unusualness.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0733",
        "title": "ConceptVision: A Flexible Scene Classification Framework",
        "authors": [
            "Ahmet Iscen",
            "Eren Golge",
            "Ilker Sarac",
            "Pinar Duygulu"
        ],
        "abstract": "We introduce ConceptVision, a method that aims for high accuracy in categorizing large number of scenes, while keeping the model relatively simpler and efficient for scalability. The proposed method combines the advantages of both low-level representations and high-level semantic categories, and eliminates the distinctions between different levels through the definition of concepts. The proposed framework encodes the perspectives brought through different concepts by considering them in concept groups. Different perspectives are ensembled for the final decision. Extensive experiments are carried out on benchmark datasets to test the effects of different concepts, and methods used to ensemble. Comparisons with state-of-the-art studies show that we can achieve better results with incorporation of concepts in different levels with different perspectives.\n    ",
        "submission_date": "2014-01-03T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0764",
        "title": "Context-Aware Hypergraph Construction for Robust Spectral Clustering",
        "authors": [
            "Xi Li",
            "Weiming Hu",
            "Chunhua Shen",
            "Anthony Dick",
            "Zhongfei Zhang"
        ],
        "abstract": "Spectral clustering is a powerful tool for unsupervised data analysis. In this paper, we propose a context-aware hypergraph similarity measure (CAHSM), which leads to robust spectral clustering in the case of noisy data. We construct three types of hypergraph---the pairwise hypergraph, the k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering hypergraph. The pairwise hypergraph captures the pairwise similarity of data points; the kNN hypergraph captures the neighborhood of each point; and the clustering hypergraph encodes high-order contexts within the dataset. By combining the affinity information from these three hypergraphs, the CAHSM algorithm is able to explore the intrinsic topological information of the dataset. Therefore, data clustering using CAHSM tends to be more robust. Considering the intra-cluster compactness and the inter-cluster separability of vertices, we further design a discriminative hypergraph partitioning criterion (DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is developed. Theoretical analysis and experimental evaluation demonstrate the effectiveness and robustness of the proposed algorithm.\n    ",
        "submission_date": "2014-01-04T00:00:00",
        "last_modified_date": "2014-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0870",
        "title": "Pectoral Muscles Suppression in Digital Mammograms using Hybridization of Soft Computing Methods",
        "authors": [
            "I. Laurence Aroquiaraj",
            "K. Thangavel"
        ],
        "abstract": "Breast region segmentation is an essential prerequisite in computerized analysis of mammograms. It aims at separating the breast tissue from the background of the mammogram and it includes two independent segmentations. The first segments the background region which usually contains annotations, labels and frames from the whole breast region, while the second removes the pectoral muscle portion (present in Medio Lateral Oblique (MLO) views) from the rest of the breast tissue. In this paper we propose hybridization of Connected Component Labeling (CCL), Fuzzy, and Straight line methods. Our proposed methods worked good for separating pectoral region. After removal pectoral muscle from the mammogram, further processing is confined to the breast region alone. To demonstrate the validity of our segmentation algorithm, it is extensively tested using over 322 mammographic images from the Mammographic Image Analysis Society (MIAS) database. The segmentation results were evaluated using a Mean Absolute Error (MAE), Hausdroff Distance (HD), Probabilistic Rand Index (PRI), Local Consistency Error (LCE) and Tanimoto Coefficient (TC). The hybridization of fuzzy with straight line method is given more than 96% of the curve segmentations to be adequate or better. In addition a comparison with similar approaches from the state of the art has been given, obtaining slightly improved results. Experimental results demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2014-01-05T00:00:00",
        "last_modified_date": "2014-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0898",
        "title": "Feature Selection Using Classifier in High Dimensional Data",
        "authors": [
            "Vijendra Singh",
            "Shivani Pathak"
        ],
        "abstract": "Feature selection is frequently used as a pre-processing step to machine learning. It is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion. The central objective of this paper is to reduce the dimension of the data by finding a small set of important features which can give good classification performance. We have applied filter and wrapper approach with different classifiers QDA and LDA respectively. A widely-used filter method is used for bioinformatics data i.e. a univariate criterion separately on each feature, assuming that there is no interaction between features and then applied Sequential Feature Selection method. Experimental results show that filter approach gives better performance in respect of Misclassification Error Rate.\n    ",
        "submission_date": "2014-01-05T00:00:00",
        "last_modified_date": "2014-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1190",
        "title": "Bangla Text Recognition from Video Sequence: A New Focus",
        "authors": [
            "Souvik Bhowmick",
            "Purnendu Banerjee"
        ],
        "abstract": "Extraction and recognition of Bangla text from video frame images is challenging due to complex color background, low-resolution etc. In this paper, we propose an algorithm for extraction and recognition of Bangla text form such video frames with complex background. Here, a two-step approach has been proposed. First, the text line is segmented into words using information based on line contours. First order gradient value of the text blocks are used to find the word gap. Next, a local binarization technique is applied on each word and text line is reconstructed using those words. Secondly, this binarized text block is sent to OCR for recognition purpose.\n    ",
        "submission_date": "2014-01-06T00:00:00",
        "last_modified_date": "2014-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1742",
        "title": "Content Based Image Indexing and Retrieval",
        "authors": [
            "Avinash N Bhute",
            "B. B. Meshram"
        ],
        "abstract": "In this paper, we present the efficient content based image retrieval systems which employ the color, texture and shape information of images to facilitate the retrieval process. For efficient feature extraction, we extract the color, texture and shape feature of images automatically using edge detection which is widely used in signal processing and image compression. For facilitated the speedy retrieval we are implements the antipole-tree algorithm for indexing the images.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1778",
        "title": "Large Scale Visual Recommendations From Street Fashion Images",
        "authors": [
            "Vignesh Jagadeesh",
            "Robinson Piramuthu",
            "Anurag Bhardwaj",
            "Wei Di",
            "Neel Sundaresan"
        ],
        "abstract": "We describe a completely automated large scale visual recommendation system for fashion. Our focus is to efficiently harness the availability of large quantities of online fashion images and their rich meta-data. Specifically, we propose four data driven models in the form of Complementary Nearest Neighbor Consensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov Chain LDA for solving this problem. We analyze relative merits and pitfalls of these algorithms through extensive experimentation on a large-scale data set and baseline them against existing ideas from color science. We also illustrate key fashion insights learned through these experiments and show how they can be employed to design better recommendation systems. Finally, we also outline a large-scale annotated data set of fashion images (Fashion-136K) that can be exploited for future vision research.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1990",
        "title": "Brazilian License Plate Detection Using Histogram of Oriented Gradients and Sliding Windows",
        "authors": [
            "R. F. Prates",
            "G. C\u00e1mara-Ch\u00e1vez",
            "William R. Schwartz",
            "D. Menotti"
        ],
        "abstract": "Due to the increasingly need for automatic traffic monitoring, vehicle license plate detection is of high interest to perform automatic toll collection, traffic law enforcement, parking lot access control, among others. In this paper, a sliding window approach based on Histogram of Oriented Gradients (HOG) features is used for Brazilian license plate detection. This approach consists in scanning the whole image in a multiscale fashion such that the license plate is located precisely. The main contribution of this work consists in a deep study of the best setup for HOG descriptors on the detection of Brazilian license plates, in which HOG have never been applied before. We also demonstrate the reliability of this method ensured by a recall higher than 98% (with a precision higher than 78%) in a publicly available data set.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2058",
        "title": "Gesture recognition based mouse events",
        "authors": [
            "Rachit Puri"
        ],
        "abstract": "This paper presents the maneuver of mouse pointer and performs various mouse operations such as left click, right click, double click, drag etc using gestures recognition technique. Recognizing gestures is a complex task which involves many aspects such as motion modeling, motion analysis, pattern recognition and machine learning. Keeping all the essential factors in mind a system has been created which recognizes the movement of fingers and various patterns formed by them. Color caps have been used for fingers to distinguish it from the background color such as skin color. Thus recognizing the gestures various mouse events have been performed. The application has been created on MATLAB environment with operating system as windows 7.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2416",
        "title": "Satellite image classification and segmentation using non-additive entropy",
        "authors": [
            "Lucas Assirati",
            "Alexandre Souto Martinez",
            "Odemir Martinez Bruno"
        ],
        "abstract": "Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallis entropy on the pattern recognition and segmentation of coloured images obtained by satellites, via \"Google Earth\". By segmentation we mean split an image to locate regions of interest. Here, we discriminate and define an image partition classes according to a training basis. This training basis consists of three pattern classes: aquatic, urban and vegetation regions. Our numerical experiments demonstrate that the Tsallis entropy, used as a feature vector composed of distinct entropic indexes $q$ outperforms the standard entropy. There are several applications of our proposed methodology, once satellite images can be used to monitor migration form rural to urban regions, agricultural activities, oil spreading on the ocean etc.\n    ",
        "submission_date": "2014-01-10T00:00:00",
        "last_modified_date": "2014-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2529",
        "title": "A Study of Image Analysis with Tangent Distance",
        "authors": [
            "Elif Vural",
            "Pascal Frossard"
        ],
        "abstract": "The computation of the geometric transformation between a reference and a target image, known as registration or alignment, corresponds to the projection of the target image onto the transformation manifold of the reference image (the set of images generated by its geometric transformations). It, however, often takes a nontrivial form such that the exact computation of projections on the manifold is difficult. The tangent distance method is an effective algorithm to solve this problem by exploiting a linear approximation of the manifold. As theoretical studies about the tangent distance algorithm have been largely overlooked, we present in this work a detailed performance analysis of this useful algorithm, which can eventually help its implementation. We consider a popular image registration setting using a multiscale pyramid of lowpass filtered versions of the (possibly noisy) reference and target images, which is particularly useful for recovering large transformations. We first show that the alignment error has a nonmonotonic variation with the filter size, due to the opposing effects of filtering on both manifold nonlinearity and image noise. We then study the convergence of the multiscale tangent distance method to the optimal solution. We finally examine the performance of the tangent distance method in image classification applications. Our theoretical findings are confirmed by experiments on image transformation models involving translations, rotations and scalings. Our study is the first detailed study of the tangent distance algorithm that leads to a better understanding of its efficacy and to the proper selection of its design parameters.\n    ",
        "submission_date": "2014-01-11T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2686",
        "title": "A parameterless scale-space approach to find meaningful modes in histograms - Application to image and spectrum segmentation",
        "authors": [
            "J\u00e9r\u00f4me Gilles",
            "Kathryn Heal"
        ],
        "abstract": "In this paper, we present an algorithm to automatically detect meaningful modes in a histogram. The proposed method is based on the behavior of local minima in a scale-space representation. We show that the detection of such meaningful modes is equivalent in a two classes clustering problem on the length of minima scale-space curves. The algorithm is easy to implement, fast, and does not require any parameters. We present several results on histogram and spectrum segmentation, grayscale image segmentation and color image reduction.\n    ",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2804",
        "title": "Insights into analysis operator learning: From patch-based sparse models to higher-order MRFs",
        "authors": [
            "Yunjin Chen",
            "Ren\u00e9 Ranftl",
            "Thomas Pock"
        ],
        "abstract": "This paper addresses a new learning algorithm for the recently introduced co-sparse analysis model. First, we give new insights into the co-sparse analysis model by establishing connections to filter-based MRF models, such as the Field of Experts (FoE) model of Roth and Black. For training, we introduce a technique called bi-level optimization to learn the analysis operators. Compared to existing analysis operator learning approaches, our training procedure has the advantage that it is unconstrained with respect to the analysis operator. We investigate the effect of different aspects of the co-sparse analysis model and show that the sparsity promoting function (also called penalty function) is the most important factor in the model. In order to demonstrate the effectiveness of our training approach, we apply our trained models to various classical image restoration problems. Numerical experiments show that our trained models clearly outperform existing analysis operator learning approaches and are on par with state-of-the-art image denoising algorithms. Our approach develops a framework that is intuitive to understand and easy to implement.\n    ",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2818",
        "title": "Multilinear Wavelets: A Statistical Shape Space for Human Faces",
        "authors": [
            "Alan Brunton",
            "Timo Bolkart",
            "Stefanie Wuhrer"
        ],
        "abstract": "We present a statistical model for $3$D human faces in varying expression, which decomposes the surface of the face using a wavelet transform, and learns many localized, decorrelated multilinear models on the resulting coefficients. Using this model we are able to reconstruct faces from noisy and occluded $3$D face scans, and facial motion sequences. Accurate reconstruction of face shape is important for applications such as tele-presence and gaming. The localized and multi-scale nature of our model allows for recovery of fine-scale detail while retaining robustness to severe noise and occlusion, and is computationally efficient and scalable. We validate these properties experimentally on challenging data in the form of static scans and motion sequences. We show that in comparison to a global multilinear model, our model better preserves fine detail and is computationally faster, while in comparison to a localized PCA model, our model better handles variation in expression, is faster, and allows us to fix identity parameters for a given subject.\n    ",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2871",
        "title": "Tensor Representation and Manifold Learning Methods for Remote Sensing Images",
        "authors": [
            "Lefei Zhang"
        ],
        "abstract": "One of the main purposes of earth observation is to extract interested information and knowledge from remote sensing (RS) images with high efficiency and accuracy. However, with the development of RS technologies, RS system provide images with higher spatial and temporal resolution and more spectral channels than before, and it is inefficient and almost impossible to manually interpret these images. Thus, it is of great interests to explore automatic and intelligent algorithms to quickly process such massive RS data with high accuracy. This thesis targets to develop some efficient information extraction algorithms for RS images, by relying on the advanced technologies in machine learning. More precisely, we adopt the manifold learning algorithms as the mainline and unify the regularization theory, tensor-based method, sparse learning and transfer learning into the same framework. The main contributions of this thesis are as follows.\n    ",
        "submission_date": "2014-01-13T00:00:00",
        "last_modified_date": "2014-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2899",
        "title": "Application of the Modified Fractal Signature Method for Terrain Classification from Synthetic Aperture Radar Images",
        "authors": [
            "A. Malamou",
            "C. Pandis",
            "P. Frangos",
            "P. Stefaneas",
            "A. Karakasiliotis",
            "D. Kodokostas"
        ],
        "abstract": "In this paper the Modified Fractal Signature method is applied to real Synthetic Aperture Radar images provided to our research group by SET 163 Working Group on SAR radar techniques. This method uses the blanket technique to provide useful information for SAR image classification. It is based on the calculation of the volume of a blanket, corresponding to the image to be classified, and then on the calculation of the corresponding Fractal Area curve and Fractal Dimension curve of the image. The main idea concerning this proposed technique is the fact that different terrain types encountered in SAR images yield different values of Fractal Area curves and Fractal Dimension curves, upon which classification of different types of terrain is possible. As a result, a classification technique for five different terrain types, i.e. urban, suburban, rural, mountain and sea, is presented in this paper.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2902",
        "title": "An Alternate Approach for Designing a Domain Specific Image Search Prototype Using Histogram",
        "authors": [
            "Sukanta Sinha",
            "Rana Dattagupta",
            "Debajyoti Mukhopadhyay"
        ],
        "abstract": "Everyone knows that thousand of words are represented by a single image. As a result image search has become a very popular mechanism for the Web searchers. Image search means, the search results are produced by the search engine should be a set of images along with their Web page Unified Resource Locator. Now Web searcher can perform two types of image search, they are Text to Image and Image to Image search. In Text to Image search, search query should be a text. Based on the input text data system will generate a set of images along with their Web page URL as an output. On the other hand, in Image to Image search, search query should be an image and based on this image system will generate a set of images along with their Web page URL as an output. According to the current scenarios, Text to Image search mechanism always not returns perfect result. It matches the text data and then displays the corresponding images as an output, which is not always perfect. To resolve this problem, Web researchers have introduced the Image to Image search mechanism. In this paper, we have also proposed an alternate approach of Image to Image search mechanism using Histogram.\n    ",
        "submission_date": "2013-11-28T00:00:00",
        "last_modified_date": "2013-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3409",
        "title": "Low-Rank Modeling and Its Applications in Image Analysis",
        "authors": [
            "Xiaowei Zhou",
            "Can Yang",
            "Hongyu Zhao",
            "Weichuan Yu"
        ],
        "abstract": "Low-rank modeling generally refers to a class of methods that solve problems by representing variables of interest as low-rank matrices. It has achieved great success in various fields including computer vision, data mining, signal processing and bioinformatics. Recently, much progress has been made in theories, algorithms and applications of low-rank modeling, such as exact low-rank matrix recovery via convex programming and matrix completion applied to collaborative filtering. These advances have brought more and more attentions to this topic. In this paper, we review the recent advance of low-rank modeling, the state-of-the-art algorithms, and related applications in image analysis. We first give an overview to the concept of low-rank modeling and challenging problems in this area. Then, we summarize the models and algorithms for low-rank matrix recovery and illustrate their advantages and limitations with numerical experiments. Next, we introduce a few applications of low-rank modeling in the context of image analysis. Finally, we conclude this paper with some discussions.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3584",
        "title": "Experiments of Distance Measurements in a Foliage Plant Retrieval System",
        "authors": [
            "Abdul Kadir",
            "Lukito Edi Nugroho",
            "Adhi Susanto",
            "Paulus Insap Santosa"
        ],
        "abstract": "One of important components in an image retrieval system is selecting a distance measure to compute rank between two objects. In this paper, several distance measures were researched to implement a foliage plant retrieval system. Sixty kinds of foliage plants with various leaf color and shape were used to test the performance of 7 different kinds of distance measures: city block distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2 statistics, Jensen Shannon divergence and Kullback Leibler divergence. The results show that city block and Euclidean distance measures gave the best performance among the others.\n    ",
        "submission_date": "2013-11-20T00:00:00",
        "last_modified_date": "2013-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3590",
        "title": "An Enhanced Method For Evaluating Automatic Video Summaries",
        "authors": [
            "Karim M. Mahmoud"
        ],
        "abstract": "Evaluation of automatic video summaries is a challenging problem. In the past years, some evaluation methods are presented that utilize only a single feature like color feature to detect similarity between automatic video summaries and ground-truth user summaries. One of the drawbacks of using a single feature is that sometimes it gives a false similarity detection which makes the assessment of the quality of the generated video summary less perceptual and not accurate. In this paper, a novel method for evaluating automatic video summaries is presented. This method is based on comparing automatic video summaries generated by video summarization techniques with ground-truth user summaries. The objective of this evaluation method is to quantify the quality of video summaries, and allow comparing different video summarization techniques utilizing both color and texture features of the video frames and using the Bhattacharya distance as a dissimilarity measure due to its advantages. Our Experiments show that the proposed evaluation method overcomes the drawbacks of other methods and gives a more perceptual evaluation of the quality of the automatic video summaries.\n    ",
        "submission_date": "2014-01-14T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3700",
        "title": "Convex Relaxations of SE(2) and SE(3) for Visual Pose Estimation",
        "authors": [
            "Matanya B. Horowitz",
            "Nikolai Matni",
            "Joel W. Burdick"
        ],
        "abstract": "This paper proposes a new method for rigid body pose estimation based on spectrahedral representations of the tautological orbitopes of $SE(2)$ and $SE(3)$. The approach can use dense point cloud data from stereo vision or an RGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data. The method is a convex relaxation of the classical pose estimation problem, and is based on explicit linear matrix inequality (LMI) representations for the convex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxed pose estimation problem can be framed as a robust least squares problem with the optimization variable constrained to these convex sets. Although this formulation is a relaxation of the original problem, numerical experiments indicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or $SE(3)$ - in many interesting settings. We additionally show that this method is guaranteed to be exact for a large class of pose estimation problems.\n    ",
        "submission_date": "2014-01-15T00:00:00",
        "last_modified_date": "2014-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3818",
        "title": "Structured Priors for Sparse-Representation-Based Hyperspectral Image Classification",
        "authors": [
            "Xiaoxia Sun",
            "Qing Qu",
            "Nasser M. Nasrabadi",
            "Trac D. Tran"
        ],
        "abstract": "Pixel-wise classification, where each pixel is assigned to a predefined class, is one of the most important procedures in hyperspectral image (HSI) analysis. By representing a test pixel as a linear combination of a small subset of labeled pixels, a sparse representation classifier (SRC) gives rather plausible results compared with that of traditional classifiers such as the support vector machine (SVM). Recently, by incorporating additional structured sparsity priors, the second generation SRCs have appeared in the literature and are reported to further improve the performance of HSI. These priors are based on exploiting the spatial dependencies between the neighboring pixels, the inherent structure of the dictionary, or both. In this paper, we review and compare several structured priors for sparse-representation-based HSI classification. We also propose a new structured prior called the low rank group prior, which can be considered as a modification of the low rank prior. Furthermore, we will investigate how different structured priors improve the result for the HSI classification.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4105",
        "title": "Learning $\\ell_1$-based analysis and synthesis sparsity priors using bi-level optimization",
        "authors": [
            "Yunjin Chen",
            "Thomas Pock",
            "Horst Bischof"
        ],
        "abstract": "We consider the analysis operator and synthesis dictionary learning problems based on the the $\\ell_1$ regularized sparse representation model. We reveal the internal relations between the $\\ell_1$-based analysis model and synthesis model. We then introduce an approach to learn both analysis operator and synthesis dictionary simultaneously by using a unified framework of bi-level optimization. Our aim is to learn a meaningful operator (dictionary) such that the minimum energy solution of the analysis (synthesis)-prior based model is as close as possible to the ground-truth. We solve the bi-level optimization problem using the implicit differentiation technique. Moreover, we demonstrate the effectiveness of our leaning approach by applying the learned analysis operator (dictionary) to the image denoising task and comparing its performance with state-of-the-art methods. Under this unified framework, we can compare the performance of the two types of priors.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4107",
        "title": "Revisiting loss-specific training of filter-based MRFs for image restoration",
        "authors": [
            "Yunjin Chen",
            "Thomas Pock",
            "Ren\u00e9 Ranftl",
            "Horst Bischof"
        ],
        "abstract": "It is now well known that Markov random fields (MRFs) are particularly effective for modeling image priors in low-level vision. Recent years have seen the emergence of two main approaches for learning the parameters in MRFs: (1) probabilistic learning using sampling-based algorithms and (2) loss-specific training based on MAP estimate. After investigating existing training approaches, it turns out that the performance of the loss-specific training has been significantly underestimated in existing work. In this paper, we revisit this approach and use techniques from bi-level optimization to solve it. We show that we can get a substantial gain in the final performance by solving the lower-level problem in the bi-level framework with high accuracy using our newly proposed algorithm. As a result, our trained model is on par with highly specialized image denoising algorithms and clearly outperforms probabilistically trained MRF models. Our findings suggest that for the loss-specific training scheme, solving the lower-level problem with higher accuracy is beneficial. Our trained model comes along with the additional advantage, that inference is extremely efficient. Our GPU-based implementation takes less than 1s to produce state-of-the-art performance.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4112",
        "title": "A bi-level view of inpainting - based image compression",
        "authors": [
            "Yunjin Chen",
            "Ren\u00e9 Ranftl",
            "Thomas Pock"
        ],
        "abstract": "Inpainting based image compression approaches, especially linear and non-linear diffusion models, are an active research topic for lossy image compression. The major challenge in these compression models is to find a small set of descriptive supporting points, which allow for an accurate reconstruction of the original image. It turns out in practice that this is a challenging problem even for the simplest Laplacian interpolation model. In this paper, we revisit the Laplacian interpolation compression model and introduce two fast algorithms, namely successive preconditioning primal dual algorithm and the recently proposed iPiano algorithm, to solve this problem efficiently. Furthermore, we extend the Laplacian interpolation based compression model to a more general form, which is based on principles from bi-level optimization. We investigate two different variants of the Laplacian model, namely biharmonic interpolation and smoothed Total Variation regularization. Our numerical results show that significant improvements can be obtained from the biharmonic interpolation model, and it can recover an image with very high quality from only 5% pixels.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4221",
        "title": "Distortion-driven Turbulence Effect Removal using Variational Model",
        "authors": [
            "Yuan Xie",
            "Wensheng Zhang",
            "Dacheng Tao",
            "Wenrui Hu",
            "Yanyun Qu",
            "Hanzi Wang"
        ],
        "abstract": "It remains a challenge to simultaneously remove geometric distortion and space-time-varying blur in frames captured through a turbulent atmospheric medium. To solve, or at least reduce these effects, we propose a new scheme to recover a latent image from observed frames by integrating a new variational model and distortion-driven spatial-temporal kernel regression. The proposed scheme first constructs a high-quality reference image from the observed frames using low-rank decomposition. Then, to generate an improved registered sequence, the reference image is iteratively optimized using a variational model containing a new spatial-temporal regularization. The proposed fast algorithm efficiently solves this model without the use of partial differential equations (PDEs). Next, to reduce blur variation, distortion-driven spatial-temporal kernel regression is carried out to fuse the registered sequence into one image by introducing the concept of the near-stationary patch. Applying a blind deconvolution algorithm to the fused image produces the final output. Extensive experimental testing shows, both qualitatively and quantitatively, that the proposed method can effectively alleviate distortion and blur and recover details of the original scene compared to state-of-the-art methods.\n    ",
        "submission_date": "2014-01-17T00:00:00",
        "last_modified_date": "2014-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4447",
        "title": "Leaf Classification Using Shape, Color, and Texture Features",
        "authors": [
            "Abdul Kadir",
            "Lukito Edi Nugroho",
            "Adhi Susanto",
            "Paulus Insap Santosa"
        ],
        "abstract": "Several methods to identify plants have been proposed by several researchers. Commonly, the methods did not capture color information, because color was not recognized as an important aspect to the identification. In this research, shape and vein, color, and texture features were incorporated to classify a leaf. In this case, a neural network called Probabilistic Neural network (PNN) was used as a classifier. The experimental result shows that the method for classification gives average accuracy of 93.75% when it was tested on Flavia dataset, that contains 32 kinds of plant leaves. It means that the method gives better performance compared to the original work.\n    ",
        "submission_date": "2013-11-20T00:00:00",
        "last_modified_date": "2013-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4489",
        "title": "An Analysis of Random Projections in Cancelable Biometrics",
        "authors": [
            "Devansh Arpit",
            "Ifeoma Nwogu",
            "Gaurav Srivastava",
            "Venu Govindaraju"
        ],
        "abstract": "With increasing concerns about security, the need for highly secure physical biometrics-based authentication systems utilizing \\emph{cancelable biometric} technologies is on the rise. Because the problem of cancelable template generation deals with the trade-off between template security and matching performance, many state-of-the-art algorithms successful in generating high quality cancelable biometrics all have random projection as one of their early processing steps. This paper therefore presents a formal analysis of why random projections is an essential step in cancelable biometrics. By formally defining the notion of an \\textit{Independent Subspace Structure} for datasets, it can be shown that random projection preserves the subspace structure of data vectors generated from a union of independent linear subspaces. The bound on the minimum number of random vectors required for this to hold is also derived and is shown to depend logarithmically on the number of data samples, not only in independent subspaces but in disjoint subspace settings as well. The theoretical analysis presented is supported in detail with empirical results on real-world face recognition datasets.\n    ",
        "submission_date": "2014-01-17T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4648",
        "title": "Visual Tracking using Particle Swarm Optimization",
        "authors": [
            "Rafid Siddiqui",
            "Siamak Khatibi"
        ],
        "abstract": "The problem of robust extraction of visual odometry from a sequence of images obtained by an eye in hand camera configuration is addressed. A novel approach toward solving planar template based tracking is proposed which performs a non-linear image alignment for successful retrieval of camera transformations. In order to obtain global optimum a bio-metaheuristic is used for optimization of similarity among the planar regions. The proposed method is validated on image sequences with real as well as synthetic transformations and found to be resilient to intensity variations. A comparative analysis of the various similarity measures as well as various state-of-art methods reveal that the algorithm succeeds in tracking the planar regions robustly and has good potential to be used in real applications.\n    ",
        "submission_date": "2014-01-19T00:00:00",
        "last_modified_date": "2014-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4788",
        "title": "Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means",
        "authors": [
            "Frank Nielsen"
        ],
        "abstract": "Bayesian classification labels observations based on given prior information, namely class-a priori and class-conditional probabilities. Bayes' risk is the minimum expected classification cost that is achieved by the Bayes' test, the optimal decision rule. When no cost incurs for correct classification and unit cost is charged for misclassification, Bayes' test reduces to the maximum a posteriori decision rule, and Bayes risk simplifies to Bayes' error, the probability of error. Since calculating this probability of error is often intractable, several techniques have been devised to bound it with closed-form formula, introducing thereby measures of similarity and divergence between distributions like the Bhattacharyya coefficient and its associated Bhattacharyya distance. The Bhattacharyya upper bound can further be tightened using the Chernoff information that relies on the notion of best error exponent. In this paper, we first express Bayes' risk using the total variation distance on scaled distributions. We then elucidate and extend the Bhattacharyya and the Chernoff upper bound mechanisms using generalized weighted means. We provide as a byproduct novel notions of statistical divergences and affinity coefficients. We illustrate our technique by deriving new upper bounds for the univariate Cauchy and the multivariate $t$-distributions, and show experimentally that those bounds are not too distant to the computationally intractable Bayes' error.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5098",
        "title": "Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding",
        "authors": [
            "Mohamed A. El-Sayed",
            "S. Abdel-Khalek",
            "Eman Abdel-Aziz"
        ],
        "abstract": "Thresholding is an important task in image processing. It is a main tool in pattern recognition, image segmentation, edge detection and scene analysis. In this paper, we present a new thresholding technique based on two-dimensional Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the twodimensional histogram which was determined by using the gray value of the pixels and the local average gray value of the pixels, the work it was applied a generalized entropy formalism that represents a recent development in statistical mechanics. The effectiveness of the proposed method is demonstrated by using examples from the real-world and synthetic images. The performance evaluation of the proposed technique in terms of the quality of the thresholded images are presented. Experimental results demonstrate that the proposed method achieve better result than the Shannon method.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5108",
        "title": "An Identification System Using Eye Detection Based On Wavelets And Neural Networks",
        "authors": [
            "Mohamed A. El-Sayed",
            "Mohamed A. Khafagy"
        ],
        "abstract": "The randomness and uniqueness of human eye patterns is a major breakthrough in the search for quicker, easier and highly reliable forms of automatic human identification. It is being used extensively in security solutions. This includes access control to physical facilities, security systems and information databases, Suspect tracking, surveillance and intrusion detection and by various Intelligence agencies through out the world. We use the advantage of human eye uniqueness to identify people and approve its validity as a biometric. . Eye detection involves first extracting the eye from a digital face image, and then encoding the unique patterns of the eye in such a way that they can be compared with pre-registered eye patterns. The eye detection system consists of an automatic segmentation system that is based on the wavelet transform, and then the Wavelet analysis is used as a pre-processor for a back propagation neural network with conjugate gradient learning. The inputs to the neural network are the wavelet maxima neighborhood coefficients of face images at a particular scale. The output of the neural network is the classification of the input into an eye or non-eye region. An accuracy of 90% is observed for identifying test images under different conditions included in training stage.\n    ",
        "submission_date": "2014-01-20T00:00:00",
        "last_modified_date": "2014-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5245",
        "title": "Edge detection of binary images using the method of masks",
        "authors": [
            "Ayman M Bahaa-Eldeen",
            "Abdel-Moneim A. Wahdan",
            "Hani M. K. Mahdi"
        ],
        "abstract": "In this work the method of masks, creating and using of inverted image masks, together with binary operation of image data are used in edge detection of binary images, monochrome images, which yields about 300 times faster than ordinary methods. The method is divided into three stages: Mask construction, Fundamental edge detection, and Edge Construction Comparison with an ordinary method and a fuzzy based method is carried out.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2014-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5311",
        "title": "Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition",
        "authors": [
            "Changxing Ding",
            "Jonghyun Choi",
            "Dacheng Tao",
            "Larry S. Davis"
        ],
        "abstract": "To perform unconstrained face recognition robust to variations in illumination, pose and expression, this paper presents a new scheme to extract \"Multi-Directional Multi-Level Dual-Cross Patterns\" (MDML-DCPs) from face images. Specifically, the MDMLDCPs scheme exploits the first derivative of Gaussian operator to reduce the impact of differences in illumination and then computes the DCP feature at both the holistic and component levels. DCP is a novel face image descriptor inspired by the unique textural structure of human faces. It is computationally efficient and only doubles the cost of computing local binary patterns, yet is extremely robust to pose and expression variations. MDML-DCPs comprehensively yet efficiently encodes the invariant characteristics of a face image from multiple levels into patterns that are highly discriminative of inter-personal differences but robust to intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC 2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local descriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face identification and face verification tasks. More impressively, the best performance is achieved on the challenging LFW and FRGC 2.0 databases by deploying MDML-DCPs in a simple recognition scheme.\n    ",
        "submission_date": "2014-01-21T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5535",
        "title": "Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification",
        "authors": [
            "Shu Kong",
            "Zhuolin Jiang",
            "Qiang Yang"
        ],
        "abstract": "We now know that mid-level features can greatly enhance the performance of image learning, but how to automatically learn the image features efficiently and in an unsupervised manner is still an open question. In this paper, we present a very efficient mid-level feature learning approach (MidFea), which only involves simple operations such as $k$-means clustering, convolution, pooling, vector quantization and random projection. We explain why this simple method generates the desired features, and argue that there is no need to spend much time in learning low-level feature extractors. Furthermore, to boost the performance, we propose to model the neuron selectivity (NS) principle by building an additional layer over the mid-level features before feeding the features into the classifier. We show that the NS-layer learns category-specific neurons with both bottom-up inference and top-down analysis, and thus supports fast inference for a query image. We run extensive experiments on several public databases to demonstrate that our approach can achieve state-of-the-art performances for face recognition, gender classification, age estimation and object categorization. In particular, we demonstrate that our approach is more than an order of magnitude faster than some recently proposed sparse coding based methods.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5559",
        "title": "License Plate Recognition (LPR): A Review with Experiments for Malaysia Case Study",
        "authors": [
            "Nuzulha Khilwani Ibrahim",
            "Emaliana Kasmuri",
            "Norazira A Jalil",
            "Mohd Adili Norasikin",
            "Sazilah Salam",
            "Mohamad Riduwan Md Nawawi"
        ],
        "abstract": "Most vehicle license plate recognition use neural network techniques to enhance its computing capability. The image of the vehicle license plate is captured and processed to produce a textual output for further processing. This paper reviews image processing and neural network techniques applied at different stages which are preprocessing, filtering, feature extraction, segmentation and recognition in such way to remove the noise of the image, to enhance the image quality and to expedite the computing process by converting the characters in the image into respective text. An exemplar experiment has been done in MATLAB to show the basic process of the image processing especially for license plate in Malaysia case study. An algorithm is adapted into the solution for parking management system. The solution then is implemented as proof of concept to the algorithm.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5632",
        "title": "Enhancing Template Security of Face Biometrics by Using Edge Detection and Hashing",
        "authors": [
            "Manoj Krishnaswamy",
            "G. Hemantha Kumar"
        ],
        "abstract": "In this paper we address the issues of using edge detection techniques on facial images to produce cancellable biometric templates and a novel method for template verification against tampering. With increasing use of biometrics, there is a real threat for the conventional systems using face databases, which store images of users in raw and unaltered form. If compromised not only it is irrevocable, but can be misused for cross-matching across different databases. So it is desirable to generate and store revocable templates for the same user in different applications to prevent cross-matching and to enhance security, while maintaining privacy and ethics. By comparing different edge detection methods it has been observed that the edge detection based on the Roberts Cross operator performs consistently well across multiple face datasets, in which the face images have been taken under a variety of conditions. We have proposed a novel scheme using hashing, for extra verification, in order to harden the security of the stored biometric templates.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5891",
        "title": "Hierarchical pixel clustering for image segmentation",
        "authors": [
            "M. Kharinov"
        ],
        "abstract": "In the paper a piecewise constant image approximations of sequential number of pixel clusters or segments are treated. A majorizing of optimal approximation sequence by hierarchical sequence of image approximations is studied. Transition from pixel clustering to image segmentation by reducing of segment numbers in clusters is provided. Algorithms are proved by elementary formulas.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6013",
        "title": "Efficient Background Modeling Based on Sparse Representation and Outlier Iterative Removal",
        "authors": [
            "Linhao Li",
            "Ping Wang",
            "Qinghua Hu",
            "Sijia Cai"
        ],
        "abstract": "Background modeling is a critical component for various vision-based applications. Most traditional methods tend to be inefficient when solving large-scale problems. In this paper, we introduce sparse representation into the task of large scale stable background modeling, and reduce the video size by exploring its 'discriminative' frames. A cyclic iteration process is then proposed to extract the background from the discriminative frame set. The two parts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm. The algorithm operates in tensor space to obey the natural data structure of videos. Experimental results show that a few discriminative frames determine the performance of the background extraction. Further, SOIR can achieve high accuracy and high speed simultaneously when dealing with real video sequences. Thus, SOIR has an advantage in solving large-scale tasks.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6108",
        "title": "Face Verification Using Kernel Principle Component Analysis",
        "authors": [
            "V. Karthikeyan",
            "Manjupriya",
            "C.K. Chithra",
            "M. Divya"
        ],
        "abstract": "In the beginning stage, face verification is done using easy method of geometric algorithm models, but the verification route has now developed into a scientific progress of complicated geometric representation and matching process. In modern time the skill have enhanced face detection system into the vigorous focal point. Researchers currently undergoing strong research on finding face recognition system for wider area information taken under hysterical elucidation dissimilarity. The proposed face recognition system consists of a narrative exposition indiscreet preprocessing method, a hybrid Fourier-based facial feature extraction and a score fusion scheme. We take in conventional the face detection in unlike cheer up circumstances and at unusual setting. Image processing, Image detection, Feature removal and Face detection are the methods used for Face Verification System . This paper focuses mainly on the issue of toughness to lighting variations. The proposed system has obtained an average of verification rate on Two-Dimensional images under different lightening conditions.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6112",
        "title": "Face Verification System based on Integral Normalized Gradient Image(INGI)",
        "authors": [
            "V. Karthikeyan",
            "M. Divya",
            "C.K. Chithra",
            "K. Manju Priya"
        ],
        "abstract": "Character identification plays a vital role in the contemporary world of Image processing. It can solve many composite problems and makes humans work easier. An instance is Handwritten Character detection. Handwritten recognition is not a novel expertise, but it has not gained community notice until Now. The eventual aim of designing Handwritten Character recognition structure with an accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character recognition system uses the Neural Networks to distinguish them. Neural Network and structural characteristics are used to instruct and recognize written characters. After training and testing the exactness rate reached 99%. This correctness rate is extremely high. In this paper we are exploring image processing through the Hilditch algorithm foundation and structural characteristics of a character in the image. And we recognized some character of the Tamil language, and we are trying to identify all the character of Tamil In our future works.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6126",
        "title": "Delegating Custom Object Detection Tasks to a Universal Classification System",
        "authors": [
            "Andrew Gleibman"
        ],
        "abstract": "In this paper, a concept of multipurpose object detection system, recently introduced in our previous work, is clarified. The business aspect of this method is transformation of a classifier into an object detector/locator via an image grid. This is a universal framework for locating objects of interest through classification. The framework standardizes and simplifies implementation of custom systems by doing only a custom analysis of the classification results on the image grid.\n    ",
        "submission_date": "2013-12-19T00:00:00",
        "last_modified_date": "2013-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6127",
        "title": "Brain Tumor Detection Based On Symmetry Information",
        "authors": [
            "Narkhede Sachin G",
            "Vaishali Khairnar"
        ],
        "abstract": "Advances in computing technology have allowed researchers across many fields of endeavor to collect and maintain vast amounts of observational statistical data such as clinical data, biological patient data, data regarding access of web sites, financial data, and the like. This paper addresses some of the challenging issues on brain magnetic resonance (MR) image tumor segmentation caused by the weak correlation between magnetic resonance imaging (MRI) intensity and anatomical meaning. With the objective of utilizing more meaningful information to improve brain tumor segmentation, an approach which employs bilateral symmetry information as an additional feature for segmentation is proposed. This is motivated by potential performance improvement in the general automatic brain tumor segmentation systems which are important for many medical and scientific applications\n    ",
        "submission_date": "2013-11-23T00:00:00",
        "last_modified_date": "2013-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6129",
        "title": "Image enhancement using fusion by wavelet transform and laplacian pyramid",
        "authors": [
            "S.M.Mukane",
            "Y.S.Ghodake",
            "P.S.Khandagle"
        ],
        "abstract": "The idea of combining multiple image modalities to provide a single, enhanced image is well established different fusion methods have been proposed in literature. This paper is based on image fusion using laplacian pyramid and wavelet transform method. Images of same size are used for experimentation. Images used for the experimentation are standard images and averaging filter is used of equal weights in original images to burl. Performance of image fusion technique is measured by mean square error, normalized absolute error and peak signal to noise ratio. From the performance analysis it has been observed that MSE is decreased in case of both the methods where as PSNR increased, NAE decreased in case of laplacian pyramid where as constant for wavelet transform method.\n    ",
        "submission_date": "2013-11-19T00:00:00",
        "last_modified_date": "2013-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6196",
        "title": "Spatially regularized reconstruction of fibre orientation distributions in the presence of isotropic diffusion",
        "authors": [
            "Q. Zhou",
            "O. Michailovich",
            "Y. Rathi"
        ],
        "abstract": "The connectivity and structural integrity of the white matter of the brain is nowadays known to be implicated into a wide range of brain-related disorders. However, it was not before the advent of diffusion Magnetic Resonance Imaging (dMRI) that researches have been able to examine the properties of white matter in vivo. Presently, among a range of various methods of dMRI, high angular resolution diffusion imaging (HARDI) is known to excel in its ability to provide reliable information about the local orientations of neural fasciculi (aka fibre tracts). Moreover, as opposed to the more traditional diffusion tensor imaging (DTI), HARDI is capable of distinguishing the orientations of multiple fibres passing through a given spatial voxel. Unfortunately, the ability of HARDI to discriminate between neural fibres that cross each other at acute angles is always limited, which is the main reason behind the development of numerous post-processing tools, aiming at the improvement of the directional resolution of HARDI. Among such tools is spherical deconvolution (SD). Due to its ill-posed nature, however, SD standardly relies on a number of a priori assumptions which are to render its results unique and stable. In this paper, we propose a different approach to the problem of SD in HARDI, which accounts for the spatial continuity of neural fibres as well as the presence of isotropic diffusion. Subsequently, we demonstrate how the proposed solution can be used to successfully overcome the effect of partial voluming, while preserving the spatial coherency of cerebral diffusion at moderate-to-severe noise levels. In a series of both in silico and in vivo experiments, the performance of the proposed method is compared with that of several available alternatives, with the comparative results clearly supporting the viability and usefulness of our approach.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2014-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6393",
        "title": "Automatic Detection of Calibration Grids in Time-of-Flight Images",
        "authors": [
            "Miles Hansard",
            "Radu Horaud",
            "Michel Amat",
            "Georgios Evangelidis"
        ],
        "abstract": "It is convenient to calibrate time-of-flight cameras by established methods, using images of a chequerboard pattern. The low resolution of the amplitude image, however, makes it difficult to detect the board reliably. Heuristic detection methods, based on connected image-components, perform very poorly on this data. An alternative, geometrically-principled method is introduced here, based on the Hough transform. The projection of a chequerboard is represented by two pencils of lines, which are identified as oriented clusters in the gradient-data of the image. A projective Hough transform is applied to each of the two clusters, in axis-aligned coordinates. The range of each transform is properly bounded, because the corresponding gradient vectors are approximately parallel. Each of the two transforms contains a series of collinear peaks; one for every line in the given pencil. This pattern is easily detected, by sweeping a dual line through the transform. The proposed Hough-based method is compared to the standard OpenCV detection routine, by application to several hundred time-of-flight images. It is shown that the new method detects significantly more calibration boards, over a greater variety of poses, without any overall loss of accuracy. This conclusion is based on an analysis of both geometric and photometric error.\n    ",
        "submission_date": "2014-01-24T00:00:00",
        "last_modified_date": "2014-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6606",
        "title": "Continuous Localization and Mapping of a Pan Tilt Zoom Camera for Wide Area Tracking",
        "authors": [
            "Giuseppe Lisanti",
            "Iacopo Masi",
            "Federico Pernici",
            "Alberto Del Bimbo"
        ],
        "abstract": "Pan-tilt-zoom (PTZ) cameras are powerful to support object identification and recognition in far-field scenes. However, the effective use of PTZ cameras in real contexts is complicated by the fact that a continuous on-line camera calibration is needed and the absolute pan, tilt and zoom positional values provided by the camera actuators cannot be used because are not synchronized with the video stream. So, accurate calibration must be directly extracted from the visual content of the frames. Moreover, the large and abrupt scale changes, the scene background changes due to the camera operation and the need of camera motion compensation make target tracking with these cameras extremely challenging. In this paper, we present a solution that provides continuous on-line calibration of PTZ cameras which is robust to rapid camera motion, changes of the environment due to illumination or moving objects and scales beyond thousands of landmarks. The method directly derives the relationship between the position of a target in the 3D world plane and the corresponding scale and position in the 2D image, and allows real-time tracking of multiple targets with high and stable degree of accuracy even at far distances and any zooming level.\n    ",
        "submission_date": "2014-01-26T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6638",
        "title": "Painting Analysis Using Wavelets and Probabilistic Topic Models",
        "authors": [
            "Tong Wu",
            "Gungor Polatkan",
            "David Steel",
            "William Brown",
            "Ingrid Daubechies",
            "Robert Calderbank"
        ],
        "abstract": "In this paper, computer-based techniques for stylistic analysis of paintings are applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto di Bondone. Features are extracted by combining a dual-tree complex wavelet transform with a hidden Markov tree (HMT) model. Hierarchical clustering is used to identify stylistic keywords in image patches, and keyword frequencies are calculated for sub-images that each contains many patches. A generative hierarchical Bayesian model learns stylistic patterns of keywords; these patterns are then used to characterize the styles of the sub-images; this in turn, permits to discriminate between paintings. Results suggest that such unsupervised probabilistic topic models can be useful to distill characteristic elements of style.\n    ",
        "submission_date": "2014-01-26T00:00:00",
        "last_modified_date": "2014-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7486",
        "title": "Use HMM and KNN for classifying corneal data",
        "authors": [
            "Payam Porkar Rezaeiye",
            "mehrnoosh bazrafkan",
            "ali akbar movassagh",
            "Mojtaba Sedigh Fazli",
            "Gholam hossein bazyari"
        ],
        "abstract": "These days to gain classification system with high accuracy that can classify complicated pattern are so useful in medicine and industry. In this article a process for getting the best classifier for Lasik data is suggested. However at first it's been tried to find the best line and curve by this classifier in order to gain classifier fitting, and in the end by using the Markov method a classifier for topographies is gained.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7517",
        "title": "Information quantity in a pixel of digital image",
        "authors": [
            "M. Kharinov"
        ],
        "abstract": "The paper is devoted to the problem of integer-valued estimating of information quantity in a pixel of digital image. The definition of an integer estimation of information quantity based on constructing of the certain binary hierarchy of pixel clusters is proposed. The methods for constructing hierarchies of clusters and generating of hierarchical sequences of image approximations that minimally differ from the image by a standard deviation are developed. Experimental results on integer-valued estimation of information quantity are compared with the results obtained by utilizing of the classical formulas.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7713",
        "title": "A Generalized Probabilistic Framework for Compact Codebook Creation",
        "authors": [
            "Lingqiao Liu",
            "Lei Wang",
            "Chunhua Shen"
        ],
        "abstract": "Compact and discriminative visual codebooks are preferred in many visual recognition tasks. In the literature, a number of works have taken the approach of hierarchically merging visual words of an initial large-sized codebook, but implemented this approach with different merging criteria. In this work, we propose a single probabilistic framework to unify these merging criteria, by identifying two key factors: the function used to model class-conditional distribution and the method used to estimate the distribution parameters. More importantly, by adopting new distribution functions and/or parameter estimation methods, our framework can readily produce a spectrum of novel merging criteria. Three of them are specifically focused in this work. In the first criterion, we adopt the multinomial distribution with Bayesian method; In the second criterion, we integrate Gaussian distribution with maximum likelihood parameter estimation. In the third criterion, which shows the best merging performance, we propose a max-margin-based parameter estimation method and apply it with multinomial distribution. Extensive experimental study is conducted to systematically analyse the performance of the above three criteria and compare them with existing ones. As demonstrated, the best criterion obtained in our framework achieves the overall best merging performance among the comparable merging criteria developed in the literature.\n    ",
        "submission_date": "2014-01-30T00:00:00",
        "last_modified_date": "2014-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7715",
        "title": "Video Compressive Sensing for Dynamic MRI",
        "authors": [
            "Jianing V. Shi",
            "Wotao Yin",
            "Aswin C. Sankaranarayanan",
            "Richard G. Baraniuk"
        ],
        "abstract": "We present a video compressive sensing framework, termed kt-CSLDS, to accelerate the image acquisition process of dynamic magnetic resonance imaging (MRI). We are inspired by a state-of-the-art model for video compressive sensing that utilizes a linear dynamical system (LDS) to model the motion manifold. Given compressive measurements, the state sequence of an LDS can be first estimated using system identification techniques. We then reconstruct the observation matrix using a joint structured sparsity assumption. In particular, we minimize an objective function with a mixture of wavelet sparsity and joint sparsity within the observation matrix. We derive an efficient convex optimization algorithm through alternating direction method of multipliers (ADMM), and provide a theoretical guarantee for global convergence. We demonstrate the performance of our approach for video compressive sensing, in terms of reconstruction accuracy. We also investigate the impact of various sampling strategies. We apply this framework to accelerate the acquisition process of dynamic MRI and show it achieves the best reconstruction accuracy with the least computational time compared with existing algorithms in the literature.\n    ",
        "submission_date": "2014-01-30T00:00:00",
        "last_modified_date": "2014-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7743",
        "title": "Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method",
        "authors": [
            "T.Balaji",
            "Dr.M.Sumathi"
        ],
        "abstract": "Remote sensing image classification can be performed in many different ways to extract meaningful features. One common approach is to perform edge detection. A second approach is to try and detect whole shapes, given the fact that these shapes usually tend to have distinctive properties such as object foreground or background. To get optimal results, these two approaches can be combined. This paper adopts a combinatorial optimization method to adaptively select threshold based features to improve remote sensing image. Feature selection is an important combinatorial optimization problem in the remote sensing image classification. The feature selection method has to achieve three characteristics: first the performance issues by facilitating data collection and reducing storage space and classification time, second to perform semantics analysis helping to understand the problem, and third to improve prediction accuracy by avoiding the curse of dimensionality. The goal of this thresholding an image is to classify pixels as either dark or light and evaluation of classification results. Interactive adaptive thresholding is a form of thresholding that takes into account spatial variations in illumination of remote sensing image. We present a technique for remote sensing based adaptive thresholding using the interactive satellite image of the input. However, our solution is more robust to illumination changes in the remote sensing image. Additionally, our method is simple and easy to implement but it is effective algorithm to classify the image pixels. This technique is suitable for preprocessing the remote sensing image classification, making it a valuable tool for interactive remote based applications such as augmented reality of the classification procedure.\n    ",
        "submission_date": "2014-01-30T00:00:00",
        "last_modified_date": "2014-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8053",
        "title": "Hallucinating optimal high-dimensional subspaces",
        "authors": [
            "Ognjen Arandjelovic"
        ],
        "abstract": "Linear subspace representations of appearance variation are pervasive in computer vision. This paper addresses the problem of robustly matching such subspaces (computing the similarity between them) when they are used to describe the scope of variations within sets of images of different (possibly greatly so) scales. A naive solution of projecting the low-scale subspace into the high-scale image space is described first and subsequently shown to be inadequate, especially at large scale discrepancies. A successful approach is proposed instead. It consists of (i) an interpolated projection of the low-scale subspace into the high-scale space, which is followed by (ii) a rotation of this initial estimate within the bounds of the imposed ``downsampling constraint''. The optimal rotation is found in the closed-form which best aligns the high-scale reconstruction of the low-scale subspace with the reference it is compared to. The method is evaluated on the problem of matching sets of (i) face appearances under varying illumination and (ii) object appearances under varying viewpoint, using two large data sets. In comparison to the naive matching, the proposed algorithm is shown to greatly increase the separation of between-class and within-class similarities, as well as produce far more meaningful modes of common appearance on which the match score is based.\n    ",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2014-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8092",
        "title": "Cross-calibration of Time-of-flight and Colour Cameras",
        "authors": [
            "Miles Hansard",
            "Georgios Evangelidis",
            "Quentin Pelorson",
            "Radu Horaud"
        ],
        "abstract": "Time-of-flight cameras provide depth information, which is complementary to the photometric appearance of the scene in ordinary images. It is desirable to merge the depth and colour information, in order to obtain a coherent scene representation. However, the individual cameras will have different viewpoints, resolutions and fields of view, which means that they must be mutually calibrated. This paper presents a geometric framework for this multi-view and multi-modal calibration problem. It is shown that three-dimensional projective transformations can be used to align depth and parallax-based representations of the scene, with or without Euclidean reconstruction. A new evaluation procedure is also developed; this allows the reprojection error to be decomposed into calibration and sensor-dependent components. The complete approach is demonstrated on a network of three time-of-flight and six colour cameras. The applications of such a system, to a range of automatic scene-interpretation problems, are discussed.\n    ",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2014-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8261",
        "title": "Infrared face recognition: a comprehensive review of methodologies and databases",
        "authors": [
            "Reza Shoja Ghiass",
            "Ognjen Arandjelovic",
            "Hakim Bendada",
            "Xavier Maldague"
        ],
        "abstract": "Automatic face recognition is an area with immense practical potential which includes a wide range of commercial and law enforcement applications. Hence it is unsurprising that it continues to be one of the most active research areas of computer vision. Even after over three decades of intense research, the state-of-the-art in face recognition continues to improve, benefitting from advances in a range of different research fields such as image processing, pattern recognition, computer graphics, and physiology. Systems based on visible spectrum images, the most researched face recognition modality, have reached a significant level of maturity with some practical success. However, they continue to face challenges in the presence of illumination, pose and expression changes, as well as facial disguises, all of which can significantly decrease recognition accuracy. Amongst various approaches which have been proposed in an attempt to overcome these limitations, the use of infrared (IR) imaging has emerged as a particularly promising research direction. This paper presents a comprehensive and timely review of the literature on this subject. Our key contributions are: (i) a summary of the inherent properties of infrared imaging which makes this modality promising in the context of face recognition, (ii) a systematic review of the most influential approaches, with a focus on emerging common trends as well as key differences between alternative methodologies, (iii) a description of the main databases of infrared facial images available to the researcher, and lastly (iv) a discussion of the most promising avenues for future research.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0170",
        "title": "Collaborative Receptive Field Learning",
        "authors": [
            "Shu Kong",
            "Zhuolin Jiang",
            "Qiang Yang"
        ],
        "abstract": "The challenge of object categorization in images is largely due to arbitrary translations and scales of the foreground objects. To attack this difficulty, we propose a new approach called collaborative receptive field learning to extract specific receptive fields (RF's) or regions from multiple images, and the selected RF's are supposed to focus on the foreground objects of a common category. To this end, we solve the problem by maximizing a submodular function over a similarity graph constructed by a pool of RF candidates. However, measuring pairwise distance of RF's for building the similarity graph is a nontrivial problem. Hence, we introduce a similarity metric called pyramid-error distance (PED) to measure their pairwise distances through summing up pyramid-like matching errors over a set of low-level features. Besides, in consistent with the proposed PED, we construct a simple nonparametric classifier for classification. Experimental results show that our method effectively discovers the foreground objects in images, and improves classification performance.\n    ",
        "submission_date": "2014-02-02T00:00:00",
        "last_modified_date": "2014-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0289",
        "title": "A Robust Framework for Moving-Object Detection and Vehicular Traffic Density Estimation",
        "authors": [
            "Pranam Janney",
            "Glenn Geers"
        ],
        "abstract": "Intelligent machines require basic information such as moving-object detection from videos in order to deduce higher-level semantic information. In this paper, we propose a methodology that uses a texture measure to detect moving objects in video. The methodology is computationally inexpensive, requires minimal parameter fine-tuning and also is resilient to noise, illumination changes, dynamic background and low frame rate. Experimental results show that performance of the proposed approach is higher than those of state-of-the-art approaches. We also present a framework for vehicular traffic density estimation using the foreground object detection technique and present a comparison between the foreground object detection-based framework and the classical density state modelling-based framework for vehicular traffic density estimation.\n    ",
        "submission_date": "2014-02-03T00:00:00",
        "last_modified_date": "2014-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0453",
        "title": "Fine-Grained Visual Categorization via Multi-stage Metric Learning",
        "authors": [
            "Qi Qian",
            "Rong Jin",
            "Shenghuo Zhu",
            "Yuanqing Lin"
        ],
        "abstract": "Fine-grained visual categorization (FGVC) is to categorize objects into subordinate classes instead of basic classes. One major challenge in FGVC is the co-occurrence of two issues: 1) many subordinate classes are highly correlated and are difficult to distinguish, and 2) there exists the large intra-class variation (e.g., due to object pose). This paper proposes to explicitly address the above two issues via distance metric learning (DML). DML addresses the first issue by learning an embedding so that data points from the same class will be pulled together while those from different classes should be pushed apart from each other; and it addresses the second issue by allowing the flexibility that only a portion of the neighbors (not all data points) from the same class need to be pulled together. However, feature representation of an image is often high dimensional, and DML is known to have difficulty in dealing with high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$ for storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a multi-stage metric learning framework that divides the large-scale high dimensional learning problem to a series of simple subproblems, achieving $\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC benchmark datasets verifies that our method is both effective and efficient compared to the state-of-the-art FGVC approaches.\n    ",
        "submission_date": "2014-02-03T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0595",
        "title": "Scene Labeling with Contextual Hierarchical Models",
        "authors": [
            "Mojtaba Seyedhosseini",
            "Tolga Tasdizen"
        ],
        "abstract": "Scene labeling is the problem of assigning an object label to each pixel. It unifies the image segmentation and object recognition problems. The importance of using contextual information in scene labeling frameworks has been widely realized in the field. We propose a contextual framework, called contextual hierarchical model (CHM), which learns contextual information in a hierarchical framework for scene labeling. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. This training strategy allows for optimization of a joint posterior probability at multiple resolutions through the hierarchy. Contextual hierarchical model is purely based on the input image patches and does not make use of any fragments or shape examples. Hence, it is applicable to a variety of problems such as object segmentation and edge detection. We demonstrate that CHM outperforms state-of-the-art on Stanford background and Weizmann horse datasets. It also outperforms state-of-the-art edge detection methods on NYU depth dataset and achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0785",
        "title": "Signal to Noise Ratio in Lensless Compressive Imaging",
        "authors": [
            "Hong Jiang",
            "Gang Huang",
            "Paul Wilford"
        ],
        "abstract": "We analyze the signal to noise ratio (SNR) in a lensless compressive imaging (LCI) architecture. The architecture consists of a sensor of a single detecting element and an aperture assembly of an array of programmable elements. LCI can be used in conjunction with compressive sensing to capture images in a compressed form of compressive measurements. In this paper, we perform SNR analysis of the LCI and compare it with imaging with a pinhole or a lens. We will show that the SNR in the LCI is independent of the image resolution, while the SNR in either pinhole aperture imaging or lens aperture imaging decreases as the image resolution increases. Consequently, the SNR in the LCI is much higher if the image resolution is large enough.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0859",
        "title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in Generative Computer Vision Models",
        "authors": [
            "Varun Jampani",
            "Sebastian Nowozin",
            "Matthew Loper",
            "Peter V. Gehler"
        ],
        "abstract": "Computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. Bayesian posterior inference could then, in principle, explain the observation. While intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. As a result the community has favoured efficient discriminative approaches. We still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. We implement this idea in a principled way with an \"informed sampler\" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. We concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as \"Inverse Graphics\". The informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2015-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0936",
        "title": "An Optimization Method For Slice Interpolation Of Medical Images",
        "authors": [
            "Ahmadreza Baghaie",
            "Zeyun Yu"
        ],
        "abstract": "Slice interpolation is a fast growing field in medical image processing. Intensity-based interpolation and object-based interpolation are two major groups of methods in the literature. In this paper, we describe an object-oriented, optimization method based on a modified version of curvature-based image registration, in which a displacement field is computed for the missing slice between two known slices and used to interpolate the intensities of the missing slice. The proposed approach is evaluated quantitatively by using the Mean Squared Difference (MSD) as a metric. The produced results also show visual improvement in preserving sharp edges in images.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0978",
        "title": "Patchwise Joint Sparse Tracking with Occlusion Detection",
        "authors": [
            "Ali Zarezade",
            "Hamid R. Rabiee",
            "Ali Soltani-Farani",
            "Ahmad Khajenezhad"
        ],
        "abstract": "This paper presents a robust tracking approach to handle challenges such as occlusion and appearance change. Here, the target is partitioned into a number of patches. Then, the appearance of each patch is modeled using a dictionary composed of corresponding target patches in previous frames. In each frame, the target is found among a set of candidates generated by a particle filter, via a likelihood measure that is shown to be proportional to the sum of patch-reconstruction errors of each candidate. Since the target's appearance often changes slowly in a video sequence, it is assumed that the target in the current frame and the best candidates of a small number of previous frames, belong to a common subspace. This is imposed using joint sparse representation to enforce the target and previous best candidates to have a common sparsity pattern. Moreover, an occlusion detection scheme is proposed that uses patch-reconstruction errors and a prior probability of occlusion, extracted from an adaptive Markov chain, to calculate the probability of occlusion per patch. In each frame, occluded patches are excluded when updating the dictionary. Extensive experimental results on several challenging sequences shows that the proposed method outperforms state-of-the-art trackers.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1151",
        "title": "Image Acquisition in an Underwater Vision System with NIR and VIS Illumination",
        "authors": [
            "Wojciech Biega\u0144ski",
            "Andrzej Kasi\u0144ski"
        ],
        "abstract": "The paper describes the image acquisition system able to capture images in two separated bands of light, used to underwater autonomous navigation. The channels are: the visible light spectrum and near infrared spectrum. The characteristics of natural, underwater environment were also described together with the process of the underwater image creation. The results of an experiment with comparison of selected images acquired in these channels are discussed.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1331",
        "title": "An Estimation Method of Measuring Image Quality for Compressed Images of Human Face",
        "authors": [
            "Abhishek Bhattacharya",
            "Tanusree Chatterjee"
        ],
        "abstract": "Nowadays digital image compression and decompression techniques are very much important. So our aim is to calculate the quality of face and other regions of the compressed image with respect to the original image. Image segmentation is typically used to locate objects and boundaries (lines, curves etc.)in images. After segmentation the image is changed into something which is more meaningful to analyze. Using Universal Image Quality Index(Q),Structural Similarity Index(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be shown that face region is less compressed than any other region of the image.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1348",
        "title": "A Cellular Automata based Optimal Edge Detection Technique using Twenty-Five Neighborhood Model",
        "authors": [
            "Deepak Ranjan Nayak",
            "Sumit Kumar Sahu",
            "Jahangir Mohammed"
        ],
        "abstract": "Cellular Automata (CA) are common and most simple models of parallel computations. Edge detection is one of the crucial task in image processing, especially in processing biological and medical images. CA can be successfully applied in image processing. This paper presents a new method for edge detection of binary images based on two dimensional twenty five neighborhood cellular automata. The method considers only linear rules of CA for extraction of edges under null boundary condition. The performance of this approach is compared with some existing edge detection techniques. This comparison shows that the proposed method to be very promising for edge detection of binary images. All the algorithms and results used in this paper are prepared in MATLAB.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1359",
        "title": "Real-time Pedestrian Surveillance with Top View Cumulative Grids",
        "authors": [
            "Kai Berger",
            "Jeyarajan Thiyagalingam"
        ],
        "abstract": "This manuscript presents an efficient approach to map pedestrian surveillance footage to an aerial view for global assessment of features. The analysis of the footages relies on low level computer vision and enable real-time surveillance. While we neglect object tracking, we introduce cumulative grids on top view scene flow visualization to highlight situations of interest in the footage. Our approach is tested on multiview footage both from RGB cameras and, for the first time in the field, on RGB-D-sensors.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1371",
        "title": "Quantile Representation for Indirect Immunofluorescence Image Classification",
        "authors": [
            "David M. J. Tax",
            "Veronika Cheplygina",
            "Marco Loog"
        ],
        "abstract": "In the diagnosis of autoimmune diseases, an important task is to classify images of slides containing several HEp-2 cells. All cells from one slide share the same label, and by classifying cells from one slide independently, some information on the global image quality and intensity is lost. Considering one whole slide as a collection (a bag) of feature vectors, however, poses the problem of how to handle this bag. A simple, and surprisingly effective, approach is to summarize the bag of feature vectors by a few quantile values per feature. This characterizes the full distribution of all instances, thereby assuming that all instances in a bag are informative. This representation is particularly useful when each bag contains many feature vectors, which is the case in the classification of the immunofluorescence images. Experiments on the classification of indirect immunofluorescence images show the usefulness of this approach.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1503",
        "title": "Tracking via Motion Estimation with Physically Motivated Inter-Region Constraints",
        "authors": [
            "Omar Arif",
            "Ganesh Sundaramoorthi",
            "Byung-Woo Hong",
            "Anthony Yezzi"
        ],
        "abstract": "In this paper, we propose a method for tracking structures (e.g., ventricles and myocardium) in cardiac images (e.g., magnetic resonance) by propagating forward in time a previous estimate of the structures via a new deformation estimation scheme that is motivated by physical constraints of fluid motion. The method employs within structure motion estimation (so that differing motions among different structures are not mixed) while simultaneously satisfying the physical constraint in fluid motion that at the interface between a fluid and a medium, the normal component of the fluid's motion must match the normal component of the motion of the medium. We show how to estimate the motion according to the previous considerations in a variational framework, and in particular, show that these conditions lead to PDEs with boundary conditions at the interface that resemble Robin boundary conditions and induce coupling between structures. We illustrate the use of this motion estimation scheme in propagating a segmentation across frames and show that it leads to more accurate segmentation than traditional motion estimation that does not make use of physical constraints. Further, the method is naturally suited to interactive segmentation methods, which are prominently used in practice in commercial applications for cardiac analysis, where typically a segmentation from the previous frame is used to predict a segmentation in the next frame. We show that our propagation scheme reduces the amount of user interaction by predicting more accurate segmentations than commonly used and recent interactive commercial techniques.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1720",
        "title": "Performance of Hull-Detection Algorithms For Proton Computed Tomography Reconstruction",
        "authors": [
            "Blake Schultze",
            "Micah Witt",
            "Yair Censor",
            "Reinhard Schulte",
            "Keith Evan Schubert"
        ],
        "abstract": "Proton computed tomography (pCT) is a novel imaging modality developed for patients receiving proton radiation therapy. The purpose of this work was to investigate hull-detection algorithms used for preconditioning of the large and sparse linear system of equations that needs to be solved for pCT image reconstruction. The hull-detection algorithms investigated here included silhouette/space carving (SC), modified silhouette/space carving (MSC), and space modeling (SM). Each was compared to the cone-beam version of filtered backprojection (FBP) used for hull-detection. Data for testing these algorithms included simulated data sets of a digital head phantom and an experimental data set of a pediatric head phantom obtained with a pCT scanner prototype at Loma Linda University Medical Center. SC was the fastest algorithm, exceeding the speed of FBP by more than 100 times. FBP was most sensitive to the presence of noise. Ongoing work will focus on optimizing threshold parameters in order to define a fast and efficient method for hull-detection in pCT image reconstruction.\n    ",
        "submission_date": "2014-02-07T00:00:00",
        "last_modified_date": "2014-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1879",
        "title": "Sparse Illumination Learning and Transfer for Single-Sample Face Recognition with Image Corruption and Misalignment",
        "authors": [
            "Liansheng Zhuang",
            "Tsung-Han Chan",
            "Allen Y. Yang",
            "S. Shankar Sastry",
            "Yi Ma"
        ],
        "abstract": "Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required gallery images to one sample per class. To compensate for the missing illumination information traditionally provided by multiple gallery images, a sparse illumination learning and transfer (SILT) technique is introduced. The illumination in SILT is learned by fitting illumination examples of auxiliary face images from one or more additional subjects with a sparsely-used illumination dictionary. By enforcing a sparse representation of the query image in the illumination dictionary, the SILT can effectively recover and transfer the illumination and pose information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the state of the art in the single-sample regime and with less restrictions. In particular, the single-sample face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple gallery images per class. Furthermore, the face recognition accuracy exceeds those of the SRC and Extended SRC algorithms using hand labeled alignment initialization.\n    ",
        "submission_date": "2014-02-08T00:00:00",
        "last_modified_date": "2014-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1971",
        "title": "Direct Processing of Run Length Compressed Document Image for Segmentation and Characterization of a Specified Block",
        "authors": [
            "Mohammed Javed",
            "P. Nagabhushan",
            "B.B. Chaudhuri"
        ],
        "abstract": "Extracting a block of interest referred to as segmenting a specified block in an image and studying its characteristics is of general research interest, and could be a challenging if such a segmentation task has to be carried out directly in a compressed image. This is the objective of the present research work. The proposal is to evolve a method which would segment and extract a specified block, and carry out its characterization without decompressing a compressed image, for two major reasons that most of the image archives contain images in compressed format and decompressing an image indents additional computing time and space. Specifically in this research work, the proposal is to work on run-length compressed document images.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1973",
        "title": "Dictionary learning for fast classification based on soft-thresholding",
        "authors": [
            "Alhussein Fawzi",
            "Mike Davies",
            "Pascal Frossard"
        ],
        "abstract": "Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations at test time is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme that applies the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly learns the dictionary and linear classifier, is cast as a difference of convex (DC) program and solved efficiently with an iterative DC solver. We conduct experiments on several datasets, and show that our learning algorithm that leverages the structure of the classification problem outperforms generic learning procedures. Our simple classifier based on soft-thresholding also competes with the recent sparse coding classifiers, when the dictionary is learned appropriately. The adopted classification scheme further requires less computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the adequately trained soft-thresholding mapping for classification and paves the way towards the development of very efficient classification methods for vision problems.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2013",
        "title": "Foreground segmentation based on multi-resolution and matting",
        "authors": [
            "Xintong Yu",
            "Xiaohan Liu",
            "Yisong Chen"
        ],
        "abstract": "We propose a foreground segmentation algorithm that does foreground extraction under different scales and refines the result by matting. First, the input image is filtered and resampled to 5 different resolutions. Then each of them is segmented by adaptive figure-ground classification and the best segmentation is automatically selected by an evaluation score that maximizes the difference between foreground and background. This segmentation is upsampled to the original size, and a corresponding trimap is built. Closed-form matting is employed to label the boundary region, and the result is refined by a final figure-ground classification. Experiments show the success of our method in treating challenging images with cluttered background and adapting to loose initial bounding-box.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2016",
        "title": "Leveraging Long-Term Predictions and Online-Learning in Agent-based Multiple Person Tracking",
        "authors": [
            "Wenxi Liu",
            "Antoni B. Chan",
            "Rynson W. H. Lau",
            "Dinesh Manocha"
        ],
        "abstract": "We present a multiple-person tracking algorithm, based on combining particle filters and RVO, an agent-based crowd model that infers collision-free velocities so as to predict pedestrian's motion. In addition to position and velocity, our tracking algorithm can estimate the internal goals (desired destination or desired velocity) of the tracked pedestrian in an online manner, thus removing the need to specify this information beforehand. Furthermore, we leverage the longer-term predictions of RVO by deriving a higher-order particle filter, which aggregates multiple predictions from different prior time steps. This yields a tracker that can recover from short-term occlusions and spurious noise in the appearance model. Experimental results show that our tracking algorithm is suitable for predicting pedestrians' behaviors online without needing scene priors or hand-annotated goal information, and improves tracking in real-world crowded scenes under low frame rates.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2020",
        "title": "Binary Stereo Matching",
        "authors": [
            "Kang Zhang",
            "Jiyang Li",
            "Yijing Li",
            "Weidong Hu",
            "Lifeng Sun",
            "Shiqiang Yang"
        ],
        "abstract": "In this paper, we propose a novel binary-based cost computation and aggregation approach for stereo matching problem. The cost volume is constructed through bitwise operations on a series of binary strings. Then this approach is combined with traditional winner-take-all strategy, resulting in a new local stereo matching algorithm called binary stereo matching (BSM). Since core algorithm of BSM is based on binary and integer computations, it has a higher computational efficiency than previous methods. Experimental results on Middlebury benchmark show that BSM has comparable performance with state-of-the-art local stereo methods in terms of both quality and speed. Furthermore, experiments on images with radiometric differences demonstrate that BSM is more robust than previous methods under these changes, which is common under real illumination.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2031",
        "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification",
        "authors": [
            "Wen Wang",
            "Zhen Cui",
            "Hong Chang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2188",
        "title": "Handwritten Character Recognition In Malayalam Scripts- A Review",
        "authors": [
            "Anitha Mary M.O. Chacko",
            "P.M Dhanya"
        ],
        "abstract": "Handwritten character recognition is one of the most challenging and ongoing areas of research in the field of pattern recognition. HCR research is matured for foreign languages like Chinese and Japanese but the problem is much more complex for Indian languages. The problem becomes even more complicated for South Indian languages due to its large character set and the presence of vowels modifiers and compound characters. This paper provides an overview of important contributions and advances in offline as well as online handwritten character recognition of Malayalam scripts.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2426",
        "title": "Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision",
        "authors": [
            "Keith Dillon",
            "Yeshaiahu Fainman"
        ],
        "abstract": "In this paper we broadly consider techniques which utilize projections on rays for data collection, with particular emphasis on optical techniques. We formulate a variety of imaging techniques as either special cases or extensions of tomographic reconstruction. We then consider how the techniques must be extended to describe objects containing occlusion, as with a self-occluding opaque object. We formulate the reconstruction problem as a regularized nonlinear optimization problem to simultaneously solve for object brightness and attenuation, where the attenuation can become infinite. We demonstrate various simulated examples for imaging opaque objects, including sparse point sources, a conventional multiview reconstruction technique, and a super-resolving technique which exploits occlusion to resolve an image.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2606",
        "title": "A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected Component Analysis",
        "authors": [
            "Dibyendu Mukherjee"
        ],
        "abstract": "Connected component analysis (CCA) has been heavily used to label binary images and classify segments. However, it has not been well-exploited to segment multi-valued natural images. This work proposes a novel multi-value segmentation algorithm that utilizes CCA to segment color images. A user defined distance measure is incorporated in the proposed modified CCA to identify and segment similar image regions. The raw output of the algorithm consists of distinctly labelled segmented regions. The proposed algorithm has a unique design architecture that provides several benefits: 1) it can be used to segment any multi-channel multi-valued image; 2) the distance measure/segmentation criteria can be application-specific and 3) an absolute linear-time implementation allows easy extension for real-time video segmentation. Experimental demonstrations of the aforesaid benefits are presented along with the comparison results on multiple datasets with current benchmark algorithms. A number of possible application areas are also identified and results on real-time video segmentation has been presented to show the promise of the proposed method.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2673",
        "title": "Real-Time Hand Shape Classification",
        "authors": [
            "Jakub Nalepa",
            "Michal Kawulok"
        ],
        "abstract": "The problem of hand shape classification is challenging since a hand is characterized by a large number of degrees of freedom. Numerous shape descriptors have been proposed and applied over the years to estimate and classify hand poses in reasonable time. In this paper we discuss our parallel framework for real-time hand shape classification applicable in real-time applications. We show how the number of gallery images influences the classification accuracy and execution time of the parallel algorithm. We present the speedup and efficiency analyses that prove the efficacy of the parallel implementation. Noteworthy, different methods can be used at each step of our parallel framework. Here, we combine the shape contexts with the appearance-based techniques to enhance the robustness of the algorithm and to increase the classification score. An extensive experimental study proves the superiority of the proposed approach over existing state-of-the-art methods.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2681",
        "title": "Packing and Padding: Coupled Multi-index for Accurate Image Retrieval",
        "authors": [
            "Liang Zheng",
            "Shengjin Wang",
            "Ziqiong Liu",
            "Qi Tian"
        ],
        "abstract": "In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a low discriminative power, so false positive matches occur prevalently. Apart from the information loss during quantization, another cause is that the SIFT feature only describes the local gradient distribution. To address this problem, this paper proposes a coupled Multi-Index (c-MI) framework to perform feature fusion at indexing level. Basically, complementary features are coupled into a multi-dimensional inverted index. Each dimension of c-MI corresponds to one kind of feature, and the retrieval process votes for images similar in both SIFT and other feature spaces. Specifically, we exploit the fusion of local color feature into c-MI. While the precision of visual match is greatly enhanced, we adopt Multiple Assignment to improve recall. The joint cooperation of SIFT and color features significantly reduces the impact of false positive matches.\n",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2720",
        "title": "Noise Analysis for Lensless Compressive Imaging",
        "authors": [
            "Hong Jiang",
            "Gang Huang",
            "Paul Wilford"
        ],
        "abstract": "We analyze the signal to noise ratio (SNR) in a recently proposed lensless compressive imaging architecture. The architecture consists of a sensor of a single detector element and an aperture assembly of an array of aperture elements, each of which has a programmable transmittance. This lensless compressive imaging architecture can be used in conjunction with compressive sensing to capture images in a compressed form of compressive measurements. In this paper, we perform noise analysis of this lensless compressive imaging architecture and compare it with pinhole aperture imaging and lens aperture imaging. We will show that the SNR in the lensless compressive imaging is independent of the image resolution, while that in either pinhole aperture imaging or lens aperture imaging decreases as the image resolution increases. Consequently, the SNR in the lensless compressive imaging can be much higher if the image resolution is large enough.\n    ",
        "submission_date": "2014-02-12T00:00:00",
        "last_modified_date": "2014-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2826",
        "title": "Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles",
        "authors": [
            "Aniket Bera",
            "Dinesh Manocha"
        ],
        "abstract": "We present a novel, realtime algorithm to compute the trajectory of each pedestrian in moderately dense crowd scenes. Our formulation is based on an adaptive particle filtering scheme that uses a multi-agent motion model based on velocity-obstacles, and takes into account local interactions as well as physical and personal constraints of each pedestrian. Our method dynamically changes the number of particles allocated to each pedestrian based on different confidence metrics. Additionally, we use a new high-definition crowd video dataset, which is used to evaluate the performance of different pedestrian tracking algorithms. This dataset consists of videos of indoor and outdoor scenes, recorded at different locations with 30-80 pedestrians. We highlight the performance benefits of our algorithm over prior techniques using this dataset. In practice, our algorithm can compute trajectories of tens of pedestrians on a multi-core desktop CPU at interactive rates (27-30 frames per second). To the best of our knowledge, our approach is 4-5 times faster than prior methods, which provide similar accuracy.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2941",
        "title": "Multispectral Palmprint Encoding and Recognition",
        "authors": [
            "Zohaib Khan",
            "Faisal Shafait",
            "Yiqun Hu",
            "Ajmal Mian"
        ],
        "abstract": "Palmprints are emerging as a new entity in multi-modal biometrics for human identification and verification. Multispectral palmprint images captured in the visible and infrared spectrum not only contain the wrinkles and ridge structure of a palm, but also the underlying pattern of veins; making them a highly discriminating biometric identifier. In this paper, we propose a feature encoding scheme for robust and highly accurate representation and matching of multispectral palmprints. To facilitate compact storage of the feature, we design a binary hash table structure that allows for efficient matching in large databases. Comprehensive experiments for both identification and verification scenarios are performed on two public datasets -- one captured with a contact-based sensor (PolyU dataset), and the other with a contact-free sensor (CASIA dataset). Recognition results in various experimental setups show that the proposed method consistently outperforms existing state-of-the-art methods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA) are the lowest reported in literature on both dataset and clearly indicate the viability of palmprint as a reliable and promising biometric. All source codes are publicly available.\n    ",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3261",
        "title": "Hand-Eye and Robot-World Calibration by Global Polynomial Optimization",
        "authors": [
            "Jan Heller",
            "Didier Henrion",
            "Tomas Pajdla"
        ],
        "abstract": "The need to relate measurements made by a camera to a different known coordinate system arises in many engineering applications. Historically, it appeared for the first time in the connection with cameras mounted on robotic systems. This problem is commonly known as hand-eye calibration. In this paper, we present several formulations of hand-eye calibration that lead to multivariate polynomial optimization problems. We show that the method of convex linear matrix inequality (LMI) relaxations can be used to effectively solve these problems and to obtain globally optimal solutions. Further, we show that the same approach can be used for the simultaneous hand-eye and robot-world calibration. Finally, we validate the proposed solutions using both synthetic and real datasets.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2014-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3344",
        "title": "Intrinsically Motivated Learning of Visual Motion Perception and Smooth Pursuit",
        "authors": [
            "Chong Zhang",
            "Yu Zhao",
            "Jochen Triesch",
            "Bertram E. Shi"
        ],
        "abstract": "We extend the framework of efficient coding, which has been used to model the development of sensory processing in isolation, to model the development of the perception/action cycle. Our extension combines sparse coding and reinforcement learning so that sensory processing and behavior co-develop to optimize a shared intrinsic motivational signal: the fidelity of the neural encoding of the sensory input under resource constraints. Applying this framework to a model system consisting of an active eye behaving in a time varying environment, we find that this generic principle leads to the simultaneous development of both smooth pursuit behavior and model neurons whose properties are similar to those of primary visual cortical neurons selective for different directions of visual motion. We suggest that this general principle may form the basis for a unified and integrated explanation of many perception/action loops.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3557",
        "title": "Improving Streaming Video Segmentation with Early and Mid-Level Visual Processing",
        "authors": [
            "Subarna Tripathi",
            "Youngbae Hwang",
            "Serge Belongie",
            "Truong Nguyen"
        ],
        "abstract": "Despite recent advances in video segmentation, many opportunities remain to improve it using a variety of low and mid-level visual cues. We propose improvements to the leading streaming graph-based hierarchical video segmentation (streamGBH) method based on early and mid level visual processing. The extensive experimental analysis of our approach validates the improvement of hierarchical supervoxel representation by incorporating motion and color with effective filtering. We also pose and illuminate some open questions towards intermediate level video analysis as further extension to streamGBH. We exploit the supervoxels as an initialization towards estimation of dominant affine motion regions, followed by merging of such motion regions in order to hierarchically segment a video in a novel motion-segmentation framework which aims at subsequent applications such as foreground recognition.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3657",
        "title": "A Narrative Vehicle Protection Representation for Vehicle Speed Regulator Under Driver Exhaustion -- A Study",
        "authors": [
            "V. Karthikeyan",
            "B. Praveen Kumar",
            "S. Suresh Babu",
            "R. Purusothaman",
            "shijin Thomas"
        ],
        "abstract": "Driver fatigue is one of the important factors that cause traffic accidents, and the ever-increasing number due to diminished drivers vigilance level has become a problem of serious concern to society. Drivers with a diminished vigilance level suffer from a marked decline in their abilities of perception, recognition, and vehicle control, and therefore pose serious danger to their own life and the lives of other people. Exhaustion resulting from sleep deprivation or sleep disorders is an important factor in the creasing number of accidents. In this projected work, we discuss the various methods of the existing and the proposed method based on a real time online safety prototype that controls the vehicle speed under driver fatigue. The purpose of such a model is to advance a system to detect fatigue symptoms in drivers and control the speed of vehicle to avoid accidents. This system was tested adequately with subjects of different technology of various researchers finally the validity of the proposed model for vehicle speed controller based on driver fatigue detection is shown.\n    ",
        "submission_date": "2014-02-15T00:00:00",
        "last_modified_date": "2014-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3849",
        "title": "Scalable Kernel Clustering: Approximate Kernel k-means",
        "authors": [
            "Radha Chitta",
            "Rong Jin",
            "Timothy C. Havens",
            "Anil K. Jain"
        ],
        "abstract": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k-means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that its running time and memory requirements are significantly lower than those of kernel k-means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.\n    ",
        "submission_date": "2014-02-16T00:00:00",
        "last_modified_date": "2014-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3869",
        "title": "FTVd is beyond Fast Total Variation regularized Deconvolution",
        "authors": [
            "Yilun Wang"
        ],
        "abstract": "In this paper, we revisit the \"FTVd\" algorithm for Fast Total Variation Regularized Deconvolution, which has been widely used in the past few years. Both its original version implemented in the MATLAB software FTVd 3.0 and its related variant implemented in the latter version FTVd 4.0 are considered \\cite{Wang08FTVdsoftware}. We propose that the intermediate results during the iterations are the solutions of a series of combined Tikhonov and total variation regularized image deconvolution models and therefore some of them often have even better image quality than the final solution, which is corresponding to the pure total variation regularized model.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3926",
        "title": "Sparse Coding Approach for Multi-Frame Image Super Resolution",
        "authors": [
            "Toshiyuki Kato",
            "Hideitsu Hino",
            "Noboru Murata"
        ],
        "abstract": "An image super-resolution method from multiple observation of low-resolution images is proposed. The method is based on sub-pixel accuracy block matching for estimating relative displacements of observed images, and sparse signal representation for estimating the corresponding high-resolution image. Relative displacements of small patches of observed low-resolution images are accurately estimated by a computationally efficient block matching method. Since the estimated displacements are also regarded as a warping component of image degradation process, the matching results are directly utilized to generate low-resolution dictionary for sparse image representation. The matching scores of the block matching are used to select a subset of low-resolution patches for reconstructing a high-resolution patch, that is, an adaptive selection of informative low-resolution images is realized. When there is only one low-resolution image, the proposed method works as a single-frame super-resolution method. The proposed method is shown to perform comparable or superior to conventional single- and multi-frame super-resolution methods through experiments using various real-world datasets.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4067",
        "title": "Statistical Noise Analysis in SENSE Parallel MRI",
        "authors": [
            "Santiago Aja-Fernandez",
            "Gonzalo Vegas-Sanchez-Ferrero",
            "Antonio Trsitan-Vega"
        ],
        "abstract": "A complete first and second order statistical characterization of noise in SENSE reconstructed data is proposed. SENSE acquisitions have usually been modeled as Rician distributed, since the data reconstruction takes place into the spatial domain, where Gaussian noise is assumed. However, this model just holds for the first order statistics and obviates other effects induced by coils correlations and the reconstruction interpolation. Those effects are properly taken into account in this study, in order to fully justify a final SENSE noise model. As a result, some interesting features of the reconstructed image arise: (1) There is a strong correlation between adjacent lines. (2) The resulting distribution is non-stationary and therefore the variance of noise will vary from point to point across the image. Closed equations for the calculation of the variance of noise and the correlation coefficient between lines are proposed. The proposed model is totally compatible with g-factor formulations.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4069",
        "title": "Application of the Ring Theory in the Segmentation of Digital Images",
        "authors": [
            "Yasel Garc\u00e9s",
            "Esley Torres",
            "Osvaldo Pereira",
            "Roberto Rodr\u00edguez"
        ],
        "abstract": "Ring theory is one of the branches of the abstract algebra that has been broadly used in images. However, ring theory has not been very related with image segmentation. In this paper, we propose a new index of similarity among images using Zn rings and the entropy function. This new index was applied as a new stopping criterion to the Mean Shift Iterative Algorithm with the goal to reach a better segmentation. An analysis on the performance of the algorithm with this new stopping criterion is carried out. The obtained results proved that the new index is a suitable tool to compare images.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4388",
        "title": "Automatic Detection of Font Size Straight from Run Length Compressed Text Documents",
        "authors": [
            "Mohammed Javed",
            "P. Nagabhushan",
            "B.B. Chaudhuri"
        ],
        "abstract": "Automatic detection of font size finds many applications in the area of intelligent OCRing and document image analysis, which has been traditionally practiced over uncompressed documents, although in real life the documents exist in compressed form for efficient storage and transmission. It would be novel and intelligent if the task of font size detection could be carried out directly from the compressed data of these documents without decompressing, which would result in saving of considerable amount of processing time and space. Therefore, in this paper we present a novel idea of learning and detecting font size directly from run-length compressed text documents at line level using simple line height features, which paves the way for intelligent OCRing and document analysis directly from compressed documents. In the proposed model, the given mixed-case text documents of different font size are segmented into compressed text lines and the features extracted such as line height and ascender height are used to capture the pattern of font size in the form of a regression line, using which the automatic detection of font size is done during the recognition stage. The method is experimented with a dataset of 50 compressed documents consisting of 780 text lines of single font size and 375 text lines of mixed font size resulting in an overall accuracy of 99.67%.\n    ",
        "submission_date": "2014-02-18T00:00:00",
        "last_modified_date": "2014-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4566",
        "title": "Transduction on Directed Graphs via Absorbing Random Walks",
        "authors": [
            "Jaydeep De",
            "Xiaowei Zhang",
            "Li Cheng"
        ],
        "abstract": "In this paper we consider the problem of graph-based transductive classification, and we are particularly interested in the directed graph scenario which is a natural form for many real world applications. Different from existing research efforts that either only deal with undirected graphs or circumvent directionality by means of symmetrization, we propose a novel random walk approach on directed graphs using absorbing Markov chains, which can be regarded as maximizing the accumulated expected number of visits from the unlabeled transient states. Our algorithm is simple, easy to implement, and works with large-scale graphs. In particular, it is capable of preserving the graph structure even when the input graph is sparse and changes over time, as well as retaining weak signals presented in the directed edges. We present its intimate connections to a number of existing methods, including graph kernels, graph Laplacian based methods, and interestingly, spanning forest of graphs. Its computational complexity and the generalization error are also studied. Empirically our algorithm is systematically evaluated on a wide range of applications, where it has shown to perform competitively comparing to a suite of state-of-the-art methods.\n    ",
        "submission_date": "2014-02-19T00:00:00",
        "last_modified_date": "2014-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4893",
        "title": "Anisotropic Mesh Adaptation for Image Representation",
        "authors": [
            "Xianping Li"
        ],
        "abstract": "Triangular meshes have gained much interest in image representation and have been widely used in image processing. This paper introduces a framework of anisotropic mesh adaptation (AMA) methods to image representation and proposes a GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme. Different than many other methods that triangulate sample points to form the mesh, the AMA methods start directly with a triangular mesh and then adapt the mesh based on a user-defined metric tensor to represent the image. The AMA methods have clear mathematical framework and provides flexibility for both image representation and image reconstruction. A mesh patching technique is developed for the implementation of the GPRAMA method, which leads to an improved version of the popular GPRFS-ED method. The GPRAMA method can achieve better quality than the GPRFS-ED method but with lower computational cost.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4936",
        "title": "Enhanced Secure Algorithm for Fingerprint Recognition",
        "authors": [
            "Amira Mohammad Abdel-Mawgoud Saleh"
        ],
        "abstract": "Fingerprint recognition requires a minimal effort from the user, does not capture other information than strictly necessary for the recognition process, and provides relatively good performance. A critical step in fingerprint identification system is thinning of the input fingerprint image. The performance of a minutiae extraction algorithm relies heavily on the quality of the thinning algorithm. So, a fast fingerprint thinning algorithm is proposed. The algorithm works directly on the gray-scale image as binarization of fingerprint causes many spurious minutiae and also removes many important features. The performance of the thinning algorithm is evaluated and experimental results show that the proposed thinning algorithm is both fast and accurate. A new minutiae-based fingerprint matching technique is proposed. The main idea is that each fingerprint is represented by a minutiae table of just two columns in the database. The number of different minutiae types (terminations and bifurcations) found in each track of a certain width around the core point of the fingerprint is recorded in this table. Each row in the table represents a certain track, in the first column, the number of terminations in each track is recorded, in the second column, the number of bifurcations in each track is recorded. The algorithm is rotation and translation invariant, and needs less storage size. Experimental results show that recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally, the integrity of the data transmission via communication channels must be secure all the way from the scanner to the application. After applying Gaussian noise addition, and JPEG compression with high and moderate quality factors on the watermarked fingerprint images, recognition accuracy decreases slightly to reach 96%.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4963",
        "title": "Vesselness via Multiple Scale Orientation Scores",
        "authors": [
            "Julius Hannink",
            "Remco Duits",
            "Erik Bekkers"
        ],
        "abstract": "The multi-scale Frangi vesselness filter is an established tool in (retinal) vascular imaging. However, it cannot cope with crossings or bifurcations, since it only looks for elongated structures. Therefore, we disentangle crossing structures in the image via (multiple scale) invertible orientation scores. The described vesselness filter via scale-orientation scores performs considerably better at enhancing vessels throughout crossings and bifurcations than the Frangi version. Both methods are evaluated on a public dataset. Performance is measured by comparing ground truth data to the segmentation results obtained by basic thresholding and morphological component analysis of the filtered images.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5073",
        "title": "Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "We propose a new approach, {\\it two-dimensional fused binary compressive sensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bit measurements, exploiting 2D group sparsity for 1-bit compressive sensing recovery. The proposed method is a modified 2D version of the previous {\\it binary iterative hard thresholding} (2DBIHT) algorithm, where the objective function includes a 2D one-sided $\\ell_1$ (or $\\ell_2$) penalty function encouraging agreement with the observed data, an indicator function of $K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. The subgradient of the 2D one-sided $\\ell_1$ (or $\\ell_2$) penalty and the projection onto the $K$-sparsity and TV or MTV constraint can be computed efficiently, allowing the appliaction of algorithms of the {\\it forward-backward splitting} (a.k.a. {\\it iterative shrinkage-thresholding}) family. Experiments on the recovery of 2D sparse piece-wise smooth signals show that the proposed approach is able to take advantage of the piece-wise smoothness of the original signal, achieving more accurate recovery than 2DBIHT. More specifically, 2DFBCS with the MTV and the $\\ell_2$ penalty performs best amongst the algorithms tested.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5074",
        "title": "Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group Sparsity",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "We propose a new method, {\\it binary fused compressive sensing} (BFCS), to recover sparse piece-wise smooth signals from 1-bit compressive measurements. The proposed algorithm is a modification of the previous {\\it binary iterative hard thresholding} (BIHT) algorithm, where, in addition to the sparsity constraint, the total-variation of the recovered signal is upper constrained. As in BIHT, the data term of the objective function is an one-sided $\\ell_1$ (or $\\ell_2$) norm. Experiments on the recovery of sparse piece-wise smooth signals show that the proposed algorithm is able to take advantage of the piece-wise smoothness of the original signal, achieving more accurate recovery than BIHT.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5076",
        "title": "Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "We propose a new method, {\\it robust binary fused compressive sensing} (RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressive measurements. The proposed method is a modification of our previous {\\it binary fused compressive sensing} (BFCS) algorithm, which is based on the {\\it binary iterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of the objective function is a one-sided $\\ell_1$ (or $\\ell_2$) norm. Experiments show that the proposed algorithm is able to take advantage of the piece-wise smoothness of the original signal and detect sign flips and correct them, achieving more accurate recovery than BFCS and BIHT.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5564",
        "title": "Structure Tensor Based Image Interpolation Method",
        "authors": [
            "Ahmadreza Baghaie",
            "Zeyun Yu"
        ],
        "abstract": "Feature preserving image interpolation is an active area in image processing field. In this paper a new direct edge directed image super-resolution algorithm based on structure tensors is proposed. Using an isotropic Gaussian filter, the structure tensor at each pixel of the input image is computed and the pixels are classified to three distinct classes; uniform region, corners and edges, according to the eigenvalues of the structure tensor. Due to application of the isotropic Gaussian filter, the classification is robust to noise presented in image. Based on the tangent eigenvector of the structure tensor, the edge direction is determined and used for interpolation along the edges. In comparison to some previous edge directed image interpolation methods, the proposed method achieves higher quality in both subjective and objective aspects. Also the proposed method outperforms previous methods in case of noisy and JPEG compressed images. Furthermore, without the need for optimization in the process, the algorithm can achieve higher speed.\n    ",
        "submission_date": "2014-02-22T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5619",
        "title": "A Novel Histogram Based Robust Image Registration Technique",
        "authors": [
            "V. Karthikeyan"
        ],
        "abstract": "In this paper, a method for Automatic Image Registration (AIR) through histogram is proposed. Automatic image registration is one of the crucial steps in the analysis of remotely sensed data. A new acquired image must be transformed, using image registration techniques, to match the orientation and scale of previous related images. This new approach combines several segmentations of the pair of images to be registered. A relaxation parameter on the histogram modes delineation is introduced. It is followed by characterization of the extracted objects through the objects area, axis ratio, and perimeter and fractal dimension. The matched objects are used for rotation and translation estimation. It allows for the registration of pairs of images with differences in rotation and translation. This method contributes to subpixel accuracy.\n    ",
        "submission_date": "2014-02-23T00:00:00",
        "last_modified_date": "2014-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5623",
        "title": "Localization of License Plate Using Morphological Operations",
        "authors": [
            "V.Karthikeyan",
            "V.J.Vijayalakshmi"
        ],
        "abstract": "It is believed that there are currently millions of vehicles on the roads worldwide. The over speed of vehicles,theft of vehicles, disobeying traffic rules in public, an unauthorized person entering the restricted area are keep on increasing. In order restrict against these criminal activities, we need an automatic public security system. Each vehicle has their own Vehicle Identification Number (VIN) as their primary identifier. The VIN is actually a License Number which states a legal license to participate in the public traffic. The proposed paper is to identify the vehicle with the help of vehicles License Plate (LP).LPRS is one the most important part of the Intelligent Transportation System (ITS) to locate the LP. In this paper certain existing algorithm drawbacks are overcome by the proposed morphological operations for LPRS. Morphological operation is chosen due to its higher efficiency, noise filter capacity, accuracy, exact localization of LP and speed.\n    ",
        "submission_date": "2014-02-23T00:00:00",
        "last_modified_date": "2014-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5697",
        "title": "Exemplar-based Linear Discriminant Analysis for Robust Object Tracking",
        "authors": [
            "Changxin Gao",
            "Feifei Chen",
            "Jin-Gang Yu",
            "Rui Huang",
            "Nong Sang"
        ],
        "abstract": "Tracking-by-detection has become an attractive tracking technique, which treats tracking as a category detection problem. However, the task in tracking is to search for a specific object, rather than an object category as in detection. In this paper, we propose a novel tracking framework based on exemplar detector rather than category detector. The proposed tracker is an ensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Each detector is quite specific and discriminative, because it is trained by a single object instance and massive negatives. To improve its adaptivity, we update both object and background models. Experimental results on several challenging video sequences demonstrate the effectiveness and robustness of our tracking algorithm.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5792",
        "title": "A Novel Scheme for Intelligent Recognition of Pornographic Images",
        "authors": [
            "Seyed Mostafa Kia",
            "Hossein Rahmani",
            "Reza Mortezaei",
            "Mohsen Ebrahimi Moghaddam",
            "Amer Namazi"
        ],
        "abstract": "Harmful contents are rising in internet day by day and this motivates the essence of more research in fast and reliable obscene and immoral material filtering. Pornographic image recognition is an important component in each filtering system. In this paper, a new approach for detecting pornographic images is introduced. In this approach, two new features are suggested. These two features in combination with other simple traditional features provide decent difference between porn and non-porn images. In addition, we applied fuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron) and NF (Neuro-Fuzzy) outputs. To test the proposed method, performance of system was evaluated over 18354 download images from internet. The attained precision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% on test dataset. Achieved results verify the performance of proposed system versus other related works.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5805",
        "title": "Automatic Estimation of Live Coffee Leaf Infection based on Image Processing Techniques",
        "authors": [
            "Eric Hitimana",
            "Oubong Gwun"
        ],
        "abstract": "Image segmentation is the most challenging issue in computer vision applications. And most difficulties for crops management in agriculture are the lack of appropriate methods for detecting the leaf damage for pests treatment. In this paper we proposed an automatic method for leaf damage detection and severity estimation of coffee leaf by avoiding defoliation. After enhancing the contrast of the original image using LUT based gamma correction, the image is processed to remove the background, and the output leaf is clustered using Fuzzy c-means segmentation in V channel of YUV color space to maximize all leaf damage detection, and finally, the severity of leaf is estimated in terms of ratio for leaf pixel distribution between the normal and the detected leaf damage. The results in each proposed method was compared to the current researches and the accuracy is obvious either in the background removal or damage detection.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5859",
        "title": "A Novel Face Recognition Method using Nearest Line Projection",
        "authors": [
            "Huanguo Zhang",
            "Sha Lv",
            "Wei Li",
            "Xun Qu"
        ],
        "abstract": "Face recognition is a popular application of pat- tern recognition methods, and it faces challenging problems including illumination, expression, and pose. The most popular way is to learn the subspaces of the face images so that it could be project to another discriminant space where images of different persons can be separated. In this paper, a nearest line projection algorithm is developed to represent the face images for face recognition. Instead of projecting an image to its nearest image, we try to project it to its nearest line spanned by two different face images. The subspaces are learned so that each face image to its nearest line is minimized. We evaluated the proposed algorithm on some benchmark face image database, and also compared it to some other image projection algorithms. The experiment results showed that the proposed algorithm outperforms other ones.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5923",
        "title": "A Testbed for Cross-Dataset Analysis",
        "authors": [
            "Tatiana Tommasi",
            "Tinne Tuytelaars",
            "Barbara Caputo"
        ],
        "abstract": "Since its beginning visual recognition research has tried to capture the huge variability of the visual world in several image collections. The number of available datasets is still progressively growing together with the amount of samples per object category. However, this trend does not correspond directly to an increasing in the generalization capabilities of the developed recognition systems. Each collection tends to have its specific characteristics and to cover just some aspects of the visual world: these biases often narrow the effect of the methods defined and tested separately over each image set. Our work makes a first step towards the analysis of the dataset bias problem on a large scale. We organize twelve existing databases in a unique corpus and we present the visual community with a useful feature repository for future research.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6383",
        "title": "Large-margin Learning of Compact Binary Image Encodings",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "The use of high-dimensional features has become a normal practice in many computer vision applications. The large dimension of these features is a limiting factor upon the number of data points which may be effectively stored and processed, however. We address this problem by developing a novel approach to learning a compact binary encoding, which exploits both pair-wise proximity and class-label information on training data set. Exploiting this extra information allows the development of encodings which, although compact, outperform the original high-dimensional features in terms of final classification or retrieval performance. The method is general, in that it is applicable to both non-parametric and parametric learning methods. This generality means that the embedded features are suitable for a wide variety of computer vision tasks, such as image classification and content-based image retrieval. Experimental results demonstrate that the new compact descriptor achieves an accuracy comparable to, and in some cases better than, the visual descriptor in the original space despite being significantly more compact. Moreover, any convex loss function and convex regularization penalty (e.g., $ \\ell_p $ norm with $ p \\ge 1 $) can be incorporated into the framework, which provides future flexibility.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6387",
        "title": "Active spline model: A shape based model-interactive segmentation",
        "authors": [
            "Jen Hong Tan",
            "U. Rajendra Acharya"
        ],
        "abstract": "Rarely in literature a method of segmentation cares for the edit after the algorithm delivers. They provide no solution when segmentation goes wrong. We propose to formulate point distribution model in terms of centripetal-parameterized Catmull-Rom spline. Such fusion brings interactivity to model-based segmentation, so that edit is better handled. When the delivered segment is unsatisfactory, user simply shifts points to vary the curve. We ran the method on three disparate imaging modalities and achieved an average overlap of 0.879 for automated lung segmentation on chest radiographs. The edit afterward improved the average overlap to 0.945, with a minimum of 0.925. The source code and the demo video are available at ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6416",
        "title": "Deconstruction of compound objects from image sets",
        "authors": [
            "Anton van den Hengel",
            "John Bastian",
            "Anthony Dick",
            "Lachlan Fleming"
        ],
        "abstract": "We propose a method to recover the structure of a compound object from multiple silhouettes. Structure is expressed as a collection of 3D primitives chosen from a pre-defined library, each with an associated pose. This has several advantages over a volume or mesh representation both for estimation and the utility of the recovered model. The main challenge in recovering such a model is the combinatorial number of possible arrangements of parts. We address this issue by exploiting the sparse nature of the problem, and show that our method scales to objects constructed from large libraries of parts.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6650",
        "title": "A Novel Method for the Recognition of Isolated Handwritten Arabic Characters",
        "authors": [
            "Ahmed Sahlol",
            "Cheng Suen"
        ],
        "abstract": "There are many difficulties facing a handwritten Arabic recognition system such as unlimited variation in human handwriting, similarities of distinct character shapes, interconnections of neighbouring characters and their position in the word. The typical Optical Character Recognition (OCR) systems are based mainly on three stages, preprocessing, features extraction and recognition. This paper proposes new methods for handwritten Arabic character recognition which is based on novel preprocessing operations including different kinds of noise removal also different kind of features like structural, Statistical and Morphological features from the main body of the character and also from the secondary components. Evaluation of the accuracy of the selected features is made. The system was trained and tested by back propagation neural network with CENPRMI dataset. The proposed algorithm obtained promising results as it is able to recognize 88% of our test set accurately. In Comparable with other related works we find that our result is the highest among other published works.\n    ",
        "submission_date": "2014-02-26T00:00:00",
        "last_modified_date": "2014-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6932",
        "title": "Low-Cost Compressive Sensing for Color Video and Depth",
        "authors": [
            "Xin Yuan",
            "Patrick Llull",
            "Xuejun Liao",
            "Jianbo Yang",
            "Guillermo Sapiro",
            "David J. Brady",
            "Lawrence Carin"
        ],
        "abstract": "A simple and inexpensive (low-power and low-bandwidth) modification is made to a conventional off-the-shelf color video camera, from which we recover {multiple} color frames for each of the original measured frames, and each of the recovered frames can be focused at a different depth. The recovery of multiple frames for each measured frame is made possible via high-speed coding, manifested via translation of a single coded aperture; the inexpensive translation is constituted by mounting the binary code on a piezoelectric device. To simultaneously recover depth information, a {liquid} lens is modulated at high speed, via a variable voltage. Consequently, during the aforementioned coding process, the liquid lens allows the camera to sweep the focus through multiple depths. In addition to designing and implementing the camera, fast recovery is achieved by an anytime algorithm exploiting the group-sparsity of wavelet/DCT coefficients.\n    ",
        "submission_date": "2014-02-27T00:00:00",
        "last_modified_date": "2014-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.7162",
        "title": "Visual Saliency Model using SIFT and Comparison of Learning Approaches",
        "authors": [
            "Hamdi Yalin Yalic"
        ],
        "abstract": "Humans' ability to detect and locate salient objects on images is remarkably fast and successful. Performing this process by using eye tracking equipment is expensive and cannot be easily applied, and computer modeling of this human behavior is still a problem to be solved. In our study, one of the largest public eye-tracking databases which has fixation points of 15 observers on 1003 images is used. In addition to low, medium and high-level features which have been used in previous studies, SIFT features extracted from the images are used to improve the classification accuracy of the models. A second contribution of this paper is the comparison and statistical analysis of different machine learning methods that can be used to train our model. As a result, a best feature set and learning model to predict where humans look at images, is determined.\n    ",
        "submission_date": "2014-02-28T00:00:00",
        "last_modified_date": "2014-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0087",
        "title": "Temporal Image Fusion",
        "authors": [
            "Francisco J. Estrada"
        ],
        "abstract": "This paper introduces temporal image fusion. The proposed technique builds upon previous research in exposure fusion and expands it to deal with the limited Temporal Dynamic Range of existing sensors and camera technologies. In particular, temporal image fusion enables the rendering of long-exposure effects on full frame-rate video, as well as the generation of arbitrarily long exposures from a sequence of images of the same scene taken over time. We explore the problem of temporal under-exposure, and show how it can be addressed by selectively enhancing dynamic structure. Finally, we show that the use of temporal image fusion together with content-selective image filters can produce a range of striking visual effects on a given input sequence.\n    ",
        "submission_date": "2014-03-01T00:00:00",
        "last_modified_date": "2014-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0240",
        "title": "Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation",
        "authors": [
            "Ivo F. Sbalzarini",
            "Sophie Schneider",
            "Janick Cardinale"
        ],
        "abstract": "Bio-image analysis is challenging due to inhomogeneous intensity distributions and high levels of noise in the images. Bayesian inference provides a principled way for regularizing the problem using prior knowledge. A fundamental choice is how one measures \"distances\" between shapes in an image. It has been shown that the straightforward geometric L2 distance is degenerate and leads to pathological situations. This is avoided when using Sobolev gradients, rendering the segmentation problem less ill-posed. The high computational cost and implementation overhead of Sobolev gradients, however, have hampered practical applications. We show how particle methods as applied to image segmentation allow for a simple and computationally efficient implementation of Sobolev gradients. We show that the evaluation of Sobolev gradients amounts to particle-particle interactions along the contour in an image. We extend an existing particle-based segmentation algorithm to using Sobolev gradients. Using synthetic and real-world images, we benchmark the results for both 2D and 3D images using piecewise smooth and piecewise constant region models. The present particle approximation of Sobolev gradients is 2.8 to 10 times faster than the previous reference implementation, but retains the known favorable properties of Sobolev gradients. This speedup is achieved by using local particle-particle interactions instead of solving a global Poisson equation at each iteration. The computational time per iteration is higher for Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition the optimization problem, however, a smaller number of overall iterations may be necessary for the algorithm to converge, which can in some cases amortize the higher per-iteration cost.\n    ",
        "submission_date": "2014-03-02T00:00:00",
        "last_modified_date": "2014-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0284",
        "title": "Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval",
        "authors": [
            "Liang Zheng",
            "Shengjin Wang",
            "Wengang Zhou",
            "Qi Tian"
        ],
        "abstract": "The Bag-of-Words (BoW) representation is well applied to recent state-of-the-art image retrieval works. Typically, multiple vocabularies are generated to correct quantization artifacts and improve recall. However, this routine is corrupted by vocabulary correlation, i.e., overlapping among different vocabularies. Vocabulary correlation leads to an over-counting of the indexed features in the overlapped area, or the intersection set, thus compromising the retrieval accuracy. In order to address the correlation problem while preserve the benefit of high recall, this paper proposes a Bayes merging approach to down-weight the indexed features in the intersection set. Through explicitly modeling the correlation problem in a probabilistic view, a joint similarity on both image- and feature-level is estimated for the indexed features in the intersection set.\n",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0309",
        "title": "Object Tracking via Non-Euclidean Geometry: A Grassmann Approach",
        "authors": [
            "Sareh Shirazi",
            "Mehrtash T. Harandi",
            "Brian C. Lovell",
            "Conrad Sanderson"
        ],
        "abstract": "A robust visual tracking system requires an object appearance model that is able to handle occlusion, pose, and illumination variations in the video stream. This can be difficult to accomplish when the model is trained using only a single image. In this paper, we first propose a tracking approach based on affine subspaces (constructed from several images) which are able to accommodate the abovementioned variations. We use affine subspaces not only to represent the object, but also the candidate areas that the object may occupy. We furthermore propose a novel approach to measure affine subspace-to-subspace distance via the use of non-Euclidean geometry of Grassmann manifolds. The tracking problem is then considered as an inference task in a Markov Chain Monte Carlo framework via particle filtering. Quantitative evaluation on challenging video sequences indicates that the proposed approach obtains considerably better performance than several recent state-of-the-art methods such as Tracking-Learning-Detection and MILtrack.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0315",
        "title": "Summarisation of Short-Term and Long-Term Videos using Texture and Colour",
        "authors": [
            "Johanna Carvajal",
            "Chris McCool",
            "Conrad Sanderson"
        ],
        "abstract": "We present a novel approach to video summarisation that makes use of a Bag-of-visual-Textures (BoT) approach. Two systems are proposed, one based solely on the BoT approach and another which exploits both colour information and BoT features. On 50 short-term videos from the Open Video Project we show that our BoT and fusion systems both achieve state-of-the-art performance, obtaining an average F-measure of 0.83 and 0.86 respectively, a relative improvement of 9% and 13% when compared to the previous state-of-the-art. When applied to a new underwater surveillance dataset containing 33 long-term videos, the proposed system reduces the amount of footage by a factor of 27, with only minor degradation in the information content. This order of magnitude reduction in video data represents significant savings in terms of time and potential labour cost when manually reviewing such footage.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0316",
        "title": "Cross-Scale Cost Aggregation for Stereo Matching",
        "authors": [
            "Kang Zhang",
            "Yuqiang Fang",
            "Dongbo Min",
            "Lifeng Sun",
            "Shiqiang Yang. Shuicheng Yan",
            "Qi Tian"
        ],
        "abstract": "Human beings process stereoscopic correspondence across multiple scales. However, this bio-inspiration is ignored by state-of-the-art cost aggregation methods for dense stereo correspondence. In this paper, a generic cross-scale cost aggregation framework is proposed to allow multi-scale interaction in cost aggregation. We firstly reformulate cost aggregation from a unified optimization perspective and show that different cost aggregation methods essentially differ in the choices of similarity kernels. Then, an inter-scale regularizer is introduced into optimization and solving this new optimization problem leads to the proposed framework. Since the regularization term is independent of the similarity kernel, various cost aggregation methods can be integrated into the proposed general framework. We show that the cross-scale framework is important as it effectively and efficiently expands state-of-the-art cost aggregation methods and leads to significant improvements, when evaluated on Middlebury, KITTI and New Tsukuba datasets.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0320",
        "title": "Matching Image Sets via Adaptive Multi Convex Hull",
        "authors": [
            "Shaokang Chen",
            "Arnold Wiliem",
            "Conrad Sanderson",
            "Brian C. Lovell"
        ],
        "abstract": "Traditional nearest points methods use all the samples in an image set to construct a single convex or affine hull model for classification. However, strong artificial features and noisy data may be generated from combinations of training samples when significant intra-class variations and/or noise occur in the image set. Existing multi-model approaches extract local models by clustering each image set individually only once, with fixed clusters used for matching with various image sets. This may not be optimal for discrimination, as undesirable environmental conditions (eg. illumination and pose variations) may result in the two closest clusters representing different characteristics of an object (eg. frontal face being compared to non-frontal face). To address the above problem, we propose a novel approach to enhance nearest points based methods by integrating affine/convex hull classification with an adapted multi-model approach. We first extract multiple local convex hulls from a query image set via maximum margin clustering to diminish the artificial variations and constrain the noise in local convex hulls. We then propose adaptive reference clustering (ARC) to constrain the clustering of each gallery image set by forcing the clusters to have resemblance to the clusters in the query image set. By applying ARC, noisy clusters in the query set can be discarded. Experiments on Honda, MoBo and ETH-80 datasets show that the proposed method outperforms single model approaches and other recent techniques, such as Sparse Approximated Nearest Points, Mutual Subspace Method and Manifold Discriminant Analysis.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0485",
        "title": "Face Recognition Methods & Applications",
        "authors": [
            "Divyarajsinh N. Parmar",
            "Brijesh B. Mehta"
        ],
        "abstract": "Face recognition presents a challenging problem in the field of image analysis and computer vision. The security of information is becoming very significant and difficult. Security cameras are presently common in airports, Offices, University, ATM, Bank and in any locations with a security system. Face recognition is a biometric system used to identify or verify a person from a digital image. Face Recognition system is used in security. Face recognition system should be able to automatically detect a face in an image. This involves extracts its features and then recognize it, regardless of lighting, expression, illumination, ageing, transformations (translate, rotate and scale image) and pose, which is a difficult task. This paper contains three sections. The first section describes the common methods like holistic matching method, feature extraction method and hybrid methods. The second section describes applications with examples and finally third section describes the future research directions of face recognition.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0699",
        "title": "Multi-Shot Person Re-Identification via Relational Stein Divergence",
        "authors": [
            "Azadeh Alavi",
            "Yan Yang",
            "Mehrtash Harandi",
            "Conrad Sanderson"
        ],
        "abstract": "Person re-identification is particularly challenging due to significant appearance changes across separate camera views. In order to re-identify people, a representative human signature should effectively handle differences in illumination, pose and camera parameters. While general appearance-based methods are modelled in Euclidean spaces, it has been argued that some applications in image and video analysis are better modelled via non-Euclidean manifold geometry. To this end, recent approaches represent images as covariance matrices, and interpret such matrices as points on Riemannian manifolds. As direct classification on such manifolds can be difficult, in this paper we propose to represent each manifold point as a vector of similarities to class representers, via a recently introduced form of Bregman matrix divergence known as the Stein divergence. This is followed by using a discriminative mapping of similarity vectors for final classification. The use of similarity vectors is in contrast to the traditional approach of embedding manifolds into tangent spaces, which can suffer from representing the manifold structure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS datasets for the person re-identification task show that the proposed approach obtains better performance than recent techniques such as Histogram Plus Epitome, Partial Least Squares, and Symmetry-Driven Accumulation of Local Features.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0700",
        "title": "Random Projections on Manifolds of Symmetric Positive Definite Matrices for Image Classification",
        "authors": [
            "Azadeh Alavi",
            "Arnold Wiliem",
            "Kun Zhao",
            "Brian C. Lovell",
            "Conrad Sanderson"
        ],
        "abstract": "Recent advances suggest that encoding images through Symmetric Positive Definite (SPD) matrices and then interpreting such matrices as points on Riemannian manifolds can lead to increased classification performance. Taking into account manifold geometry is typically done via (1) embedding the manifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert Spaces (RKHS). While embedding into tangent spaces allows the use of existing Euclidean-based learning algorithms, manifold shape is only approximated which can cause loss of discriminatory information. The RKHS approach retains more of the manifold structure, but may require non-trivial effort to kernelise Euclidean-based learning algorithms. In contrast to the above approaches, in this paper we offer a novel solution that allows SPD matrices to be used with unmodified Euclidean-based learning algorithms, with the true manifold shape well-preserved. Specifically, we propose to project SPD matrices using a set of random projection hyperplanes over RKHS into a random projection space, which leads to representing each matrix as a vector of projection coefficients. Experiments on face recognition, person re-identification and texture classification show that the proposed approach outperforms several recent methods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian Locality Preserving Projection and Relational Divergence Classification.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0728",
        "title": "A Novel Method for Vectorization",
        "authors": [
            "Tolga Birdal",
            "Emrah Bala"
        ],
        "abstract": "Vectorization of images is a key concern uniting computer graphics and computer vision communities. In this paper we are presenting a novel idea for efficient, customizable vectorization of raster images, based on Catmull Rom spline fitting. The algorithm maintains a good balance between photo-realism and photo abstraction, and hence is applicable to applications with artistic concerns or applications where less information loss is crucial. The resulting algorithm is fast, parallelizable and can satisfy general soft realtime requirements. Moreover, the smoothness of the vectorized images aesthetically outperforms outputs of many polygon-based methods\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0820",
        "title": "Geometry-based Adaptive Symbolic Approximation for Fast Sequence Matching on Manifolds",
        "authors": [
            "Rushil Anirudh",
            "Pavan Turaga"
        ],
        "abstract": "In this paper, we consider the problem of fast and efficient indexing techniques for sequences evolving in non-Euclidean spaces. This problem has several applications in the areas of human activity analysis, where there is a need to perform fast search, and recognition in very high dimensional spaces. The problem is made more challenging when representations such as landmarks, contours, and human skeletons etc. are naturally studied in a non-Euclidean setting where even simple operations are much more computationally intensive than their Euclidean counterparts. We propose a geometry and data adaptive symbolic framework that is shown to enable the deployment of fast and accurate algorithms for activity recognition, dynamic texture recognition, motif discovery. Toward this end, we present generalizations of key concepts of piece-wise aggregation and symbolic approximation for the case of non-Euclidean manifolds. We show that one can replace expensive geodesic computations with much faster symbolic computations with little loss of accuracy in activity recognition and discovery applications. The framework is general enough to work across both Euclidean and non-Euclidean spaces, depending on appropriate feature representations without compromising on the ultra-low bandwidth, high speed and high accuracy. The proposed methods are ideally suited for real-time systems and low complexity scenarios.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.0829",
        "title": "Multiview Hessian regularized logistic regression for action recognition",
        "authors": [
            "W. Liu",
            "H. Liu",
            "D. Tao",
            "Y. Wang",
            "Ke Lu"
        ],
        "abstract": "With the rapid development of social media sharing, people often need to manage the growing volume of multimedia data such as large scale video classification and annotation, especially to organize those videos containing human activities. Recently, manifold regularized semi-supervised learning (SSL), which explores the intrinsic data probability distribution and then improves the generalization ability with only a small number of labeled data, has emerged as a promising paradigm for semiautomatic video classification. In addition, human action videos often have multi-modal content and different representations. To tackle the above problems, in this paper we propose multiview Hessian regularized logistic regression (mHLR) for human action recognition. Compared with existing work, the advantages of mHLR lie in three folds: (1) mHLR combines multiple Hessian regularization, each of which obtained from a particular representation of instance, to leverage the exploring of local geometry; (2) mHLR naturally handle multi-view instances with multiple representations; (3) mHLR employs a smooth loss function and then can be effectively optimized. We carefully conduct extensive experiments on the unstructured social activity attribute (USAA) dataset and the experimental results demonstrate the effectiveness of the proposed multiview Hessian regularized logistic regression for human action recognition.\n    ",
        "submission_date": "2014-03-03T00:00:00",
        "last_modified_date": "2014-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1024",
        "title": "On learning to localize objects with minimal supervision",
        "authors": [
            "Hyun Oh Song",
            "Ross Girshick",
            "Stefanie Jegelka",
            "Julien Mairal",
            "Zaid Harchaoui",
            "Trevor Darrell"
        ],
        "abstract": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.\n    ",
        "submission_date": "2014-03-05T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1056",
        "title": "K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian Detection",
        "authors": [
            "Andres Sanin",
            "Conrad Sanderson",
            "Mehrtash T. Harandi",
            "Brian C. Lovell"
        ],
        "abstract": "For covariance-based image descriptors, taking into account the curvature of the corresponding feature space has been shown to improve discrimination performance. This is often done through representing the descriptors as points on Riemannian manifolds, with the discrimination accomplished on a tangent space. However, such treatment is restrictive as distances between arbitrary points on the tangent space do not represent true geodesic distances, and hence do not represent the manifold structure accurately. In this paper we propose a general discriminative model based on the combination of several tangent spaces, in order to preserve more details of the structure. The model can be used as a weak learner in a boosting-based pedestrian detection framework. Experiments on the challenging INRIA and DaimlerChrysler datasets show that the proposed model leads to considerably higher performance than methods based on histograms of oriented gradients as well as previous Riemannian-based techniques.\n    ",
        "submission_date": "2014-03-05T00:00:00",
        "last_modified_date": "2014-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1327",
        "title": "Multi-view Face Analysis Based on Gabor Features",
        "authors": [
            "Hongli Liu",
            "Weifeng Liu",
            "Yanjiang Wang"
        ],
        "abstract": "Facial analysis has attracted much attention in the technology for human-machine interface. Different methods of classification based on sparse representation and Gabor kernels have been widely applied in the fields of facial analysis. However, most of these methods treat face from a whole view standpoint. In terms of the importance of different facial views, in this paper, we present multi-view face analysis based on sparse representation and Gabor wavelet coefficients. To evaluate the performance, we conduct face analysis experiments including face recognition (FR) and face expression recognition (FER) on JAFFE database. Experiments are conducted from two parts: (1) Face images are divided into three facial parts which are forehead, eye and mouth. (2) Face images are divided into 8 parts by the orientation of Gabor kernels. Experimental results demonstrate that the proposed methods can significantly boost the performance and perform better than the other methods.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1353",
        "title": "Collaborative Representation for Classification, Sparse or Non-sparse?",
        "authors": [
            "Yang Wu",
            "Vansteenberge Jarich",
            "Masayuki Mukunoki",
            "Michihiko Minoh"
        ],
        "abstract": "Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the $l_1$-norm based regularization to the computationally much more efficient $l_2$-norm based non-sparse version would lead to a similar or even better performance. However, that's not always the case. Given a new classification task, it's still unclear which regularization strategy (i.e., making the coefficients sparse or non-sparse) is a better choice without trying both for comparison. In this paper, we present as far as we know the first study on solving this issue, based on plenty of diverse classification experiments. We propose a scoring function for pre-selecting the regularization strategy using only the dataset size, the feature dimensionality and a discrimination score derived from a given feature representation. Moreover, we show that when dictionary learning is taking into account, non-sparse representation has a more significant superiority to sparse representation. This work is expected to enrich our understanding of sparse/non-sparse collaborative representation for classification and motivate further research activities.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1362",
        "title": "Illumination,Expression and Occlusion Invariant Pose-Adaptive Face Recognition System for Real-Time Applications",
        "authors": [
            "Shireesha Chintalapati",
            "M. V. Raghunadh"
        ],
        "abstract": "Face recognition in real-time scenarios is mainly affected by illumination, expression and pose variations and also by occlusion. This paper presents the framework for pose adaptive component-based face recognition system. The framework proposed deals with all the above mentioned issues. The steps involved in the presented framework are (i) facial landmark localisation, (ii) facial component extraction, (iii) pre-processing of facial image (iv) facial pose estimation (v) feature extraction using Local Binary Pattern Histograms of each component followed by (vi) fusion of pose adaptive classification of components. By employing pose adaptive classification, the recognition process is carried out on some part of database, based on estimated pose, instead of applying the recognition process on the whole database. Pre-processing techniques employed to overcome the problems due to illumination variation are also discussed in this paper. Component-based techniques provide better recognition rates when face images are occluded compared to the holistic methods. Our method is simple, feasible and provides better results when compared to other holistic methods.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1626",
        "title": "Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing",
        "authors": [
            "Zhiwu Lu",
            "Zhenyong Fu",
            "Tao Xiang",
            "Liwei Wang",
            "Ji-Rong Wen"
        ],
        "abstract": "This paper presents a weakly supervised sparse learning approach to the problem of noisily tagged image parsing, or segmenting all the objects within a noisily tagged image and identifying their categories (i.e. tags). Different from the traditional image parsing that takes pixel-level labels as strong supervisory information, our noisily tagged image parsing is provided with noisy tags of all the images (i.e. image-level labels), which is a natural setting for social image collections (e.g. Flickr). By oversegmenting all the images into regions, we formulate noisily tagged image parsing as a weakly supervised sparse learning problem over all the regions, where the initial labels of each region are inferred from image-level labels. Furthermore, we develop an efficient algorithm to solve such weakly supervised sparse learning problem. The experimental results on two benchmark datasets show the effectiveness of our approach. More notably, the reported surprising results shed some light on answering the question: can image-level labels replace pixel-level labels (hard to access) as supervisory information for image parsing.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1653",
        "title": "Automated Tracking and Estimation for Control of Non-rigid Cloth",
        "authors": [
            "Marc D. Killpack"
        ],
        "abstract": "This report is a summary of research conducted on cloth tracking for automated textile manufacturing during a two semester long research course at Georgia Tech. This work was completed in 2009. Advances in current sensing technology such as the Microsoft Kinect would now allow me to relax certain assumptions and generally improve the tracking performance. This is because a major part of my approach described in this paper was to track features in a 2D image and use these to estimate the cloth deformation. Innovations such as the Kinect would improve estimation due to the automatic depth information obtained when tracking 2D pixel locations. Additionally, higher resolution camera images would probably give better quality feature tracking. However, although I would use different technology now to implement this tracker, the algorithm described and implemented in this paper is still a viable approach which is why I am publishing this as a tech report for reference. In addition, although the related work is a bit exhaustive, it will be useful to a reader who is new to methods for tracking and estimation as well as modeling of cloth.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1660",
        "title": "Feature Extraction of ECG Signal Using HHT Algorithm",
        "authors": [
            "Neha Soorma",
            "Jaikaran Singh",
            "Mukesh Tiwari"
        ],
        "abstract": "This paper describe the features extraction algorithm for electrocardiogram (ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signal for an individual human being is different due to unique heart structure. The purpose of feature extraction of ECG signal would allow successful abnormality detection and efficient prognosis due to heart disorder. Some major important features will be extracted from ECG signals such as amplitude, duration, pre-gradient, post-gradient and so on. Therefore, we need a strong mathematical model to extract such useful parameter. Here an adaptive mathematical analysis model is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huang transform, is implemented to analyze the non-linear and nonstationary data. It is unique and different from the existing methods of data analysis and does not require an a priori functional basis. The effectiveness of the proposed scheme is verified through the simulation.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1687",
        "title": "Rigid-Motion Scattering for Texture Classification",
        "authors": [
            "Laurent SIfre",
            "St\u00e9phane Mallat"
        ],
        "abstract": "A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network. Convolutions are calculated on the rigid-motion group, with wavelets defined on the translation and rotation variables. It preserves joint rotation and translation information, while providing global invariants at any desired scale. Texture classification is studied, through the characterization of stationary processes from a single realization. State-of-the-art results are obtained on multiple texture data bases, with important rotation and scaling variabilities.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1840",
        "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features",
        "authors": [
            "Yunchao Gong",
            "Liwei Wang",
            "Ruiqi Guo",
            "Svetlana Lazebnik"
        ],
        "abstract": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1902",
        "title": "Quality-based Multimodal Classification Using Tree-Structured Sparsity",
        "authors": [
            "Soheil Bahrampour",
            "Asok Ray",
            "Nasser M. Nasrabadi",
            "Kenneth W. Jenkins"
        ],
        "abstract": "Recent studies have demonstrated advantages of information fusion based on sparsity models for multimodal classification. Among several sparsity models, tree-structured sparsity provides a flexible framework for extraction of cross-correlated information from different sources and for enforcing group sparsity at multiple granularities. However, the existing algorithm only solves an approximated version of the cost functional and the resulting solution is not necessarily sparse at group levels. This paper reformulates the tree-structured sparse model for multimodal classification task. An accelerated proximal algorithm is proposed to solve the optimization problem, which is an efficient tool for feature-level fusion among either homogeneous or heterogeneous sources of information. In addition, a (fuzzy-set-theoretic) possibilistic scheme is proposed to weight the available modalities, based on their respective reliability, in a joint optimization problem for finding the sparsity codes. This approach provides a general framework for quality-based fusion that offers added robustness to several sparsity-based multimodal classification algorithms. To demonstrate their efficacy, the proposed methods are evaluated on three different applications - multiview face recognition, multimodal face recognition, and target classification.\n    ",
        "submission_date": "2014-03-08T00:00:00",
        "last_modified_date": "2014-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1974",
        "title": "Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the Greening of Potatoes",
        "authors": [
            "Jaspinder Pal Singh"
        ],
        "abstract": "Potato quality control has improved in the last years thanks to automation techniques like machine vision, mainly making the classification task between different quality degrees faster, safer and less subjective. In our study we are going to design a computer vision algorithm for grading of potatoes according to the greening of the surface color of potato. The ratio of green pixels to the total number of pixels of the potato surface is found. The higher the ratio the worse is the potato. First the image is converted into serial data and then processing is done in RGB colour space. Green part of the potato is also shown by de-serializing the output. The same algorithm is then synthesized on FPGA and the result shows thousand times speed improvement in case of hardware synthesis.\n    ",
        "submission_date": "2014-03-08T00:00:00",
        "last_modified_date": "2014-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2031",
        "title": "Texture Defect Detection in Gradient Space",
        "authors": [
            "V.Asha",
            "N.U.Bhajantri",
            "P.Nagabhushan"
        ],
        "abstract": "In this paper, we propose a machine vision algorithm for automatically detecting defects in patterned textures with the help of gradient space and its energy. Experiments on real fabric images with defects show that the proposed method can be used for automatic detection of fabric defects in textile industries.\n    ",
        "submission_date": "2014-03-09T00:00:00",
        "last_modified_date": "2014-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2330",
        "title": "Subspace clustering using a symmetric low-rank representation",
        "authors": [
            "Jie Chen",
            "Hua Mao",
            "Yongsheng Sang",
            "Zhang Yi"
        ],
        "abstract": "In this paper, we propose a low-rank representation with symmetric constraint (LRRSC) method for robust subspace clustering. Given a collection of data points approximately drawn from multiple subspaces, the proposed technique can simultaneously recover the dimension and members of each subspace. LRRSC extends the original low-rank representation algorithm by integrating a symmetric constraint into the low-rankness property of high-dimensional data representation. The symmetric low-rank representation, which preserves the subspace structures of high-dimensional data, guarantees weight consistency for each pair of data points so that highly correlated data points of subspaces are represented together. Moreover, it can be efficiently calculated by solving a convex optimization problem. We provide a rigorous proof for minimizing the nuclear-norm regularized least square problem with a symmetric constraint. The affinity matrix for spectral clustering can be obtained by further exploiting the angular information of the principal directions of the symmetric low-rank representation. This is a critical step towards evaluating the memberships between data points. Experimental results on benchmark databases demonstrate the effectiveness and robustness of LRRSC compared with several state-of-the-art subspace clustering algorithms.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2017-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2482",
        "title": "Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted Means",
        "authors": [
            "Haijuan Hu",
            "Bing Li",
            "Quansheng Liu"
        ],
        "abstract": "We first establish a law of large numbers and a convergence theorem in distribution to show the rate of convergence of the non-local means filter for removing Gaussian noise. We then introduce the notion of degree of similarity to measure the role of similarity for the non-local means filter. Based on the convergence theorems, we propose a patch-based weighted means filter for removing impulse noise and its mixture with Gaussian noise by combining the essential idea of the trilateral filter and that of the non-local means filter. Our experiments show that our filter is competitive compared to recently proposed methods.\n    ",
        "submission_date": "2014-03-11T00:00:00",
        "last_modified_date": "2014-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2802",
        "title": "Learning Deep Face Representation",
        "authors": [
            "Haoqiang Fan",
            "Zhimin Cao",
            "Yuning Jiang",
            "Qi Yin",
            "Chinchilla Doudou"
        ],
        "abstract": "Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy ($85.8\\%$ on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance ($97.3\\%$) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2871",
        "title": "Shape-Based Plagiarism Detection for Flowchart Figures in Texts",
        "authors": [
            "Senosy Arrish",
            "Fadhil Noer Afif",
            "Ahmadu Maidorawa",
            "Naomie Salim"
        ],
        "abstract": "Plagiarism detection is well known phenomenon in the academic arena. Copying other people is considered as serious offence that needs to be checked. There are many plagiarism detection systems such as turn-it-in that has been developed to provide this checks. Most, if not all, discard the figures and charts before checking for plagiarism. Discarding the figures and charts results in look holes that people can take advantage. That means people can plagiarized figures and charts easily without the current plagiarism systems detecting it. There are very few papers which talks about flowcharts plagiarism detection. Therefore, there is a need to develop a system that will detect plagiarism in figures and charts. This paper presents a method for detecting flow chart figure plagiarism based on shape-based image processing and multimedia retrieval. The method managed to retrieve flowcharts with ranked similarity according to different matching sets.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2895",
        "title": "Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras",
        "authors": [
            "M. Mart\u00ednez-Zarzuela",
            "M. Pedraza-Hueso",
            "F.J. D\u00edaz-Pernas",
            "D. Gonz\u00e1lez-Ortega",
            "M. Ant\u00f3n-Rodr\u00edguez"
        ],
        "abstract": "This article describes the design and development of a system for remote indoor 3D monitoring using an undetermined number of Microsoft(R) Kinect sensors. In the proposed client-server system, the Kinect cameras can be connected to different computers, addressing this way the hardware limitation of one sensor per USB controller. The reason behind this limitation is the high bandwidth needed by the sensor, which becomes also an issue for the distributed system TCP/IP communications. Since traffic volume is too high, 3D data has to be compressed before it can be sent over the network. The solution consists in selfcoding the Kinect data into RGB images and then using a standard multimedia codec to compress color maps. Information from different sources is collected into a central client computer, where point clouds are transformed to reconstruct the scene in 3D. An algorithm is proposed to merge the skeletons detected locally by each Kinect conveniently, so that monitoring of people is robust to self and inter-user occlusions. Final skeletons are labeled and trajectories of every joint can be saved for event reconstruction or further analysis.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2980",
        "title": "3D Well-composed Polyhedral Complexes",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Maria-Jose Jimenez",
            "Belen Medrano"
        ],
        "abstract": "A binary three-dimensional (3D) image $I$ is well-composed if the boundary surface of its continuous analog is a 2D manifold. Since 3D images are not often well-composed, there are several voxel-based methods (\"repairing\" algorithms) for turning them into well-composed ones but these methods either do not guarantee the topological equivalence between the original image and its corresponding well-composed one or involve sub-sampling the whole image.\n",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3021",
        "title": "Image reconstruction from limited range projections using orthogonal moments",
        "authors": [
            "Huazhong Shu",
            "Jian Zhou",
            "Guo-Niu Han",
            "Limin M. Luo",
            "Jean-Louis Coatrieux"
        ],
        "abstract": "A set of orthonormal polynomials is proposed for image reconstruction from projection data. The relationship between the projection moments and image moments is discussed in detail, and some interesting properties are demonstrated. Simulation results are provided to validate the method and to compare its performance with previous works.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3022",
        "title": "Efficient Legendre moment computation for grey level images",
        "authors": [
            "Guanyu Yang",
            "Huazhong Shu",
            "Christine Toumoulin",
            "Guo-Niu Han",
            "Limin M. Luo"
        ],
        "abstract": "Legendre orthogonal moments have been widely used in the field of image analysis. Because their computation by a direct method is very time expensive, recent efforts have been devoted to the reduction of computational complexity. Nevertheless, the existing algorithms are mainly focused on binary images. We propose here a new fast method for computing the Legendre moments, which is not only suitable for binary images but also for grey levels. We first set up the recurrence formula of one-dimensional (1D) Legendre moments by using the recursive property of Legendre polynomials. As a result, the 1D Legendre moments of order p, Lp = Lp(0), can be expressed as a linear combination of Lp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0) is thus obtained from the array of L1(a) and L0(a) where a is an integer number less than p. To further decrease the computation complexity, an algorithm, in which no multiplication is required, is used to compute these quantities. The method is then extended to the calculation of the two-dimensional Legendre moments Lpq. We show that the proposed method is more efficient than the direct method.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3083",
        "title": "A Novel Method to Extract Rocks from Mars Images",
        "authors": [
            "Shuliang Wang",
            "Yasen Chen"
        ],
        "abstract": "In this paper, a novel method is proposed to extract rocks from Martian surface images by using 8 data field. It models the interaction between two pixels of an image in the context of imagery 9 characteristics. First, foreground rocks are differed from background information by binarizing 10 image on roughly partitioned images. Second, foreground rocks are grouped into clusters by 11 locating the centers and edges of clusters in data field via hierarchical grids. Third, the target 12 rocks are discovered for the Mars Exploration Rover (MER) to keep healthy paths. The 13 experiment with images taken by MER shows the proposed method is practical and potential.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2014-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3118",
        "title": "Parallel WiSARD object tracker: a ram-based tracking system",
        "authors": [
            "Rodrigo da Silva Moreira",
            "Nelson Francisco Favilla Ebecken"
        ],
        "abstract": "This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new object tracker based on the WiSARD weightless neural network that is robust against quantization errors. Object tracking in video is an important and challenging task in many applications. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. Tracking is a high-level application and requires the object location frame by frame in real time. This paper proposes a fast hybrid image segmentation (threshold and edge detection) in YcbCr color model and a parallel RAM based discriminator that improves efficiency when quantization errors occur. The original WiSARD training algorithm was changed to allow the tracking.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3155",
        "title": "Spectral Unmixing via Data-guided Sparsity",
        "authors": [
            "Feiyun Zhu",
            "Ying Wang",
            "Bin Fan",
            "Gaofeng Meng",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "abstract": "Hyperspectral unmixing, the process of estimating a common set of spectral bases and their corresponding composite percentages at each pixel, is an important task for hyperspectral analysis, visualization and understanding. From an unsupervised learning perspective, this problem is very challenging---both the spectral bases and their composite percentages are unknown, making the solution space too large. To reduce the solution space, many approaches have been proposed by exploiting various priors. In practice, these priors would easily lead to some unsuitable solution. This is because they are achieved by applying an identical strength of constraints to all the factors, which does not hold in practice. To overcome this limitation, we propose a novel sparsity based method by learning a data-guided map to describe the individual mixed level of each pixel. Through this data-guided map, the $\\ell_{p}(0<p<1)$ constraint is applied in an adaptive manner. Such implementation not only meets the practical situation, but also guides the spectral bases toward the pixels under highly sparse constraint. What's more, an elegant optimization scheme as well as its convergence proof have been provided in this paper. Extensive experiments on several datasets also demonstrate that the data-guided map is feasible, and high quality unmixing results could be obtained by our method.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3522",
        "title": "An inertial forward-backward algorithm for monotone inclusions",
        "authors": [
            "Dirk A. Lorenz",
            "Thomas Pock"
        ],
        "abstract": "In this paper, we propose an inertial forward backward splitting algorithm to compute a zero of the sum of two monotone operators, with one of the two operators being co-coercive. The algorithm is inspired by the accelerated gradient method of Nesterov, but can be applied to a much larger class of problems including convex-concave saddle point problems and general monotone inclusions. We prove convergence of the algorithm in a Hilbert space setting and show that several recently proposed first-order methods can be obtained as special cases of the general algorithm. Numerical results show that the proposed algorithm converges faster than existing methods, while keeping the computational cost of each iteration basically unchanged.\n    ",
        "submission_date": "2014-03-14T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3602",
        "title": "Spontaneous expression classification in the encrypted domain",
        "authors": [
            "Segun Aina",
            "Yogachandran Rahulamathavan",
            "Raphael C.-W. Phan",
            "Jonathon A. Chambers"
        ],
        "abstract": "To date, most facial expression analysis have been based on posed image databases and is carried out without being able to protect the identity of the subjects whose expressions are being recognised. In this paper, we propose and implement a system for classifying facial expressions of images in the encrypted domain based on a Paillier cryptosystem implementation of Fisher Linear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We present results of experiments carried out on a recently developed natural visible and infrared facial expression (NVIE) database of spontaneous images. To the best of our knowledge, this is the first system that will allow the recog-nition of encrypted spontaneous facial expressions by a remote server on behalf of a client.\n    ",
        "submission_date": "2014-03-14T00:00:00",
        "last_modified_date": "2014-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3683",
        "title": "Removal and Contraction Operations in $n$D Generalized Maps for Efficient Homology Computation",
        "authors": [
            "Guillaume Damiand",
            "Rocio Gonzalez-Diaz",
            "Samuel Peltier"
        ],
        "abstract": "In this paper, we show that contraction operations preserve the homology of $n$D generalized maps, under some conditions. Removal and contraction operations are used to propose an efficient algorithm that compute homology generators of $n$D generalized maps. Its principle consists in simplifying a generalized map as much as possible by using removal and contraction operations. We obtain a generalized map having the same homology than the initial one, while the number of cells decreased significantly.\n",
        "submission_date": "2014-03-14T00:00:00",
        "last_modified_date": "2014-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3724",
        "title": "VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer vision at Large Scale",
        "authors": [
            "William Gray Roncal",
            "Michael Pekala",
            "Verena Kaynig-Fittkau",
            "Dean M. Kleissas",
            "Joshua T. Vogelstein",
            "Hanspeter Pfister",
            "Randal Burns",
            "R. Jacob Vogelstein",
            "Mark A. Chevillet",
            "Gregory D. Hager"
        ],
        "abstract": "An open challenge problem at the forefront of modern neuroscience is to obtain a comprehensive mapping of the neural pathways that underlie human brain function; an enhanced understanding of the wiring diagram of the brain promises to lead to new breakthroughs in diagnosing and treating neurological disorders. Inferring brain structure from image data, such as that obtained via electron microscopy (EM), entails solving the problem of identifying biological structures in large data volumes. Synapses, which are a key communication structure in the brain, are particularly difficult to detect due to their small size and limited contrast. Prior work in automated synapse detection has relied upon time-intensive biological preparations (post-staining, isotropic slice thicknesses) in order to simplify the problem.\n",
        "submission_date": "2014-03-14T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3829",
        "title": "Geometric VLAD for Large Scale Image Search",
        "authors": [
            "Zixuan Wang",
            "Wei Di",
            "Anurag Bhardwaj",
            "Vignesh Jagadeesh",
            "Robinson Piramuthu"
        ],
        "abstract": "We present a novel compact image descriptor for large scale image search. Our proposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector of Locally Aggregated Descriptors) that incorporates weak geometry information into the VLAD framework. The proposed geometry cues are derived as a membership function over keypoint angles which contain evident and informative information but yet often discarded. A principled technique for learning the membership function by clustering angles is also presented. Further, to address the overhead of iterative codebook training over real-time datasets, a novel codebook adaptation strategy is outlined. Finally, we demonstrate the efficacy of proposed gVLAD based retrieval framework where we achieve more than 15% improvement in mAP over existing benchmarks.\n    ",
        "submission_date": "2014-03-15T00:00:00",
        "last_modified_date": "2014-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3964",
        "title": "Image processing using miniKanren",
        "authors": [
            "Hirotaka Niitsuma"
        ],
        "abstract": "An integral image is one of the most efficient optimization technique for image processing. However an integral image is only a special case of delayed stream or memoization. This research discusses generalizing concept of integral image optimization technique, and how to generate an integral image optimized program code automatically from abstracted image processing algorithm. In oder to abstruct algorithms, we forces to miniKanren.\n    ",
        "submission_date": "2014-03-16T00:00:00",
        "last_modified_date": "2014-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4232",
        "title": "Automatic Image Registration in Infrared-Visible Videos using Polygon Vertices",
        "authors": [
            "Tanushri Chakravorty",
            "Guillaume-Alexandre Bilodeau",
            "Eric Granger"
        ],
        "abstract": "In this paper, an automatic method is proposed to perform image registration in visible and infrared pair of video sequences for multiple targets. In multimodal image analysis like image fusion systems, color and IR sensors are placed close to each other and capture a same scene simultaneously, but the videos are not properly aligned by default because of different fields of view, image capturing information, working principle and other camera specifications. Because the scenes are usually not planar, alignment needs to be performed continuously by extracting relevant common information. In this paper, we approximate the shape of the targets by polygons and use affine transformation for aligning the two video sequences. After background subtraction, keypoints on the contour of the foreground blobs are detected using DCE (Discrete Curve Evolution)technique. These keypoints are then described by the local shape at each point of the obtained polygon. The keypoints are matched based on the convexity of polygon's vertices and Euclidean distance between them. Only good matches for each local shape polygon in a frame, are kept. To achieve a global affine transformation that maximises the overlapping of infrared and visible foreground pixels, the matched keypoints of each local shape polygon are stored temporally in a buffer for a few number of frames. The matrix is evaluated at each frame using the temporal buffer and the best matrix is selected, based on an overlapping ratio criterion. Our experimental results demonstrate that this method can provide highly accurate registered images and that we outperform a previous related method.\n    ",
        "submission_date": "2014-03-17T00:00:00",
        "last_modified_date": "2014-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4334",
        "title": "Bregman Divergences for Infinite Dimensional Covariance Matrices",
        "authors": [
            "Mehrtash Harandi",
            "Mathieu Salzmann",
            "Fatih Porikli"
        ],
        "abstract": "We introduce an approach to computing and comparing Covariance Descriptors (CovDs) in infinite-dimensional spaces. CovDs have become increasingly popular to address classification problems in computer vision. While CovDs offer some robustness to measurement variations, they also throw away part of the information contained in the original data by only retaining the second-order statistics over the measurements. Here, we propose to overcome this limitation by first mapping the original data to a high-dimensional Hilbert space, and only then compute the CovDs. We show that several Bregman divergences can be computed between the resulting CovDs in Hilbert space via the use of kernels. We then exploit these divergences for classification purposes. Our experiments demonstrate the benefits of our approach on several tasks, such as material and texture recognition, person re-identification, and action recognition from motion capture data.\n    ",
        "submission_date": "2014-03-18T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4682",
        "title": "Structured Sparse Method for Hyperspectral Unmixing",
        "authors": [
            "Feiyun Zhu",
            "Ying Wang",
            "Shiming Xiang",
            "Bin Fan",
            "Chunhong Pan"
        ],
        "abstract": "Hyperspectral Unmixing (HU) has received increasing attention in the past decades due to its ability of unveiling information latent in hyperspectral data. Unfortunately, most existing methods fail to take advantage of the spatial information in data. To overcome this limitation, we propose a Structured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method from the following two aspects. First, we incorporate a graph Laplacian to encode the manifold structures embedded in the hyperspectral data space. In this way, the highly similar neighboring pixels can be grouped together. Second, the lasso penalty is employed in SS-NMF for the fact that pixels in the same manifold structure are sparsely mixed by a common set of relevant bases. These two factors act as a new structured sparse constraint. With this constraint, our method can learn a compact space, where highly similar pixels are grouped to share correlated sparse representations. Experiments on real hyperspectral data sets with different noise levels demonstrate that our method outperforms the state-of-the-art methods significantly.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4777",
        "title": "MFCC based Enlargement of the Training Set for Emotion Recognition in Speech",
        "authors": [
            "Inma Mohino-Herranz",
            "Roberto Gil-Pita",
            "Sagrario Alonso-Diaz",
            "Manuel Rosa-Zurera"
        ],
        "abstract": "Emotional state recognition through speech is being a very interesting research topic nowadays. Using subliminal information of speech, denominated as prosody, it is possible to recognize the emotional state of the person. One of the main problems in the design of automatic emotion recognition systems is the small number of available patterns. This fact makes the learning process more difficult, due to the generalization problems that arise under these conditions. In this work we propose a solution to this problem consisting in enlarging the training set through the creation the new virtual patterns. In the case of emotional speech, most of the emotional information is included in speed and pitch variations. So, a change in the average pitch that does not modify neither the speed nor the pitch variations does not affect the expressed emotion. Thus, we use this prior information in order to create new patterns applying a gender dependent pitch shift modification in the feature extraction process of the classification system. For this purpose, we propose a frequency scaling modification of the Mel Frequency Cepstral Coefficients, used to classify the emotion. For this purpose, we propose a gender dependent frequency scaling modification. This proposed process allows us to synthetically increase the number of available patterns in the training set, thus increasing the generalization capability of the system and reducing the test error. Results carried out with two different classifiers with different degree of generalization capability demonstrate the suitability of the proposal.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5403",
        "title": "A Non-Local Structure Tensor Based Approach for Multicomponent Image Recovery Problems",
        "authors": [
            "Giovanni Chierchia",
            "Nelly Pustelnik",
            "Beatrice Pesquet-Popescu",
            "Jean-Christophe Pesquet"
        ],
        "abstract": "Non-Local Total Variation (NLTV) has emerged as a useful tool in variational methods for image recovery problems. In this paper, we extend the NLTV-based regularization to multicomponent images by taking advantage of the Structure Tensor (ST) resulting from the gradient of a multicomponent image. The proposed approach allows us to penalize the non-local variations, jointly for the different components, through various $\\ell_{1,p}$ matrix norms with $p \\ge 1$. To facilitate the choice of the hyper-parameters, we adopt a constrained convex optimization approach in which we minimize the data fidelity term subject to a constraint involving the ST-NLTV regularization. The resulting convex optimization problem is solved with a novel epigraphical projection method. This formulation can be efficiently implemented thanks to the flexibility offered by recent primal-dual proximal algorithms. Experiments are carried out for multispectral and hyperspectral images. The results demonstrate the interest of introducing a non-local structure tensor regularization and show that the proposed approach leads to significant improvements in terms of convergence speed over current state-of-the-art methods.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5473",
        "title": "Image Fusion Techniques in Remote Sensing",
        "authors": [
            "Reham Gharbia",
            "Ahmad Taher Azar",
            "Ali El Baz",
            "Aboul Ella Hassanien"
        ],
        "abstract": "Remote sensing image fusion is an effective way to use a large volume of data from multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOS and QuickBird provide both panchromatic (Pan) images at a higher spatial resolution and multispectral (MS) images at a lower spatial resolution and many remote sensing applications require both high spatial and high spectral resolutions, especially for GIS based applications. An effective image fusion technique can produce such remotely sensed images. Image fusion is the combination of two or more different images to form a new image by using a certain algorithm to obtain more and better information about an object or a study area than. The image fusion is performed at three different processing levels which are pixel level, feature level and decision level according to the stage at which the fusion takes place. There are many image fusion methods that can be used to produce high resolution multispectral images from a high resolution pan image and low resolution multispectral images. This paper explores the major remote sensing data fusion techniques at pixel level and reviews the concept, principals, limitations and advantages for each technique. This paper focused on traditional techniques like intensity hue-saturation- (HIS), Brovey, principal component analysis (PCA) and Wavelet.\n    ",
        "submission_date": "2014-03-11T00:00:00",
        "last_modified_date": "2014-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5475",
        "title": "An Efficient Method for Face Recognition System In Various Assorted Conditions",
        "authors": [
            "V. Karthikeyan",
            "K. Vijayalakshmi",
            "P. Jeyakumar"
        ],
        "abstract": "In the beginning stage, face verification is done using easy method of geometric algorithm models, but the verification route has now developed into a scientific progress of complicated geometric representation and identical procedure. In recent years the technologies have boosted face recognition system into the healthy focus. Researchers currently undergoing strong research on finding face recognition system for wider area information taken under hysterical elucidation dissimilarity. The proposed face recognition system consists of a narrative expositionindiscreet preprocessing method, a hybrid Fourier-based facial feature extraction and a score fusion scheme. We have verified the face recognition in different lightening conditions (day or night) and at different locations (indoor or outdoor). Preprocessing, Image detection, Feature- extraction and Face recognition are the methods used for face verification system. This paper focuses mainly on the issue of toughness to lighting variations. The proposed system has obtained an average of 88.1% verification rate on Two-Dimensional images under different lightening conditions.\n    ",
        "submission_date": "2014-03-04T00:00:00",
        "last_modified_date": "2014-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5590",
        "title": "Continuous Optimization for Fields of Experts Denoising Works",
        "authors": [
            "Petter Strandmark",
            "Sameer Agarwal"
        ],
        "abstract": "Several recent papers use image denoising with a Fields of Experts prior to benchmark discrete optimization methods. We show that a non-linear least squares solver significantly outperforms all known discrete methods on this problem.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5718",
        "title": "SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images",
        "authors": [
            "Yu-Shiang Wong",
            "Hung-Kuo Chu",
            "Niloy J. Mitra"
        ],
        "abstract": "RGBD images with high quality annotations in the form of geometric (i.e., segmentation) and structural (i.e., how do the segments are mutually related in 3D) information provide valuable priors to a large number of scene and image manipulation applications. While it is now simple to acquire RGBD images, annotating them, automatically or manually, remains challenging especially in cluttered noisy environments. We present SmartAnnotator, an interactive system to facilitate annotating RGBD images. The system performs the tedious tasks of grouping pixels, creating potential abstracted cuboids, inferring object interactions in 3D, and comes up with various hypotheses. The user simply has to flip through a list of suggestions for segment labels, finalize a selection, and the system updates the remaining hypotheses. As objects are finalized, the process speeds up with fewer ambiguities to resolve. Further, as more scenes are annotated, the system makes better suggestions based on structural and geometric priors learns from the previous annotation sessions. We test our system on a large number of database scenes and report significant improvements over naive low-level annotation tools.\n    ",
        "submission_date": "2014-03-23T00:00:00",
        "last_modified_date": "2014-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5869",
        "title": "Block Motion Based Dynamic Texture Analysis: A Review",
        "authors": [
            "Akhlaqur Rahman",
            "Sumaira Tasnim"
        ],
        "abstract": "Dynamic texture refers to image sequences of non-rigid objects that exhibit some regularity in their movement. Videos of smoke, fire etc. fall under the category of dynamic texture. Researchers have investigated different ways to analyze dynamic textures since early nineties. Both appearance based (image intensities) and motion based approaches are investigated. Motion based approaches turn out to be more effective. A group of researchers have investigated ways to utilize the motion vectors readily available with the blocks in video codes like MGEG/H26X. In this paper we provide a review of the dynamic texture analysis methods using block motion. Research into dynamic texture analysis using block motion includes recognition, motion computation, segmentation, and synthesis. We provide a comprehensive review of these approaches.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5919",
        "title": "SRA: Fast Removal of General Multipath for ToF Sensors",
        "authors": [
            "Daniel Freedman",
            "Eyal Krupka",
            "Yoni Smolin",
            "Ido Leichter",
            "Mirko Schmidt"
        ],
        "abstract": "A major issue with Time of Flight sensors is the presence of multipath interference. We present Sparse Reflections Analysis (SRA), an algorithm for removing this interference which has two main advantages. First, it allows for very general forms of multipath, including interference with three or more paths, diffuse multipath resulting from Lambertian surfaces, and combinations thereof. SRA removes this general multipath with robust techniques based on $L_1$ optimization. Second, due to a novel dimension reduction, we are able to produce a very fast version of SRA, which is able to run at frame rate. Experimental results on both synthetic data with ground truth, as well as real images of challenging scenes, validate the approach.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6002",
        "title": "Brain Tumor Detection Based On Mathematical Analysis and Symmetry Information",
        "authors": [
            "Narkhede Sachin G.",
            "Vaishali Khairnar",
            "Sujata Kadu"
        ],
        "abstract": "Image segmentation some of the challenging issues on brain magnetic resonance image tumor segmentation caused by the weak correlation between magnetic resonance imaging intensity and anatomical ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6173",
        "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail",
        "authors": [
            "Anna Senina",
            "Marcus Rohrbach",
            "Wei Qiu",
            "Annemarie Friedrich",
            "Sikandar Amin",
            "Mykhaylo Andriluka",
            "Manfred Pinkal",
            "Bernt Schiele"
        ],
        "abstract": "Humans can easily describe what they see in a coherent way and at varying level of detail. However, existing approaches for automatic video description are mainly focused on single sentence generation and produce descriptions at a fixed level of detail. In this paper, we address both of these limitations: for a variable level of detail we produce coherent multi-sentence descriptions of complex videos. We follow a two-step approach where we first learn to predict a semantic representation (SR) from video and then generate natural language descriptions from the SR. To produce consistent multi-sentence descriptions, we model across-sentence consistency at the level of the SR by enforcing a consistent topic. We also contribute both to the visual recognition of objects proposing a hand-centric approach as well as to the robust generation of sentences using a word lattice. Human judges rate our multi-sentence descriptions as more readable, correct, and relevant than related work. To understand the difference between more detailed and shorter descriptions, we collect and analyze a video description corpus of three levels of detail.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6183",
        "title": "Development and evaluation of a 3D model observer with nonlinear spatiotemporal contrast sensitivity",
        "authors": [
            "Ali R. N. Avanaki",
            "Kathryn S. Espig",
            "Andrew D. A. Maidment",
            "Cedric Marchessoux",
            "Predrag R. Bakic",
            "Tom R. L. Kimpe"
        ],
        "abstract": "We investigate improvements to our 3D model observer with the goal of better matching human observer performance as a function of viewing distance, effective contrast, maximum luminance, and browsing speed. Two nonlinear methods of applying the human contrast sensitivity function (CSF) to a 3D model observer are proposed, namely the Probability Map (PM) and Monte Carlo (MC) methods. In the PM method, the visibility probability for each frequency component of the image stack, p, is calculated taking into account Barten's spatiotemporal CSF, the component modulation, and the human psychometric function. The probability p is considered to be equal to the perceived amplitude of the frequency component and thus can be used by a traditional model observer (e.g., LG-msCHO) in the space-time domain. In the MC method, each component is randomly kept with probability p or discarded with 1-p. The amplitude of the retained components is normalized to unity. The methods were tested using DBT stacks of an anthropomorphic breast phantom processed in a comprehensive simulation pipeline. Our experiments indicate that both the PM and MC methods yield results that match human observer performance better than the linear filtering method as a function of viewing distance, effective contrast, maximum luminance, and browsing speed.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6260",
        "title": "Capturing and Recognizing Objects Appearance Employing Eigenspace",
        "authors": [
            "M. Ashrafuzzaman",
            "M.M .Rahman",
            "M.M.A. Hashem"
        ],
        "abstract": "This paper presents a method of capturing objects appearances from its environment and it also describes how to recognize unknown appearances creating an eigenspace. This representation and recognition can be done automatically taking objects various appearances by using robotic vision from a defined environment. This technique also allows extracting objects from some sort of complicated scenes. In this case, some of object appearances are taken with defined occlusions and eigenspaces are created by accepting both of non-occluded and occluded appearances together. Eigenspace is constructed successfully every times when a new object appears, and various appearances accumulated gradually. A sequence of appearances is generated from its accumulated shapes, which is used for recognition of the unknown objects appearances. Various objects environments are shown in the experiment to capture objects appearances and experimental results show effectiveness of the proposed approach.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6275",
        "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies",
        "authors": [
            "Vibhav Vineet",
            "Jonathan Warrell",
            "Philip H.S. Torr"
        ],
        "abstract": "A large number of problems in computer vision can be modelled as energy minimization problems in a Markov Random Field (MRF) or Conditional Random Field (CRF) framework. Graph-cuts based $\\alpha$-expansion is a standard move-making method to minimize the energy functions with sub-modular pairwise terms. However, certain problems require more complex pairwise terms where the $\\alpha$-expansion method is generally not applicable.\n",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6290",
        "title": "Spectral Sparse Representation for Clustering: Evolved from PCA, K-means, Laplacian Eigenmap, and Ratio Cut",
        "authors": [
            "Zhenfang Hu",
            "Gang Pan",
            "Yueming Wang",
            "Zhaohui Wu"
        ],
        "abstract": "Dimensionality reduction, cluster analysis, and sparse representation are basic components in machine learning. However, their relationships have not yet been fully investigated. In this paper, we find that the spectral graph theory underlies a series of these elementary methods and can unify them into a complete framework. The methods include PCA, K-means, Laplacian eigenmap (LE), ratio cut (Rcut), and a new sparse representation method developed by us, called spectral sparse representation (SSR). Further, extended relations to conventional over-complete sparse representations (e.g., method of optimal directions, KSVD), manifold learning (e.g., kernel PCA, multidimensional scaling, Isomap, locally linear embedding), and subspace clustering (e.g., sparse subspace clustering, low-rank representation) are incorporated. We show that, under an ideal condition from the spectral graph theory, PCA, K-means, LE, and Rcut are unified together. And when the condition is relaxed, the unification evolves to SSR, which lies in the intermediate between PCA/LE and K-mean/Rcut. An efficient algorithm, NSCrt, is developed to solve the sparse codes of SSR. SSR combines merits of both sides: its sparse codes reduce dimensionality of data meanwhile revealing cluster structure. For its inherent relation to cluster analysis, the codes of SSR can be directly used for clustering. Scut, a clustering approach derived from SSR reaches the state-of-the-art performance in the spectral clustering family. The one-shot solution obtained by Scut is comparable to the optimal result of K-means that are run many times. Experiments on various data sets demonstrate the properties and strengths of SSR, NSCrt, and Scut.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2017-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6318",
        "title": "Stabilizing dual-energy X-ray computed tomography reconstructions using patch-based regularization",
        "authors": [
            "Brian H. Tracey",
            "Eric L. Miller"
        ],
        "abstract": "Recent years have seen growing interest in exploiting dual- and multi-energy measurements in computed tomography (CT) in order to characterize material properties as well as object shape. Material characterization is performed by decomposing the scene into constitutive basis functions, such as Compton scatter and photoelectric absorption functions. While well motivated physically, the joint recovery of the spatial distribution of photoelectric and Compton properties is severely complicated by the fact that the data are several orders of magnitude more sensitive to Compton scatter coefficients than to photoelectric absorption, so small errors in Compton estimates can create large artifacts in the photoelectric estimate. To address these issues, we propose a model-based iterative approach which uses patch-based regularization terms to stabilize inversion of photoelectric coefficients, and solve the resulting problem though use of computationally attractive Alternating Direction Method of Multipliers (ADMM) solution techniques. Using simulations and experimental data acquired on a commercial scanner, we demonstrate that the proposed processing can lead to more stable material property estimates which should aid materials characterization in future dual- and multi-energy CT systems.\n    ",
        "submission_date": "2014-03-25T00:00:00",
        "last_modified_date": "2014-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6382",
        "title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition",
        "authors": [
            "Ali Sharif Razavian",
            "Hossein Azizpour",
            "Josephine Sullivan",
            "Stefan Carlsson"
        ],
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.\n    ",
        "submission_date": "2014-03-23T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6774",
        "title": "Optimized imaging using non-rigid registration",
        "authors": [
            "Benjamin Berkels",
            "Peter Binev",
            "Douglas A. Blom",
            "Wolfgang Dahmen",
            "Robert C. Sharpley",
            "Thomas Vogt"
        ],
        "abstract": "The extraordinary improvements of modern imaging devices offer access to data with unprecedented information content. However, widely used image processing methodologies fall far short of exploiting the full breadth of information offered by numerous types of scanning probe, optical, and electron microscopies. In many applications, it is necessary to keep measurement intensities below a desired threshold. We propose a methodology for extracting an increased level of information by processing a series of data sets suffering, in particular, from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process. An important role is played by a nonrigid pixel-wise registration method that can cope with low signal-to-noise ratios. This is accompanied by formulating objective quality measures which replace human intervention and visual inspection in the processing chain. Scanning transmission electron microscopy of siliceous zeolite material exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6888",
        "title": "Fast Localization of Facial Landmark Points",
        "authors": [
            "Nenad Marku\u0161",
            "Miroslav Frljak",
            "Igor S. Pand\u017ei\u0107",
            "J\u00f6rgen Ahlberg",
            "Robert Forchheimer"
        ],
        "abstract": "Localization of salient facial landmark points, such as eye corners or the tip of the nose, is still considered a challenging computer vision problem despite recent efforts. This is especially evident in unconstrained environments, i.e., in the presence of background clutter and large head pose variations. Most methods that achieve state-of-the-art accuracy are slow, and, thus, have limited applications. We describe a method that can accurately estimate the positions of relevant facial landmarks in real-time even on hardware with limited processing power, such as mobile devices. This is achieved with a sequence of estimators based on ensembles of regression trees. The trees use simple pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast. We test the developed system on several publicly available datasets and analyse its processing speed on various devices. Experimental results show that our method has practical value.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6950",
        "title": "Pyramidal Fisher Motion for Multiview Gait Recognition",
        "authors": [
            "F.M. Castro",
            "M.J. Marin-Jimenez",
            "R. Medina-Carnicer"
        ],
        "abstract": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person. Thus, obtaining a pyramidal representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves promising results in the problem of gait recognition.\n    ",
        "submission_date": "2014-03-27T00:00:00",
        "last_modified_date": "2014-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6958",
        "title": "Compressive Pattern Matching on Multispectral Data",
        "authors": [
            "S. Rousseau",
            "D. Helbert",
            "P. Carr\u00e9",
            "J. Blanc-Talon"
        ],
        "abstract": "We introduce a new constrained minimization problem that performs template and pattern detection on a multispectral image in a compressive sensing context. We use an original minimization problem from Guo and Osher that uses $L_1$ minimization techniques to perform template detection in a multispectral image. We first adapt this minimization problem to work with compressive sensing data. Then we extend it to perform pattern detection using a formal transform called the spectralization along a pattern. That extension brings out the problem of measurement reconstruction. We introduce shifted measurements that allow us to reconstruct all the measurement with a small overhead and we give an optimality constraint for simple patterns. We present numerical results showing the performances of the original minimization problem and the compressed ones with different measurement rates and applied on remotely sensed data.\n    ",
        "submission_date": "2014-03-27T00:00:00",
        "last_modified_date": "2014-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7311",
        "title": "Performance Evaluation of Raster Based Shape Vectors in Object Recognition",
        "authors": [
            "Akbar Khan",
            "Pratap Reddy L"
        ],
        "abstract": "Object recognition is still an impediment in the field of computer vision and multimedia ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7321",
        "title": "Learning detectors quickly using structured covariance matrices",
        "authors": [
            "Jack Valmadre",
            "Sridha Sridharan",
            "Simon Lucey"
        ],
        "abstract": "Computer vision is increasingly becoming interested in the rapid estimation of object detectors. Canonical hard negative mining strategies are slow as they require multiple passes of the large negative training set. Recent work has demonstrated that if the distribution of negative examples is assumed to be stationary, then Linear Discriminant Analysis (LDA) can learn comparable detectors without ever revisiting the negative set. Even with this insight, however, the time to learn a single object detector can still be on the order of tens of seconds on a modern desktop computer. This paper proposes to leverage the resulting structured covariance matrix to obtain detectors with identical performance in orders of magnitude less time and memory. We elucidate an important connection to the correlation filter literature, demonstrating that these can also be trained without ever revisiting the negative set.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7365",
        "title": "Expectation-Maximization Technique and Spatial-Adaptation Applied to Pel-Recursive Motion Estimation",
        "authors": [
            "Vania Vieira Estrela",
            "Marcos Henrique da Silva Bassani"
        ],
        "abstract": "Pel-recursive motion estimation isa well-established approach. However, in the presence of noise, it becomes an ill-posed problem that requires regularization. In this paper, motion vectors are estimated in an iterative fashion by means of the Expectation-Maximization (EM) algorithm and a Gaussian data model. Our proposed algorithm also utilizes the local image properties of the scene to improve the motion vector estimates following a spatially adaptive approach. Numerical experiments are presented that demonstrate the merits of our method.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7783",
        "title": "Extraction of Line Word Character Segments Directly from Run Length Compressed Printed Text Documents",
        "authors": [
            "Mohammed Javed",
            "P. Nagabhushan",
            "B.B. Chaudhuri"
        ],
        "abstract": "Segmentation of a text-document into lines, words and characters, which is considered to be the crucial pre-processing stage in Optical Character Recognition (OCR) is traditionally carried out on uncompressed documents, although most of the documents in real life are available in compressed form, for the reasons such as transmission and storage efficiency. However, this implies that the compressed image should be decompressed, which indents additional computing resources. This limitation has motivated us to take up research in document image analysis using compressed documents. In this paper, we think in a new way to carry out segmentation at line, word and character level in run-length compressed printed-text-documents. We extract the horizontal projection profile curve from the compressed file and using the local minima points perform line segmentation. However, tracing vertical information which leads to tracking words-characters in a run-length compressed file is not very straight forward. Therefore, we propose a novel technique for carrying out simultaneous word and character segmentation by popping out column runs from each row in an intelligent sequence. The proposed algorithms have been validated with 1101 text-lines, 1409 words and 7582 characters from a data-set of 35 noise and skew free compressed documents of Bengali, Kannada and English Scripts.\n    ",
        "submission_date": "2014-03-30T00:00:00",
        "last_modified_date": "2014-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7876",
        "title": "Correlation Filters with Limited Boundaries",
        "authors": [
            "Hamed Kiani Galoogahi",
            "Terence Sim",
            "Simon Lucey"
        ],
        "abstract": "Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(NDlogD) in the frequency domain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the number of signals. Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties. In this paper we demonstrate, however, that this computational efficiency comes at a cost. Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance. In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, and (ii) dramatically reduces boundary effects. Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency.\n    ",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7877",
        "title": "ROML: A Robust Feature Correspondence Approach for Matching Objects in A Set of Images",
        "authors": [
            "Kui Jia",
            "Tsung-Han Chan",
            "Zinan Zeng",
            "Shenghua Gao",
            "Gang Wang",
            "Tianzhu Zhang",
            "Yi Ma"
        ],
        "abstract": "Feature-based object matching is a fundamental problem for many applications in computer vision, such as object recognition, 3D reconstruction, tracking, and motion segmentation. In this work, we consider simultaneously matching object instances in a set of images, where both inlier and outlier features are extracted. The task is to identify the inlier features and establish their consistent correspondences across the image set. This is a challenging combinatorial problem, and the problem complexity grows exponentially with the image number. To this end, we propose a novel framework, termed ROML, to address this problem. ROML optimizes simultaneously a partial permutation matrix (PPM) for each image, and feature correspondences are established by the obtained PPMs. Two of our key contributions are summarized as follows. (1) We formulate the problem as rank and sparsity minimization for PPM optimization, and treat simultaneous optimization of multiple PPMs as a regularized consensus problem in the context of distributed optimization. (2) We use the ADMM method to solve the thus formulated ROML problem, in which a subproblem associated with a single PPM optimization appears to be a difficult integer quadratic program (IQP). We prove that under wildly applicable conditions, this IQP is equivalent to a linear sum assignment problem (LSAP), which can be efficiently solved to an exact solution. Extensive experiments on rigid/non-rigid object matching, matching instances of a common object category, and common object localization show the efficacy of our proposed method.\n    ",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.8003",
        "title": "Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using Global Shape Regularization",
        "authors": [
            "Fabian Rathke",
            "Stefan Schmidt",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "With the introduction of spectral-domain optical coherence tomography (OCT), resulting in a significant increase in acquisition speed, the fast and accurate segmentation of 3-D OCT scans has become evermore important. This paper presents a novel probabilistic approach, that models the appearance of retinal layers as well as the global shape variations of layer boundaries. Given an OCT scan, the full posterior distribution over segmentations is approximately inferred using a variational method enabling efficient probabilistic inference in terms of computationally tractable model components: Segmenting a full 3-D volume takes around a minute. Accurate segmentations demonstrate the benefit of using global shape regularization: We segmented 35 fovea-centered 3-D volumes with an average unsigned error of 2.46 $\\pm$ 0.22 {\\mu}m as well as 80 normal and 66 glaucomatous 2-D circular scans with errors of 2.92 $\\pm$ 0.53 {\\mu}m and 4.09 $\\pm$ 0.98 {\\mu}m respectively. Furthermore, we utilized the inferred posterior distribution to rate the quality of the segmentation, point out potentially erroneous regions and discriminate normal from pathological scans. No pre- or postprocessing was required and we used the same set of parameters for all data sets, underlining the robustness and out-of-the-box nature of our approach.\n    ",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2014-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.8067",
        "title": "Robust Subspace Recovery via Bi-Sparsity Pursuit",
        "authors": [
            "Xiao Bian",
            "Hamid Krim"
        ],
        "abstract": "Successful applications of sparse models in computer vision and machine learning imply that in many real-world applications, high dimensional data is distributed in a union of low dimensional subspaces. Nevertheless, the underlying structure may be affected by sparse errors and/or outliers. In this paper, we propose a bi-sparse model as a framework to analyze this problem and provide a novel algorithm to recover the union of subspaces in presence of sparse corruptions. We further show the effectiveness of our method by experiments on both synthetic data and real-world vision data.\n    ",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2014-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.8098",
        "title": "Hyperspectral image superresolution: An edge-preserving convex formulation",
        "authors": [
            "Miguel Sim\u00f5es",
            "Jos\u00e9 Bioucas-Dias",
            "Luis B. Almeida",
            "Jocelyn Chanussot"
        ],
        "abstract": "Hyperspectral remote sensing images (HSIs) are characterized by having a low spatial resolution and a high spectral resolution, whereas multispectral images (MSIs) are characterized by low spectral and high spatial resolutions. These complementary characteristics have stimulated active research in the inference of images with high spatial and spectral resolutions from HSI-MSI pairs.\n",
        "submission_date": "2014-03-31T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0106",
        "title": "Traffic Monitoring Using M2M Communication",
        "authors": [
            "Shiu Kumar",
            "Eun Sik Ham",
            "Seong Ro Lee"
        ],
        "abstract": "This paper presents an intelligent traffic monitoring system using wireless vision sensor network that captures and processes the real-time video image to obtain the traffic flow rate and vehicle speeds along different urban roadways. This system will display the traffic states on the front roadways that can guide the drivers to select the right way and avoid potential traffic congestions. On the other hand, it will also monitor the vehicle speeds and store the vehicle details, for those breaking the roadway speed limits, in its database. The real-time traffic data is processed by the Personal Computer (PC) at the sub roadway station and the traffic flow rate data is transmitted to the main roadway station Arduino 3G via email, where the data is extracted and traffic flow rate displayed.\n    ",
        "submission_date": "2014-04-01T00:00:00",
        "last_modified_date": "2014-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0334",
        "title": "Active Deformable Part Models",
        "authors": [
            "Menglong Zhu",
            "Nikolay Atanasov",
            "George J. Pappas",
            "Kostas Daniilidis"
        ],
        "abstract": "This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets.\n    ",
        "submission_date": "2014-04-01T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0336",
        "title": "A Continuous Max-Flow Approach to General Hierarchical Multi-Labeling Problems",
        "authors": [
            "John S.H. Baxter",
            "Martin Rajchl",
            "Jing Yuan",
            "Terry M. Peters"
        ],
        "abstract": "Multi-region segmentation algorithms often have the onus of incorporating complex anatomical knowledge representing spatial or geometric relationships between objects, and general-purpose methods of addressing this knowledge in an optimization-based manner have thus been lacking. This paper presents Generalized Hierarchical Max-Flow (GHMF) segmentation, which captures simple anatomical part-whole relationships in the form of an unconstrained hierarchy. Regularization can then be applied to both parts and wholes independently, allowing for spatial grouping and clustering of labels in a globally optimal convex optimization framework. For the purposes of ready integration into a variety of segmentation tasks, the hierarchies can be presented in run-time, allowing for the segmentation problem to be readily specified and alternatives explored without undue programming effort or recompilation.\n    ",
        "submission_date": "2014-04-01T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0437",
        "title": "Theory and Application of Shapelets to the Analysis of Surface Self-assembly Imaging",
        "authors": [
            "Robert Suderman",
            "Daniel Lizotte",
            "Nasser Mohieddin Abukhdeir"
        ],
        "abstract": "A method for quantitative analysis of local pattern strength and defects in surface self-assembly imaging is presented and applied to images of stripe and hexagonal ordered domains. The presented method uses \"shapelet\" functions which were originally developed for quantitative analysis of images of galaxies ($\\propto 10^{20}\\mathrm{m}$). In this work, they are used instead to quantify the presence of translational order in surface self-assembled films ($\\propto 10^{-9}\\mathrm{m}$) through reformulation into \"steerable\" filters. The resulting method is both computationally efficient (with respect to the number of filter evaluations), robust to variation in pattern feature shape, and, unlike previous approaches, is applicable to a wide variety of pattern types. An application of the method is presented which uses a nearest-neighbour analysis to distinguish between uniform (defect-free) and non-uniform (strained, defect-containing) regions within imaged self-assembled domains, both with striped and hexagonal patterns.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0533",
        "title": "A Comparative Study of Modern Inference Techniques for Structured Discrete Energy Minimization Problems",
        "authors": [
            "J\u00f6rg H. Kappes",
            "Bjoern Andres",
            "Fred A. Hamprecht",
            "Christoph Schn\u00f6rr",
            "Sebastian Nowozin",
            "Dhruv Batra",
            "Sungwoong Kim",
            "Bernhard X. Kausler",
            "Thorben Kr\u00f6ger",
            "Jan Lellmann",
            "Nikos Komodakis",
            "Bogdan Savchynskyy",
            "Carsten Rother"
        ],
        "abstract": "Szeliski et al. published an influential study in 2006 on energy minimization methods for Markov Random Fields (MRF). This study provided valuable insights in choosing the best optimization technique for certain classes of problems. While these insights remain generally useful today, the phenomenal success of random field models means that the kinds of inference problems that have to be solved changed significantly. Specifically, the models today often include higher order interactions, flexible connectivity structures, large la\\-bel-spaces of different cardinalities, or learned energy tables. To reflect these changes, we provide a modernized and enlarged study. We present an empirical comparison of 32 state-of-the-art optimization techniques on a corpus of 2,453 energy minimization instances from diverse applications in computer vision. To ensure reproducibility, we evaluate all methods in the OpenGM 2 framework and report extensive results regarding runtime and solution quality. Key insights from our study agree with the results of Szeliski et al. for the types of models they studied. However, on new and challenging types of models our findings disagree and suggest that polyhedral methods and integer programming solvers are competitive in terms of runtime and solution quality over a large range of model types.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0566",
        "title": "Weyl group orbit functions in image processing",
        "authors": [
            "Goce Chadzitaskos",
            "Lenka H\u00e1kov\u00e1",
            "Ond\u0159ej Kaj\u00ednek"
        ],
        "abstract": "We deal with the Fourier-like analysis of functions on discrete grids in two-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For these cases we present the convolution theorem. We provide an example of application of image processing using the $C-$ functions and the convolutions for spatial filtering of the treated image.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0600",
        "title": "MBIS: Multivariate Bayesian Image Segmentation Tool",
        "authors": [
            "Oscar Esteban",
            "Gert Wollny",
            "Subrahmanyam Gorthi",
            "Maria-J. Ledesma-Carbayo",
            "Jean-Philippe Thiran",
            "Andres Santos",
            "Meritxell Bach-Cuadra"
        ],
        "abstract": "We present MBIS (Multivariate Bayesian Image Segmentation tool), a clustering tool based on the mixture of multivariate normal distributions model. MBIS supports multi-channel bias field correction based on a B-spline model. A second methodological novelty is the inclusion of graph-cuts optimization for the stationary anisotropic hidden Markov random field model. Along with MBIS, we release an evaluation framework that contains three different experiments on multi-site data. We first validate the accuracy of segmentation and the estimated bias field for each channel. MBIS outperforms a widely used segmentation tool in a cross-comparison evaluation. The second experiment demonstrates the robustness of results on atlas-free segmentation of two image sets from scan-rescan protocols on 21 healthy subjects. Multivariate segmentation is more replicable than the monospectral counterpart on T1-weighted images. Finally, we provide a third experiment to illustrate how MBIS can be used in a large-scale study of tissue volume change with increasing age in 584 healthy subjects. This last result is meaningful as multivariate segmentation performs robustly without the need for prior knowledge\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0627",
        "title": "Extraction of Projection Profile, Run-Histogram and Entropy Features Straight from Run-Length Compressed Text-Documents",
        "authors": [
            "Mohammed Javed",
            "P. Nagabhushan",
            "B.B. Chaudhuri"
        ],
        "abstract": "Document Image Analysis, like any Digital Image Analysis requires identification and extraction of proper features, which are generally extracted from uncompressed images, though in reality images are made available in compressed form for the reasons such as transmission and storage efficiency. However, this implies that the compressed image should be decompressed, which indents additional computing resources. This limitation induces the motivation to research in extracting features directly from the compressed image. In this research, we propose to extract essential features such as projection profile, run-histogram and entropy for text document analysis directly from run-length compressed text-documents. The experimentation illustrates that features are extracted directly from the compressed image without going through the stage of decompression, because of which the computing time is reduced. The feature values so extracted are exactly identical to those extracted from uncompressed images.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0736",
        "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
        "authors": [
            "Remi Denton",
            "Wojciech Zaremba",
            "Joan Bruna",
            "Yann LeCun",
            "Rob Fergus"
        ],
        "abstract": "  We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1116",
        "title": "Resolving Multi-path Interference in Time-of-Flight Imaging via Modulation Frequency Diversity and Sparse Regularization",
        "authors": [
            "Ayush Bhandari",
            "Achuta Kadambi",
            "Refael Whyte",
            "Christopher Barsi",
            "Micha Feigin",
            "Adrian Dorrington",
            "Ramesh Raskar"
        ],
        "abstract": "Time-of-flight (ToF) cameras calculate depth maps by reconstructing phase shifts of amplitude-modulated signals. For broad illumination or transparent objects, reflections from multiple scene points can illuminate a given pixel, giving rise to an erroneous depth map. We report here a sparsity regularized solution that separates K-interfering components using multiple modulation frequency measurements. The method maps ToF imaging to the general framework of spectral estimation theory and has applications in improving depth profiles and exploiting multiple scattering.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1129",
        "title": "An Efficient Two-Stage Sparse Representation Method",
        "authors": [
            "Chengyu Peng",
            "Hong Cheng",
            "Manchor Ko"
        ],
        "abstract": "There are a large number of methods for solving under-determined linear inverse problem. Many of them have very high time complexity for large datasets. We propose a new method called Two-Stage Sparse Representation (TSSR) to tackle this problem. We decompose the representing space of signals into two parts, the measurement dictionary and the sparsifying basis. The dictionary is designed to approximate a sub-Gaussian distribution to exploit its concentration property. We apply sparse coding to the signals on the dictionary in the first stage, and obtain the training and testing coefficients respectively. Then we design the basis to approach an identity matrix in the second stage, to acquire the Restricted Isometry Property (RIP) and universality property. The testing coefficients are encoded over the basis and the final representing coefficients are obtained. We verify that the projection of testing coefficients onto the basis is a good approximation of the signal onto the representing space. Since the projection is conducted on a much sparser space, the runtime is greatly reduced. For concrete realization, we provide an instance for the proposed TSSR. Experiments on four biometrics databases show that TSSR is effective and efficient, comparing with several classical methods for solving linear inverse problem.\n    ",
        "submission_date": "2014-04-04T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1151",
        "title": "Recognition of Handwritten MODI Numerals using Hu and Zernike features",
        "authors": [
            "Sadanand A. Kulkarni",
            "Prashant L. Borde",
            "Ramesh R. Manza",
            "Pravin L. Yannawar"
        ],
        "abstract": " Handwritten automatic character recognition has attracted many researchers all over the world to contribute automatic character recognition domain. Shape identification and feature extraction is very important part of any character recognition system and success of method is highly dependent on selection of features. However feature extraction is the most important step in defining the shape of the character as precisely and as uniquely as possible. This is indeed the most important step and complex task as well and achieved success by using invariance property, irrespective of position and orientation. Zernike moments describes shape, identify rotation invariant due to its Orthogonality property. MODI is an ancient script of India had cursive and complex representation of characters. The work described in this paper presents efficiency of Zernike moments over Hus moment for automatic recognition of handwritten MODI numerals.\n    ",
        "submission_date": "2014-04-04T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1292",
        "title": "Review of Face Detection Systems Based Artificial Neural Networks Algorithms",
        "authors": [
            "Omaima N. A. AL-Allaf"
        ],
        "abstract": "Face detection is one of the most relevant applications of image processing and biometric systems. Artificial neural networks (ANN) have been used in the field of image processing and pattern recognition. There is lack of literature surveys which give overview about the studies and researches related to the using of ANN in face detection. Therefore, this research includes a general review of face detection studies and systems which based on different ANN approaches and algorithms. The strengths and limitations of these literature studies and systems were included also.\n    ",
        "submission_date": "2014-03-20T00:00:00",
        "last_modified_date": "2014-03-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1561",
        "title": "Fast Supervised Hashing with Decision Trees for High-Dimensional Data",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Qinfeng Shi",
            "Anton van den Hengel",
            "David Suter"
        ],
        "abstract": "Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the Hamming space. Non-linear hash functions have demonstrated the advantage over linear ones due to their powerful generalization capability. In the literature, kernel functions are typically used to achieve non-linearity in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time. Here we propose to use boosted decision trees for achieving non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem and an efficient GraphCut based block search method for solving large-scale inference. Then we learn hash functions by training boosted decision trees to fit the binary codes. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods in retrieval precision and training time. Especially for high-dimensional data, our method is orders of magnitude faster than many methods in terms of training time.\n    ",
        "submission_date": "2014-04-06T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1682",
        "title": "Pseudo-Zernike Based Multi-Pass Automatic Target Recognition From Multi-Channel SAR",
        "authors": [
            "Carmine Clemente",
            "Luca Pallotta",
            "Ian Proudler",
            "Antonio De Maio",
            "John J. Soraghan",
            "Alfonso Farina"
        ],
        "abstract": "The capability to exploit multiple sources of information is of fundamental importance in a battlefield scenario. Information obtained from different sources, and separated in space and time, provide the opportunity to exploit diversities in order to mitigate uncertainty. For the specific challenge of Automatic Target Recognition (ATR) from radar platforms, both channel (e.g. polarization) and spatial diversity can provide useful information for such a specific and critical task. In this paper the use of pseudo-Zernike moments applied to multi-channel multi-pass data is presented exploiting diversities and invariant properties leading to high confidence ATR, small computational complexity and data transfer requirements. The effectiveness of the proposed approach, in different configurations and data source availability is demonstrated using real data.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1777",
        "title": "Neural Codes for Image Retrieval",
        "authors": [
            "Artem Babenko",
            "Anton Slesarev",
            "Alexandr Chigorin",
            "Victor Lempitsky"
        ],
        "abstract": "It has been shown that the activations invoked by an image within the top layers of a large convolutional neural network provide a high-level descriptor of the visual content of the image. In this paper, we investigate the use of such descriptors (neural codes) within the image retrieval application. In the experiments with several standard retrieval benchmarks, we establish that neural codes perform competitively even when the convolutional neural network has been trained for an unrelated classification task (e.g.\\ Image-Net). We also evaluate the improvement in the retrieval performance of neural codes, when the network is retrained on a dataset of images that are similar to images encountered at test time.\n",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1831",
        "title": "Improving Bilayer Product Quantization for Billion-Scale Approximate Nearest Neighbors in High Dimensions",
        "authors": [
            "Artem Babenko",
            "Victor Lempitsky"
        ],
        "abstract": "The top-performing systems for billion-scale high-dimensional approximate nearest neighbor (ANN) search are all based on two-layer architectures that include an indexing structure and a compressed datapoints layer. An indexing structure is crucial as it allows to avoid exhaustive search, while the lossy data compression is needed to fit the dataset into RAM. Several of the most successful systems use product quantization (PQ) for both the indexing and the dataset compression layers. These systems are however limited in the way they exploit the interaction of product quantization processes that happen at different stages of these systems.\n",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1869",
        "title": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids",
        "authors": [
            "Forrest Iandola",
            "Matt Moskewicz",
            "Sergey Karayev",
            "Ross Girshick",
            "Trevor Darrell",
            "Kurt Keutzer"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper presents DenseNet, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier. Future work will involve training efficient object detectors with DenseNet feature descriptors.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2005",
        "title": "Automatic Tracker Selection w.r.t Object Detection Performance",
        "authors": [
            "Duc Phu Chau",
            "Fran\u00e7ois Bremond",
            "Monique Thonnat",
            "Slawomir Bak"
        ],
        "abstract": "The tracking algorithm performance depends on video content. This paper presents a new multi-object tracking approach which is able to cope with video content variations. First the object detection is improved using Kanade- Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, an appropriate tracker is selected among a KLT-based tracker and a discriminative appearance-based tracker. This selection is supported by an online tracking evaluation. The approach has been experimented on three public video datasets. The experimental results show a better performance of the proposed approach compared to recent state of the art trackers.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2014",
        "title": "Entropy Computation of Document Images in Run-Length Compressed Domain",
        "authors": [
            "P. Nagabhushan",
            "Mohammed Javed",
            "B.B. Chaudhuri"
        ],
        "abstract": "Compression of documents, images, audios and videos have been traditionally practiced to increase the efficiency of data storage and transfer. However, in order to process or carry out any analytical computations, decompression has become an unavoidable pre-requisite. In this research work, we have attempted to compute the entropy, which is an important document analytic directly from the compressed documents. We use Conventional Entropy Quantifier (CEQ) and Spatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropies obtained are useful in applications like establishing equivalence, word spotting and document retrieval. Experiments have been performed with all the data sets of [1], at character, word and line levels taking compressed documents in run-length compressed domain. The algorithms developed are computational and space efficient, and results obtained match 100% with the results reported in [1].\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2086",
        "title": "Cascades of Regression Tree Fields for Image Restoration",
        "authors": [
            "Uwe Schmidt",
            "Jeremy Jancsary",
            "Sebastian Nowozin",
            "Stefan Roth",
            "Carsten Rother"
        ],
        "abstract": "Conditional random fields (CRFs) are popular discriminative models for computer vision and have been successfully applied in the domain of image restoration, especially to image denoising. For image deblurring, however, discriminative approaches have been mostly lacking. We posit two reasons for this: First, the blur kernel is often only known at test time, requiring any discriminative approach to cope with considerable variability. Second, given this variability it is quite difficult to construct suitable features for discriminative prediction. To address these challenges we first show a connection between common half-quadratic inference for generative image priors and Gaussian CRFs. Based on this analysis, we then propose a cascade model for image restoration that consists of a Gaussian CRF at each stage. Each stage of our cascade is semi-parametric, i.e. it depends on the instance-specific parameters of the restoration problem, such as the blur kernel. We train our model by loss minimization with synthetically generated training data. Our experiments show that when applied to non-blind image deblurring, the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur. Moreover, we demonstrate its suitability for image denoising, where we achieve competitive results for grayscale and color images.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2268",
        "title": "A Compact Linear Programming Relaxation for Binary Sub-modular MRF",
        "authors": [
            "Junyan Wang",
            "Sai-Kit Yeung"
        ],
        "abstract": "We propose a novel compact linear programming (LP) relaxation for binary sub-modular MRF in the context of object segmentation. Our model is obtained by linearizing an $l_1^+$-norm derived from the quadratic programming (QP) form of the MRF energy. The resultant LP model contains significantly fewer variables and constraints compared to the conventional LP relaxation of the MRF energy. In addition, unlike QP which can produce ambiguous labels, our model can be viewed as a quasi-total-variation minimization problem, and it can therefore preserve the discontinuities in the labels. We further establish a relaxation bound between our LP model and the conventional LP model. In the experiments, we demonstrate our method for the task of interactive object segmentation. Our LP model outperforms QP when converting the continuous labels to binary labels using different threshold values on the entire Oxford interactive segmentation dataset. The computational complexity of our LP is of the same order as that of the QP, and it is significantly lower than the conventional LP relaxation.\n    ",
        "submission_date": "2014-04-09T00:00:00",
        "last_modified_date": "2014-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2571",
        "title": "RANCOR: Non-Linear Image Registration with Total Variation Regularization",
        "authors": [
            "Martin Rajchl",
            "John S.H. Baxter",
            "Wu Qiu",
            "Ali R. Khan",
            "Aaron Fenster",
            "Terry M. Peters",
            "Jing Yuan"
        ],
        "abstract": "Optimization techniques have been widely used in deformable registration, allowing for the incorporation of similarity metrics with regularization mechanisms. These regularization mechanisms are designed to mitigate the effects of trivial solutions to ill-posed registration problems and to otherwise ensure the resulting deformation fields are well-behaved. This paper introduces a novel deformable registration algorithm, RANCOR, which uses iterative convexification to address deformable registration problems under total-variation regularization. Initial comparative results against four state-of-the-art registration algorithms are presented using the Internet Brain Segmentation Repository (IBSR) database.\n    ",
        "submission_date": "2014-04-09T00:00:00",
        "last_modified_date": "2014-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2903",
        "title": "Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition",
        "authors": [
            "Marius Leordeanu",
            "Rahul Sukthankar"
        ],
        "abstract": "We propose a general multi-class visual recognition model, termed the Classifier Graph, which aims to generalize and integrate ideas from many of today's successful hierarchical recognition approaches. Our graph-based model has the advantage of enabling rich interactions between classes from different levels of interpretation and abstraction. The proposed multi-class system is efficiently learned using step by step updates. The structure consists of simple logistic linear layers with inputs from features that are automatically selected from a large pool. Each newly learned classifier becomes a potential new feature. Thus, our feature pool can consist both of initial manually designed features as well as learned classifiers from previous steps (graph nodes), each copied many times at different scales and locations. In this manner we can learn and grow both a deep, complex graph of classifiers and a rich pool of features at different levels of abstraction and interpretation. Our proposed graph of classifiers becomes a multi-class system with a recursive structure, suitable for deep detection and recognition of several classes simultaneously.\n    ",
        "submission_date": "2014-04-02T00:00:00",
        "last_modified_date": "2014-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2999",
        "title": "A Reverse Hierarchy Model for Predicting Eye Fixations",
        "authors": [
            "Tianlin Shi",
            "Liang Ming",
            "Xiaolin Hu"
        ],
        "abstract": "A number of psychological and physiological evidences suggest that early visual attention works in a coarse-to-fine way, which lays a basis for the reverse hierarchy theory (RHT). This theory states that attention propagates from the top level of the visual hierarchy that processes gist and abstract information of input, to the bottom level that processes local details. Inspired by the theory, we develop a computational model for saliency detection in images. First, the original image is downsampled to different scales to constitute a pyramid. Then, saliency on each layer is obtained by image super-resolution reconstruction from the layer above, which is defined as unpredictability from this coarse-to-fine reconstruction. Finally, saliency on each layer of the pyramid is fused into stochastic fixations through a probabilistic model, where attention initiates from the top layer and propagates downward through the pyramid. Extensive experiments on two standard eye-tracking datasets show that the proposed method can achieve competitive results with state-of-the-art models.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3012",
        "title": "Bayesian image segmentations by Potts prior and loopy belief propagation",
        "authors": [
            "Kazuyuki Tanaka",
            "Shun Kataoka",
            "Muneki Yasuda",
            "Yuji Waizumi",
            "Chiou-Ting Hsu"
        ],
        "abstract": "This paper presents a Bayesian image segmentation model based on Potts prior and loopy belief propagation. The proposed Bayesian model involves several terms, including the pairwise interactions of Potts models, and the average vectors and covariant matrices of Gauss distributions in color image modeling. These terms are often referred to as hyperparameters in statistical machine learning theory. In order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the Potts prior. The algorithm is given based on loopy belief propagation. In addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in LBP's for Potts models influence our hyperparameter estimation procedures.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3184",
        "title": "Decreasing Weighted Sorted $\\ell_1$ Regularization",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "We consider a new family of regularizers, termed {\\it weighted sorted $\\ell_1$ norms} (WSL1), which generalizes the recently introduced {\\it octagonal shrinkage and clustering algorithm for regression} (OSCAR) and also contains the $\\ell_1$ and $\\ell_{\\infty}$ norms as particular instances. We focus on a special case of the WSL1, the {\\sl decreasing WSL1} (DWSL1), where the elements of the argument vector are sorted in non-increasing order and the weights are also non-increasing. In this paper, after showing that the DWSL1 is indeed a norm, we derive two key tools for its use as a regularizer: the dual norm and the Moreau proximity operator.\n    ",
        "submission_date": "2014-04-11T00:00:00",
        "last_modified_date": "2014-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3291",
        "title": "Cost-Effective HITs for Relative Similarity Comparisons",
        "authors": [
            "Michael J. Wilber",
            "Iljung S. Kwak",
            "Serge J. Belongie"
        ],
        "abstract": "Similarity comparisons of the form \"Is object a more similar to b than to c?\" are useful for computer vision and machine learning applications. Unfortunately, an embedding of $n$ points is specified by $n^3$ triplets, making collecting every triplet an expensive task. In noticing this difficulty, other researchers have investigated more intelligent triplet sampling techniques, but they do not study their effectiveness or their potential drawbacks. Although it is important to reduce the number of collected triplets, it is also important to understand how best to display a triplet collection task to a user. In this work we explore an alternative display for collecting triplets and analyze the monetary cost and speed of the display. We propose best practices for creating cost effective human intelligence tasks for collecting triplets. We show that rather than changing the sampling algorithm, simple changes to the crowdsourcing UI can lead to much higher quality embeddings. We also provide a dataset as well as the labels collected from crowd workers.\n    ",
        "submission_date": "2014-04-12T00:00:00",
        "last_modified_date": "2014-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3312",
        "title": "Shrinkage Optimized Directed Information using Pictorial Structures for Action Recognition",
        "authors": [
            "Xu Chen",
            "Alfred Hero",
            "Silvio Savarese"
        ],
        "abstract": "In this paper, we propose a novel action recognition framework. The method uses pictorial structures and shrinkage optimized directed information assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model the directional temporal dependency and bidirectional spatial dependency. As a variant of mutual information, directional information captures the directional information flow and temporal structure of video sequences across frames. Meanwhile, within each frame, Markov random fields are utilized to model the spatial relations among different parts of a human body and the body parts of different people. The proposed SODA+MRF model is robust to view point transformations and detect complex interactions accurately. We compare the proposed method against several baseline methods to highlight the effectiveness of the SODA+MRF model. We demonstrate that our algorithm has superior action recognition performance on the UCF action recognition dataset, the Olympic sports dataset and the collective activity dataset over several state-of-the-art methods.\n    ",
        "submission_date": "2014-04-12T00:00:00",
        "last_modified_date": "2014-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3366",
        "title": "Learning Deep Convolutional Features for MRI Based Alzheimer's Disease Classification",
        "authors": [
            "Fayao Liu",
            "Chunhua Shen"
        ],
        "abstract": "Effective and accurate diagnosis of Alzheimer's disease (AD) or mild cognitive impairment (MCI) can be critical for early treatment and thus has attracted more and more attention nowadays. Since first introduced, machine learning methods have been gaining increasing popularity for AD related research. Among the various identified biomarkers, magnetic resonance imaging (MRI) are widely used for the prediction of AD or MCI. However, before a machine learning algorithm can be applied, image features need to be extracted to represent the MRI images. While good representations can be pivotal to the classification performance, almost all the previous studies typically rely on human labelling to find the regions of interest (ROI) which may be correlated to AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires domain knowledge and is costly and tedious.\n",
        "submission_date": "2014-04-13T00:00:00",
        "last_modified_date": "2014-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3538",
        "title": "Proceedings of The 38th Annual Workshop of the Austrian Association for Pattern Recognition (\u00d6AGM), 2014",
        "authors": [
            "Vladimir Kolmogorov",
            "Christoph Lampert",
            "Emilie Morvant",
            "Rustem Takhanov"
        ],
        "abstract": "The 38th Annual Workshop of the Austrian Association for Pattern Recognition (\u00d6AGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides a platform for researchers and industry to discuss traditional and new areas of computer vision. This year the main topic is: Pattern Recognition: interdisciplinary challenges and opportunities.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3543",
        "title": "Recover Canonical-View Faces in the Wild with Deep Neural Networks",
        "authors": [
            "Zhenyao Zhu",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Face images in the wild undergo large intra-personal variations, such as poses, illuminations, occlusions, and low resolutions, which cause great challenges to face-related applications. This paper addresses this challenge by proposing a new deep learning framework that can recover the canonical view of face images. It dramatically reduces the intra-person variances, while maintaining the inter-person discriminativeness. Unlike the existing face reconstruction methods that were either evaluated in controlled 2D environment or employed 3D information, our approach directly learns the transformation from the face images with a complex set of variations to their canonical views. At the training stage, to avoid the costly process of labeling canonical-view images from the training set by hand, we have devised a new measurement to automatically select or synthesize a canonical-view image for each identity. As an application, this face recovery approach is used for face verification. Facial features are learned from the recovered canonical-view face images by using a facial component-based convolutional neural network. Our approach achieves the state-of-the-art performance on the LFW dataset.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3596",
        "title": "Face Detection with a 3D Model",
        "authors": [
            "Adrian Barbu",
            "Nathan Lay",
            "Gary Gramajo"
        ],
        "abstract": "This paper presents a part-based face detection approach where the spatial relationship between the face parts is represented by a hidden 3D model with six parameters. The computational complexity of the search in the six dimensional pose space is addressed by proposing meaningful 3D pose candidates by image-based regression from detected face keypoint locations. The 3D pose candidates are evaluated using a parameter sensitive classifier based on difference features relative to the 3D pose. A compatible subset of candidates is then obtained by non-maximal suppression. Experiments on two standard face detection datasets show that the proposed 3D model based approach obtains results comparable to or better than state of the art.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3606",
        "title": "PCANet: A Simple Deep Learning Baseline for Image Classification?",
        "authors": [
            "Tsung-Han Chan",
            "Kui Jia",
            "Shenghua Gao",
            "Jiwen Lu",
            "Zinan Zeng",
            "Yi Ma"
        ],
        "abstract": "In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.\n    ",
        "submission_date": "2014-04-14T00:00:00",
        "last_modified_date": "2014-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3840",
        "title": "Surpassing Human-Level Face Verification Performance on LFW with GaussianFace",
        "authors": [
            "Chaochao Lu",
            "Xiaoou Tang"
        ],
        "abstract": "Face verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the human-level performance in face verification (97.53%) on LFW is surpassed.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3933",
        "title": "Scalable Matting: A Sub-linear Approach",
        "authors": [
            "Philip G. Lee",
            "Ying Wu"
        ],
        "abstract": "Natural image matting, which separates foreground from background, is a very important intermediate step in recent computer vision algorithms. However, it is severely underconstrained and difficult to solve. State-of-the-art approaches include matting by graph Laplacian, which significantly improves the underconstrained nature by reducing the solution space. However, matting by graph Laplacian is still very difficult to solve and gets much harder as the image size grows: current iterative methods slow down as $\\mathcal{O}\\left(n^2 \\right)$ in the resolution $n$. This creates uncomfortable practical limits on the resolution of images that we can matte. Current literature mitigates the problem, but they all remain super-linear in complexity. We expose properties of the problem that remain heretofore unexploited, demonstrating that an optimization technique originally intended to solve PDEs can be adapted to take advantage of this knowledge to solve the matting problem, not heuristically, but exactly and with sub-linear complexity. This makes ours the most efficient matting solver currently known by a very wide margin and allows matting finally to be practical and scalable in the future as consumer photos exceed many dozens of megapixels, and also relieves matting from being a bottleneck for vision algorithms that depend on it.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3991",
        "title": "Spiralet Sparse Representation",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "abstract": "This is the first report on Working Paper WP-RFM-14-01. The potential and capability of sparse representations is well-known. However, their (multivariate variable) vectorial form, which is completely fine in many fields and disciplines, results in removal and filtering of important \"spatial\" relations that are implicitly carried by two-dimensional [or multi-dimensional] objects, such as images. In this paper, a new approach, called spiralet sparse representation, is proposed in order to develop an augmented representation and therefore a modified sparse representation and theory, which is capable to preserve the data associated to the spatial relations.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4316",
        "title": "Generic Object Detection With Dense Neural Patterns and Regionlets",
        "authors": [
            "Will Y. Zou",
            "Xiaoyu Wang",
            "Miao Sun",
            "Yuanqing Lin"
        ],
        "abstract": "This paper addresses the challenge of establishing a bridge between deep convolutional neural networks and conventional object detection frameworks for accurate and efficient generic object detection. We introduce Dense Neural Patterns, short for DNPs, which are dense local features derived from discriminatively trained deep convolutional neural networks. DNPs can be easily plugged into conventional detection frameworks in the same way as other dense local features(like HOG or LBP). The effectiveness of the proposed approach is demonstrated with the Regionlets object detection framework. It achieved 46.1% mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL VOC 2010 dataset, which dramatically improves the original Regionlets approach without DNPs.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4467",
        "title": "Cube-Cut: Vertebral Body Segmentation in MRI-Data through Cubic-Shaped Divergences",
        "authors": [
            "Robert Schwarzenberg",
            "Bernd Freisleben",
            "Christopher Nimsky",
            "Jan Egger"
        ],
        "abstract": "In this article, we present a graph-based method using a cubic template for volumetric segmentation of vertebrae in magnetic resonance imaging (MRI) acquisitions. The user can define the degree of deviation from a regular cube via a smoothness value Delta. The Cube-Cut algorithm generates a directed graph with two terminal nodes (s-t-network), where the nodes of the graph correspond to a cubic-shaped subset of the image's voxels. The weightings of the graph's terminal edges, which connect every node with a virtual source s or a virtual sink t, represent the affinity of a voxel to the vertebra (source) and to the background (sink). Furthermore, a set of infinite weighted and non-terminal edges implements the smoothness term. After graph construction, a minimal s-t-cut is calculated within polynomial computation time, which splits the nodes into two disjoint units. Subsequently, the segmentation result is determined out of the source-set. A quantitative evaluation of a C++ implementation of the algorithm resulted in an average Dice Similarity Coefficient (DSC) of 81.33% and a running time of less than a minute.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4661",
        "title": "Learning Fine-grained Image Similarity with Deep Ranking",
        "authors": [
            "Jiang Wang",
            "Yang song",
            "Thomas Leung",
            "Chuck Rosenberg",
            "Jinbin Wang",
            "James Philbin",
            "Bo Chen",
            "Ying Wu"
        ],
        "abstract": "Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2014-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4774",
        "title": "Online Group Feature Selection",
        "authors": [
            "Wang Jing",
            "Zhao Zhong-Qiu",
            "Hu Xuegang",
            "Cheung Yiu-ming",
            "Wang Meng",
            "Wu Xindong"
        ],
        "abstract": "Online feature selection with dynamic features has become an active research area in recent years. However, in some real-world applications such as image analysis and email spam filtering, features may arrive by groups. Existing online feature selection methods evaluate features individually, while existing group feature selection methods cannot handle online processing. Motivated by this, we formulate the online group feature selection problem, and propose a novel selection approach for this problem. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we use spectral analysis to select discriminative features in each group when it arrives. In the inter-group selection, we use Lasso to select a globally optimal subset of features. This 2-stage procedure continues until there are no more features to come or some predefined stopping conditions are met. Extensive experiments conducted on benchmark and real-world data sets demonstrate that our proposed approach outperforms other state-of-the-art online feature selection methods.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4780",
        "title": "Robust Face Recognition via Adaptive Sparse Representation",
        "authors": [
            "Jing Wang",
            "Canyi Lu",
            "Meng Wang",
            "Peipei Li",
            "Shuicheng Yan",
            "Xuegang Hu"
        ],
        "abstract": "Sparse Representation (or coding) based Classification (SRC) has gained great success in face recognition in recent years. However, SRC emphasizes the sparsity too much and overlooks the correlation information which has been demonstrated to be critical in real-world face recognition problems. Besides, some work considers the correlation but overlooks the discriminative ability of sparsity. Different from these existing techniques, in this paper, we propose a framework called Adaptive Sparse Representation based Classification (ASRC) in which sparsity and correlation are jointly considered. Specifically, when the samples are of low correlation, ASRC selects the most discriminative samples for representation, like SRC; when the training samples are highly correlated, ASRC selects most of the correlated and discriminative samples for representation, rather than choosing some related samples randomly. In general, the representation model is adaptive to the correlation structure, which benefits from both $\\ell_1$-norm and $\\ell_2$-norm.\n",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4800",
        "title": "Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes",
        "authors": [
            "Ayushi Sinha",
            "William Gray Roncal",
            "Narayanan Kasthuri",
            "Ming Chuang",
            "Priya Manavalan",
            "Dean M. Kleissas",
            "Joshua T. Vogelstein",
            "R. Jacob Vogelstein",
            "Randal Burns",
            "Jeff W. Lichtman",
            "Michael Kazhdan"
        ],
        "abstract": "In this paper, we present a new pipeline which automatically identifies and annotates axoplasmic reticula, which are small subcellular structures present only in axons. We run our algorithm on the Kasthuri11 dataset, which was color corrected using gradient-domain techniques to adjust contrast. We use a bilateral filter to smooth out the noise in this data while preserving edges, which highlights axoplasmic reticula. These axoplasmic reticula are then annotated using a morphological region growing algorithm. Additionally, we perform Laplacian sharpening on the bilaterally filtered data to enhance edges, and repeat the morphological region growing algorithm to annotate more axoplasmic reticula. We track our annotations through the slices to improve precision, and to create long objects to aid in segment merging. This method annotates axoplasmic reticula with high precision. Our algorithm can easily be adapted to annotate axoplasmic reticula in different sets of brain data by changing a few thresholds. The contribution of this work is the introduction of a straightforward and robust pipeline which annotates axoplasmic reticula with high precision, contributing towards advancements in automatic feature annotations in neural EM data.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4805",
        "title": "iPiano: Inertial Proximal Algorithm for Non-Convex Optimization",
        "authors": [
            "Peter Ochs",
            "Yunjin Chen",
            "Thomas Brox",
            "Thomas Pock"
        ],
        "abstract": "In this paper we study an algorithm for solving a minimization problem composed of a differentiable (possibly non-convex) and a convex (possibly non-differentiable) function. The algorithm iPiano combines forward-backward splitting with an inertial force. It can be seen as a non-smooth split version of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for the proposed class of problems yields global convergence of the function values and the arguments. This makes the algorithm robust for usage on non-convex problems. The convergence result is obtained based on the \\KL inequality. This is a very weak restriction, which was used to prove convergence for several other gradient methods. First, an abstract convergence theorem for a generic algorithm is proved, and, then iPiano is shown to satisfy the requirements of this theorem. Furthermore, a convergence rate is established for the general problem class. We demonstrate iPiano on computer vision problems: image denoising with learned priors and diffusion based image compression.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4880",
        "title": "Bias Correction and Modified Profile Likelihood under the Wishart Complex Distribution",
        "authors": [
            "Abra\u00e3o D. C. Nascimento",
            "Alejandro C. Frery",
            "Renato J. Cintra"
        ],
        "abstract": "This paper proposes improved methods for the maximum likelihood (ML) estimation of the equivalent number of looks $L$. This parameter has a meaningful interpretation in the context of polarimetric synthetic aperture radar (PolSAR) images. Due to the presence of coherent illumination in their processing, PolSAR systems generate images which present a granular noise called speckle. As a potential solution for reducing such interference, the parameter $L$ controls the signal-noise ratio. Thus, the proposal of efficient estimation methodologies for $L$ has been sought. To that end, we consider firstly that a PolSAR image is well described by the scaled complex Wishart distribution. In recent years, Anfinsen et al. derived and analyzed estimation methods based on the ML and on trace statistical moments for obtaining the parameter $L$ of the unscaled version of such probability law. This paper generalizes that approach. We present the second-order bias expression proposed by Cox and Snell for the ML estimator of this parameter. Moreover, the formula of the profile likelihood modified by Barndorff-Nielsen in terms of $L$ is discussed. Such derivations yield two new ML estimators for the parameter $L$, which are compared to the estimators proposed by Anfinsen et al. The performance of these estimators is assessed by means of Monte Carlo experiments, adopting three statistical measures as comparison criterion: the mean square error, the bias, and the coefficient of variation. Equivalently to the simulation study, an application to actual PolSAR data concludes that the proposed estimators outperform all the others in homogeneous scenarios.\n    ",
        "submission_date": "2014-04-18T00:00:00",
        "last_modified_date": "2014-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4923",
        "title": "Unified Structured Learning for Simultaneous Human Pose Estimation and Garment Attribute Classification",
        "authors": [
            "Jie Shen",
            "Guangcan Liu",
            "Jia Chen",
            "Yuqiang Fang",
            "Jianbin Xie",
            "Yong Yu",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we utilize structured learning to simultaneously address two intertwined problems: human pose estimation (HPE) and garment attribute classification (GAC), which are valuable for a variety of computer vision and multimedia applications. Unlike previous works that usually handle the two problems separately, our approach aims to produce a jointly optimal estimation for both HPE and GAC via a unified inference procedure. To this end, we adopt a preprocessing step to detect potential human parts from each image (i.e., a set of \"candidates\") that allows us to have a manageable input space. In this way, the simultaneous inference of HPE and GAC is converted to a structured learning problem, where the inputs are the collections of candidate ensembles, the outputs are the joint labels of human parts and garment attributes, and the joint feature representation involves various cues such as pose-specific features, garment-specific features, and cross-task features that encode correlations between human parts and garment attributes. Furthermore, we explore the \"strong edge\" evidence around the potential human parts so as to derive more powerful representations for oriented human parts. Such evidences can be seamlessly integrated into our structured learning model as a kind of energy function, and the learning process could be performed by standard structured Support Vector Machines (SVM) algorithm. However, the joint structure of the two problems is a cyclic graph, which hinders efficient inference. To resolve this issue, we compute instead approximate optima by using an iterative procedure, where in each iteration the variables of one problem are fixed. In this way, satisfactory solutions can be efficiently computed by dynamic programming. Experimental results on two benchmark datasets show the state-of-the-art performance of our approach.\n    ",
        "submission_date": "2014-04-19T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4942",
        "title": "Geometric Abstraction from Noisy Image-Based 3D Reconstructions",
        "authors": [
            "Thomas Holzmann",
            "Christof Hoppe",
            "Stefan Kluckner",
            "Horst Bischof"
        ],
        "abstract": "Creating geometric abstracted models from image-based scene reconstructions is difficult due to noise and irregularities in the reconstructed model. In this paper, we present a geometric modeling method for noisy reconstructions dominated by planar horizontal and orthogonal vertical structures. We partition the scene into horizontal slices and create an inside/outside labeling represented by a floor plan for each slice by solving an energy minimization problem. Consecutively, we create an irregular discretization of the volume according to the individual floor plans and again label each cell as inside/outside by minimizing an energy function. By adjusting the smoothness parameter, we introduce different levels of detail. In our experiments, we show results with varying regularization levels using synthetically generated and real-world data.\n    ",
        "submission_date": "2014-04-19T00:00:00",
        "last_modified_date": "2014-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5009",
        "title": "Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference",
        "authors": [
            "Peng Wang",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Philip Torr"
        ],
        "abstract": "We propose a Branch-and-Cut (B&C) method for solving general MAP-MRF inference problems. The core of our method is a very efficient bounding procedure, which combines scalable semidefinite programming (SDP) and a cutting-plane method for seeking violated constraints. In order to further speed up the computation, several strategies have been exploited, including model reduction, warm start and removal of inactive constraints.\n",
        "submission_date": "2014-04-20T00:00:00",
        "last_modified_date": "2015-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5344",
        "title": "A higher-order MRF based variational model for multiplicative noise reduction",
        "authors": [
            "Yunjin Chen",
            "Wensen Feng",
            "Ren\u00e9 Ranftl",
            "Hong Qiao",
            "Thomas Pock"
        ],
        "abstract": "The Fields of Experts (FoE) image prior model, a filter-based higher-order Markov Random Fields (MRF) model, has been shown to be effective for many image restoration problems. Motivated by the successes of FoE-based approaches, in this letter, we propose a novel variational model for multiplicative noise reduction based on the FoE image prior model. The resulted model corresponds to a non-convex minimization problem, which can be solved by a recently published non-convex optimization algorithm. Experimental results based on synthetic speckle noise and real synthetic aperture radar (SAR) images suggest that the performance of our proposed method is on par with the best published despeckling algorithm. Besides, our proposed model comes along with an additional advantage, that the inference is extremely efficient. {Our GPU based implementation takes less than 1s to produce state-of-the-art despeckling performance.}\n    ",
        "submission_date": "2014-04-21T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5351",
        "title": "Fast Approximate Matching of Cell-Phone Videos for Robust Background Subtraction",
        "authors": [
            "Raffay Hamid",
            "Atish Das Sarma",
            "Dennis DeCoste",
            "Neel Sundaresan"
        ],
        "abstract": "We identify a novel instance of the background subtraction problem that focuses on extracting near-field foreground objects captured using handheld cameras. Given two user-generated videos of a scene, one with and the other without the foreground object(s), our goal is to efficiently generate an output video with only the foreground object(s) present in it. We cast this challenge as a spatio-temporal frame matching problem, and propose an efficient solution for it that exploits the temporal smoothness of the video sequences. We present theoretical analyses for the error bounds of our approach, and validate our findings using a detailed set of simulation experiments. Finally, we present the results of our approach tested on multiple real videos captured using handheld cameras, and compare them to several alternate foreground extraction approaches.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5588",
        "title": "Large Margin Image Set Representation and Classification",
        "authors": [
            "Jim Jing-Yan Wang",
            "Majed Alzahrani",
            "Xin Gao"
        ],
        "abstract": "In this paper, we propose a novel image set representation and classification method by maximizing the margin of image sets. The margin of an image set is defined as the difference of the distance to its nearest image set from different classes and the distance to its nearest image set of the same class. By modeling the image sets by using both their image samples and their affine hull models, and maximizing the margins of the images sets, the image set representation parameter learning problem is formulated as an minimization problem, which is further optimized by an expectation -maximization (EM) strategy with accelerated proximal gradient (APG) optimization in an iterative algorithm. To classify a given test image set, we assign it to the class which could provide the largest margin. Experiments on two applications of video-sequence-based face recognition demonstrate that the proposed method significantly outperforms state-of-the-art image set classification methods in terms of both effectiveness and efficiency.\n    ",
        "submission_date": "2014-04-22T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.5765",
        "title": "Find my mug: Efficient object search with a mobile robot using semantic segmentation",
        "authors": [
            "Daniel Wolf",
            "Markus Bajones",
            "Johann Prankl",
            "Markus Vincze"
        ],
        "abstract": "In this paper, we propose an efficient semantic segmentation framework for indoor scenes, tailored to the application on a mobile robot. Semantic segmentation can help robots to gain a reasonable understanding of their environment, but to reach this goal, the algorithms not only need to be accurate, but also fast and robust. Therefore, we developed an optimized 3D point cloud processing framework based on a Randomized Decision Forest, achieving competitive results at sufficiently high frame rates. We evaluate the capabilities of our method on the popular NYU depth dataset and our own data and demonstrate its feasibility by deploying it on a mobile service robot, for which we could optimize an object search procedure using our results.\n    ",
        "submission_date": "2014-04-23T00:00:00",
        "last_modified_date": "2014-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6031",
        "title": "Maximum Margin Vector Correlation Filter",
        "authors": [
            "Vishnu Naresh Boddeti",
            "B.V.K. Vijaya Kumar"
        ],
        "abstract": "Correlation Filters (CFs) are a class of classifiers which are designed for accurate pattern localization. Traditionally CFs have been used with scalar features only, which limits their ability to be used with vector feature representations like Gabor filter banks, SIFT, HOG, etc. In this paper we present a new CF named Maximum Margin Vector Correlation Filter (MMVCF) which extends the traditional CF designs to vector features. MMVCF further combines the generalization capability of large margin based classifiers like Support Vector Machines (SVMs) and the localization properties of CFs for better robustness to outliers. We demonstrate the efficacy of MMVCF for object detection and landmark localization on a variety of databases and demonstrate that MMVCF consistently shows improved pattern localization capability in comparison to SVMs.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6055",
        "title": "A General Homogeneous Matrix Formulation to 3D Rotation Geometric Transformations",
        "authors": [
            "Feng Lu",
            "Ziqiang Chen"
        ],
        "abstract": "We present algebraic projective geometry definitions of 3D rotations so as to bridge a small gap between the applications and the definitions of 3D rotations in homogeneous matrix form. A general homogeneous matrix formulation to 3D rotation geometric transformations is proposed which suits for the cases when the rotation axis is unnecessarily through the coordinate system origin given their rotation axes and rotation angles.\n",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2024-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6071",
        "title": "Rough Clustering Based Unsupervised Image Change Detection",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper introduces an unsupervised technique to detect the changed region of multitemporal images on a same reference plane with the help of rough clustering. The proposed technique is a soft-computing approach, based on the concept of rough set with rough clustering and Pawlak's accuracy. It is less noisy and avoids pre-deterministic knowledge about the distribution of the changed and unchanged regions. To show the effectiveness, the proposed technique is compared with some other approaches.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6075",
        "title": "Unsupervised Text Extraction from G-Maps",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper represents an text extraction method from Google maps, GIS maps/images. Due to an unsupervised approach there is no requirement of any prior knowledge or training set about the textual and non-textual parts. Fuzzy CMeans clustering technique is used for image segmentation and Prewitt method is used to detect the edges. Connected component analysis and gridding technique enhance the correctness of the results. The proposed method reaches 98.5% accuracy level on the basis of experimental data sets.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6272",
        "title": "Scalable Similarity Learning using Large Margin Neighborhood Embedding",
        "authors": [
            "Zhaowen Wang",
            "Jianchao Yang",
            "Zhe Lin",
            "Jonathan Brandt",
            "Shiyu Chang",
            "Thomas Huang"
        ],
        "abstract": "Classifying large-scale image data into object categories is an important problem that has received increasing research attention. Given the huge amount of data, non-parametric approaches such as nearest neighbor classifiers have shown promising results, especially when they are underpinned by a learned distance or similarity measurement. Although metric learning has been well studied in the past decades, most existing algorithms are impractical to handle large-scale data sets. In this paper, we present an image similarity learning method that can scale well in both the number of images and the dimensionality of image descriptors. To this end, similarity comparison is restricted to each sample's local neighbors and a discriminative similarity measure is induced from large margin neighborhood embedding. We also exploit the ensemble of projections so that high-dimensional features can be processed in a set of lower-dimensional subspaces in parallel without much performance compromise. The similarity function is learned online using a stochastic gradient descent algorithm in which the triplet sampling strategy is customized for quick convergence of classification performance. The effectiveness of our proposed model is validated on several data sets with scales varying from tens of thousands to one million images. Recognition accuracies competitive with the state-of-the-art performance are achieved with much higher efficiency and scalability.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6351",
        "title": "Improving weather radar by fusion and classification",
        "authors": [
            "Harald Ganster",
            "Martina Uray",
            "Sylwia Steginska",
            "Gerardus Croonen",
            "Rudolf Kaltenb\u00f6ck",
            "Karin Hennermann"
        ],
        "abstract": "In air traffic management (ATM) all necessary operations (tactical planing, sector configuration, required staffing, runway configuration, routing of approaching aircrafts) rely on accurate measurements and predictions of the current weather situation. An essential basis of information is delivered by weather radar images (WXR), which, unfortunately, exhibit a vast amount of disturbances. Thus, the improvement of these datasets is the key factor for more accurate predictions of weather phenomena and weather conditions. Image processing methods based on texture analysis and geometric operators allow to identify regions including artefacts as well as zones of missing information. Correction of these zones is implemented by exploiting multi-spectral satellite data (Meteosat Second Generation). Results prove that the proposed system for artefact detection and data correction significantly improves the quality of WXR data and, thus, enables more reliable weather now- and forecast leading to increased ATM safety.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6413",
        "title": "Indoor Activity Detection and Recognition for Sport Games Analysis",
        "authors": [
            "Georg Waltner",
            "Thomas Mauthner",
            "Horst Bischof"
        ],
        "abstract": "Activity recognition in sport is an attractive field for computer vision research. Game, player and team analysis are of great interest and research topics within this field emerge with the goal of automated analysis. The very specific underlying rules of sports can be used as prior knowledge for the recognition task and present a constrained environment for evaluation. This paper describes recognition of single player activities in sport with special emphasis on volleyball. Starting from a per-frame player-centered activity recognition, we incorporate geometry and contextual information via an activity context descriptor that collects information about all player's activities over a certain timespan relative to the investigated player. The benefit of this context information on single player activity recognition is evaluated on our new real-life dataset presenting a total amount of almost 36k annotated frames containing 7 activity classes within 6 videos of professional volleyball games. Our incorporation of the contextual information improves the average player-centered classification performance of 77.56% by up to 18.35% on specific classes, proving that spatio-temporal context is an important clue for activity recognition.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6736",
        "title": "Robust and Efficient Subspace Segmentation via Least Squares Regression",
        "authors": [
            "Can-Yi Lu",
            "Hai Min",
            "Zhong-Qiu Zhao",
            "Lin Zhu",
            "De-Shuang Huang",
            "Shuicheng Yan"
        ],
        "abstract": "This paper studies the subspace segmentation problem which aims to segment data drawn from a union of multiple linear subspaces. Recent works by using sparse representation, low rank representation and their extensions attract much attention. If the subspaces from which the data drawn are independent or orthogonal, they are able to obtain a block diagonal affinity matrix, which usually leads to a correct segmentation. The main differences among them are their objective functions. We theoretically show that if the objective function satisfies some conditions, and the data are sufficiently drawn from independent subspaces, the obtained affinity matrix is always block diagonal. Furthermore, the data sampling can be insufficient if the subspaces are orthogonal. Some existing methods are all special cases. Then we present the Least Squares Regression (LSR) method for subspace segmentation. It takes advantage of data correlation, which is common in real data. LSR encourages a grouping effect which tends to group highly correlated data together. Experimental results on the Hopkins 155 database and Extended Yale Database B show that our method significantly outperforms state-of-the-art methods. Beyond segmentation accuracy, all experiments demonstrate that LSR is much more efficient.\n    ",
        "submission_date": "2014-04-27T00:00:00",
        "last_modified_date": "2014-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7059",
        "title": "Stereo on a budget",
        "authors": [
            "Dana Menaker",
            "Shai Avidan"
        ],
        "abstract": "We propose an algorithm for recovering depth using less than two images. Instead of having both cameras send their entire image to the host computer, the left camera sends its image to the host while the right camera sends only a fraction $\\epsilon$ of its image. The key aspect is that the cameras send the information without communicating at all. Hence, the required communication bandwidth is significantly reduced.\n",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7174",
        "title": "Computer vision-based recognition of liquid surfaces and phase boundaries in transparent vessels, with emphasis on chemistry applications",
        "authors": [
            "Sagi Eppel",
            "Tal Kachman"
        ],
        "abstract": "The ability to recognize the liquid surface and the liquid level in transparent containers is perhaps the most commonly used evaluation method when dealing with fluids. Such recognition is essential in determining the liquid volume, fill level, phase boundaries and phase separation in various fluid systems. The recognition of liquid surfaces is particularly important in solution chemistry, where it is essential to many laboratory techniques (e.g., extraction, distillation, titration). A general method for the recognition of interfaces between liquid and air or between phase-separating liquids could have a wide range of applications and contribute to the understanding of the visual properties of such interfaces. This work examines a computer vision method for the recognition of liquid surfaces and liquid levels in various transparent containers. The method can be applied to recognition of both liquid-air and liquid-liquid surfaces. No prior knowledge of the number of phases is required. The method receives the image of the liquid container and the boundaries of the container in the image and scans all possible curves that could correspond to the outlines of liquid surfaces in the image. The method then compares each curve to the image to rate its correspondence with the outline of the real liquid surface by examining various image properties in the area surrounding each point of the curve. The image properties that were found to give the best indication of the liquid surface are the relative intensity change, the edge density change and the gradient direction relative to the curve normal.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7211",
        "title": "Spatially Directional Predictive Coding for Block-based Compressive Sensing of Natural Images",
        "authors": [
            "Jian Zhang",
            "Debin Zhao",
            "Feng Jiang"
        ],
        "abstract": "A novel coding strategy for block-based compressive sens-ing named spatially directional predictive coding (SDPC) is proposed, which efficiently utilizes the intrinsic spatial cor-relation of natural images. At the encoder, for each block of compressive sensing (CS) measurements, the optimal pre-diction is selected from a set of prediction candidates that are generated by four designed directional predictive modes. Then, the resulting residual is processed by scalar quantiza-tion (SQ). At the decoder, the same prediction is added onto the de-quantized residuals to produce the quantized CS measurements, which is exploited for CS reconstruction. Experimental results substantiate significant improvements achieved by SDPC-plus-SQ in rate distortion performance as compared with SQ alone and DPCM-plus-SQ.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7212",
        "title": "Structural Group Sparse Representation for Image Compressive Sensing Recovery",
        "authors": [
            "Jian Zhang",
            "Debin Zhao",
            "Feng Jiang",
            "Wen Gao"
        ],
        "abstract": "Compressive Sensing (CS) theory shows that a signal can be decoded from many fewer measurements than suggested by the Nyquist sampling theory, when the signal is sparse in some domain. Most of conventional CS recovery approaches, however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet and gradient domain) for the entirety of a signal, which are irrespective of the nonstationarity of natural signals and cannot achieve high enough degree of sparsity, thus resulting in poor rate-distortion performance. In this paper, we propose a new framework for image compressive sensing recovery via structural group sparse representation (SGSR) modeling, which enforces image sparsity and self-similarity simultaneously under a unified framework in an adaptive group domain, thus greatly confining the CS solution space. In addition, an efficient iterative shrinkage/thresholding algorithm based technique is developed to solve the above optimization problem. Experimental results demonstrate that the novel CS recovery strategy achieves significant performance improvements over the current state-of-the-art schemes and exhibits nice convergence.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7306",
        "title": "Generalized Nonconvex Nonsmooth Low-Rank Minimization",
        "authors": [
            "Canyi Lu",
            "Jinhui Tang",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "As surrogate functions of $L_0$-norm, many nonconvex penalty functions have been proposed to enhance the sparse vector recovery. It is easy to extend these nonconvex penalty functions on singular values of a matrix to enhance low-rank matrix recovery. However, different from convex optimization, solving the nonconvex low-rank minimization problem is much more challenging than the nonconvex sparse minimization problem. We observe that all the existing nonconvex penalty functions are concave and monotonically increasing on $[0,\\infty)$. Thus their gradients are decreasing functions. Based on this property, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to solve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem. By setting the weight vector as the gradient of the concave penalty function, the WSVT problem has a closed form solution. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthetic data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7566",
        "title": "Image Compressive Sensing Recovery Using Adaptively Learned Sparsifying Basis via L0 Minimization",
        "authors": [
            "Jian Zhang",
            "Chen Zhao",
            "Debin Zhao",
            "Wen Gao"
        ],
        "abstract": "From many fewer acquired measurements than suggested by the Nyquist sampling theory, compressive sensing (CS) theory demonstrates that, a signal can be reconstructed with high probability when it exhibits sparsity in some domain. Most of the conventional CS recovery approaches, however, exploited a set of fixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of a signal, which are irrespective of the non-stationarity of natural signals and cannot achieve high enough degree of sparsity, thus resulting in poor CS recovery performance. In this paper, we propose a new framework for image compressive sensing recovery using adaptively learned sparsifying basis via L0 minimization. The intrinsic sparsity of natural images is enforced substantially by sparsely representing overlapped image patches using the adaptively learned sparsifying basis in the form of L0 norm, greatly reducing blocking artifacts and confining the CS solution space. To make our proposed scheme tractable and robust, a split Bregman iteration based technique is developed to solve the non-convex L0 minimization problem efficiently. Experimental results on a wide range of natural images for CS recovery have shown that our proposed algorithm achieves significant performance improvements over many current state-of-the-art schemes and exhibits good convergence property.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7584",
        "title": "High-Speed Tracking with Kernelized Correlation Filters",
        "authors": [
            "Jo\u00e3o F. Henriques",
            "Rui Caseiro",
            "Pedro Martins",
            "Jorge Batista"
        ],
        "abstract": "The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies -- any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the Discrete Fourier Transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new Kernelized Correlation Filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7592",
        "title": "Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video",
        "authors": [
            "Jacob Grosek",
            "J. Nathan Kutz"
        ],
        "abstract": "This paper introduces the method of dynamic mode decomposition (DMD) for robustly separating video frames into background (low-rank) and foreground (sparse) components in real-time. The method is a novel application of a technique used for characterizing nonlinear dynamical systems in an equation-free manner by decomposing the state of the system into low-rank terms whose Fourier components in time are known. DMD terms with Fourier frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given video frames, and the terms with Fourier frequencies bounded away from the origin are their sparse counterparts. An approximate low-rank/sparse separation is achieved at the computational cost of just one singular value decomposition and one linear equation solve, thus producing results orders of magnitude faster than a leading separation method, namely robust principal component analysis (RPCA). The DMD method that is developed here is demonstrated to work robustly in real-time with personal laptop-class computing power and without any parameter tuning, which is a transformative improvement in performance that is ideal for video surveillance and recognition applications.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7594",
        "title": "Selecting a Small Set of Optimal Gestures from an Extensive Lexicon",
        "authors": [
            "Jacob Grosek",
            "J. Nathan Kutz"
        ],
        "abstract": "Finding the best set of gestures to use for a given computer recognition problem is an essential part of optimizing the recognition performance while being mindful to those who may articulate the gestures. An objective function, called the ellipsoidal distance ratio metric (EDRM), for determining the best gestures from a larger lexicon library is presented, along with a numerical method for incorporating subjective preferences. In particular, we demonstrate an efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$ gestures where typically $n \\ll m$ using a weighting of both subjective and objective measures.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7748",
        "title": "A graph-based mathematical morphology reader",
        "authors": [
            "Laurent Najman",
            "Jean Cousty"
        ],
        "abstract": "This survey paper aims at providing a \"literary\" anthology of mathematical morphology on graphs. It describes in the English language many ideas stemming from a large number of different papers, hence providing a unified view of an active and diverse field of research.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0006",
        "title": "Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction",
        "authors": [
            "Moritz Kassner",
            "William Patera",
            "Andreas Bulling"
        ],
        "abstract": "Commercial head-mounted eye trackers provide useful features to customers in industry and research but are expensive and rely on closed source hardware and software. This limits the application areas and use of mobile eye tracking to expert users and inhibits user-driven development, customisation, and extension. In this paper we present Pupil -- an accessible, affordable, and extensible open source platform for mobile eye tracking and gaze-based interaction. Pupil comprises 1) a light-weight headset with high-resolution cameras, 2) an open source software framework for mobile eye tracking, as well as 3) a graphical user interface (GUI) to playback and visualize video and gaze data. Pupil features high-resolution scene and eye cameras for monocular and binocular gaze estimation. The software and GUI are platform-independent and include state-of-the-art algorithms for real-time pupil detection and tracking, calibration, and accurate gaze estimation. Results of a performance evaluation show that Pupil can provide an average gaze estimation accuracy of 0.6 degree of visual angle (0.08 degree precision) with a latency of the processing pipeline of only 0.045 seconds.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0085",
        "title": "Relative Facial Action Unit Detection",
        "authors": [
            "Mahmoud Khademi",
            "Louis-Philippe Morency"
        ],
        "abstract": "This paper presents a subject-independent facial action unit (AU) detection method by introducing the concept of relative AU detection, for scenarios where the neutral face is not provided. We propose a new classification objective function which analyzes the temporal neighborhood of the current frame to decide if the expression recently increased, decreased or showed no change. This approach is a significant change from the conventional absolute method which decides about AU classification using the current frame, without an explicit comparison with its neighboring frames. Our proposed method improves robustness to individual differences such as face scale and shape, age-related wrinkles, and transitions among expressions (e.g., lower intensity of expressions). Our experiments on three publicly available datasets (Extended Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement of our approach over conventional absolute techniques. Keywords: facial action coding system (FACS); relative facial action unit detection; temporal information;\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0174",
        "title": "VSCAN: An Enhanced Video Summarization using Density-based Spatial Clustering",
        "authors": [
            "Karim M. Mohamed",
            "Mohamed A. Ismail",
            "Nagia M. Ghanem"
        ],
        "abstract": "In this paper, we present VSCAN, a novel approach for generating static video summaries. This approach is based on a modified DBSCAN clustering algorithm to summarize the video content utilizing both color and texture features of the video frames. The paper also introduces an enhanced evaluation method that depends on color and texture features. Video Summaries generated by VSCAN are compared with summaries generated by other approaches found in the literature and those created by users. Experimental results indicate that the video summaries generated by VSCAN have a higher quality than those generated by other approaches.\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0234",
        "title": "Retrieval in Long Surveillance Videos using User Described Motion and Object Attributes",
        "authors": [
            "Greg Castanon",
            "Mohamed Elgharib",
            "Venkatesh Saligrama",
            "Pierre-Marc Jodoin"
        ],
        "abstract": "We present a content-based retrieval method for long surveillance videos both for wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is to retrieve video segments, with a focus on detecting objects moving on routes, that match user-defined events of interest. The sheer size and remote locations where surveillance videos are acquired, necessitates highly compressed representations that are also meaningful for supporting user-defined queries. To address these challenges we archive long-surveillance video through lightweight processing based on low-level local spatio-temporal extraction of motion and object features. These are then hashed into an inverted index using locality-sensitive hashing (LSH). This local approach allows for query flexibility as well as leads to significant gains in compression. Our second task is to extract partial matches to the user-created query and assembles them into full matches using Dynamic Programming (DP). DP exploits causality to assemble the indexed low level features into a video segment which matches the query route. We examine CCTV and Airborne footage, whose low contrast makes motion extraction more difficult. We generate robust motion estimates for Airborne data using a tracklets generation algorithm while we use Horn and Schunck approach to generate motion estimates for CCTV. Our approach handles long routes, low contrasts and occlusion. We derive bounds on the rate of false positives and demonstrate the effectiveness of the approach for counting, motion pattern recognition and abandoned object applications.\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0312",
        "title": "Microsoft COCO: Common Objects in Context",
        "authors": [
            "Tsung-Yi Lin",
            "Michael Maire",
            "Serge Belongie",
            "Lubomir Bourdev",
            "Ross Girshick",
            "James Hays",
            "Pietro Perona",
            "Deva Ramanan",
            "C. Lawrence Zitnick",
            "Piotr Doll\u00e1r"
        ],
        "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.\n    ",
        "submission_date": "2014-05-01T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0545",
        "title": "Optimal measurement of visual motion across spatial and temporal scales",
        "authors": [
            "Sergei Gepshtein",
            "Ivan Tyukin"
        ],
        "abstract": "Sensory systems use limited resources to mediate the perception of a great variety of objects and events. Here a normative framework is presented for exploring how the problem of efficient allocation of resources can be solved in visual perception. Starting with a basic property of every measurement, captured by Gabor's uncertainty relation about the location and frequency content of signals, prescriptions are developed for optimal allocation of sensors for reliable perception of visual motion. This study reveals that a large-scale characteristic of human vision (the spatiotemporal contrast sensitivity function) is similar to the optimal prescription, and it suggests that some previously puzzling phenomena of visual sensitivity, adaptation, and perceptual organization have simple principled explanations.\n    ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0601",
        "title": "Supervised Descent Method for Solving Nonlinear Least Squares Problems in Computer Vision",
        "authors": [
            "Xuehan Xiong",
            "Fernando De la Torre"
        ],
        "abstract": "Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved with nonlinear optimization methods. It is generally accepted that second order descent methods are the most robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, second order descent methods have two main drawbacks: (1) the function might not be analytically differentiable and numerical approximations are impractical, and (2) the Hessian may be large and not positive definite. To address these issues, this paper proposes generic descent maps, which are average \"descent directions\" and rescaling factors learned in a supervised fashion. Using generic descent maps, we derive a practical algorithm - Supervised Descent Method (SDM) - for minimizing Nonlinear Least Squares (NLS) problems. During training, SDM learns a sequence of decent maps that minimize the NLS. In testing, SDM minimizes the NLS objective using the learned descent maps without computing the Jacobian or the Hessian. We prove the conditions under which the SDM is guaranteed to converge. We illustrate the effectiveness and accuracy of SDM in three computer vision problems: rigid image alignment, non-rigid image alignment, and 3D pose estimation. In particular, we show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code has been made available at ",
        "submission_date": "2014-05-03T00:00:00",
        "last_modified_date": "2014-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0632",
        "title": "Rule of Three for Superresolution of Still Images with Applications to Compression and Denoising",
        "authors": [
            "Mario Mastriani"
        ],
        "abstract": "We describe a new method for superresolution of still images (in the wavelet domain) based on the reconstruction of missing details subbands pixels at a given ith level via Rule of Three (Ro3) between pixels of approximation subband of such level, and pixels of approximation and detail subbands of (i+1)th level. The histogramic profiles demonstrate that Ro3 is the appropriate mechanism to recover missing detail subband pixels in these cases. Besides, with the elimination of the details subbands pixels (in an eventual compression scheme), we obtain a bigger compression rate. Experimental results demonstrate that our approach compares favorably to more typical methods of denoising and compression in wavelet domain. Our method does not compress, but facilitates the action of the real compressor, in our case, Joint Photographic Experts Group (JPEG) and JPEg2000, that is, Ro3 acts as a catalyst compression\n    ",
        "submission_date": "2014-05-04T00:00:00",
        "last_modified_date": "2014-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0892",
        "title": "A Continuous Max-Flow Approach to Multi-Labeling Problems under Arbitrary Region Regularization",
        "authors": [
            "John S.H. Baxter",
            "Martin Rajchl",
            "Jing Yuan",
            "Terry M. Peters"
        ],
        "abstract": "The incorporation of region regularization into max-flow segmentation has traditionally focused on ordering and part-whole relationships. A side effect of the development of such models is that it constrained regularization only to those cases, rather than allowing for arbitrary region regularization. Directed Acyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations by allowing for the algorithm designer to specify an arbitrary directed acyclic graph to structure a max-flow segmentation. This allows for individual 'parts' to be a member of multiple distinct 'wholes.'\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.0921",
        "title": "Gabor Filter and Rough Clustering Based Edge Detection",
        "authors": [
            "Chandranath Adak"
        ],
        "abstract": "This paper introduces an efficient edge detection method based on Gabor filter and rough clustering. The input image is smoothed by Gabor function, and the concept of rough clustering is used to focus on edge detection with soft computational approach. Hysteresis thresholding is used to get the actual output, i.e. edges of the input image. To show the effectiveness, the proposed technique is compared with some other edge detection methods.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1005",
        "title": "Comparing apples to apples in the evaluation of binary coding methods",
        "authors": [
            "Mohammad Rastegari",
            "Shobeir Fakhraei",
            "Jonghyun Choi",
            "David Jacobs",
            "Larry S. Davis"
        ],
        "abstract": "We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means).\n    ",
        "submission_date": "2014-05-05T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1020",
        "title": "Study on performance improvement of oil paint image filter algorithm using parallel pattern library",
        "authors": [
            "Siddhartha Mukherjee"
        ],
        "abstract": "This paper gives a detailed study on the performance of oil paint image filter algorithm with various parameters applied on an image of RGB model. Oil Paint image processing, being very performance hungry, current research tries to find improvement using parallel pattern library. With increasing kernel-size, the processing time of oil paint image filter algorithm increases exponentially.\n    ",
        "submission_date": "2014-03-19T00:00:00",
        "last_modified_date": "2014-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1207",
        "title": "Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes",
        "authors": [
            "Jian Yang",
            "Jianjun Qian",
            "Lei Luo",
            "Fanlong Zhang",
            "Yicheng Gao"
        ],
        "abstract": "Recently regression analysis becomes a popular tool for face recognition. The existing regression methods all use the one-dimensional pixel-based error model, which characterizes the representation error pixel by pixel individually and thus neglects the whole structure of the error image. We observe that occlusion and illumination changes generally lead to a low-rank error image. To make use of this low-rank structural information, this paper presents a two-dimensional image matrix based error model, i.e. matrix regression, for face representation and classification. Our model uses the minimal nuclear norm of representation error image as a criterion, and the alternating direction method of multipliers method to calculate the regression coefficients. Compared with the current regression methods, the proposed Nuclear Norm based Matrix Regression (NMR) model is more robust for alleviating the effect of illumination, and more intuitive and powerful for removing the structural noise caused by occlusion. We experiment using four popular face image databases, the Extended Yale B database, the AR database, the Multi-PIE and the FRGC database. Experimental results demonstrate the performance advantage of NMR over the state-of-the-art regression based face recognition methods.\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1213",
        "title": "Human Pose Estimation from RGB Input Using Synthetic Training Data",
        "authors": [
            "Oscar Danielsson",
            "Omid Aghazadeh"
        ],
        "abstract": "We address the problem of estimating the pose of humans using RGB image input. More specifically, we are using a random forest classifier to classify pixels into joint-based body part categories, much similar to the famous Kinect pose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth. Since the random forest requires a large number of training examples, we are using computer graphics generated, synthetic training data. In addition, we assume that we have access to a large number of real images with bounding box labels, extracted for example by a pedestrian detector or a tracking system. We propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain. We demonstrate on a publicly available dataset [6] that the proposed objective function yields a classifier that significantly outperforms a baseline classifier trained using the standard entropy objective [10].\n    ",
        "submission_date": "2014-05-06T00:00:00",
        "last_modified_date": "2014-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1402",
        "title": "New Algorithmic Approaches to Point Constellation Recognition",
        "authors": [
            "Thomas Bourgeat",
            "Julien Bringer",
            "Herve Chabanne",
            "Robin Champenois",
            "Jeremie Clement",
            "Houda Ferradi",
            "Marc Heinrich",
            "Paul Melotti",
            "David Naccache",
            "Antoine Voizard"
        ],
        "abstract": "Point constellation recognition is a common problem with many pattern matching applications. Whilst useful in many contexts, this work is mainly motivated by fingerprint matching. Fingerprints are traditionally modelled as constellations of oriented points called minutiae. The fingerprint verifier's task consists in comparing two point constellations. The compared constellations may differ by rotation and translation or by much more involved transforms such as distortion or occlusion. This paper presents three new constellation matching algorithms. The first two methods generalize an algorithm by Bringer and Despiegel. Our third proposal creates a very interesting analogy between mechanical system simulation and the constellation recognition problem.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1403",
        "title": "MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source",
        "authors": [
            "Rui Song",
            "Hyunsuk Ko",
            "C.C. Jay Kuo"
        ],
        "abstract": "A new stereoscopic image quality assessment database rendered using the 2D-image-plus-depth source, called MCL-3D, is described and the performance benchmarking of several known 2D and 3D image quality metrics using the MCL-3D database is presented in this work. Nine image-plus-depth sources are first selected, and a depth image-based rendering (DIBR) technique is used to render stereoscopic image pairs. Distortions applied to either the texture image or the depth image before stereoscopic image rendering include: Gaussian blur, additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compression and transmission error. Furthermore, the distortion caused by imperfect rendering is also examined. The MCL-3D database contains 693 stereoscopic image pairs, where one third of them are of resolution 1024x728 and two thirds are of resolution 1920x1080. The pair-wise comparison was adopted in the subjective test for user friendliness, and the Mean Opinion Score (MOS) can be computed accordingly. Finally, we evaluate the performance of several 2D and 3D image quality metrics applied to MCL-3D. All texture images, depth images, rendered image pairs in MCL-3D and their MOS values obtained in the subjective test are available to the public (",
        "submission_date": "2014-03-23T00:00:00",
        "last_modified_date": "2014-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1678",
        "title": "RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal Component Analysis",
        "authors": [
            "Chinh Dang",
            "Abdolreza Moghadam",
            "Hayder Radha"
        ],
        "abstract": "Key frame extraction algorithms consider the problem of selecting a subset of the most informative frames from a video to summarize its content.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1681",
        "title": "Representative Selection for Big Data via Sparse Graph and Geodesic Grassmann Manifold Distance",
        "authors": [
            "Chinh Dang",
            "Hayder Radha"
        ],
        "abstract": "This paper addresses the problem of identifying a very small subset of data points that belong to a significantly larger massive dataset (i.e., Big Data). The small number of selected data points must adequately represent and faithfully characterize the massive Big Data. Such identification process is known as representative selection [19]. We propose a novel representative selection framework by generating an l1 norm sparse graph for a given Big-Data dataset. The Big Data is partitioned recursively into clusters using a spectral clustering algorithm on the generated sparse graph. We consider each cluster as one point in a Grassmann manifold, and measure the geodesic distance among these points. The distances are further analyzed using a min-max algorithm [1] to extract an optimal subset of clusters. Finally, by considering a sparse subgraph of each selected cluster, we detect a representative using principal component centrality [11]. We refer to the proposed representative selection framework as a Sparse Graph and Grassmann Manifold (SGGM) based approach. To validate the proposed SGGM framework, we apply it onto the problem of video summarization where only few video frames, known as key frames, are selected among a much longer video sequence. A comparison of the results obtained by the proposed algorithm with the ground truth, which is agreed by multiple human judges, and with some state-of-the-art methods clearly indicates the viability of the SGGM framework.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1717",
        "title": "Entropy Based Cartoon Texture Separation",
        "authors": [
            "Kutlu Emre Yilmaz"
        ],
        "abstract": "Separating an image into cartoon and texture components comes useful in image processing applications, such as image compression, image segmentation, image inpainting. Yves Meyer's influential cartoon texture decomposition model involves deriving an energy functional by choosing appropriate spaces and functionals. Minimizers of the derived energy functional are cartoon and texture components of an image. In this study, cartoon part of an image is separated, by reconstructing it from pixels of multi scale Total-Variation filtered versions of the original image which is sought to be decomposed into cartoon and texture parts. An information theoretic pixel by pixel selection criteria is employed to choose the contributing pixels and their scales.\n    ",
        "submission_date": "2014-05-07T00:00:00",
        "last_modified_date": "2014-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1815",
        "title": "Implementation And Performance Evaluation Of Background Subtraction Algorithms",
        "authors": [
            "Deepjoy Das",
            "Dr. Sarat Saharia"
        ],
        "abstract": "The study evaluates three background subtraction techniques. The techniques ranges from very basic algorithm to state of the art published techniques categorized based on speed, memory requirements and accuracy. Such a review can effectively guide the designer to select the most suitable method for a given application in a principled way. The algorithms used in the study ranges from varying levels of accuracy and computational complexity. Few of them can also deal with real time challenges like rain, snow, hails, swaying branches, objects overlapping, varying light intensity or slow moving objects.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1965",
        "title": "Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes using High-Resolution Neural EM Data",
        "authors": [
            "Ayushi Sinha",
            "William Gray Roncal",
            "Narayanan Kasthuri",
            "Jeff W. Lichtman",
            "Randal Burns",
            "Michael Kazhdan"
        ],
        "abstract": "Accurately estimating the wiring diagram of a brain, known as a connectome, at an ultrastructure level is an open research problem. Specifically, precisely tracking neural processes is difficult, especially across many image slices. Here, we propose a novel method to automatically identify and annotate small subcellular structures present in axons, known as axoplasmic reticula, through a 3D volume of high-resolution neural electron microscopy data. Our method produces high precision annotations, which can help improve automatic segmentation by using our results as seeds for segmentation, and as cues to aid segment merging.\n    ",
        "submission_date": "2014-04-16T00:00:00",
        "last_modified_date": "2014-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1966",
        "title": "Texture Based Image Segmentation of Chili Pepper X-Ray Images Using Gabor Filter",
        "authors": [
            "M.Rajalakshmi",
            "Dr. P.Subashini"
        ],
        "abstract": "Texture segmentation is the process of partitioning an image into regions with different textures containing a similar group of pixels. Detecting the discontinuity of the filter's output and their statistical properties help in segmenting and classifying a given image with different texture regions. In this proposed paper, chili x-ray image texture segmentation is performed by using Gabor filter. The texture segmented result obtained from Gabor filter fed into three texture filters, namely Entropy, Standard Deviation and Range filter. After performing texture analysis, features can be extracted by using Statistical methods. In this paper Gray Level Co-occurrence Matrices and First order statistics are used as feature extraction methods. Features extracted from statistical methods are given to Support Vector Machine (SVM) classifier. Using this methodology, it is found that texture segmentation is followed by the Gray Level Co-occurrence Matrix feature extraction method gives a higher accuracy rate of 84% when compared with First order feature extraction method.\n",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1967",
        "title": "Image Resolution and Contrast Enhancement of Satellite Geographical Images with Removal of Noise using Wavelet Transforms",
        "authors": [
            "Prajakta P. Khairnar",
            "C. A. Manjare"
        ],
        "abstract": "In this paper the technique for resolution and contrast enhancement of satellite geographical images based on discrete wavelet transform (DWT), stationary wavelet transform (SWT) and singular value decomposition (SVD) has been proposed. In this, the noise is added in the input low resolution and low contrast image. The median filter is used remove noise from the input image. This low resolution, low contrast image without noise is decomposed into four sub-bands by using DWT and SWT. The resolution enhancement technique is based on the interpolation of high frequency components obtained by DWT and input image. SWT is used to enhance input image. DWT is used to decompose an image into four frequency sub bands and these four sub-bands are interpolated using bicubic interpolation technique. All these sub-bands are reconstructed as high resolution image by using inverse DWT (IDWT). To increase the contrast the proposed technique uses DWT and SVD. GHE is used to equalize an image. The equalized image is decomposed into four sub-bands using DWT and new LL sub-band is reconstructed using SVD. All sub-bands are reconstructed using IDWT to generate high resolution and contrast image over conventional techniques. The experimental result shows superiority of the proposed technique over conventional techniques.\n",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.1999",
        "title": "Model-Driven Applications of Fractional Derivatives and Integrals",
        "authors": [
            "William A. Sethares",
            "Sel\u00e7uk \u015e. Bay\u0131n"
        ],
        "abstract": "Fractional order derivatives and integrals (differintegrals) are viewed from a frequency-domain perspective using the formalism of Riesz, providing a computational tool as well as a way to interpret the operations in the frequency domain. Differintegrals provide a logical extension of current techniques, generalizing the notion of integral and differential operators and acting as kind of frequency-domain filtering that has many of the advantages of a nonlocal linear operator. Several important properties of differintegrals are presented, and sample applications are given to one- and two-dimensional signals. Computer code to carry out the computations is made available on the author's website.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2062",
        "title": "Precision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps",
        "authors": [
            "Pengfei Wan",
            "Gene Cheung",
            "Philip A. Chou",
            "Dinei Florencio",
            "Cha Zhang",
            "Oscar C. Au"
        ],
        "abstract": "In texture-plus-depth representation of a 3D scene, depth maps from different camera viewpoints are typically lossily compressed via the classical transform coding / coefficient quantization paradigm. In this paper we propose to reduce distortion of the decoded depth maps due to quantization. The key observation is that depth maps from different viewpoints constitute multiple descriptions (MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-like iterative procedure to project a reconstructed signal from one depth map to the other and back, so that the converged depth maps have higher precision than the original quantized versions.\n    ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2128",
        "title": "Variational Image Segmentation Model Coupled with Image Restoration Achievements",
        "authors": [
            "Xiaohao Cai"
        ],
        "abstract": "Image segmentation and image restoration are two important topics in image processing with great achievements. In this paper, we propose a new multiphase segmentation model by combining image restoration and image segmentation models. Utilizing image restoration aspects, the proposed segmentation model can effectively and robustly tackle high noisy images, blurry images, images with missing pixels, and vector-valued images. In particular, one of the most important segmentation models, the piecewise constant Mumford-Shah model, can be extended easily in this way to segment gray and vector-valued images corrupted for example by noise, blur or missing pixels after coupling a new data fidelity term which comes from image restoration topics. It can be solved efficiently using the alternating minimization algorithm, and we prove the convergence of this algorithm with three variables under mild condition. Experiments on many synthetic and real-world images demonstrate that our method gives better segmentation results in comparison to others state-of-the-art segmentation models especially for blurry images and images with missing pixels values.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2227",
        "title": "An Overview of Face Liveness Detection",
        "authors": [
            "Saptarshi Chakraborty",
            "Dhrubajyoti Das"
        ],
        "abstract": "Face recognition is a widely used biometric approach. Face recognition technology has developed rapidly in recent years and it is more direct, user friendly and convenient compared to other methods. But face recognition systems are vulnerable to spoof attacks made by non-real faces. It is an easy way to spoof face recognition systems by facial pictures such as portrait photographs. A secure system needs Liveness detection in order to guard against such spoofing. In this work, face liveness detection approaches are categorized based on the various types techniques used for liveness detection. This categorization helps understanding different spoof attacks scenarios and their relation to the developed solutions. A review of the latest works regarding face liveness detection works is presented. The main aim is to provide a simple path for the future development of novel and more secured face liveness detection approach.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2246",
        "title": "Graph Regularized Non-negative Matrix Factorization By Maximizing Correntropy",
        "authors": [
            "Le Li",
            "Jianjun Yang",
            "Kaili Zhao",
            "Yang Xu",
            "Honggang Zhang",
            "Zhuoyi Fan"
        ],
        "abstract": "Non-negative matrix factorization (NMF) has proved effective in many clustering and classification tasks. The classic ways to measure the errors between the original and the reconstructed matrix are $l_2$ distance or Kullback-Leibler (KL) divergence. However, nonlinear cases are not properly handled when we use these error measures. As a consequence, alternative measures based on nonlinear kernels, such as correntropy, are proposed. However, the current correntropy-based NMF only targets on the low-level features without considering the intrinsic geometrical distribution of data. In this paper, we propose a new NMF algorithm that preserves local invariance by adding graph regularization into the process of max-correntropy-based matrix factorization. Meanwhile, each feature can learn corresponding kernel from the data. The experiment results of Caltech101 and Caltech256 show the benefits of such combination against other NMF algorithms for the unsupervised image clustering.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2316",
        "title": "Better Feature Tracking Through Subspace Constraints",
        "authors": [
            "Bryan Poling",
            "Gilad Lerman",
            "Arthur Szlam"
        ],
        "abstract": "Feature tracking in video is a crucial task in computer vision. Usually, the tracking problem is handled one feature at a time, using a single-feature tracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives. While this approach works quite well when dealing with high-quality video and \"strong\" features, it often falters when faced with dark and noisy video containing low-quality features. We present a framework for jointly tracking a set of features, which enables sharing information between the different features in the scene. We show that our method can be employed to track features for both rigid and nonrigid motions (possibly of few moving bodies) even when some features are occluded. Furthermore, it can be used to significantly improve tracking results in poorly-lit scenes (where there is a mix of good and bad features). Our approach does not require direct modeling of the structure or the motion of the scene, and runs in real time on a single CPU core.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2362",
        "title": "Image Segmentation Using Frequency Locking of Coupled Oscillators",
        "authors": [
            "Yan Fang",
            "Matthew J. Cotter",
            "Donald M. Chiarulli",
            "Steven P. Levitan"
        ],
        "abstract": "Synchronization of coupled oscillators is observed at multiple levels of neural systems, and has been shown to play an important function in visual perception. We propose a computing system based on locally coupled oscillator networks for image segmentation. The system can serve as the preprocessing front-end of an image processing pipeline where the common frequencies of clusters of oscillators reflect the segmentation results. To demonstrate the feasibility of our design, the system is simulated and tested on a human face image dataset and its performance is compared with traditional intensity threshold based algorithms. Our system shows both better performance and higher noise tolerance than traditional methods.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2403",
        "title": "Hyperspectral pan-sharpening: a variational convex constrained formulation to impose parallel level lines, solved with ADMM",
        "authors": [
            "Alexis Huck",
            "Fran\u00e7ois de Vieilleville",
            "Pierre Weiss",
            "Manuel Grizonnet"
        ],
        "abstract": "In this paper, we address the issue of hyperspectral pan-sharpening, which consists in fusing a (low spatial resolution) hyperspectral image HX and a (high spatial resolution) panchromatic image P to obtain a high spatial resolution hyperspectral image. The problem is addressed under a variational convex constrained formulation. The objective favors high resolution spectral bands with level lines parallel to those of the panchromatic image. This term is balanced with a total variation term as regularizer. Fit-to-P data and fit-to-HX data constraints are effectively considered as mathematical constraints, which depend on the statistics of the data noise measurements. The developed Alternating Direction Method of Multipliers (ADMM) optimization scheme enables us to solve this problem efficiently despite the non differentiabilities and the huge number of unknowns.\n    ",
        "submission_date": "2014-05-10T00:00:00",
        "last_modified_date": "2014-05-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2496",
        "title": "Anomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of Solid Media",
        "authors": [
            "Jeffrey M. Druce",
            "Jarvis D. Haupt",
            "Stefano Gonella"
        ],
        "abstract": "This paper proposes a strategy for the detection and triangulation of structural anomalies in solid media. The method revolves around the construction of sparse representations of the medium's dynamic response, obtained by learning instructive dictionaries which form a suitable basis for the response data. The resulting sparse coding problem is recast as a modified dictionary learning task with additional spatial sparsity constraints enforced on the atoms of the learned dictionaries, which provides them with a prescribed spatial topology that is designed to unveil anomalous regions in the physical domain. The proposed methodology is model agnostic, i.e., it forsakes the need for a physical model and requires virtually no a priori knowledge of the structure's material properties, as all the inferences are exclusively informed by the data through the layers of information that are available in the intrinsic salient structure of the material's dynamic response. This characteristic makes the approach powerful for anomaly identification in systems with unknown or heterogeneous property distribution, for which a model is unsuitable or unreliable. The method is validated using both synthetically\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2539",
        "title": "A Review of Image Mosaicing Techniques",
        "authors": [
            "Dushyant Vaghela",
            "Prof. Kapildev Naina"
        ],
        "abstract": "Image Mosaicing is a method of constructing multiple images of the same scene into a larger image. The output of the image mosaic will be the union of two input images. Image-mosaicing algorithms are used to get mosaiced image. Image Mosaicing processed is basically divided in to 5 phases. Which includes; Feature point extraction, Image registration, Homography computation, Warping and Blending if Image. Various corner detection algorithm is being used for Feature extraction. This corner produces an efficient and informative output mosaiced image. Image mosaicing is widely used in creating 3D images, medical imaging, computer vision, data from satellites, and military automatic target recognition.\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2641",
        "title": "Multi Modal Face Recognition Using Block Based Curvelet Features",
        "authors": [
            "Jyothi K",
            "Prabhakar C.J"
        ],
        "abstract": "In this paper, we present multimodal 2D +3D face recognition method using block based curvelet features. The 3D surface of face (Depth Map) is computed from the stereo face images using stereo vision technique. The statistical measures such as mean, standard deviation, variance and entropy are extracted from each block of curvelet subband for both depth and intensity images ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2908",
        "title": "Resource-Aware Programming for Robotic Vision",
        "authors": [
            "Johny Paul",
            "Walter Stechele",
            "Manfred Kr\u00f6hnert",
            "Tamim Asfour"
        ],
        "abstract": "Humanoid robots are designed to operate in human centered environments. They face changing, dynamic environments in which they need to fulfill a multitude of challenging tasks. Such tasks differ in complexity, resource requirements, and execution time. Latest computer architectures of humanoid robots consist of several industrial PCs containing single- or dual-core processors. According to the SIA roadmap for semiconductors, many-core chips with hundreds to thousands of cores are expected to be available in the next decade. Utilizing the full power of a chip with huge amounts of resources requires new computing paradigms and methodologies.\n",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2941",
        "title": "Cross-view Action Modeling, Learning and Recognition",
        "authors": [
            "Jiang wang",
            "Xiaohan Nie",
            "Yin Xia",
            "Ying Wu",
            "Song-Chun Zhu"
        ],
        "abstract": "Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.\n    ",
        "submission_date": "2014-05-12T00:00:00",
        "last_modified_date": "2014-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3195",
        "title": "An Intelligent Pixel Replication Technique by Binary Decomposition for Digital Image Zooming",
        "authors": [
            "Kaeser M Sabrin",
            "M Haider Ali"
        ],
        "abstract": "Image zooming is the process of enlarging the spatial resolution of a given digital image. We present a novel technique that intelligently modifies the classical pixel replication method for zooming. Our method decomposes a given image into layer of binary images, interpolates them by magnifying the binary patterns preserving their geometric shape and finally aggregates them all to obtain the zoomed image. Although the quality of our zoomed images is much higher than that of nearest neighbor and bilinear interpolation and comparable with bicubic interpolation, the running time of our technique is extremely fast like nearest neighbor interpolation and much faster than bilinear and bicubic interpolation.\n    ",
        "submission_date": "2014-05-13T00:00:00",
        "last_modified_date": "2014-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3351",
        "title": "Group-based Sparse Representation for Image Restoration",
        "authors": [
            "Jian Zhang",
            "Debin Zhao",
            "Wen Gao"
        ],
        "abstract": "Traditional patch-based sparse representation modeling of natural images usually suffer from two problems. First, it has to solve a large-scale optimization problem with high computational complexity in dictionary learning. Second, each patch is considered independently in dictionary learning and sparse coding, which ignores the relationship among patches, resulting in inaccurate sparse coding coefficients. In this paper, instead of using patch as the basic unit of sparse representation, we exploit the concept of group as the basic unit of sparse representation, which is composed of nonlocal patches with similar structures, and establish a novel sparse representation modeling of natural images, called group-based sparse representation (GSR). The proposed GSR is able to sparsely represent natural images in the domain of group, which enforces the intrinsic local sparsity and nonlocal self-similarity of images simultaneously in a unified framework. Moreover, an effective self-adaptive dictionary learning method for each group with low complexity is designed, rather than dictionary learning from natural images. To make GSR tractable and robust, a split Bregman based technique is developed to solve the proposed GSR-driven minimization problem for image restoration efficiently. Extensive experiments on image inpainting, image deblurring and image compressive sensing recovery manifest that the proposed GSR modeling outperforms many current state-of-the-art schemes in both PSNR and visual perception.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3352",
        "title": "Newton-Type Iterative Solver for Multiple View $L2$ Triangulation",
        "authors": [
            "F. Lu",
            "Z. Chen"
        ],
        "abstract": "In this note, we show that the L2 optimal solutions to most real multiple view L2 triangulation problems can be efficiently obtained by two-stage Newton-like iterative methods, while the difficulty of such problems mainly lies in how to verify the L2 optimality. Such a working two-stage bundle adjustment approach features: first, the algorithm is initialized by symmedian point triangulation, a multiple-view generalization of the mid-point method; second, a symbolic-numeric method is employed to compute derivatives accurately; third, globalizing strategy such as line search or trust region is smoothly applied to the underlying iteration which assures algorithm robustness in general cases.\n",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3382",
        "title": "Active Mining of Parallel Video Streams",
        "authors": [
            "Samaneh Khoshrou",
            "Jaime S. Cardoso",
            "Luis F. Teixeira"
        ],
        "abstract": "The practicality of a video surveillance system is adversely limited by the amount of queries that can be placed on human resources and their vigilance in response. To transcend this limitation, a major effort under way is to include software that (fully or at least semi) automatically mines video footage, reducing the burden imposed to the system. Herein, we propose a semi-supervised incremental learning framework for evolving visual streams in order to develop a robust and flexible track classification system. Our proposed method learns from consecutive batches by updating an ensemble in each time. It tries to strike a balance between performance of the system and amount of data which needs to be labelled. As no restriction is considered, the system can address many practical problems in an evolving multi-camera scenario, such as concept drift, class evolution and various length of video streams which have not been addressed before. Experiments were performed on synthetic as well as real-world visual data in non-stationary environments, showing high accuracy with fairly little human collaboration.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3531",
        "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets",
        "authors": [
            "Ken Chatfield",
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.\n    ",
        "submission_date": "2014-05-14T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3866",
        "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions",
        "authors": [
            "Max Jaderberg",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "abstract": "The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5x speedup with no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4054",
        "title": "Optimized Cartesian $K$-Means",
        "authors": [
            "Jianfeng Wang",
            "Jingdong Wang",
            "Jingkuan Song",
            "Xin-Shun Xu",
            "Heng Tao Shen",
            "Shipeng Li"
        ],
        "abstract": "Product quantization-based approaches are effective to encode high-dimensional data points for approximate nearest neighbor search. The space is decomposed into a Cartesian product of low-dimensional subspaces, each of which generates a sub codebook. Data points are encoded as compact binary codes using these sub codebooks, and the distance between two data points can be approximated efficiently from their codes by the precomputed lookup tables. Traditionally, to encode a subvector of a data point in a subspace, only one sub codeword in the corresponding sub codebook is selected, which may impose strict restrictions on the search accuracy. In this paper, we propose a novel approach, named Optimized Cartesian $K$-Means (OCKM), to better encode the data points for more accurate approximate nearest neighbor search. In OCKM, multiple sub codewords are used to encode the subvector of a data point in a subspace. Each sub codeword stems from different sub codebooks in each subspace, which are optimally generated with regards to the minimization of the distortion errors. The high-dimensional data point is then encoded as the concatenation of the indices of multiple sub codewords from all the subspaces. This can provide more flexibility and lower distortion errors than traditional methods. Experimental results on the standard real-life datasets demonstrate the superiority over state-of-the-art approaches for approximate nearest neighbor search.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4308",
        "title": "Coarse-to-Fine Classification via Parametric and Nonparametric Models for Computer-Aided Diagnosis",
        "authors": [
            "Meizhu Liu",
            "Le Lu",
            "Xiaojing Ye",
            "Shipeng Yu"
        ],
        "abstract": "Classification is one of the core problems in Computer-Aided Diagnosis (CAD), targeting for early cancer detection using 3D medical imaging interpretation. High detection sensitivity with desirably low false positive (FP) rate is critical for a CAD system to be accepted as a valuable or even indispensable tool in radiologists' workflow. Given various spurious imagery noises which cause observation uncertainties, this remains a very challenging task. In this paper, we propose a novel, two-tiered coarse-to-fine (CTF) classification cascade framework to tackle this problem. We first obtain classification-critical data samples (e.g., samples on the decision boundary) extracted from the holistic data distributions using a robust parametric model (e.g., \\cite{Raykar08}); then we build a graph-embedding based nonparametric classifier on sampled data, which can more accurately preserve or formulate the complex classification boundary. These two steps can also be considered as effective \"sample pruning\" and \"feature pursuing + $k$NN/template matching\", respectively. Our approach is validated comprehensively in colorectal polyp detection and lung nodule detection CAD systems, as the top two deadly cancers, using hospital scale, multi-site clinical datasets. The results show that our method achieves overall better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers, such as the support vector machine variants \\cite{Wang08}, boosting \\cite{Slabaugh10}, logistic regression \\cite{Ravesteijn10}, relevance vector machine \\cite{Raykar08}, $k$-nearest neighbor \\cite{Murphy09} or spectral projections on graph \\cite{Cai08}.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2014-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4389",
        "title": "Efficient Tracking of a Moving Object using Inter-Frame Coding",
        "authors": [
            "Shraddha Mehta",
            "Vaishali Kalariya"
        ],
        "abstract": "Video surveillance has long been in use to monitor security sensitive areas such as banks, department stores, highways, crowded public places and ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4390",
        "title": "Real Time Object Tracking Based on Inter-frame Coding: A Review",
        "authors": [
            "Shraddha Mehta",
            "Vaishali Kalariya"
        ],
        "abstract": "Inter-frame Coding plays significant role for video Compression and Computer Vision. Computer vision systems have been incorporated in many real life applications (e.g. surveillance systems, medical imaging, robot navigation and identity verification systems). Object tracking is a key computer vision topic, which aims at detecting the position of a moving object from a video sequence. The application of Inter-frame Coding for low frame rate video, as well as for low resolution video. Various methods based on Top-down approach just like kernel based or mean shift technique are used to track the object for video, So, Inter-frame Coding algorithms are widely adopted by video coding standards, mainly due to their simplicity and good distortion performance for object tracking.\n    ",
        "submission_date": "2014-05-17T00:00:00",
        "last_modified_date": "2014-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4506",
        "title": "Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice",
        "authors": [
            "Xiaojiang Peng",
            "Limin Wang",
            "Xingxing Wang",
            "Yu Qiao"
        ],
        "abstract": "Video based action recognition is one of the important and challenging problems in computer vision research. Bag of Visual Words model (BoVW) with local features has become the most popular method and obtained the state-of-the-art performance on several realistic datasets, such as the HMDB51, UCF50, and UCF101. BoVW is a general pipeline to construct a global representation from a set of local features, which is mainly composed of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. Many efforts have been made in each step independently in different scenarios and their effect on action recognition is still unknown. Meanwhile, video data exhibits different views of visual pattern, such as static appearance and motion dynamics. Multiple descriptors are usually extracted to represent these different views. Many feature fusion methods have been developed in other areas and their influence on action recognition has never been investigated before. This paper aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practice to produce a state-of-the-art action recognition system. Specifically, we explore two kinds of local features, ten kinds of encoding methods, eight kinds of pooling and normalization strategies, and three kinds of fusion methods. We conclude that every step is crucial for contributing to the final recognition rate. Furthermore, based on our comprehensive study, we propose a simple yet effective representation, called hybrid representation, by exploring the complementarity of different BoVW frameworks and local descriptors. Using this representation, we obtain the state-of-the-art on the three challenging datasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%).\n    ",
        "submission_date": "2014-05-18T00:00:00",
        "last_modified_date": "2014-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4574",
        "title": "Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount Classification",
        "authors": [
            "Kristjan H. Greenewald",
            "Alfred O. Hero III"
        ],
        "abstract": "We consider the application of KronPCA spatio-temporal modeling techniques [Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction of spatiotemporal features for video dismount classification. KronPCA performs a low-rank type of dimensionality reduction that is adapted to spatio-temporal data and is characterized by the T frame multiframe mean and covariance of p spatial features. For further regularization and improved inverse estimation, we also use the diagonally corrected KronPCA shrinkage methods we presented in [Greenewald et al 2013]. We apply this very general method to the modeling of the multivariate temporal behavior of HOG features extracted from pedestrian bounding boxes in video, with gender classification in a challenging dataset chosen as a specific application. The learned covariances for each class are used to extract spatiotemporal features which are then classified, achieving competitive classification performance.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4583",
        "title": "ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy Functions",
        "authors": [
            "Wei Feng",
            "Jiaya Jia",
            "Zhi-Qiang Liu"
        ],
        "abstract": "Many recent advances in computer vision have demonstrated the impressive power of dense and nonsubmodular energy functions in solving visual labeling problems. However, minimizing such energies is challenging. None of existing techniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do this well. In this paper, we present an efficient method, namely ESSP, to optimize binary MRFs with arbitrary pairwise potentials, which could be nonsubmodular and with dense connectivity. We also provide a comparative study of our approach and several recent promising methods. From our study, we make some reasonable recommendations of combining existing methods that perform the best in different situations for this challenging problem. Experimental results validate that for dense and nonsubmodular energy functions, the proposed approach can usually obtain lower energies than the best combination of other techniques using comparably reasonable time.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4802",
        "title": "Use of Computer Vision to Detect Tangles in Tangled Objects",
        "authors": [
            "Paritosh Parmar"
        ],
        "abstract": "Untangling of structures like ropes and wires by autonomous robots can be useful in areas such as personal robotics, industries and electrical wiring & repairing by robots. This problem can be tackled by using computer vision system in robot. This paper proposes a computer vision based method for analyzing visual data acquired from camera for perceiving the overlap of wires, ropes, hoses i.e. detecting tangles. Information obtained after processing image according to the proposed method comprises of position of tangles in tangled object and which wire passes over which wire. This information can then be used to guide robot to untangle wire/s. Given an image, preprocessing is done to remove noise. Then edges of wire are detected. After that, the image is divided into smaller blocks and each block is checked for wire overlap/s and finding other relevant information. TANGLED-100 dataset was introduced, which consists of images of tangled linear deformable objects. Method discussed in here was tested on the TANGLED-100 dataset. Accuracy achieved during experiments was found to be 74.9%. Robotic simulations were carried out to demonstrate the use of the proposed method in applications of robot. Proposed method is a general method that can be used by robots working in different situations.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4930",
        "title": "Adapted Approach for Fruit Disease Identification using Images",
        "authors": [
            "Shiv Ram Dubey",
            "Anand Singh Jalal"
        ],
        "abstract": "Diseases in fruit cause devastating problem in economic losses and production in agricultural industry worldwide. In this paper, an adaptive approach for the identification of fruit diseases is proposed and experimentally validated. The image processing based proposed approach is composed of the following main steps; in the first step K-Means clustering technique is used for the defect segmentation, in the second step some state of the art features are extracted from the segmented image, and finally images are classified into one of the classes by using a Multi-class Support Vector Machine. We have considered diseases of apple as a test case and evaluated our approach for three types of apple diseases namely apple scab, apple blotch and apple rot. Our experimental results express that the proposed solution can significantly support accurate detection and automatic identification of fruit diseases. The classification accuracy for the proposed solution is achieved up to 93%.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4969",
        "title": "Sparsity Based Methods for Overparameterized Variational Problems",
        "authors": [
            "Raja Giryes",
            "Michael Elad",
            "Alfred M. Bruckstein"
        ],
        "abstract": "Two complementary approaches have been extensively used in signal and image processing leading to novel results, the sparse representation methodology and the variational strategy. Recently, a new sparsity based model has been proposed, the cosparse analysis framework, which may potentially help in bridging sparse approximation based methods to the traditional total-variation minimization. Based on this, we introduce a sparsity based framework for solving overparameterized variational problems. The latter has been used to improve the estimation of optical flow and also for general denoising of signals and images. However, the recovery of the space varying parameters involved was not adequately addressed by traditional variational methods. We first demonstrate the efficiency of the new framework for one dimensional signals in recovering a piecewise linear and polynomial function. Then, we illustrate how the new technique can be used for denoising and segmentation of images.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5047",
        "title": "Single camera pose estimation using Bayesian filtering and Kinect motion priors",
        "authors": [
            "Michael Burke",
            "Joan Lasenby"
        ],
        "abstract": "Traditional approaches to upper body pose estimation using monocular vision rely on complex body models and a large variety of geometric constraints. We argue that this is not ideal and somewhat inelegant as it results in large processing burdens, and instead attempt to incorporate these constraints through priors obtained directly from training data. A prior distribution covering the probability of a human pose occurring is used to incorporate likely human poses. This distribution is obtained offline, by fitting a Gaussian mixture model to a large dataset of recorded human body poses, tracked using a Kinect sensor. We combine this prior information with a random walk transition model to obtain an upper body model, suitable for use within a recursive Bayesian filtering framework. Our model can be viewed as a mixture of discrete Ornstein-Uhlenbeck processes, in that states behave as random walks, but drift towards a set of typically observed poses. This model is combined with measurements of the human head and hand positions, using recursive Bayesian estimation to incorporate temporal information. Measurements are obtained using face detection and a simple skin colour hand detector, trained using the detected face. The suggested model is designed with analytical tractability in mind and we show that the pose tracking can be Rao-Blackwellised using the mixture Kalman filter, allowing for computational efficiency while still incorporating bio-mechanical properties of the upper body. In addition, the use of the proposed upper body model allows reliable three-dimensional pose estimates to be obtained indirectly for a number of joints that are often difficult to detect using traditional object recognition strategies. Comparisons with Kinect sensor results and the state of the art in 2D pose estimation highlight the efficacy of the proposed approach.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5164",
        "title": "Multi-ellipses detection on images inspired by collective animal behavior",
        "authors": [
            "Erik Cuevas",
            "Maurici Gonzalez",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "This paper presents a novel and effective technique for extracting multiple ellipses from an image. The approach employs an evolutionary algorithm to mimic the way animals behave collectively assuming the overall detection process as a multi-modal optimization problem. In the algorithm, searcher agents emulate a group of animals that interact to each other using simple biological rules which are modeled as evolutionary operators. In turn, such operators are applied to each agent considering that the complete group has a memory to store optimal solutions (ellipses) seen so-far by applying a competition principle. The detector uses a combination of five edge points as parameters to determine ellipse candidates (possible solutions) while a matching function determines if such ellipse candidates are actually present in the image. Guided by the values of such matching functions, the set of encoded candidate ellipses are evolved through the evolutionary algorithm so that the best candidates can be fitted into the actual ellipses within the image. Just after the optimization process ends, an analysis over the embedded memory is executed in order to find the best obtained solution (the best ellipse) and significant local minima (remaining ellipses). Experimental results over several complex synthetic and natural images have validated the efficiency of the proposed technique regarding accuracy, speed and robustness.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5248",
        "title": "Dynamic Hierarchical Bayesian Network for Arabic Handwritten Word Recognition",
        "authors": [
            "Khaoula jayech",
            "Nesrine Trimech",
            "Mohamed Ali Mahjoub",
            "Najoua Essoukri Ben Amara"
        ],
        "abstract": "This paper presents a new probabilistic graphical model used to model and recognize words representing the names of Tunisian cities. In fact, this work is based on a dynamic hierarchical Bayesian network. The aim is to find the best model of Arabic handwriting to reduce the complexity of the recognition process by permitting the partial recognition. Actually, we propose a segmentation of the word based on smoothing the vertical histogram projection using different width values to reduce the error of segmentation. Then, we extract the characteristics of each cell using the Zernike and HU moments, which are invariant to rotation, translation and scaling. Our approach is tested using the IFN / ENIT database, and the experiment results are very promising.\n    ",
        "submission_date": "2014-05-20T00:00:00",
        "last_modified_date": "2014-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5406",
        "title": "Circle detection on images using Learning Automata",
        "authors": [
            "Erik Cuevas",
            "Fernando Wario",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "Circle detection over digital images has received considerable attention from the computer vision community over the last few years devoting a tremendous amount of research seeking for an optimal detector. This article presents an algorithm for the automatic detection of circular shapes from complicated and noisy images with no consideration of conventional Hough transform principles. The proposed algorithm is based on Learning Automata (LA) which is a probabilistic optimization method that explores an unknown random environment by progressively improving the performance via a reinforcement signal (objective function). The approach uses the encoding of three non-collinear points as a candidate circle over the edge image. A reinforcement signal (matching function) indicates if such candidate circles are actually present in the edge map. Guided by the values of such reinforcement signal, the probability set of the encoded candidate circles is modified through the LA algorithm so that they can fit to the actual circles on the edge map. Experimental results over several complex synthetic and natural images have validated the efficiency of the proposed technique regarding accuracy, speed and robustness.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5422",
        "title": "Robust Fuzzy corner detector",
        "authors": [
            "Erik Cuevas",
            "Daniel Zaldivar",
            "Marco Perez",
            "Edgar Sanchez",
            "Marte Ramirez"
        ],
        "abstract": "Reliable corner detection is an important task in determining the shape of different regions within an image. Real-life image data are always imprecise due to inherent uncertainties that may arise from the imaging process such as defocusing, illumination changes, noise, etc. Therefore, the localization and detection of corners has become a difficult task to accomplish under such imperfect situations. On the other hand, Fuzzy systems are well known for their efficient handling of impreciseness and incompleteness, which make them inherently suitable for modelling corner properties by means of a rule-based fuzzy system. The paper presents a corner detection algorithm which employs such fuzzy reasoning. The robustness of the proposed algorithm is compared to well-known conventional corner detectors and its performance is also tested over a number of benchmark images to illustrate the efficiency of the algorithm under uncertainty.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5488",
        "title": "On Learning Where To Look",
        "authors": [
            "Marc'Aurelio Ranzato"
        ],
        "abstract": "Current automatic vision systems face two major challenges: scalability and extreme variability of appearance. First, the computational time required to process an image typically scales linearly with the number of pixels in the image, therefore limiting the resolution of input images to thumbnail size. Second, variability in appearance and pose of the objects constitute a major hurdle for robust recognition and detection. In this work, we propose a model that makes baby steps towards addressing these challenges. We describe a learning based method that recognizes objects through a series of glimpses. This system performs an amount of computation that scales with the complexity of the input rather than its number of pixels. Moreover, the proposed method is potentially more robust to changes in appearance since its parameters are learned in a data driven manner. Preliminary experiments on a handwritten dataset of digits demonstrate the computational advantages of this approach.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5494",
        "title": "Iterative Non-Local Shrinkage Algorithm for MR Image Reconstruction",
        "authors": [
            "Yasir Q. Moshin",
            "Greg Ongie",
            "Mathews Jacob"
        ],
        "abstract": "We introduce a fast iterative non-local shrinkage algorithm to recover MRI data from undersampled Fourier measurements. This approach is enabled by the reformulation of current non-local schemes as an alternating algorithm to minimize a global criterion. The proposed algorithm alternates between a non-local shrinkage step and a quadratic subproblem. We derive analytical shrinkage rules for several penalties that are relevant in non-local regularization. The redundancy in the searches used to evaluate the shrinkage steps are exploited using filtering operations. The resulting algorithm is observed to be considerably faster than current alternating non-local algorithms. The comparisons of the proposed scheme with state-of-the-art regularization schemes show a considerable reduction in alias artifacts and preservation of edges.\n    ",
        "submission_date": "2014-05-15T00:00:00",
        "last_modified_date": "2014-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5531",
        "title": "Fast algorithm for Multiple-Circle detection on images using Learning Automata",
        "authors": [
            "Erik Cuevas",
            "Fernando Wario",
            "Valentin Osuna",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "Hough transform (HT) has been the most common method for circle detection exhibiting robustness but adversely demanding a considerable computational load and large storage. Alternative approaches include heuristic methods that employ iterative optimization procedures for detecting multiple circles under the inconvenience that only one circle can be marked at each optimization cycle demanding a longer execution time. On the other hand, Learning Automata (LA) is a heuristic method to solve complex multi-modal optimization problems. Although LA converges to just one global minimum, the final probability distribution holds valuable information regarding other local minima which have emerged during the optimization process. The detection process is considered as a multi-modal optimization problem, allowing the detection of multiple circular shapes through only one optimization procedure. The algorithm uses a combination of three edge points as parameters to determine circles candidates. A reinforcement signal determines if such circle candidates are actually present at the image. Guided by the values of such reinforcement signal, the set of encoded candidate circles are evolved using the LA so that they can fit into actual circular shapes over the edge-only map of the image. The overall approach is a fast multiple-circle detector despite facing complicated conditions.\n    ",
        "submission_date": "2014-05-21T00:00:00",
        "last_modified_date": "2014-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5732",
        "title": "Self-tuned Visual Subclass Learning with Shared Samples An Incremental Approach",
        "authors": [
            "Hossein Azizpour",
            "Stefan Carlsson"
        ],
        "abstract": "Computer vision tasks are traditionally defined and evaluated using semantic categories. However, it is known to the field that semantic classes do not necessarily correspond to a unique visual class (e.g. inside and outside of a car). Furthermore, many of the feasible learning techniques at hand cannot model a visual class which appears consistent to the human eye. These problems have motivated the use of 1) Unsupervised or supervised clustering as a preprocessing step to identify the visual subclasses to be used in a mixture-of-experts learning regime. 2) Felzenszwalb et al. part model and other works model mixture assignment with latent variables which is optimized during learning 3) Highly non-linear classifiers which are inherently capable of modelling multi-modal input space but are inefficient at the test time. In this work, we promote an incremental view over the recognition of semantic classes with varied appearances. We propose an optimization technique which incrementally finds maximal visual subclasses in a regularized risk minimization framework. Our proposed approach unifies the clustering and classification steps in a single algorithm. The importance of this approach is its compliance with the classification via the fact that it does not need to know about the number of clusters, the representation and similarity measures used in pre-processing clustering methods a priori. Following this approach we show both qualitatively and quantitatively significant results. We show that the visual subclasses demonstrate a long tail distribution. Finally, we show that state of the art object detection methods (e.g. DPM) are unable to use the tails of this distribution comprising 50\\% of the training samples. In fact we show that DPM performance slightly increases on average by the removal of this half of the data.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5737",
        "title": "Semi-supervised Spectral Clustering for Classification",
        "authors": [
            "Arif Mahmood",
            "Ajmal S. Mian"
        ],
        "abstract": "We propose a Classification Via Clustering (CVC) algorithm which enables existing clustering methods to be efficiently employed in classification problems. In CVC, training and test data are co-clustered and class-cluster distributions are used to find the label of the test data. To determine an efficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC) algorithm is proposed. Clusters are obtained by hierarchically applying two-way NCut by using signs of the Fiedler vector of the normalized graph Laplacian. To this end, a Direct Fiedler Vector Computation algorithm is proposed. The graph cut is based on the data structure and does not consider labels. Labels are used only to define the stopping criterion for graph cut. We propose clustering to be performed on the Grassmannian manifolds facilitating the formation of spectral ensembles. The proposed algorithm outperformed state-of-the-art image-set classification algorithms on five standard datasets.\n    ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.5769",
        "title": "Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT",
        "authors": [
            "Philipp Fischer",
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "abstract": "Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.\nThis paper has been merged with ",
        "submission_date": "2014-05-22T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6012",
        "title": "On the Optimal Solution of Weighted Nuclear Norm Minimization",
        "authors": [
            "Qi Xie",
            "Deyu Meng",
            "Shuhang Gu",
            "Lei Zhang",
            "Wangmeng Zuo",
            "Xiangchu Feng",
            "Zongben Xu"
        ],
        "abstract": "In recent years, the nuclear norm minimization (NNM) problem has been attracting much attention in computer vision and machine learning. The NNM problem is capitalized on its convexity and it can be solved efficiently. The standard nuclear norm regularizes all singular values equally, which is however not flexible enough to fit real scenarios. Weighted nuclear norm minimization (WNNM) is a natural extension and generalization of NNM. By assigning properly different weights to different singular values, WNNM can lead to state-of-the-art results in applications such as image denoising. Nevertheless, so far the global optimal solution of WNNM problem is not completely solved yet due to its non-convexity in general cases. In this article, we study the theoretical properties of WNNM and prove that WNNM can be equivalently transformed into a quadratic programming problem with linear constraints. This implies that WNNM is equivalent to a convex problem and its global optimum can be readily achieved by off-the-shelf convex optimization solvers. We further show that when the weights are non-descending, the globally optimal solution of WNNM can be obtained in closed-form.\n    ",
        "submission_date": "2014-05-23T00:00:00",
        "last_modified_date": "2014-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6130",
        "title": "A Study of Local Binary Pattern Method for Facial Expression Detection",
        "authors": [
            "Ms.Drashti H. Bhatt",
            "Mr.Kirit R. Rathod",
            "Mr.Shardul J. Agravat"
        ],
        "abstract": "Face detection is a basic task for expression recognition. The reliability of face detection & face recognition approach has a major role on the performance and usability of the entire system. There are several ways to undergo face detection & recognition. We can use Image Processing Operations, various classifiers, filters or virtual machines for the former. Various strategies are being available for Facial Expression Detection. The field of facial expression detection can have various applications along with its importance & can be interacted between human being & computer. Many few options are available to identify a face in an image in accurate & efficient manner. Local Binary Pattern (LBP) based texture algorithms have gained popularity in these years. LBP is an effective approach to have facial expression recognition & is a feature-based approach.\n    ",
        "submission_date": "2014-02-04T00:00:00",
        "last_modified_date": "2014-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6132",
        "title": "Comparative analysis of common edge detection techniques in context of object extraction",
        "authors": [
            "S.K. Katiyar",
            "P.V. Arun"
        ],
        "abstract": "Edges characterize boundaries and are therefore a problem of practical importance in remote ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6133",
        "title": "A review over the applicability of image entropy in analyses of remote sensing datasets",
        "authors": [
            "S.K. Katiyar",
            "P.V. Arun"
        ],
        "abstract": "Entropy is the measure of uncertainty in any data and is adopted for maximisation of mutual information in many remote sensing operations. The availability of wide entropy variations motivated us for an investigation over the suitability preference of these versions to specific operations.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6135",
        "title": "Cellular Automata based adaptive resampling technique for the processing of remotely sensed imagery",
        "authors": [
            "S.K. Katiyar",
            "P.V. Arun"
        ],
        "abstract": "Resampling techniques are being widely used at different stages of satellite image processing. The existing methodologies cannot perfectly recover features from a completely under sampled image and hence an intelligent adaptive resampling methodology is required. We address these issues and adopt an error metric from the available literature to define interpolation quality. We also propose a new resampling scheme that adapts itself with regard to the pixel and texture variation in the image. The proposed CNN based hybrid method has been found to perform better than the existing methods as it adapts itself with reference to the image features.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6136",
        "title": "An evolutionary computational based approach towards automatic image registration",
        "authors": [
            "P.V. Arun",
            "S.K. Katiyar"
        ],
        "abstract": "Image registration is a key component of various image processing operations which involve the analysis of different image data sets. Automatic image registration domains have witnessed the application of many intelligent methodologies over the past decade; however inability to properly model object shape as well as contextual information had limited the attainable accuracy. In this paper, we propose a framework for accurate feature shape modeling and adaptive resampling using advanced techniques such as Vector Machines, Cellular Neural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to be effective in improving feature matching as well as resampling stages of registration and complexity of the approach has been considerably reduced using corset optimization The salient features of this work are cellular neural network approach based SIFT feature point optimisation, adaptive resampling and intelligent object modelling. Developed methodology has been compared with contemporary methods using different statistical measures. Investigations over various satellite images revealed that considerable success was achieved with the approach. System has dynamically used spectral and spatial information for representing contextual knowledge using CNN-prolog approach. Methodology also illustrated to be effective in providing intelligent interpretation and adaptive resampling.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6137",
        "title": "An enhanced neural network based approach towards object extraction",
        "authors": [
            "S.K. Katiyar",
            "P.V. Arun"
        ],
        "abstract": "The improvements in spectral and spatial resolution of the satellite images have facilitated the automatic extraction and identification of the features from satellite images and aerial photographs. An automatic object extraction method is presented for extracting and identifying the various objects from satellite images and the accuracy of the system is verified with regard to IRS satellite images. The system is based on neural network and simulates the process of visual interpretation from remote sensing images and hence increases the efficiency of image analysis. This approach obtains the basic characteristics of the various features and the performance is enhanced by the automatic learning approach, intelligent interpretation, and intelligent interpolation. The major advantage of the method is its simplicity and that the system identifies the features not only based on pixel value but also based on the shape, haralick features etc of the objects. Further the system allows flexibility for identifying the features within the same category based on size and shape. The successful application of the system verified its effectiveness and the accuracy of the system were assessed by ground truth verification.\n    ",
        "submission_date": "2014-02-05T00:00:00",
        "last_modified_date": "2014-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6159",
        "title": "A Bi-clustering Framework for Consensus Problems",
        "authors": [
            "Mariano Tepper",
            "Guillermo Sapiro"
        ],
        "abstract": "We consider grouping as a general characterization for problems such as clustering, community detection in networks, and multiple parametric model estimation. We are interested in merging solutions from different grouping algorithms, distilling all their good qualities into a consensus solution. In this paper, we propose a bi-clustering framework and perspective for reaching consensus in such grouping problems. In particular, this is the first time that the task of finding/fitting multiple parametric models to a dataset is formally posed as a consensus problem. We highlight the equivalence of these tasks and establish the connection with the computational Gestalt program, that seeks to provide a psychologically-inspired detection theory for visual events. We also present a simple but powerful bi-clustering algorithm, specially tuned to the nature of the problem we address, though general enough to handle many different instances inscribed within our characterization. The presentation is accompanied with diverse and extensive experimental results in clustering, community detection, and multiple parametric model estimation in image processing applications.\n    ",
        "submission_date": "2014-04-30T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6166",
        "title": "Real Time Speckle Image De-Noising",
        "authors": [
            "D.Sachin Kumar",
            "P.R.Seshadri",
            "N.Vaishnav",
            "Dr.Saraswathi Janaki"
        ],
        "abstract": "The paper presents real time speckle de-noising based on activity computation algorithm and wavelet transform. Speckles arise in an image when laser light is reflected from an illuminated surface. The process involves detection of speckles in an image by obtaining a number of frames of the same object under different illumination or angle and comparing the frames for the granular computation and de-noising the same on presence of greater activity index. The project can be implemented in FPGA (Field Programmable Gate Array) technology. The results can be shown that the used activity computation algorithm and wavelet transform has better accuracy in the process of speckle detection and de-noising.\n    ",
        "submission_date": "2014-04-10T00:00:00",
        "last_modified_date": "2014-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6168",
        "title": "Human Face as human single identity",
        "authors": [
            "Spits Warnars"
        ],
        "abstract": "Human face as a physical human recognition can be used as a unique identity for computer to recognize human by transforming human face with face algorithm as simple text number which can be primary key for human. Human face as single identity for human will be done by making a huge and large world centre human face database, where the human face around the world will be recorded from time to time and from generation to generation. Architecture database will be divided become human face image database which will save human face images and human face output code which will save human face output code as a transformation human face image with face algorithm. As an improvement the slightly and simple human face output code database will make human face searching process become more fast. Transaction with human face as a transaction without card can make human no need their card for the transaction and office automation and banking system as an example for implementation architecture. As an addition suspect human face database can be extended for fighting crime and terrorism by doing surveillance and searching suspect human face around the world.\n    ",
        "submission_date": "2014-04-08T00:00:00",
        "last_modified_date": "2014-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6177",
        "title": "Automated Fabric Defect Inspection: A Survey of Classifiers",
        "authors": [
            "Md. Tarek Habib",
            "Rahat Hossain Faisal",
            "M. Rokonuzzaman",
            "Farruk Ahmed"
        ],
        "abstract": "Quality control at each stage of production in textile industry has become a key factor to retaining the existence in the highly competitive global market. Problems of manual fabric defect inspection are lack of accuracy and high time consumption, where early and accurate fabric defect detection is a significant phase of quality control. Computer vision based, i.e. automated fabric defect inspection systems are thought by many researchers of different countries to be very useful to resolve these problems. There are two major challenges to be resolved to attain a successful automated fabric defect inspection system. They are defect detection and defect classification. In this work, we discuss different techniques used for automated fabric defect classification, then show a survey of classifiers used in automated fabric defect inspection systems, and finally, compare these classifiers by using performance metrics. This work is expected to be very useful for the researchers in the area of automated fabric defect inspection to understand and evaluate the many potential options in this field.\n    ",
        "submission_date": "2014-02-14T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6261",
        "title": "Geometric Polynomial Constraints in Higher-Order Graph Matching",
        "authors": [
            "Mayank Bansal",
            "Kostas Daniilidis"
        ],
        "abstract": "Correspondence is a ubiquitous problem in computer vision and graph matching has been a natural way to formalize correspondence as an optimization problem. Recently, graph matching solvers have included higher-order terms representing affinities beyond the unary and pairwise level. Such higher-order terms have a particular appeal for geometric constraints that include three or more correspondences like the PnP 2D-3D pose problems. In this paper, we address the problem of finding correspondences in the absence of unary or pairwise constraints as it emerges in problems where unary appearance similarity like SIFT matches is not available. Current higher order matching approaches have targeted problems where higher order affinity can simply be formulated as a difference of invariances such as lengths, angles, or cross-ratios. In this paper, we present a method of how to apply geometric constraints modeled as polynomial equation systems. As opposed to RANSAC where such systems have to be solved and then tested for inlier hypotheses, our constraints are derived as a single affinity weight based on $n>2$ hypothesized correspondences without solving the polynomial system. Since the result is directly a correspondence without a transformation model, our approach supports correspondence matching in the presence of multiple geometric transforms like articulated motions.\n    ",
        "submission_date": "2014-05-24T00:00:00",
        "last_modified_date": "2014-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6275",
        "title": "Improvements and Experiments of a Compact Statistical Background Model",
        "authors": [
            "Dong Liang",
            "Shun'ichi Kaneko"
        ],
        "abstract": "Change detection plays an important role in most video-based applications. The first stage is to build appropriate background model, which is now becoming increasingly complex as more sophisticated statistical approaches are introduced to cover challenging situations and provide reliable detection. This paper reports a simple and intuitive statistical model based on deeper learning spatial correlation among pixels: For each observed pixel, we select a group of supporting pixels with high correlation, and then use a single Gaussian to model the intensity deviations between the observed pixel and the supporting ones. In addition, a multi-channel model updating is integrated on-line and a temporal intensity constraint for each pixel is defined. Although this method is mainly designed for coping with sudden illumination changes, experimental results using all the video sequences provided on ",
        "submission_date": "2014-05-24T00:00:00",
        "last_modified_date": "2014-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6434",
        "title": "Multi-view Metric Learning for Multi-view Video Summarization",
        "authors": [
            "Yanwei Fu",
            "Lingbo Wang",
            "Yanwen Guo"
        ],
        "abstract": "Traditional methods on video summarization are designed to generate summaries for single-view video records; and thus they cannot fully exploit the redundancy in multi-view video records. In this paper, we present a multi-view metric learning framework for multi-view video summarization that combines the advantages of maximum margin clustering with the disagreement minimization criterion. The learning framework thus has the ability to find a metric that best separates the data, and meanwhile to force the learned metric to maintain original intrinsic information between data points, for example geometric information. Facilitated by such a framework, a systematic solution to the multi-view video summarization problem is developed. To the best of our knowledge, it is the first time to address multi-view video summarization from the viewpoint of metric learning. The effectiveness of the proposed method is demonstrated by experiments.\n    ",
        "submission_date": "2014-05-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6472",
        "title": "Fast and Robust Archetypal Analysis for Representation Learning",
        "authors": [
            "Yuansi Chen",
            "Julien Mairal",
            "Zaid Harchaoui"
        ],
        "abstract": "We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization.\n    ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6563",
        "title": "Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D Articulated Bodies",
        "authors": [
            "Fabio Cuzzolin",
            "Diana Mateus",
            "Radu Horaud"
        ],
        "abstract": "In motion analysis and understanding it is important to be able to fit a suitable model or structure to the temporal series of observed data, in order to describe motion patterns in a compact way, and to discriminate between them. In an unsupervised context, i.e., no prior model of the moving object(s) is available, such a structure has to be learned from the data in a bottom-up fashion. In recent times, volumetric approaches in which the motion is captured from a number of cameras and a voxel-set representation of the body is built from the camera views, have gained ground due to attractive features such as inherent view-invariance and robustness to occlusions. Automatic, unsupervised segmentation of moving bodies along entire sequences, in a temporally-coherent and robust way, has the potential to provide a means of constructing a bottom-up model of the moving body, and track motion cues that may be later exploited for motion classification. Spectral methods such as locally linear embedding (LLE) can be useful in this context, as they preserve \"protrusions\", i.e., high-curvature regions of the 3D volume, of articulated shapes, while improving their separation in a lower dimensional space, making them in this way easier to cluster. In this paper we therefore propose a spectral approach to unsupervised and temporally-coherent body-protrusion segmentation along time sequences. Volumetric shapes are clustered in an embedding space, clusters are propagated in time to ensure coherence, and merged or split to accommodate changes in the body's topology. Experiments on both synthetic and real sequences of dense voxel-set data are shown. This supports the ability of the proposed method to cluster body-parts consistently over time in a totally unsupervised fashion, its robustness to sampling density and shape quality, and its potential for bottom-up model construction\n    ",
        "submission_date": "2014-05-26T00:00:00",
        "last_modified_date": "2014-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6914",
        "title": "Supervised Dictionary Learning by a Variational Bayesian Group Sparse Nonnegative Matrix Factorization",
        "authors": [
            "Ivan Ivek"
        ],
        "abstract": "Nonnegative matrix factorization (NMF) with group sparsity constraints is formulated as a probabilistic graphical model and, assuming some observed data have been generated by the model, a feasible variational Bayesian algorithm is derived for learning model parameters. When used in a supervised learning scenario, NMF is most often utilized as an unsupervised feature extractor followed by classification in the obtained feature subspace. Having mapped the class labels to a more general concept of groups which underlie sparsity of the coefficients, what the proposed group sparse NMF model allows is incorporating class label information to find low dimensional label-driven dictionaries which not only aim to represent the data faithfully, but are also suitable for class discrimination. Experiments performed in face recognition and facial expression recognition domains point to advantages of classification in such label-driven feature subspaces over classification in feature subspaces obtained in an unsupervised manner.\n    ",
        "submission_date": "2014-05-27T00:00:00",
        "last_modified_date": "2014-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7032",
        "title": "An FPGA-based Parallel Architecture for Face Detection using Mixed Color Models",
        "authors": [
            "Luo Tao",
            "Shi zaifeng"
        ],
        "abstract": "In this paper, a reliable method for detecting human faces in color images is proposed. This system firstly detects skin color in YCgCr and YIQ color space, then filters binary texture and the result is morphological processed, finally converts skin tone to the preferred skin color configured by users in YIQ color space. The real-time adjusting circuit is implemented and some of simulation results are given out. Experimental results demonstrate that the method has achieved high rates and low false positives, another advantage is its simplicity and minor computational costs.\n    ",
        "submission_date": "2014-05-27T00:00:00",
        "last_modified_date": "2014-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7229",
        "title": "A Multi-threshold Segmentation Approach Based on Artificial Bee Colony Optimization",
        "authors": [
            "Erik Cuevas",
            "Felipe Sencion",
            "Daniel Zaldivar",
            "Marco Perez",
            "Humberto Sossa"
        ],
        "abstract": "This paper explores the use of the Artificial Bee Colony (ABC) algorithm to compute threshold selection for image segmentation. ABC is a heuristic algorithm motivated by the intelligent behavior of honey-bees which has been successfully employed to solve complex optimization problems. In this approach, an image 1D histogram is approximated through a Gaussian mixture model whose parameters are calculated by the ABC algorithm. For the approximation scheme, each Gaussian function represents a pixel class and therefore a threshold. Unlike the Expectation Maximization (EM) algorithm, the ABC based method shows fast convergence and low sensitivity to initial conditions. Remarkably, it also improves complex time consuming computations commonly required by gradient-based methods. Experimental results demonstrate the algorithms ability to perform automatic multi threshold selection yet showing interesting advantages by comparison to other well known algorithms.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7242",
        "title": "Circle detection by Harmony Search Optimization",
        "authors": [
            "Erik Cuevas",
            "Noe Ortega",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "Automatic circle detection in digital images has received considerable attention over the last years in computer vision as several efforts have aimed for an optimal circle detector. This paper presents an algorithm for automatic detection of circular shapes that considers the overall process as an optimization problem. The approach is based on the Harmony Search Algorithm (HSA), a derivative free meta-heuristic optimization algorithm inspired by musicians while improvising new harmonies. The algorithm uses the encoding of three points as candidate circles (harmonies) over the edge-only image. An objective function evaluates (harmony quality) if such candidate circles are actually present in the edge image. Guided by the values of this objective function, the set of encoded candidate circles are evolved using the HSA so that they can fit to the actual circles on the edge map of the image (optimal harmony). Experimental results from several tests on synthetic and natural images with a varying complexity range have been included to validate the efficiency of the proposed technique regarding accuracy, speed and robustness.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7361",
        "title": "Seeking multi-thresholds for image segmentation with Learning Automata",
        "authors": [
            "Erik Cuevas",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "This paper explores the use of the Learning Automata (LA) algorithm to compute threshold selection for image segmentation as it is a critical preprocessing step for image analysis, pattern recognition and computer vision. LA is a heuristic method which is able to solve complex optimization problems with interesting results in parameter estimation. Despite other techniques commonly seek through the parameter map, LA explores in the probability space providing appropriate convergence properties and robustness. The segmentation task is therefore considered as an optimization problem and the LA is used to generate the image multi-threshold separation. In this approach, one 1D histogram of a given image is approximated through a Gaussian mixture model whose parameters are calculated using the LA algorithm. Each Gaussian function approximating the histogram represents a pixel class and therefore a threshold point. The method shows fast convergence avoiding the typical sensitivity to initial conditions such as the Expectation Maximization (EM) algorithm or the complex time-consuming computations commonly found in gradient methods. Experimental results demonstrate the algorithm ability to perform automatic multi-threshold selection and show interesting advantages as it is compared to other algorithms solving the same task.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7362",
        "title": "Circle detection using Discrete Differential Evolution Optimization",
        "authors": [
            "Erik Cuevas",
            "Daniel Zaldivar",
            "Marco Perez",
            "Marte Ramirez"
        ],
        "abstract": "This paper introduces a circle detection method based on Differential Evolution (DE) optimization. Just as circle detection has been lately considered as a fundamental component for many computer vision algorithms, DE has evolved as a successful heuristic method for solving complex optimization problems, still keeping a simple structure and an easy implementation. It has also shown advantageous convergence properties and remarkable robustness. The detection process is considered similar to a combinational optimization problem. The algorithm uses the combination of three edge points as parameters to determine circles candidates in the scene yielding a reduction of the search space. The objective function determines if some circle candidates are actually present in the image. This paper focuses particularly on one DE-based algorithm known as the Discrete Differential Evolution (DDE), which eventually has shown better results than the original DE in particular for solving combinatorial problems. In the DDE, suitable conversion routines are incorporated into the DE, aiming to operate from integer values to real values and then getting integer values back, following the crossover operation. The final algorithm is a fast circle detector that locates circles with sub-pixel accuracy even considering complicated conditions and noisy images. Experimental results on several synthetic and natural images with varying range of complexity validate the efficiency of the proposed technique considering accuracy, speed, and robustness.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7406",
        "title": "A Comparison of Nature Inspired Algorithms for Multi-threshold Image Segmentation",
        "authors": [
            "Valent\u00edn Osuna-Enciso",
            "Erik Cuevas",
            "Humberto Sossa"
        ],
        "abstract": "In the field of image analysis, segmentation is one of the most important preprocessing steps. One way to achieve segmentation is by mean of threshold selection, where each pixel that belongs to a determined class islabeled according to the selected threshold, giving as a result pixel groups that share visual characteristics in the image. Several methods have been proposed in order to solve threshold selectionproblems; in this work, it is used the method based on the mixture of Gaussian functions to approximate the 1D histogram of a gray level image and whose parameters are calculated using three nature inspired algorithms (Particle Swarm Optimization, Artificial Bee Colony Optimization and Differential Evolution). Each Gaussian function approximates thehistogram, representing a pixel class and therefore a threshold point. Experimental results are shown, comparing in quantitative and qualitative fashion as well as the main advantages and drawbacks of each algorithm, applied to multi-threshold problem.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7545",
        "title": "Feature sampling and partitioning for visual vocabulary generation on large action classification datasets",
        "authors": [
            "Michael Sapienza",
            "Fabio Cuzzolin",
            "Philip H.S. Torr"
        ],
        "abstract": "The recent trend in action recognition is towards larger datasets, an increasing number of action classes and larger visual vocabularies. State-of-the-art human action classification in challenging video data is currently based on a bag-of-visual-words pipeline in which space-time features are aggregated globally to form a histogram. The strategies chosen to sample features and construct a visual vocabulary are critical to performance, in fact often dominating performance. In this work we provide a critical evaluation of various approaches to building a vocabulary and show that good practises do have a significant impact. By subsampling and partitioning features strategically, we are able to achieve state-of-the-art results on 5 major action recognition datasets using relatively small visual vocabularies.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7626",
        "title": "Classification of Basmati Rice Grain Variety using Image Processing and Principal Component Analysis",
        "authors": [
            "Rubi Kambo",
            "Amit Yerpude"
        ],
        "abstract": "All important decisions about the variety of rice grain end product are based on the different features of rice ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7718",
        "title": "Deformation corrected compressed sensing (DC-CS): a novel framework for accelerated dynamic MRI",
        "authors": [
            "Sajan Goud Lingala",
            "Edward DiBella",
            "Mathews Jacob"
        ],
        "abstract": "We propose a novel deformation corrected compressed sensing (DC-CS) framework to recover dynamic magnetic resonance images from undersampled measurements. We introduce a generalized formulation that is capable of handling a wide class of sparsity/compactness priors on the deformation corrected dynamic signal. In this work, we consider example compactness priors such as sparsity in temporal Fourier domain, sparsity in temporal finite difference domain, and nuclear norm penalty to exploit low rank structure. Using variable splitting, we decouple the complex optimization problem to simpler and well understood sub problems; the resulting algorithm alternates between simple steps of shrinkage based denoising, deformable registration, and a quadratic optimization step. Additionally, we employ efficient continuation strategies to minimize the risk of convergence to local minima. The proposed formulation contrasts with existing DC-CS schemes that are customized for free breathing cardiac cine applications, and other schemes that rely on fully sampled reference frames or navigator signals to estimate the deformation parameters. The efficient decoupling enabled by the proposed scheme allows its application to a wide range of applications including contrast enhanced dynamic MRI. Through experiments on numerical phantom and in vivo myocardial perfusion MRI datasets, we demonstrate the utility of the proposed DC-CS scheme in providing robust reconstructions with reduced motion artifacts over classical compressed sensing schemes that utilize the compact priors on the original deformation un-corrected signal.\n    ",
        "submission_date": "2014-05-29T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7771",
        "title": "DEM Registration and Error Analysis using ASCII values",
        "authors": [
            "Suma Dawn",
            "Vikas Saxena",
            "Bhu Dev Sharma"
        ],
        "abstract": "Digital Elevation Model (DEM), while providing a bare earth look, is heavily used in many applications including construction modeling, visualization, and GIS. Their registration techniques have not been explored much. Methods like Coarse-to-fine or pyramid making are common in DEM-to-image or DEM-to-map registration. Self-consistency measure is used to detect any change in terrain elevation and hence was used for DEM-to-DEM registration. But these methods apart from being time and complexity intensive, lack in error matrix evaluation. This paper gives a method of registration of DEMs using specified height values as control points by initially converting these DEMs to ASCII files. These control points may be found by two mannerisms - either by direct detection of appropriate height data in ASCII files or by edge matching along congruous quadrangle of the control point, followed by sub-graph matching. Error analysis for the same has also been done.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7903",
        "title": "The Shortlist Method for Fast Computation of the Earth Mover's Distance and Finding Optimal Solutions to Transportation Problems",
        "authors": [
            "Carsten Gottschlich",
            "Dominic Schuhmacher"
        ],
        "abstract": "Finding solutions to the classical transportation problem is of great importance, since this optimization problem arises in many engineering and computer science applications. Especially the Earth Mover's Distance is used in a plethora of applications ranging from content-based image retrieval, shape matching, fingerprint recognition, object tracking and phishing web page detection to computing color differences in linguistics and biology. Our starting point is the well-known revised simplex algorithm, which iteratively improves a feasible solution to optimality. The Shortlist Method that we propose substantially reduces the number of candidates inspected for improving the solution, while at the same time balancing the number of pivots required. Tests on simulated benchmarks demonstrate a considerable reduction in computation time for the new method as compared to the usual revised simplex algorithm implemented with state-of-the-art initialization and pivot strategies. As a consequence, the Shortlist Method facilitates the computation of large scale transportation problems in viable time. In addition we describe a novel method for finding an initial feasible solution which we coin Modified Russell's Method.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0023",
        "title": "Circle detection using electro-magnetism optimization",
        "authors": [
            "Erik Cuevas",
            "Diego Oliva",
            "Daniel Zaldivar",
            "Marco Perez-Cisneros",
            "Humberto Sossa"
        ],
        "abstract": "This paper describes a circle detection method based on Electromagnetism-Like Optimization (EMO). Circle detection has received considerable attention over the last years thanks to its relevance for many computer vision tasks. EMO is a heuristic method for solving complex optimization problems inspired in electromagnetism principles. This algorithm searches a solution based in the attraction and repulsion among prototype candidates. In this paper the detection process is considered to be similar to an optimization problem, the algorithm uses the combination of three edge points (x, y, r) as parameters to determine circles candidates in the scene. An objective function determines if such circle candidates are actually present in the image. The EMO algorithm is used to find the circle candidate that is better related with the real circle present in the image according to the objective function. The final algorithm is a fast circle detector that locates circles with sub-pixel accuracy even considering complicated conditions and noisy images.\n    ",
        "submission_date": "2014-05-30T00:00:00",
        "last_modified_date": "2014-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0074",
        "title": "Combined Approach for Image Segmentation",
        "authors": [
            "Shradha Dakhare",
            "Harshal Chowhan",
            "Manoj B.Chandak"
        ],
        "abstract": "Many image segmentation techniques have been developed over the past two decades for segmenting the images, which help for object recognition, occlusion boundary estimation within motion or stereo systems, image compression, image editing.\n",
        "submission_date": "2014-05-31T00:00:00",
        "last_modified_date": "2014-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0132",
        "title": "Seeing the Big Picture: Deep Embedding with Contextual Evidences",
        "authors": [
            "Liang Zheng",
            "Shengjin Wang",
            "Fei He",
            "Qi Tian"
        ],
        "abstract": "In the Bag-of-Words (BoW) model based image retrieval task, the precision of visual matching plays a critical role in improving retrieval performance. Conventionally, local cues of a keypoint are employed. However, such strategy does not consider the contextual evidences of a keypoint, a problem which would lead to the prevalence of false matches. To address this problem, this paper defines \"true match\" as a pair of keypoints which are similar on three levels, i.e., local, regional, and global. Then, a principled probabilistic framework is established, which is capable of implicitly integrating discriminative cues from all these feature levels.\n",
        "submission_date": "2014-06-01T00:00:00",
        "last_modified_date": "2014-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0156",
        "title": "$l_1$-regularized Outlier Isolation and Regression",
        "authors": [
            "Sheng Han",
            "Suzhen Wang",
            "Xinyu Wu"
        ],
        "abstract": "This paper proposed a new regression model called $l_1$-regularized outlier isolation and regression (LOIRE) and a fast algorithm based on block coordinate descent to solve this model. Besides, assuming outliers are gross errors following a Bernoulli process, this paper also presented a Bernoulli estimate model which, in theory, should be very accurate and robust due to its complete elimination of affections caused by outliers. Though this Bernoulli estimate is hard to solve, it could be approximately achieved through a process which takes LOIRE as an important intermediate step. As a result, the approximate Bernoulli estimate is a good combination of Bernoulli estimate's accuracy and LOIRE regression's efficiency with several simulations conducted to strongly verify this point. Moreover, LOIRE can be further extended to realize robust rank factorization which is powerful in recovering low-rank component from massive corruptions. Extensive experimental results showed that the proposed method outperforms state-of-the-art methods like RPCA and GoDec in the aspect of computation speed with a competitive performance.\n    ",
        "submission_date": "2014-06-01T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0231",
        "title": "Ambiguous Proximity Distribution",
        "authors": [
            "Quanquan Wang",
            "Yongping Li"
        ],
        "abstract": "Proximity Distribution Kernel is an effective method for bag-of-featues based image representation. In this paper, we investigate the soft assignment of visual words to image features for proximity distribution. Visual word contribution function is proposed to model ambiguous proximity distributions. Three ambiguous proximity distributions is developed by three ambiguous contribution functions. The experiments are conducted on both classification and retrieval of medical image data sets. The results show that the performance of the proposed methods, Proximity Distribution Kernel (PDK), is better or comparable to the state-of-the-art bag-of-features based image representation methods.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0288",
        "title": "Continuous Action Recognition Based on Sequence Alignment",
        "authors": [
            "Kaustubh Kulkarni",
            "Georgios Evangelidis",
            "Jan Cech",
            "Radu Horaud"
        ],
        "abstract": "Continuous action recognition is more challenging than isolated recognition because classification and segmentation must be simultaneously carried out. We build on the well known dynamic time warping (DTW) framework and devise a novel visual alignment technique, namely dynamic frame warping (DFW), which performs isolated recognition based on per-frame representation of videos, and on aligning a test sequence with a model sequence. Moreover, we propose two extensions which enable to perform recognition concomitant with segmentation, namely one-pass DFW and two-pass DFW. These two methods have their roots in the domain of continuous recognition of speech and, to the best of our knowledge, their extension to continuous visual action recognition has been overlooked. We test and illustrate the proposed techniques with a recently released dataset (RAVEL) and with two public-domain datasets widely used in action recognition (Hollywood-1 and Hollywood-2). We also compare the performances of the proposed isolated and continuous recognition algorithms with several recently published methods.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0289",
        "title": "The constitution of visual perceptual units in the functional architecture of V1",
        "authors": [
            "Alessandro Sarti",
            "Giovanna Citti"
        ],
        "abstract": "Scope of this paper is to consider a mean field neural model which takes into account the functional neurogeometry of the visual cortex modelled as a group of rotations and translations. The model generalizes well known results of Bressloff and Cowan which, in absence of input, accounts for hallucination patterns. The main result of our study consists in showing that in presence of a visual input, the eigenmodes of the linearized operator which become stable represent perceptual units present in the image. The result is strictly related to dimensionality reduction and clustering problems.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0312",
        "title": "Generalized Max Pooling",
        "authors": [
            "Naila Murray",
            "Florent Perronnin"
        ],
        "abstract": "State-of-the-art patch-based image representations involve a pooling operation that aggregates statistics computed from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptors, but only weakly influenced by rare yet potentially highly-informative ones. Max-pooling equalizes the influence of frequent and rare descriptors but is only applicable to representations that rely on count statistics, such as the bag-of-visual-words (BOV) and its soft- and sparse-coding extensions. We propose a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector -- hence the name Generalized Max Pooling (GMP). It involves equalizing the similarity between each patch and the pooled representation, which is shown to be equivalent to re-weighting the per-patch statistics. We show on five public image classification benchmarks that the proposed GMP can lead to significant performance gains with respect to heuristic alternatives.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0588",
        "title": "Image retrieval with hierarchical matching pursuit",
        "authors": [
            "Shasha Bu",
            "Yu-Jin Zhang"
        ],
        "abstract": "A novel representation of images for image retrieval is introduced in this paper, by using a new type of feature with remarkable discriminative power. Despite the multi-scale nature of objects, most existing models perform feature extraction on a fixed scale, which will inevitably degrade the performance of the whole system. Motivated by this, we introduce a hierarchical sparse coding architecture for image retrieval to explore multi-scale cues. Sparse codes extracted on lower layers are transmitted to higher layers recursively. With this mechanism, cues from different scales are fused. Experiments on the Holidays dataset show that the proposed method achieves an excellent retrieval performance with a small code length.\n    ",
        "submission_date": "2014-06-03T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0680",
        "title": "Visual Reranking with Improved Image Graph",
        "authors": [
            "Ziqiong Liu",
            "Shengjin Wang",
            "Liang Zheng",
            "Qi Tian"
        ],
        "abstract": "This paper introduces an improved reranking method for the Bag-of-Words (BoW) based image search. Built on [1], a directed image graph robust to outlier distraction is proposed. In our approach, the relevance among images is encoded in the image graph, based on which the initial rank list is refined. Moreover, we show that the rank-level feature fusion can be adopted in this reranking method as well. Taking advantage of the complementary nature of various features, the reranking performance is further enhanced. Particularly, we exploit the reranking method combining the BoW and color information. Experiments on two benchmark datasets demonstrate that ourmethod yields significant improvements and the reranking results are competitive to the state-of-the-art methods.\n    ",
        "submission_date": "2014-06-03T00:00:00",
        "last_modified_date": "2014-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0909",
        "title": "Improvement Tracking Dynamic Programming using Replication Function for Continuous Sign Language Recognition",
        "authors": [
            "S. Ildarabadi",
            "M. Ebrahimi",
            "H. R. Pourreza"
        ],
        "abstract": "In this paper we used a Replication Function (R. F.)for improvement tracking with dynamic programming. The R. F. transforms values of gray level [0 255] to [0 1]. The resulting images of R. F. are more striking and visible in skin regions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and face. Results show that Tracking Error Rate 11% and Average Tracked Distance 7% reduced\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0924",
        "title": "Multiscale Fields of Patterns",
        "authors": [
            "Pedro F. Felzenszwalb",
            "John G. Oberlin"
        ],
        "abstract": "We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0946",
        "title": "Beyond $\u03c7^2$ Difference: Learning Optimal Metric for Boundary Detection",
        "authors": [
            "Fei He",
            "Shengjin Wang"
        ],
        "abstract": "This letter focuses on solving the challenging problem of detecting natural image boundaries. A boundary usually refers to the border between two regions with different semantic meanings. Therefore, a measurement of dissimilarity between image regions plays a pivotal role in boundary detection of natural images. To improve the performance of boundary detection, a Learning-based Boundary Metric (LBM) is proposed to replace $\\chi^2$ difference adopted by the classical algorithm mPb. Compared with $\\chi^2$ difference, LBM is composed of a single layer neural network and an RBF kernel, and is fine-tuned by supervised learning rather than human-crafted. It is more effective in describing the dissimilarity between natural image regions while tolerating large variance of image data. After substituting $\\chi^2$ difference with LBM, the F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to 0.71. Moreover, when image features are computed on a single scale, the proposed LBM algorithm still achieves competitive results compared with \\emph{mPb}, which makes use of multi-scale image features.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1134",
        "title": "Local Decorrelation For Improved Detection",
        "authors": [
            "Woonhyun Nam",
            "Piotr Doll\u00e1r",
            "Joon Hee Han"
        ],
        "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1247",
        "title": "Shared Representation Learning for Heterogeneous Face Recognition",
        "authors": [
            "Dong Yi",
            "Zhen Lei",
            "Shengcai Liao",
            "Stan Z. Li"
        ],
        "abstract": "After intensive research, heterogenous face recognition is still a challenging problem. The main difficulties are owing to the complex relationship between heterogenous face image spaces. The heterogeneity is always tightly coupled with other variations, which makes the relationship of heterogenous face images highly nonlinear. Many excellent methods have been proposed to model the nonlinear relationship, but they apt to overfit to the training set, due to limited samples. Inspired by the unsupervised algorithms in deep learning, this paper proposes an novel framework for heterogeneous face recognition. We first extract Gabor features at some localized facial points, and then use Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove the heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected together and processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases are selected to evaluate the proposed method. For Sketch-Photo problem, we obtain perfect results on the CUFS database. For NIR-VIS problem, we produce new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1476",
        "title": "A Context-aware Delayed Agglomeration Framework for Electron Microscopy Segmentation",
        "authors": [
            "Toufiq Parag",
            "Anirban Chakraborty",
            "Stephen Plaza",
            "Lou Scheffer"
        ],
        "abstract": "Electron Microscopy (EM) image (or volume) segmentation has become significantly important in recent years as an instrument for connectomics. This paper proposes a novel agglomerative framework for EM segmentation. In particular, given an over-segmented image or volume, we propose a novel framework for accurately clustering regions of the same neuron. Unlike existing agglomerative methods, the proposed context-aware algorithm divides superpixels (over-segmented regions) of different biological entities into different subsets and agglomerates them separately. In addition, this paper describes a \"delayed\" scheme for agglomerative clustering that postpones some of the merge decisions, pertaining to newly formed bodies, in order to generate a more confident boundary prediction. We report significant improvements attained by the proposed approach in segmentation accuracy over existing standard methods on 2D and 3D datasets.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1528",
        "title": "Towards building a Crowd-Sourced Sky Map",
        "authors": [
            "Dustin Lang",
            "David W. Hogg",
            "Bernhard Scholkopf"
        ],
        "abstract": "We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images. The method makes use of pixel-rank information in the individual input images to improve a \"consensus\" pixel rank in the combined image. Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons. We apply the method to images of the night sky (of unknown provenance) discovered on the Web. The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually. More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1774",
        "title": "Small Sample Learning of Superpixel Classifiers for EM Segmentation- Extended Version",
        "authors": [
            "Toufiq Parag",
            "Stephen Plaza",
            "Louis Scheffer"
        ],
        "abstract": "Pixel and superpixel classifiers have become essential tools for EM segmentation algorithms. Training these classifiers remains a major bottleneck primarily due to the requirement of completely annotating the dataset which is tedious, error-prone and costly. In this paper, we propose an interactive learning scheme for the superpixel classifier for EM segmentation. Our algorithm is \"active semi-supervised\" because it requests the labels of a small number of examples from user and applies label propagation technique to generate these queries. Using only a small set ($<20\\%$) of all datapoints, the proposed algorithm consistently generates a classifier almost as accurate as that estimated from a complete groundtruth. We provide segmentation results on multiple datasets to show the strength of these classifiers.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1881",
        "title": "Fine-grained Activity Recognition with Holistic and Pose based Features",
        "authors": [
            "Leonid Pishchulin",
            "Mykhaylo Andriluka",
            "Bernt Schiele"
        ],
        "abstract": "Holistic methods based on dense trajectories are currently the de facto standard for recognition of human activities in video. Whether holistic representations will sustain or will be superseded by higher level video encoding in terms of body pose and motion is the subject of an ongoing debate. In this paper we aim to clarify the underlying factors responsible for good performance of holistic and pose-based representations. To that end we build on our recent dataset leveraging the existing taxonomy of human activities. This dataset includes 24,920 video snippets covering 410 human activities in total. Our analysis reveals that holistic and pose-based methods are highly complementary, and their performance varies significantly depending on the activity. We find that holistic methods are mostly affected by the number and speed of trajectories, whereas pose-based methods are mostly influenced by viewpoint of the person. We observe striking performance differences across activities: for certain activities results with pose-based features are more than twice as accurate compared to holistic features, and vice versa. The best performing approach in our comparison is based on the combination of holistic and pose-based approaches, which again underlines their complementarity.\n    ",
        "submission_date": "2014-06-07T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1906",
        "title": "Refinement-Cut: User-Guided Segmentation Algorithm for Translational Science",
        "authors": [
            "Jan Egger"
        ],
        "abstract": "In this contribution, a semi-automatic segmentation algorithm for (medical) image analysis is presented. More precise, the approach belongs to the category of interactive contouring algorithms, which provide real-time feedback of the segmentation result. However, even with interactive real-time contouring approaches there are always cases where the user cannot find a satisfying segmentation, e.g. due to homogeneous appearances between the object and the background, or noise inside the object. For these difficult cases the algorithm still needs additional user support. However, this additional user support should be intuitive and rapid integrated into the segmentation process, without breaking the interactive real-time segmentation feedback. I propose a solution where the user can support the algorithm by an easy and fast placement of one or more seed points to guide the algorithm to a satisfying segmentation result also in difficult cases. These additional seed(s) restrict(s) the calculation of the segmentation for the algorithm, but at the same time, still enable to continue with the interactive real-time feedback segmentation. For a practical and genuine application in translational science, the approach has been tested on medical data from the clinical routine in 2D and 3D.\n    ",
        "submission_date": "2014-06-07T00:00:00",
        "last_modified_date": "2014-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1925",
        "title": "Shape-from-intrinsic operator",
        "authors": [
            "Davide Boscaini",
            "Davide Eynard",
            "Michael M. Bronstein"
        ],
        "abstract": "Shape-from-X is an important class of problems in the fields of geometry processing, computer graphics, and vision, attempting to recover the structure of a shape from some observations. In this paper, we formulate the problem of shape-from-operator (SfO), recovering an embedding of a mesh from intrinsic differential operators defined on the mesh. Particularly interesting instances of our SfO problem include synthesis of shape analogies, shape-from-Laplacian reconstruction, and shape exaggeration. Numerically, we approach the SfO problem by splitting it into two optimization sub-problems that are applied in an alternating scheme: metric-from-operator (reconstruction of the discrete metric from the intrinsic operator) and embedding-from-metric (finding a shape embedding that would realize a given metric, a setting of the multidimensional scaling problem).\n    ",
        "submission_date": "2014-06-07T00:00:00",
        "last_modified_date": "2014-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1943",
        "title": "Structured Dictionary Learning for Classification",
        "authors": [
            "Yuanming Suo",
            "Minh Dao",
            "Umamahesh Srinivas",
            "Vishal Monga",
            "Trac D. Tran"
        ],
        "abstract": "Sparsity driven signal processing has gained tremendous popularity in the last decade. At its core, the assumption is that the signal of interest is sparse with respect to either a fixed transformation or a signal dependent dictionary. To better capture the data characteristics, various dictionary learning methods have been proposed for both reconstruction and classification tasks. For classification particularly, most approaches proposed so far have focused on designing explicit constraints on the sparse code to improve classification accuracy while simply adopting $l_0$-norm or $l_1$-norm for sparsity regularization. Motivated by the success of structured sparsity in the area of Compressed Sensing, we propose a structured dictionary learning framework (StructDL) that incorporates the structure information on both group and task levels in the learning process. Its benefits are two-fold: (i) the label consistency between dictionary atoms and training data are implicitly enforced; and (ii) the classification performance is more robust in the cases of a small dictionary size or limited training data than other techniques. Using the subspace model, we derive the conditions for StructDL to guarantee the performance and show theoretically that StructDL is superior to $l_0$-norm or $l_1$-norm regularized dictionary learning for classification. Extensive experiments have been performed on both synthetic simulations and real world applications, such as face recognition and object classification, to demonstrate the validity of the proposed DL framework.\n    ",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2031",
        "title": "Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts",
        "authors": [
            "Xianjie Chen",
            "Roozbeh Mottaghi",
            "Xiaobai Liu",
            "Sanja Fidler",
            "Raquel Urtasun",
            "Alan Yuille"
        ],
        "abstract": "Detecting objects becomes difficult when we need to deal with large shape deformation, occlusion and low resolution. We propose a novel approach to i) handle large deformations and partial occlusions in animals (as examples of highly deformable objects), ii) describe them in terms of body parts, and iii) detect them when their body parts are hard to detect (e.g., animals depicted at low resolution). We represent the holistic object and body parts separately and use a fully connected model to arrange templates for the holistic object and body parts. Our model automatically decouples the holistic object or body parts from the model when they are hard to detect. This enables us to represent a large number of holistic object and body part combinations to better deal with different \"detectability\" patterns caused by deformations, occlusion and/or low resolution.\n",
        "submission_date": "2014-06-08T00:00:00",
        "last_modified_date": "2014-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2049",
        "title": "Image Tag Completion by Low-rank Factorization with Dual Reconstruction Structure Preserved",
        "authors": [
            "Xue Li",
            "Yu-Jin Zhang",
            "Bin Shen",
            "Bao-Di Liu"
        ],
        "abstract": "A novel tag completion algorithm is proposed in this paper, which is designed with the following features: 1) Low-rank and error s-parsity: the incomplete initial tagging matrix D is decomposed into the complete tagging matrix A and a sparse error matrix E. However, instead of minimizing its nuclear norm, A is further factor-ized into a basis matrix U and a sparse coefficient matrix V, i.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our algorithm to recover latent structures from noisy initial data and avoid performing too much denoising; 2) Local reconstruction structure consistency: to steer the completion of D, the local linear reconstruction structures in feature space and tag space are obtained and preserved by U and V respectively. Such a scheme could alleviate the negative effect of distances measured by low-level features and incomplete tags. Thus, we can seek a balance between exploiting as much information and not being mislead to suboptimal performance. Experiments conducted on Corel5k dataset and the newly issued Flickr30Concepts dataset demonstrate the effectiveness and efficiency of the proposed method.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2080",
        "title": "Training Convolutional Networks with Noisy Labels",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Joan Bruna",
            "Manohar Paluri",
            "Lubomir Bourdev",
            "Rob Fergus"
        ],
        "abstract": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2139",
        "title": "Log-Euclidean Bag of Words for Human Action Recognition",
        "authors": [
            "Masoud Faraki",
            "Maziar Palhang",
            "Conrad Sanderson"
        ],
        "abstract": "Representing videos by densely extracted local space-time features has recently become a popular approach for analysing actions. In this paper, we tackle the problem of categorising human actions by devising Bag of Words (BoW) models based on covariance matrices of spatio-temporal features, with the features formed from histograms of optical flow. Since covariance matrices form a special type of Riemannian manifold, the space of Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry should be taken into account while discriminating between covariance matrices. To this end, we propose to embed SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the generation of the codebook and histograms. Experiments on challenging human action datasets show that the proposed method obtains notable improvements in discrimination accuracy, in comparison to several state-of-the-art methods.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2016-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2199",
        "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
        "authors": [
            "Karen Simonyan",
            "Andrew Zisserman"
        ],
        "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.\n",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2227",
        "title": "Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition",
        "authors": [
            "Max Jaderberg",
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "abstract": "In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one \"reading\" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2282",
        "title": "Robust Estimation of 3D Human Poses from a Single Image",
        "authors": [
            "Chunyu Wang",
            "Yizhou Wang",
            "Zhouchen Lin",
            "Alan L. Yuille",
            "Wen Gao"
        ],
        "abstract": "Human pose estimation is a key step to action recognition. We propose a method of estimating 3D human poses from a single image, which works in conjunction with an existing 2D pose/joint detector. 3D pose estimation is challenging because multiple 3D poses may correspond to the same 2D pose after projection due to the lack of depth information. Moreover, current 2D pose estimators are usually inaccurate which may cause errors in the 3D estimation. We address the challenges in three ways: (i) We represent a 3D pose as a linear combination of a sparse set of bases learned from 3D human skeletons. (ii) We enforce limb length constraints to eliminate anthropomorphically implausible skeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error between the projection of the 3D pose and the corresponding 2D detection. The $L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the alternating direction method (ADM) to solve the optimization problem efficiently. Our approach outperforms the state-of-the-arts on three benchmark datasets.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2283",
        "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "authors": [
            "David Eigen",
            "Christian Puhrsch",
            "Rob Fergus"
        ],
        "abstract": "Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2375",
        "title": "Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency",
        "authors": [
            "Wenhao Lu",
            "Xiaochen Lian",
            "Alan Yuille"
        ],
        "abstract": "This paper addresses the problem of semantic part parsing (segmentation) of cars, ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2407",
        "title": "Optimization Methods for Convolutional Sparse Coding",
        "authors": [
            "Hilton Bristow",
            "Simon Lucey"
        ],
        "abstract": "Sparse and convolutional constraints form a natural prior for many optimization problems that arise from physical processes. Detecting motifs in speech and musical passages, super-resolving images, compressing videos, and reconstructing harmonic motions can all leverage redundancies introduced by convolution. Solving problems involving sparse and convolutional constraints remains a difficult computational problem, however. In this paper we present an overview of convolutional sparse coding in a consistent framework. The objective involves iteratively optimizing a convolutional least-squares term for the basis functions, followed by an L1-regularized least squares term for the sparse coefficients. We discuss a range of optimization methods for solving the convolutional sparse coding objective, and the properties that make each method suitable for different applications. In particular, we concentrate on computational complexity, speed to {\\epsilon} convergence, memory usage, and the effect of implied boundary conditions. We present a broad suite of examples covering different signal and application domains to illustrate the general applicability of convolutional sparse coding, and the efficacy of the available optimization methods.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2419",
        "title": "Why do linear SVMs trained on HOG features perform so well?",
        "authors": [
            "Hilton Bristow",
            "Simon Lucey"
        ],
        "abstract": "Linear Support Vector Machines trained on HOG features are now a de facto standard across many visual perception tasks. Their popularisation can largely be attributed to the step-change in performance they brought to pedestrian detection, and their subsequent successes in deformable parts models. This paper explores the interactions that make the HOG-SVM symbiosis perform so well. By connecting the feature extraction and learning processes rather than treating them as disparate plugins, we show that HOG features can be viewed as doing two things: (i) inducing capacity in, and (ii) adding prior to a linear SVM trained on pixels. From this perspective, preserving second-order statistics and locality of interactions are key to good performance. We demonstrate surprising accuracy on expression recognition and pedestrian detection tasks, by assuming only the importance of preserving such local second-order interactions.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2580",
        "title": "Identification of Orchid Species Using Content-Based Flower Image Retrieval",
        "authors": [
            "D. H. Apriyanti",
            "A.A. Arymurthy",
            "L.T. Handoko"
        ],
        "abstract": "In this paper, we developed the system for recognizing the orchid species by using the images of flower. We used MSRM (Maximal Similarity based on Region Merging) method for segmenting the flower object from the background and extracting the shape feature such as the distance from the edge to the centroid point of the flower, aspect ratio, roundness, moment invariant, fractal dimension and also extract color feature. We used HSV color feature with ignoring the V value. To retrieve the image, we used Support Vector Machine (SVM) method. Orchid is a unique flower. It has a part of flower called lip (labellum) that distinguishes it from other flowers even from other types of orchids. Thus, in this paper, we proposed to do feature extraction not only on flower region but also on lip (labellum) region. The result shows that our proposed method can increase the accuracy value of content based flower image retrieval for orchid species up to $\\pm$ 14%. The most dominant feature is Centroid Contour Distance, Moment Invariant and HSV Color. The system accuracy is 85,33% in validation phase and 79,33% in testing phase.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2639",
        "title": "A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations",
        "authors": [
            "Holger R. Roth",
            "Le Lu",
            "Ari Seff",
            "Kevin M. Cherry",
            "Joanne Hoffman",
            "Shijun Wang",
            "Jiamin Liu",
            "Evrim Turkbey",
            "Ronald M. Summers"
        ],
        "abstract": "Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards 100% sensitivity at the cost of high FP levels (40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work.\n    ",
        "submission_date": "2014-06-06T00:00:00",
        "last_modified_date": "2014-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2732",
        "title": "Deep Epitomic Convolutional Neural Networks",
        "authors": [
            "George Papandreou"
        ],
        "abstract": "Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the small-image MNIST and CIFAR-10 datasets.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2807",
        "title": "The Secrets of Salient Object Segmentation",
        "authors": [
            "Yin Li",
            "Xiaodi Hou",
            "Christof Koch",
            "James M. Rehg",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets. Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasizing the stereotypical concepts of saliency. The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth. With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation. Finally, we report significant benchmark progress on three existing datasets of segmenting salient objects\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2952",
        "title": "Bird Species Categorization Using Pose Normalized Deep Convolutional Nets",
        "authors": [
            "Steve Branson",
            "Grant Van Horn",
            "Serge Belongie",
            "Pietro Perona"
        ],
        "abstract": "We propose an architecture for fine-grained visual categorization that approaches expert human performance in the classification of bird species. Our architecture first computes an estimate of the object's pose; this is used to compute local image features which are, in turn, used for classification. The features are computed by applying deep convolutional nets to image patches that are located and normalized by the pose. We perform an empirical study of a number of pose normalization schemes, including an investigation of higher order geometric warping functions. We propose a novel graph-based clustering algorithm for learning a compact pose normalization space. We perform a detailed investigation of state-of-the-art deep convolutional feature implementations and fine-tuning feature learning for fine-grained classification. We observe that a model that integrates lower-level feature layers with pose-normalized extraction routines and higher-level feature layers with unaligned image features works best. Our experiments advance state-of-the-art performance on bird species recognition, with a large improvement of correct classification rates over previous methods (75% vs. 55-65%).\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2969",
        "title": "Truncated Nuclear Norm Minimization for Image Restoration Based On Iterative Support Detection",
        "authors": [
            "Yilun Wang",
            "Xinhua Su"
        ],
        "abstract": "Recovering a large matrix from limited measurements is a challenging task arising in many real applications, such as image inpainting, compressive sensing and medical imaging, and this kind of problems are mostly formulated as low-rank matrix approximation problems. Due to the rank operator being non-convex and discontinuous, most of the recent theoretical studies use the nuclear norm as a convex relaxation and the low-rank matrix recovery problem is solved through minimization of the nuclear norm regularized problem. However, a major limitation of nuclear norm minimization is that all the singular values are simultaneously minimized and the rank may not be well approximated \\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage algorithm, which makes use of the concept of Truncated Nuclear Norm Regularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support Detection (ISD) proposed in \\citep{wang2010sparse} to overcome the above limitation. Besides matrix completion problems considered in \\citep{hu2012fast}, the proposed method can be also extended to the general low-rank matrix recovery problems. Extensive experiments well validate the superiority of our new algorithms over other state-of-the-art methods.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2984",
        "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
        "authors": [
            "Jonathan Tompson",
            "Arjun Jain",
            "Yann LeCun",
            "Christoph Bregler"
        ],
        "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3332",
        "title": "Convolutional Kernel Networks",
        "authors": [
            "Julien Mairal",
            "Piotr Koniusz",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.\n    ",
        "submission_date": "2014-06-12T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3418",
        "title": "Fingers' Angle Calculation using Level-Set Method",
        "authors": [
            "Ankit Chaudhary",
            "J.L. Raheja",
            "K. Das",
            "S. Raheja"
        ],
        "abstract": "In the current age, use of natural communication in human computer interaction is a known and well installed thought. Hand gesture recognition and gesture based applications has gained a significant amount of popularity amongst people all over the world. It has a number of applications ranging from security to entertainment. These applications generally are real time applications and need fast, accurate communication with machines. On the other end, gesture based communications have few limitations also like bent finger information is not provided in vision based techniques. In this paper, a novel method for fingertip detection and for angle calculation of both hands bent fingers is discussed. Angle calculation has been done before with sensor based gloves/devices. This study has been conducted in the context of natural computing for calculating angles without using any wired equipment, colors, marker or any device. The pre-processing and segmentation of the region of interest is performed in a HSV color space and a binary format respectively. Fingertips are detected using level-set method and angles were calculated using geometrical analysis. This technique requires no training for system to perform the task.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3474",
        "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network",
        "authors": [
            "Sijin Li",
            "Zhi-Qiang Liu",
            "Antoni B. Chan"
        ],
        "abstract": "We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2014-06-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3906",
        "title": "Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene Understanding",
        "authors": [
            "Roozbeh Mottaghi",
            "Sanja Fidler",
            "Alan Yuille",
            "Raquel Urtasun",
            "Devi Parikh"
        ],
        "abstract": "Recent trends in image understanding have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers. In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular semantic segmentation, object detection and scene recognition. Towards this goal, we \"plug-in\" human subjects for each of the various components in a state-of-the-art conditional random field model. Comparisons among various hybrid human-machine CRFs give us indications of how much \"head room\" there is to improve scene understanding by focusing research efforts on various individual tasks.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3949",
        "title": "A Fusion of Labeled-Grid Shape Descriptors with Weighted Ranking Algorithm for Shapes Recognition",
        "authors": [
            "Jamil Ahmad",
            "Zahoor Jan",
            "Zia-ud-Din",
            "Shoaib Muhammad Khan"
        ],
        "abstract": "Retrieving similar images from a large dataset based on the image content has been a very active research area and is a very challenging task. Studies have shown that retrieving similar images based on their shape is a very effective method. For this purpose a large number of methods exist in literature. The combination of more than one feature has also been investigated for this purpose and has shown promising results. In this paper a fusion based shapes recognition method has been proposed. A set of local boundary based and region based features are derived from the labeled grid based representation of the shape and are combined with a few global shape features to produce a composite shape descriptor. This composite shape descriptor is then used in a weighted ranking algorithm to find similarities among shapes from a large dataset. The experimental analysis has shown that the proposed method is powerful enough to discriminate the geometrically similar shapes from the non-similar ones.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4007",
        "title": "Impact of Exponent Parameter Value for the Partition Matrix on the Performance of Fuzzy C Means Algorithm",
        "authors": [
            "Dibya Jyoti Bora",
            "Anil Kumar Gupta"
        ],
        "abstract": "Soft Clustering plays a very important rule on clustering real world data where a data item contributes to more than one cluster. Fuzzy logic based algorithms are always suitable for performing soft clustering tasks. Fuzzy C Means (FCM) algorithm is a very popular fuzzy logic based algorithm. In case of fuzzy logic based algorithm, the parameter like exponent for the partition matrix that we have to fix for the clustering task plays a very important rule on the performance of the algorithm. In this paper, an experimental analysis is done on FCM algorithm to observe the impact of this parameter on the performance of the algorithm.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2014-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4112",
        "title": "Semantic Graph for Zero-Shot Learning",
        "authors": [
            "Zhen-Yong Fu",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "Zero-shot learning aims to classify visual objects without any training data via knowledge transfer between seen and unseen classes. This is typically achieved by exploring a semantic embedding space where the seen and unseen classes can be related. Previous works differ in what embedding space is used and how different classes and a test image can be related. In this paper, we utilize the annotation-free semantic word space for the former and focus on solving the latter issue of modeling relatedness. Specifically, in contrast to previous work which ignores the semantic relationships between seen classes and focus merely on those between seen and unseen classes, in this paper a novel approach based on a semantic graph is proposed to represent the relationships between all the seen and unseen class in a semantic word space. Based on this semantic graph, we design a special absorbing Markov chain process, in which each unseen class is viewed as an absorbing state. After incorporating one test image into the semantic graph, the absorbing probabilities from the test data to each unseen class can be effectively computed; and zero-shot classification can be achieved by finding the class label with the highest absorbing probability. The proposed model has a closed-form solution which is linear with respect to the number of test images. We demonstrate the effectiveness and computational efficiency of the proposed method over the state-of-the-arts on the AwA (animals with attributes) dataset.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4216",
        "title": "Person Re-identification by Local Maximal Occurrence Representation and Metric Learning",
        "authors": [
            "Shengcai Liao",
            "Yang Hu",
            "Xiangyu Zhu",
            "Stan Z. Li"
        ],
        "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4296",
        "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams",
        "authors": [
            "Adrien Gaidon",
            "Gloria Zen",
            "Jose A. Rodriguez-Serrano"
        ],
        "abstract": "Learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. This is impractical when dealing with many different sources (e.g., in camera networks), or constantly changing ones such as mobile cameras (e.g., in robotics or driving assistant systems). In this paper, we address the problem of self-learning detectors in an autonomous manner, i.e. (i) detectors continuously updating themselves to efficiently adapt to streaming data sources (contrary to transductive algorithms), (ii) without any labeled data strongly related to the target data stream (contrary to self-paced learning), and (iii) without manual intervention to set and update hyper-parameters. To that end, we propose an unsupervised, on-line, and self-tuning learning algorithm to optimize a multi-task learning convex objective. Our method uses confident but laconic oracles (high-precision but low-recall off-the-shelf generic detectors), and exploits the structure of the problem to jointly learn on-line an ensemble of instance-level trackers, from which we derive an adapted category-level object detector. Our approach is validated on real-world publicly available video object datasets.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4444",
        "title": "PRISM: Person Re-Identification via Structured Matching",
        "authors": [
            "Ziming Zhang",
            "Venkatesh Saligrama"
        ],
        "abstract": "Person re-identification (re-id), an emerging problem in visual surveillance, deals with maintaining entities of individuals whilst they traverse various locations surveilled by a camera network. From a visual perspective re-id is challenging due to significant changes in visual appearance of individuals in cameras with different pose, illumination and calibration. Globally the challenge arises from the need to maintain structurally consistent matches among all the individual entities across different camera views. We propose PRISM, a structured matching method to jointly account for these challenges. We view the global problem as a weighted graph matching problem and estimate edge weights by learning to predict them based on the co-occurrences of visual patterns in the training examples. These co-occurrence based scores in turn account for appearance changes by inferring likely and unlikely visual co-occurrences appearing in training instances. We implement PRISM on single shot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in terms of matching rate while being computationally efficient.\n    ",
        "submission_date": "2014-06-13T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4484",
        "title": "Block matching algorithm based on Harmony Search optimization for motion estimation",
        "authors": [
            "Erik Cuevas"
        ],
        "abstract": "Motion estimation is one of the major problems in developing video coding applications. Among all motion estimation approaches, Block-matching (BM) algorithms are the most popular methods due to their effectiveness and simplicity for both software and hardware implementations. A BM approach assumes that the movement of pixels within a defined region of the current frame can be modeled as a translation of pixels contained in the previous frame. In this procedure, the motion vector is obtained by minimizing a certain matching metric that is produced for the current frame over a determined search window from the previous frame. Unfortunately, the evaluation of such matching measurement is computationally expensive and represents the most consuming operation in the BM process. Therefore, BM motion estimation can be viewed as an optimization problem whose goal is to find the best-matching block within a search space. The simplest available BM method is the Full Search Algorithm (FSA) which finds the most accurate motion vector through an exhaustive computation of all the elements of the search space. Recently, several fast BM algorithms have been proposed to reduce the search positions by calculating only a fixed subset of motion vectors despite lowering its accuracy. On the other hand, the Harmony Search (HS) algorithm is a population-based optimization method that is inspired by the music improvisation process in which a musician searches for harmony and continues to polish the pitches to obtain a better harmony. In this paper, a new BM algorithm that combines HS with a fitness approximation model is proposed. The approach uses motion vectors belonging to the search window as potential solutions. A fitness function evaluates the matching quality of each motion vector candidate.\n    ",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4729",
        "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is \"artificial\" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \"spatial pyramid pooling\", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.\n",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4770",
        "title": "Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour Equality",
        "authors": [
            "I. Laurence Aroquiaraj",
            "K. Thangavel"
        ],
        "abstract": "Mass classification of objects is an important area of research and application in a variety of fields. In this paper, we present an efficient computer aided mass classification method in digitized mammograms using Fuzzy K-Nearest Neighbor Equality, which performs benign or malignant classification on region of interest that contains mass. One of the major mammographic characteristics for mass classification is texture. Fuzzy K-Nearest Neighbor Equality exploits this important factor to classify the mass into benign or malignant. The statistical textural features used in characterizing the masses are Haralick and Run length features. The main aim of the method is to increase the effectiveness and efficiency of the classification process in an objective manner to reduce the numbers of false positive of malignancies. In this paper proposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifying the marked regions into benign and malignant and 94.46 sensitivity,96.81 specificity and 96.52 accuracy is achieved that is very much promising compare to the radiologists' accuracy.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4773",
        "title": "Deep Learning Face Representation by Joint Identification-Verification",
        "authors": [
            "Yi Sun",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67%.\n    ",
        "submission_date": "2014-06-18T00:00:00",
        "last_modified_date": "2014-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4845",
        "title": "Computer Vision Approach for Low Cost, High Precision Measurement of Grapevine Trunk Diameter in Outdoor Conditions",
        "authors": [
            "Diego Sebasti\u00e1n P\u00e9rez",
            "Facundo Bromberg",
            "Francisco Gonzalez Antivilo"
        ],
        "abstract": "Trunk diameter is a variable of agricultural interest, used mainly in the prediction of fruit trees production. It is correlated with leaf area and biomass of trees, and consequently gives a good estimate of the potential production of the plants. This work presents a low cost, high precision method for the measurement of trunk diameter of grapevines based on Computer Vision techniques. Several methods based on Computer Vision and other techniques are introduced in the literature. These methods present different advantages for crop management: they are amenable to be operated by unknowledgeable personnel, with lower operational costs; they result in lower stress levels to knowledgeable personnel, avoiding the deterioration of the measurement quality over time; and they make the measurement process amenable to be embedded in larger autonomous systems, allowing more measurements to be taken with equivalent costs. To date, all existing autonomous methods are either of low precision, or have a prohibitive cost for massive agricultural adoption, leaving the manual Vernier caliper or tape measure as the only choice in most situations. In this work we present a semi-autonomous measurement method that is susceptible to be fully automated, cost effective for mass adoption, and its precision is competitive (with slight improvements) over the caliper manual method.\n    ",
        "submission_date": "2014-05-16T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4966",
        "title": "Inner Product Similarity Search using Compositional Codes",
        "authors": [
            "Chao Du",
            "Jingdong Wang"
        ],
        "abstract": "This paper addresses the nearest neighbor search problem under inner product similarity and introduces a compact code-based approach. The idea is to approximate a vector using the composition of several elements selected from a source dictionary and to represent this vector by a short code composed of the indices of the selected elements. The inner product between a query vector and a database vector is efficiently estimated from the query vector and the short code of the database vector. We show the superior performance of the proposed group $M$-selection algorithm that selects $M$ elements from $M$ source dictionaries for vector approximation in terms of search accuracy and efficiency for compact codes of the same length via theoretical and empirical analysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT features, $1M$ linear models and Netflix) demonstrate the superiority of the proposed approach.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5035",
        "title": "Why are images smooth?",
        "authors": [
            "Uriel Feige"
        ],
        "abstract": "It is a well observed phenomenon that natural images are smooth, in the sense that nearby pixels tend to have similar values. We describe a mathematical model of images that makes no assumptions on the nature of the environment that images depict. It only assumes that images can be taken at different scales (zoom levels). We provide quantitative bounds on the smoothness of a typical image in our model, as a function of the number of available scales. These bounds can serve as a baseline against which to compare the observed smoothness of natural images.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5074",
        "title": "Robust Outlier Detection Technique in Data Mining: A Univariate Approach",
        "authors": [
            "Singh Vijendra",
            "Pathak Shivani"
        ],
        "abstract": "Outliers are the points which are different from or inconsistent with the rest of the data. They can be novel, new, abnormal, unusual or noisy information. Outliers are sometimes more interesting than the majority of the data. The main challenges of outlier detection with the increasing complexity, size and variety of datasets, are how to catch similar outliers as a group, and how to evaluate the outliers. This paper describes an approach which uses Univariate outlier detection as a pre-processing step to detect the outlier and then applies K-means algorithm hence to analyse the effects of the outliers on the cluster analysis of dataset.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5095",
        "title": "MRF-based Background Initialisation for Improved Foreground Detection in Cluttered Surveillance Videos",
        "authors": [
            "Vikas Reddy",
            "Conrad Sanderson",
            "Andres Sanin",
            "Brian C. Lovell"
        ],
        "abstract": "Robust foreground object segmentation via background modelling is a difficult problem in cluttered environments, where obtaining a clear view of the background to model is almost impossible. In this paper, we propose a method capable of robustly estimating the background and detecting regions of interest in such environments. In particular, we propose to extend the background initialisation component of a recent patch-based foreground detection algorithm with an elaborate technique based on Markov Random Fields, where the optimal labelling solution is computed using iterated conditional modes. Rather than relying purely on local temporal statistics, the proposed technique takes into account the spatial continuity of the entire background. Experiments with several tracking algorithms on the CAVIAR dataset indicate that the proposed method leads to considerable improvements in object tracking accuracy, when compared to methods based on Gaussian mixture models and feature histograms.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5212",
        "title": "R-CNNs for Pose Estimation and Action Detection",
        "authors": [
            "Georgia Gkioxari",
            "Bharath Hariharan",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "We present convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images. Our approach involves training an R-CNN detector with loss functions depending on the task being tackled. We evaluate our method on the challenging PASCAL VOC dataset and compare it to previous leading approaches. Our method gives state-of-the-art results for keypoint and action prediction. Additionally, we introduce a new dataset for action detection, the task of simultaneously localizing people and classifying their actions, and present results using our approach.\n    ",
        "submission_date": "2014-06-19T00:00:00",
        "last_modified_date": "2014-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5266",
        "title": "Web-Scale Training for Face Identification",
        "authors": [
            "Yaniv Taigman",
            "Ming Yang",
            "Marc'Aurelio Ranzato",
            "Lior Wolf"
        ],
        "abstract": "Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5309",
        "title": "Early Recognition of Human Activities from First-Person Videos Using Onset Representations",
        "authors": [
            "M. S. Ryoo",
            "Thomas J. Fuchs",
            "Lu Xia",
            "J. K. Aggarwal",
            "Larry Matthies"
        ],
        "abstract": "In this paper, we propose a methodology for early recognition of human activities from videos taken with a first-person viewpoint. Early recognition, which is also known as activity prediction, is an ability to infer an ongoing activity at its early stage. We present an algorithm to perform recognition of activities targeted at the camera from streaming videos, making the system to predict intended activities of the interacting person and avoid harmful events before they actually happen. We introduce the novel concept of 'onset' that efficiently summarizes pre-activity observations, and design an approach to consider event history in addition to ongoing video observation for early first-person recognition of activities. We propose to represent onset using cascade histograms of time series gradients, and we describe a novel algorithmic setup to take advantage of onset for early recognition of activities. The experimental results clearly illustrate that the proposed concept of onset enables better/earlier recognition of human activities from first-person videos.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5472",
        "title": "Predicting Motivations of Actions by Leveraging Text",
        "authors": [
            "Carl Vondrick",
            "Deniz Oktay",
            "Hamed Pirsiavash",
            "Antonio Torralba"
        ],
        "abstract": "Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5549",
        "title": "Fast Edge Detection Using Structured Forests",
        "authors": [
            "Piotr Doll\u00e1r",
            "C. Lawrence Zitnick"
        ],
        "abstract": "Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5653",
        "title": "Interactively Test Driving an Object Detector: Estimating Performance on Unlabeled Data",
        "authors": [
            "Rushil Anirudh",
            "Pavan Turaga"
        ],
        "abstract": "In this paper, we study the problem of `test-driving' a detector, i.e. allowing a human user to get a quick sense of how well the detector generalizes to their specific requirement. To this end, we present the first system that estimates detector performance interactively without extensive ground truthing using a human in the loop. We approach this as a problem of estimating proportions and show that it is possible to make accurate inferences on the proportion of classes or groups within a large data collection by observing only $5-10\\%$ of samples from the data. In estimating the false detections (for precision), the samples are chosen carefully such that the overall characteristics of the data collection are preserved. Next, inspired by its use in estimating disease propagation we apply pooled testing approaches to estimate missed detections (for recall) from the dataset. The estimates thus obtained are close to the ones obtained using ground truth, thus reducing the need for extensive labeling which is expensive and time consuming.\n    ",
        "submission_date": "2014-06-21T00:00:00",
        "last_modified_date": "2014-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5670",
        "title": "3D ShapeNets: A Deep Representation for Volumetric Shapes",
        "authors": [
            "Zhirong Wu",
            "Shuran Song",
            "Aditya Khosla",
            "Fisher Yu",
            "Linguang Zhang",
            "Xiaoou Tang",
            "Jianxiong Xiao"
        ],
        "abstract": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet -- a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5679",
        "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
        "authors": [
            "Andrej Karpathy",
            "Armand Joulin",
            "Li Fei-Fei"
        ],
        "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5710",
        "title": "Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy",
        "authors": [
            "M. C Hanumantharaju",
            "M. Ravishankar",
            "D. R Rameshbabu"
        ],
        "abstract": "This paper presents a new color image enhancement technique based on modified MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are evaluated using a new metric, namely, wavelet energy. The color image enhancement is achieved by down sampling the value component of HSV color space converted image into three scales (normal, medium and fine) following the contrast stretching operation. These down sampled value components are enhanced using the MSR algorithm. The value component is reconstructed by averaging each pixels of the lower scale image with that of the upper scale image subsequent to up sampling the lower scale image. This process replaces dark pixel by the average pixels of both the lower scale and upper scale, while retaining the bright pixels. The quality of the reconstructed images in the proposed method is found to be good and far better then the other researchers method. The performance of the proposed scheme is evaluated using new wavelet domain based assessment criterion, referred as wavelet energy. This scheme computes the energy of both original and enhanced image in wavelet domain. The number of edge details as well as wavelet energy is less in a poor quality image compared with naturally enhanced image. Experimental results presented confirms that the proposed wavelet energy based color image quality assessment technique efficiently characterizes both the local and global details of enhanced image.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5726",
        "title": "CNN: Single-label to Multi-label",
        "authors": [
            "Yunchao Wei",
            "Wei Xia",
            "Junshi Huang",
            "Bingbing Ni",
            "Jian Dong",
            "Yao Zhao",
            "Shuicheng Yan"
        ],
        "abstract": "Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3% after the fusion with our complementary result in [47] based on hand-crafted features on the VOC2012 dataset, which significantly outperforms the state-of-the-arts with a large margin of more than 7%.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5774",
        "title": "Factors of Transferability for a Generic ConvNet Representation",
        "authors": [
            "Hossein Azizpour",
            "Ali Sharif Razavian",
            "Josephine Sullivan",
            "Atsuto Maki",
            "Stefan Carlsson"
        ],
        "abstract": "Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their distance from the source task such that a correlation between the performance of tasks and their distance from the source task w.r.t. the proposed factors is observed.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5807",
        "title": "A Unified Quantitative Model of Vision and Audition",
        "authors": [
            "Peilei Liu",
            "Ting Wang"
        ],
        "abstract": "We have put forwards a unified quantitative framework of vision and audition, based on existing data and theories. According to this model, the retina is a feedforward network self-adaptive to inputs in a specific period. After fully grown, cells become specialized detectors based on statistics of stimulus history. This model has provided explanations for perception mechanisms of colour, shape, depth and motion. Moreover, based on this ground we have put forwards a bold conjecture that single ear can detect sound direction. This is complementary to existing theories and has provided better explanations for sound localization.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5824",
        "title": "VideoSET: Video Summary Evaluation through Text",
        "authors": [
            "Serena Yeung",
            "Alireza Fathi",
            "Li Fei-Fei"
        ],
        "abstract": "In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5910",
        "title": "Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions",
        "authors": [
            "Roman Shapovalov",
            "Dmitry Vetrov",
            "Anton Osokin",
            "Pushmeet Kohli"
        ],
        "abstract": "Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5947",
        "title": "Committees of deep feedforward networks trained with few data",
        "authors": [
            "Bogdan Miclut",
            "Thomas Kaester",
            "Thomas Martinetz",
            "Erhardt Barth"
        ],
        "abstract": "Deep convolutional neural networks are known to give good results on image classification tasks. In this paper we present a method to improve the classification result by combining multiple such networks in a committee. We adopt the STL-10 dataset which has very few training examples and show that our method can achieve results that are better than the state of the art. The networks are trained layer-wise and no backpropagation is used. We also explore the effects of dataset augmentation by mirroring, rotation, and scaling.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6140",
        "title": "Offline Handwritten MODI Character Recognition Using HU, Zernike Moments and Zoning",
        "authors": [
            "Sadanand A. Kulkarni",
            "Prashant L. Borde",
            "Ramesh R. Manza",
            "Pravin L. Yannawar"
        ],
        "abstract": "HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is a process of recognition of different handwritten characters from a digital image of documents. Handwritten automatic character recognition has attracted many researchers all over the world to contribute handwritten character recognition domain. Shape identification and feature extraction is very important part of any character recognition system and success of method is highly dependent on selection of features. However feature extraction is the most important step in defining the shape of the character as precisely and as uniquely as possible. This is indeed the most important step and complex task as well and achieved success by using invariance property, irrespective of position and orientation. Zernike moments describes shape, identify rotation invariant due to its Orthogonality property. MODI is an ancient script of India had cursive and complex representation of characters. The work described in this paper presents efficiency of Zernike moments over Hu 7 moment with zoning for automatic recognition of handwritten MODI characters. Offline approach is used in this paper because MODI Script was very popular and widely used for writing purpose till 19th century before Devanagari was officially adopted.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6147",
        "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation",
        "authors": [
            "Neda Salamati",
            "Diane Larlus",
            "Gabriela Csurka",
            "Sabine S\u00fcsstrunk"
        ],
        "abstract": "Recent progress in computational photography has shown that we can acquire near-infrared (NIR) information in addition to the normal visible (RGB) band, with only slight modifications to standard digital cameras. Due to the proximity of the NIR band to visible radiation, NIR images share many properties with visible images. However, as a result of the material dependent reflection in the NIR part of the spectrum, such images reveal different characteristics of the scene. We investigate how to effectively exploit these differences to improve performance on the semantic image segmentation task. Based on a state-of-the-art segmentation framework and a novel manually segmented image database (both indoor and outdoor scenes) that contain 4-channel images (RGB+NIR), we study how to best incorporate the specific characteristics of the NIR response. We show that adding NIR leads to improved performance for classes that correspond to a specific type of material in both outdoor and indoor scenes. We also discuss the results with respect to the physical properties of the NIR response.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6201",
        "title": "Saccadic Eye Movements and the Generalized Pareto Distribution",
        "authors": [
            "Reiner Lenz"
        ],
        "abstract": "We describe a statistical analysis of the eye tracker measurements in a database with 15 observers viewing 1003 images under free-viewing conditions. In contrast to the common approach of investigating the properties of the fixation points we analyze the properties of the transition phases between fixations. We introduce hyperbolic geometry as a tool to measure the step length between consecutive eye positions. We show that the step lengths, measured in hyperbolic and euclidean geometry, follow a generalized Pareto distribution. The results based on the hyperbolic distance are more robust than those based on euclidean geometry. We show how the structure of the space of generalized Pareto distributions can be used to characterize and identify individual observers.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6273",
        "title": "Image Completion for View Synthesis Using Markov Random Fields and Efficient Belief Propagation",
        "authors": [
            "Julian Habigt",
            "Klaus Diepold"
        ],
        "abstract": "View synthesis is a process for generating novel views from a scene which has been recorded with a 3-D camera setup. It has important applications in 3-D post-production and 2-D to 3-D conversion. However, a central problem in the generation of novel views lies in the handling of disocclusions. Background content, which was occluded in the original view, may become unveiled in the synthesized view. This leads to missing information in the generated view which has to be filled in a visually plausible manner. We present an inpainting algorithm for disocclusion filling in synthesized views based on Markov random fields and efficient belief propagation. We compare the result to two state-of-the-art algorithms and demonstrate a significant improvement in image quality.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6323",
        "title": "Dense Correspondences Across Scenes and Scales",
        "authors": [
            "Moria Tau",
            "Tal Hassner"
        ],
        "abstract": "We seek a practical method for establishing dense correspondences between two images with similar content, but possibly different 3D scenes. One of the challenges in designing such a system is the local scale differences of objects appearing in the two images. Previous methods often considered only small subsets of image pixels; matching only pixels for which stable scales may be reliably estimated. More recently, others have considered dense correspondences, but with substantial costs associated with generating, storing and matching scale invariant descriptors. Our work here is motivated by the observation that pixels in the image have contexts -- the pixels around them -- which may be exploited in order to estimate local scales reliably and repeatably. Specifically, we make the following contributions. (i) We show that scales estimated in sparse interest points may be propagated to neighboring pixels where this information cannot be reliably determined. Doing so allows scale invariant descriptors to be extracted anywhere in the image, not just in detected interest points. (ii) We present three different means for propagating this information: using only the scales at detected interest points, using the underlying image information to guide the propagation of this information across each image, separately, and using both images simultaneously. Finally, (iii), we provide extensive results, both qualitative and quantitative, demonstrating that accurate dense correspondences can be obtained even between very different images, with little computational costs beyond those required by existing methods.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6336",
        "title": "A multilevel thresholding algorithm using Electromagnetism Optimization",
        "authors": [
            "Diego Oliva",
            "Erik Cuevas",
            "Gonzalo Pajares",
            "Daniel Zaldivar",
            "Valentin Osuna"
        ],
        "abstract": "Segmentation is one of the most important tasks in image processing. It consist in classify the pixels into two or more groups depending on their intensity levels and a threshold value. The quality of the segmentation depends on the method applied to select the threshold. The use of the classical implementations for multilevel thresholding is computationally expensive since they exhaustively search the best values to optimize the objective function. Under such conditions, the use of optimization evolutionary approaches has been extended. The Electromagnetism Like algorithm (EMO) is an evolutionary method which mimics the attraction repulsion mechanism among charges to evolve the members of a population. Different to other algorithms, EMO exhibits interesting search capabilities whereas maintains a low computational overhead. In this paper, a multilevel thresholding (MT) algorithm based on the EMO is introduced. The approach combines the good search capabilities of EMO algorithm with objective functions proposed by the popular MT methods of Otsu and Kapur. The algorithm takes random samples from a feasible search space inside the image histogram. Such samples build each particle in the EMO context whereas its quality is evaluated considering the objective that is function employed by the Otsu or Kapur method. Guided by these objective values the set of candidate solutions are evolved through the EMO operators until an optimal solution is found. The approach generates a multilevel segmentation algorithm which can effectively identify the threshold values of a digital image in a reduced number of iterations. Experimental results show performance evidence of the implementation of EMO for digital image segmentation.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6390",
        "title": "Image patch analysis and clustering of sunspots: a dimensionality reduction approach",
        "authors": [
            "Kevin R. Moon",
            "Jimmy J. Li",
            "Veronique Delouille",
            "Fraser Watson",
            "Alfred O. Hero III"
        ],
        "abstract": "Sunspots, as seen in white light or continuum images, are associated with regions of high magnetic activity on the Sun, visible on magnetogram images. Their complexity is correlated with explosive solar activity and so classifying these active regions is useful for predicting future solar activity. Current classification of sunspot groups is visually based and suffers from bias. Supervised learning methods can reduce human bias but fail to optimally capitalize on the information present in sunspot images. This paper uses two image modalities (continuum and magnetogram) to characterize the spatial and modal interactions of sunspot and magnetic active region images and presents a new approach to cluster the images. Specifically, in the framework of image patch analysis, we estimate the number of intrinsic parameters required to describe the spatial and modal dependencies, the correlation between the two modalities and the corresponding spatial patterns, and examine the phenomena at different scales within the images. To do this, we use linear and nonlinear intrinsic dimension estimators, canonical correlation analysis, and multiresolution analysis of intrinsic dimension.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6425",
        "title": "Compressive Imaging and Characterization of Sparse Light Deflection Maps",
        "authors": [
            "Prasad Sudhakar",
            "Laurent Jacques",
            "Xavier Dubois",
            "Philippe Antoine",
            "Luc Joannes"
        ],
        "abstract": "Light rays incident on a transparent object of uniform refractive index undergo deflections, which uniquely characterize the surface geometry of the object. Associated with each point on the surface is a deflection map (or spectrum) which describes the pattern of deflections in various directions. This article presents a novel method to efficiently acquire and reconstruct sparse deflection spectra induced by smooth object surfaces. To this end, we leverage the framework of Compressed Sensing (CS) in a particular implementation of a schlieren deflectometer, i.e., an optical system providing linear measurements of deflection spectra with programmable spatial light modulation patterns. We design those modulation patterns on the principle of spread spectrum CS for reducing the number of observations. The ability of our device to simultaneously observe the deflection spectra on a dense discretization of the object surface is related to a Multiple Measurement Vector (MMV) model. This scheme allows us to estimate both the noise power and the instrumental point spread function.\n",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6507",
        "title": "Weakly-supervised Discovery of Visual Pattern Configurations",
        "authors": [
            "Hyun Oh Song",
            "Yong Jae Lee",
            "Stefanie Jegelka",
            "Trevor Darrell"
        ],
        "abstract": "The increasing prominence of weakly labeled data nurtures a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.\n    ",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6538",
        "title": "A Bimodal Co-Sparse Analysis Model for Image Processing",
        "authors": [
            "Martin Kiechle",
            "Tim Habigt",
            "Simon Hawe",
            "Martin Kleinsteuber"
        ],
        "abstract": "The success of many computer vision tasks lies in the ability to exploit the interdependency between different image modalities such as intensity and depth. Fusing corresponding information can be achieved on several levels, and one promising approach is the integration at a low level. Moreover, sparse signal models have successfully been used in many vision applications. Within this area of research, the so called co-sparse analysis model has attracted considerably less attention than its well-known counterpart, the sparse synthesis model, although it has been proven to be very useful in various image processing applications. In this paper, we propose a co-sparse analysis model that is able to capture the interdependency of two image modalities. It is based on the assumption that a pair of analysis operators exists, so that the co-supports of the corresponding bimodal image structures are correlated. We propose an algorithm that is able to learn such a coupled pair of operators from registered and noise-free training data. Furthermore, we explain how this model can be applied to solve linear inverse problems in image processing and how it can be used for image registration tasks. This paper extends the work of some of the authors by two major contributions. Firstly, a modification of the learning process is proposed that a priori guarantees unit norm and zero-mean of the rows of the operator. This accounts for the intuition that contrast in image modalities carries the most information. Secondly, the model is used in a novel bimodal image registration algorithm which estimates the transformation parameters of unregistered images of different modalities.\n    ",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6558",
        "title": "$ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image Transforms",
        "authors": [
            "Yaroslav Ganin",
            "Victor Lempitsky"
        ],
        "abstract": "We propose a new architecture for difficult image processing operations, such as natural edge detection or thin object segmentation. The architecture is based on a simple combination of convolutional neural networks with the nearest neighbor search.\n",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6560",
        "title": "Multi Circle Detection on Images Using Artificial Bee Colony (ABC) Optimization",
        "authors": [
            "Erik Cuevas",
            "Felipe Sencion-Echauri",
            "Daniel Zaldivar",
            "Marco Perez Cisneros"
        ],
        "abstract": "Hough transform (HT) has been the most common method for circle detection, exhibiting robustness, but adversely demanding considerable computational effort and large memory requirements. Alternative approaches include heuristic methods that employ iterative optimization procedures for detecting multiple circles. Since only one circle can be marked at each optimization cycle, multiple executions must be enforced in order to achieve multi detection. This paper presents an algorithm for automatic detection of multiple circular shapes that considers the overall process as a multi-modal optimization problem. The approach is based on the artificial bee colony (ABC) algorithm, a swarm optimization algorithm inspired by the intelligent foraging behavior of honey bees. Unlike the original ABC algorithm, the proposed approach presents the addition of a memory for discarded solutions. Such memory allows holding important information regarding other local optima which might have emerged during the optimization process. The detector uses a combination of three non-collinear edge points as parameters to determine circle candidates. A matching function (nectar- amount) determines if such circle candidates (bee-food-sources) are actually present in the image. Guided by the values of such matching functions, the set of encoded candidate circles are evolved through the ABC algorithm so that the best candidate (global optimum) can be fitted into an actual circle within the edge only image. Then, an analysis of the incorporated memory is executed in order to identify potential local optima, i.e., other circles.\n    ",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6568",
        "title": "Support vector machine classification of dimensionally reduced structural MRI images for dementia",
        "authors": [
            "V. A. Miller",
            "S. Erlien",
            "J. Piersol"
        ],
        "abstract": "We classify very-mild to moderate dementia in patients (CDR ranging from 0 to 2) using a support vector machine classifier acting on dimensionally reduced feature set derived from MRI brain scans of the 416 subjects available in the OASIS-Brains dataset. We use image segmentation and principal component analysis to reduce the dimensionality of the data. Our resulting feature set contains 11 features for each subject. Performance of the classifiers is evaluated using 10-fold cross-validation. Using linear and (gaussian) kernels, we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of 85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%), and test Matthews correlation coefficient of 0.594 (0.616).\n    ",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2014-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6595",
        "title": "3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for Rapid Geometry Acquisition",
        "authors": [
            "Qing Gu",
            "Kyriakos Herakleous",
            "Charalambos Poullis"
        ],
        "abstract": "Recently, there has been an increase in the demand of virtual 3D objects representing real-life objects. A plethora of methods and systems have already been proposed for the acquisition of the geometry of real-life objects ranging from those which employ active sensor technology, passive sensor technology or a combination of various techniques.\n",
        "submission_date": "2014-06-25T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6811",
        "title": "Face Image Classification by Pooling Raw Features",
        "authors": [
            "Fumin Shen",
            "Chunhua Shen",
            "Heng Tao Shen"
        ],
        "abstract": "We propose a very simple, efficient yet surprisingly effective feature extraction method for face recognition (about 20 lines of Matlab code), which is mainly inspired by spatial pyramid pooling in generic image classification. We show that features formed by simply pooling local patches over a multi-level pyramid, coupled with a linear classifier, can significantly outperform most recent face recognition methods. The simplicity of our feature extraction procedure is demonstrated by the fact that no learning is involved (except PCA whitening). We show that, multi-level spatial pooling and dense extraction of multi-scale patches play critical roles in face image classification. The extracted facial features can capture strong structural information of individual faces with no label information being used. We also find that, pre-processing on local image patches such as contrast normalization can have an important impact on the classification accuracy. In particular, on the challenging face recognition datasets of FERET and LFW-a, our method improves previous best results by more than 10% and 20%, respectively.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6818",
        "title": "Face Identification with Second-Order Pooling",
        "authors": [
            "Fumin Shen",
            "Chunhua Shen",
            "Heng Tao Shen"
        ],
        "abstract": "Automatic face recognition has received significant performance improvement by developing specialised facial image representations. On the other hand, generic object recognition has rarely been applied to the face recognition. Spatial pyramid pooling of features encoded by an over-complete dictionary has been the key component of many state-of-the-art image classification systems. Inspired by its success, in this work we develop a new face image representation method inspired by the second-order pooling in Carreira et al. [1], which was originally proposed for image segmentation. The proposed method differs from the previous methods in that, we encode the densely extracted local patches by a small-size dictionary; and the facial image signatures are obtained by pooling the second-order statistics of the encoded features. We show the importance of pooling on encoded features, which is bypassed by the original second-order pooling method to avoid the high computational cost. Equipped with a simple linear classifier, the proposed method outperforms the state-of-the-art face identification performance by large margins. For example, on the LFW databases, the proposed method performs better than the previous best by around 13% accuracy.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6946",
        "title": "An improved computer vision method for detecting white blood cells",
        "authors": [
            "Erik Cuevas",
            "Margarita Diaz",
            "Miguel Manzanares",
            "Daniel Zaldivar",
            "Marco Perez"
        ],
        "abstract": "The automatic detection of White Blood Cells (WBC) still remains as an unsolved issue in medical imaging. The analysis of WBC images has engaged researchers from fields of medicine and computer vision alike. Since WBC can be approximated by an ellipsoid form, an ellipse detector algorithm may be successfully applied in order to recognize them. This paper presents an algorithm for the automatic detection of WBC embedded into complicated and cluttered smear images that considers the complete process as a multi-ellipse detection problem. The approach, based on the Differential Evolution (DE) algorithm, transforms the detection task into an optimization problem where individuals emulate candidate ellipses. An objective function evaluates if such candidate ellipses are really present in the edge image of the smear. Guided by the values of such function, the set of encoded candidate ellipses (individuals) are evolved using the DE algorithm so that they can fit into the WBC enclosed within the edge-only map of the image. Experimental results from white blood cell images with a varying range of complexity are included to validate the efficiency of the proposed technique in terms of accuracy and robustness.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6947",
        "title": "Deep Learning Multi-View Representation for Face Recognition",
        "authors": [
            "Zhenyao Zhu",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6962",
        "title": "How good are detection proposals, really?",
        "authors": [
            "Jan Hosang",
            "Rodrigo Benenson",
            "Bernt Schiele"
        ],
        "abstract": " Current top performing Pascal VOC object detectors employ detection proposals to guide the search for objects thereby avoiding exhaustive sliding window search across images. Despite the popularity of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in depth analysis of ten object proposal methods along with four baselines regarding ground truth annotation recall (on Pascal VOC 2007 and ImageNet 2013), repeatability, and impact on DPM detector performance. Our findings show common weaknesses of existing methods, and provide insights to choose the most adequate method for different settings.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7062",
        "title": "Adaptive Mesh Representation and Restoration of Biomedical Images",
        "authors": [
            "Ke Liu",
            "Ming Xu",
            "Zeyun Yu"
        ],
        "abstract": "The triangulation of images has become an active research area in recent years for its compressive representation and ease of image processing and visualization. However, little work has been done on how to faithfully recover image intensities from a triangulated mesh of an image, a process also known as image restoration or decoding from meshes. The existing methods such as linear interpolation, least-square interpolation, or interpolation based on radial basis functions (RBFs) work to some extent, but often yield blurred features (edges, corners, etc.). The main reason for this problem is due to the isotropically-defined Euclidean distance that is taken into consideration in these methods, without considering the anisotropicity of feature intensities in an image. Moreover, most existing methods use intensities defined at mesh nodes whose intensities are often ambiguously defined on or near image edges (or feature boundaries). In the current paper, a new method of restoring an image from its triangulation representation is proposed, by utilizing anisotropic radial basis functions (ARBFs). This method considers not only the geometrical (Euclidean) distances but also the local feature orientations (anisotropic intensities). Additionally, this method is based on the intensities of mesh faces instead of mesh nodes and thus provides a more robust restoration. The two strategies together guarantee excellent feature-preserving restoration of an image with arbitrary super-resolutions from its triangulation representation, as demonstrated by various experiments provided in the paper.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7075",
        "title": "Adaptive texture energy measure method",
        "authors": [
            "Omer Faruk Ertugrul"
        ],
        "abstract": "Recent developments in image quality, data storage, and computational capacity have heightened the need for texture analysis in image process. To date various methods have been developed and introduced for assessing textures in images. One of the most popular texture analysis methods is the Texture Energy Measure (TEM) and it has been used for detecting edges, levels, waves, spots and ripples by employing predefined TEM masks to images. Despite several success- ful studies, TEM has a number of serious weaknesses in use. The major drawback is; the masks are predefined therefore they cannot be adapted to image. A new method, Adaptive Texture Energy Measure Method (aTEM), was offered to over- come this disadvantage of TEM by using adaptive masks by adjusting the contrast, sharpening and orientation angle of the mask. To assess the applicability of aTEM, it is compared with TEM. The accuracy of the classification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292 and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively by aTEM. The results of this study indicate that aTEM is a successful method for texture analysis.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7112",
        "title": "3D planar patch extraction from stereo using probabilistic region growing",
        "authors": [
            "Vasileios Zografos"
        ],
        "abstract": "This article presents a novel 3D planar patch extraction method using a probabilistic region growing algorithm. Our method works by simultaneously initiating multiple planar patches from seed points, the latter determined by an intensity-based 2D segmentation algorithm in the stereo-pair images. The patches are grown incrementally and in parallel as 3D scene points are considered for membership, using a probabilistic distance likelihood measure. In addition, we have incorporated prior information based on the noise model in the 2D images and the scene configuration but also include the intensity information resulting from the initial segmentation. This method works well across many different data-sets, involving real and synthetic examples of both regularly and non-regularly sampled data, and is fast enough that may be used for robot navigation tasks of path detection and obstacle avoidance.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7120",
        "title": "Template Matching based Object Detection Using HOG Feature Pyramid",
        "authors": [
            "Anish Acharya"
        ],
        "abstract": "This article provides a step by step development of designing a Object Detection scheme using the HOG based Feature Pyramid aligned with the concept of Template Matching.\n    ",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7128",
        "title": "On a new formulation of nonlocal image filters involving the relative rearrangement",
        "authors": [
            "Gonzalo Galiano",
            "Juli\u00e1n Velasco"
        ],
        "abstract": "Nonlocal filters are simple and powerful techniques for image denoising. In this paper we study the reformulation of a broad class of nonlocal filters in terms of two functional rearrangements: the decreasing and the relative rearrangements.\n",
        "submission_date": "2014-06-27T00:00:00",
        "last_modified_date": "2014-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7360",
        "title": "A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data",
        "authors": [
            "Ognjen Arandjelovic"
        ],
        "abstract": "In this paper we address the problem of matching patterns in the so-called verification setting in which a novel, query pattern is verified against a single training pattern: the decision sought is whether the two match (i.e. belong to the same class) or not. Unlike previous work which has universally focused on the development of more discriminative distance functions between patterns, here we consider the equally important and pervasive task of selecting a distance threshold which fits a particular operational requirement - specifically, the target false positive rate (FPR). First, we argue on theoretical grounds that a data-driven approach is inherently ill-conditioned when the desired FPR is low, because by the very nature of the challenge only a small portion of training data affects or is affected by the desired threshold. This leads us to propose a general, statistical model-based method instead. Our approach is based on the interpretation of an inter-pattern distance as implicitly defining a pattern embedding which approximately distributes patterns according to an isotropic multi-variate normal distribution in some space. This interpretation is then used to show that the distribution of training inter-pattern distances is the non-central chi2 distribution, differently parameterized for each class. Thus, to make the class-specific threshold choice we propose a novel analysis-by-synthesis iterative algorithm which estimates the three free parameters of the model (for each class) using task-specific constraints. The validity of the premises of our work and the effectiveness of the proposed method are demonstrated by applying the method to the task of set-based face verification on a large database of pseudo-random head motion videos.\n    ",
        "submission_date": "2014-06-28T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7444",
        "title": "Learning to Deblur",
        "authors": [
            "Christian J. Schuler",
            "Michael Hirsch",
            "Stefan Harmeling",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.\n    ",
        "submission_date": "2014-06-28T00:00:00",
        "last_modified_date": "2014-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7525",
        "title": "Fusion Based Holistic Road Scene Understanding",
        "authors": [
            "Wenqi Huang",
            "Xiaojin Gong"
        ],
        "abstract": "This paper addresses the problem of holistic road scene understanding based on the integration of visual and range data. To achieve the grand goal, we propose an approach that jointly tackles object-level image segmentation and semantic region labeling within a conditional random field (CRF) framework. Specifically, we first generate semantic object hypotheses by clustering 3D points, learning their prior appearance models, and using a deep learning method for reasoning their semantic categories. The learned priors, together with spatial and geometric contexts, are incorporated in CRF. With this formulation, visual and range data are fused thoroughly, and moreover, the coupled segmentation and semantic labeling problem can be inferred via Graph Cuts. Our approach is validated on the challenging KITTI dataset that contains diverse complicated road scenarios. Both quantitative and qualitative evaluations demonstrate its effectiveness.\n    ",
        "submission_date": "2014-06-29T00:00:00",
        "last_modified_date": "2014-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0010",
        "title": "Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and Shadow-free Image",
        "authors": [
            "Liangqiong Qu",
            "Jiandong Tian",
            "Zhi Han",
            "Yandong Tang"
        ],
        "abstract": "In this paper, we propose a novel, effective and fast method to obtain a color illumination invariant and shadow-free image from a single outdoor image. Different from state-of-the-art methods for shadow-free image that either need shadow detection or statistical learning, we set up a linear equation set for each pixel value vector based on physically-based shadow invariants, deduce a pixel-wise orthogonal decomposition for its solutions, and then get an illumination invariant vector for each pixel value vector on an image. The illumination invariant vector is the unique particular solution of the linear equation set, which is orthogonal to its free solutions. With this illumination invariant vector and Lab color space, we propose an algorithm to generate a shadow-free image which well preserves the texture and color information of the original image. A series of experiments on a diverse set of outdoor images and the comparisons with the state-of-the-art methods validate our method.\n    ",
        "submission_date": "2014-06-30T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0221",
        "title": "Imaging with Kantorovich-Rubinstein discrepancy",
        "authors": [
            "Jan Lellmann",
            "Dirk A. Lorenz",
            "Carola Sch\u00f6nlieb",
            "Tuomo Valkonen"
        ],
        "abstract": "We propose the use of the Kantorovich-Rubinstein norm from optimal transport in imaging problems. In particular, we discuss a variational regularisation model endowed with a Kantorovich-Rubinstein discrepancy term and total variation regularization in the context of image denoising and cartoon-texture decomposition. We point out connections of this approach to several other recently proposed methods such as total generalized variation and norms capturing oscillating patterns. We also show that the respective optimization problem can be turned into a convex-concave saddle point problem with simple constraints and hence, can be solved by standard tools. Numerical examples exhibit interesting features and favourable performance for denoising and cartoon-texture decomposition.\n    ",
        "submission_date": "2014-07-01T00:00:00",
        "last_modified_date": "2014-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0342",
        "title": "A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model and Compressed Sparse FOMFE Model",
        "authors": [
            "Jinwei Xu",
            "Jiankun Hu",
            "Xiuping Jia"
        ],
        "abstract": "Orientation field, representing the fingerprint ridge structure direction, plays a crucial role in fingerprint-related image processing tasks. Orientation field is able to be constructed by either non-parametric or parametric methods. In this paper, the advantages and disadvantages regarding to the existing non-parametric and parametric approaches are briefly summarized. With the further investigation for constructing the orientation field by parametric technique, two new models - sparse FOMFE model and compressed sparse FOMFE model are introduced, based on the rapidly developing signal sparse representation and compressed sensing theories. The experiments on high-quality fingerprint image dataset (plain and rolled print) and poor-quality fingerprint image dataset (latent print) demonstrate their feasibilities to construct the orientation field in a sparse or even compressed sparse mode. The comparisons among the state-of-art orientation field modeling approaches show that the proposed two models have the potential availability in big data-oriented fingerprint indexing tasks.\n    ",
        "submission_date": "2014-07-01T00:00:00",
        "last_modified_date": "2014-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0623",
        "title": "A Data-Driven Approach for Tag Refinement and Localization in Web Videos",
        "authors": [
            "Lamberto Ballan",
            "Marco Bertini",
            "Giuseppe Serra",
            "Alberto Del Bimbo"
        ],
        "abstract": "Tagging of visual content is becoming more and more widespread as web-based services and social networks have popularized tagging functionalities among their users. These user-generated tags are used to ease browsing and exploration of media collections, e.g. using tag clouds, or to retrieve multimedia content. However, not all media are equally tagged by users. Using the current systems is easy to tag a single photo, and even tagging a part of a photo, like a face, has become common in sites like Flickr and Facebook. On the other hand, tagging a video sequence is more complicated and time consuming, so that users just tag the overall content of a video. In this paper we present a method for automatic video annotation that increases the number of tags originally provided by users, and localizes them temporally, associating tags to keyframes. Our approach exploits collective knowledge embedded in user-generated tags and web sources, and visual similarity of keyframes and images uploaded to social sites like YouTube and Flickr, as well as web sources like Google and Bing. Given a keyframe, our method is able to select on the fly from these visual sources the training exemplars that should be the most relevant for this test sample, and proceeds to transfer labels across similar images. Compared to existing video tagging approaches that require training classifiers for each tag, our system has few parameters, is easy to implement and can deal with an open vocabulary scenario. We demonstrate the approach on tag refinement and localization on DUT-WEBV, a large dataset of web videos, and show state-of-the-art results.\n    ",
        "submission_date": "2014-07-02T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0717",
        "title": "Deep Poselets for Human Detection",
        "authors": [
            "Lubomir Bourdev",
            "Fei Yang",
            "Rob Fergus"
        ],
        "abstract": "We address the problem of detecting people in natural scenes using a part approach based on poselets. We propose a bootstrapping method that allows us to collect millions of weakly labeled examples for each poselet type. We use these examples to train a Convolutional Neural Net to discriminate different poselet types and separate them from the background class. We then use the trained CNN as a way to represent poselet patches with a Pose Discriminative Feature (PDF) vector -- a compact 256-dimensional feature vector that is effective at discriminating pose from appearance. We train the poselet model on top of PDF features and combine them with object-level CNNs for detection and bounding box prediction. The resulting model leads to state-of-the-art performance for human detection on the PASCAL datasets.\n    ",
        "submission_date": "2014-07-02T00:00:00",
        "last_modified_date": "2014-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0733",
        "title": "Cortical spatio-temporal dimensionality reduction for visual grouping",
        "authors": [
            "Giacomo Cocci",
            "Davide Barbieri",
            "Giovanna Citti",
            "Alessandro Sarti"
        ],
        "abstract": "The visual systems of many mammals, including humans, is able to integrate the geometric information of visual stimuli and to perform cognitive tasks already at the first stages of the cortical processing. This is thought to be the result of a combination of mechanisms, which include feature extraction at single cell level and geometric processing by means of cells connectivity. We present a geometric model of such connectivities in the space of detected features associated to spatio-temporal visual stimuli, and show how they can be used to obtain low-level object segmentation. The main idea is that of defining a spectral clustering procedure with anisotropic affinities over datasets consisting of embeddings of the visual stimuli into higher dimensional spaces. Neural plausibility of the proposed arguments will be discussed.\n    ",
        "submission_date": "2014-07-02T00:00:00",
        "last_modified_date": "2014-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0765",
        "title": "BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification",
        "authors": [
            "Awais Mansoor",
            "Valery Patsekin",
            "Dale Scherl",
            "J. Paul Robinson",
            "Bartlomiej Rajwa"
        ],
        "abstract": "Dental biofilm is the deposition of microbial material over a tooth substratum. Several methods have recently been reported in the literature for biofilm quantification; however, at best they provide a barely automated solution requiring significant input needed from the human expert. On the contrary, state-of-the-art automatic biofilm methods fail to make their way into clinical practice because of the lack of effective mechanism to incorporate human input to handle praxis or misclassified regions. Manual delineation, the current gold standard, is time consuming and subject to expert bias. In this paper, we introduce a new semi-automated software tool, BiofilmQuant, for dental biofilm quantification in quantitative light-induced fluorescence (QLF) images. The software uses a robust statistical modeling approach to automatically segment the QLF image into three classes (background, biofilm, and tooth substratum) based on the training data. This initial segmentation has shown a high degree of consistency and precision on more than 200 test QLF dental scans. Further, the proposed software provides the clinicians full control to fix any misclassified areas using a single click. In addition, BiofilmQuant also provides a complete solution for the longitudinal quantitative analysis of biofilm of the full set of teeth, providing greater ease of usability.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0786",
        "title": "Strengthening the Effectiveness of Pedestrian Detection with Spatially Pooled Features",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "We propose a simple yet effective approach to the problem of pedestrian detection which outperforms the current state-of-the-art. Our new features are built on the basis of low-level visual features and spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. We then directly optimise the partial area under the ROC curve (\\pAUC) measure, which concentrates detection performance in the range of most practical importance. The combination of these factors leads to a pedestrian detector which outperforms all competitors on all of the standard benchmark datasets. We advance state-of-the-art results by lowering the average miss rate from $13\\%$ to $11\\%$ on the INRIA benchmark, $41\\%$ to $37\\%$ on the ETH benchmark, $51\\%$ to $42\\%$ on the TUD-Brussels benchmark and $36\\%$ to $29\\%$ on the Caltech-USA benchmark.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0935",
        "title": "Multiple Moving Object Recognitions in video based on Log Gabor-PCA Approach",
        "authors": [
            "M. T Gopalakrishna",
            "M. Ravishankar",
            "D. R Rameshbabu"
        ],
        "abstract": "Object recognition in the video sequence or images is one of the sub-field of computer vision. Moving object recognition from a video sequence is an appealing topic with applications in various areas such as airport safety, intrusion surveillance, video monitoring, intelligent highway, etc. Moving object recognition is the most challenging task in intelligent video surveillance system. In this regard, many techniques have been proposed based on different methods. Despite of its importance, moving object recognition in complex environments is still far from being completely solved for low resolution videos, foggy videos, and also dim video sequences. All in all, these make it necessary to develop exceedingly robust techniques. This paper introduces multiple moving object recognition in the video sequence based on LoG Gabor-PCA approach and Angle based distance Similarity measures techniques used to recognize the object as a human, vehicle etc. Number of experiments are conducted for indoor and outdoor video sequences of standard datasets and also our own collection of video sequences comprising of partial night vision video sequences. Experimental results show that our proposed approach achieves an excellent recognition rate. Results obtained are satisfactory and competent.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1120",
        "title": "From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices",
        "authors": [
            "Mehrtash T. Harandi",
            "Mathieu Salzmann",
            "Richard Hartley"
        ],
        "abstract": "Representing images and videos with Symmetric Positive Definite (SPD) matrices and considering the Riemannian geometry of the resulting space has proven beneficial for many recognition tasks. Unfortunately, computation on the Riemannian manifold of SPD matrices --especially of high-dimensional ones-- comes at a high cost that limits the applicability of existing techniques. In this paper we introduce an approach that lets us handle high-dimensional SPD matrices by constructing a lower-dimensional, more discriminative SPD manifold. To this end, we model the mapping from the high-dimensional SPD manifold to the low-dimensional one with an orthonormal projection. In particular, we search for a projection that yields a low-dimensional manifold with maximum discriminative power encoded via an affinity-weighted similarity measure based on metrics on the manifold. Learning can then be expressed as an optimization problem on a Grassmann manifold. Our evaluation on several classification tasks shows that our approach leads to a significant accuracy gain over state-of-the-art methods.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1123",
        "title": "Expanding the Family of Grassmannian Kernels: An Embedding Perspective",
        "authors": [
            "Mehrtash T. Harandi",
            "Mathieu Salzmann",
            "Sadeep Jayasumana",
            "Richard Hartley",
            "Hongdong Li"
        ],
        "abstract": "Modeling videos and image-sets as linear subspaces has proven beneficial for many visual recognition tasks. However, it also incurs challenges arising from the fact that linear subspaces do not obey Euclidean geometry, but lie on a special type of Riemannian manifolds known as Grassmannian. To leverage the techniques developed for Euclidean spaces (e.g, support vector machines) with subspaces, several recent studies have proposed to embed the Grassmannian into a Hilbert space by making use of a positive definite kernel. Unfortunately, only two Grassmannian kernels are known, none of which -as we will show- is universal, which limits their ability to approximate a target function arbitrarily well. Here, we introduce several positive definite Grassmannian kernels, including universal ones, and demonstrate their superiority over previously-known kernels in various tasks, such as classification, clustering, sparse coding and hashing.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1165",
        "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition",
        "authors": [
            "Prashant Bordea",
            "Amarsinh Varpeb",
            "Ramesh Manzac",
            "Pravin Yannawara"
        ],
        "abstract": "Automatic Speech Recognition (ASR) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area. In recent year, there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions. The objective of audio-visual speech recognition system is to improve recognition accuracy. In this paper we computed visual features using Zernike moments and audio feature using Mel Frequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of Independent Standard Words) dataset which contains collection of isolated set of city names of 10 speakers. The visual features were normalized and dimension of features set was reduced by Principal Component Analysis (PCA) in order to recognize the isolated word utterance on PCA ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1208",
        "title": "Weakly Supervised Action Labeling in Videos Under Ordering Constraints",
        "authors": [
            "Piotr Bojanowski",
            "R\u00e9mi Lajugie",
            "Francis Bach",
            "Ivan Laptev",
            "Jean Ponce",
            "Cordelia Schmid",
            "Josef Sivic"
        ],
        "abstract": "We are given a set of video clips, each one annotated with an {\\em ordered} list of actions, such as \"walk\" then \"sit\" then \"answer phone\" extracted from, for example, the associated text script. We seek to temporally localize the individual actions in each clip as well as to learn a discriminative classifier for each action. We formulate the problem as a weakly supervised temporal assignment with ordering constraints. Each video clip is divided into small time intervals and each time interval of each video clip is assigned one action label, while respecting the order in which the action labels appear in the given annotations. We show that the action label assignment can be determined together with learning a classifier for each action in a discriminative manner. We evaluate the proposed model on a new and challenging dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1267",
        "title": "Calibration of Multiple Fish-Eye Cameras Using a Wand",
        "authors": [
            "Qiang Fu",
            "Quan Quan",
            "Kai-Yuan Cai"
        ],
        "abstract": "Fish-eye cameras are becoming increasingly popular in computer vision, but their use for 3D measurement is limited partly due to the lack of an accurate, efficient and user-friendly calibration procedure. For such a purpose, we propose a method to calibrate the intrinsic and extrinsic parameters (including radial distortion parameters) of two/multiple fish-eye cameras simultaneously by using a wand under general motions. Thanks to the generic camera model used, the proposed calibration method is also suitable for two/multiple conventional cameras and mixed cameras (e.g. two conventional cameras and a fish-eye camera). Simulation and real experiments demonstrate the effectiveness of the proposed method. Moreover, we develop the camera calibration toolbox, which is available online.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1339",
        "title": "Inverse Graphics with Probabilistic CAD Models",
        "authors": [
            "Tejas D. Kulkarni",
            "Vikash K. Mansinghka",
            "Pushmeet Kohli",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "Recently, multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed. However, applications to 3D perception from natural images have focused on low-dimensional latent scenes, due to challenges in both modeling and inference. Accounting for the enormous variability in 3D object shape and 2D appearance via realistic generative models seems intractable, as does inverting even simple versions of the many-to-many computations that link 3D scenes to 2D images. This paper proposes and evaluates an approach that addresses key aspects of both these challenges. We show that it is possible to solve challenging, real-world 3D vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic CAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D meshes corresponding to plausible objects and apply affine transformations to place them in a scene. Image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature. Our inference algorithm integrates single-site and locally blocked Metropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative data-driven proposals learned from training data generated from our models. We apply this approach to 3D human pose estimation and object shape reconstruction from single images, achieving quantitative and qualitative performance improvements over state-of-the-art baselines.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1352",
        "title": "Homophilic Clustering by Locally Asymmetric Geometry",
        "authors": [
            "Deli Zhao",
            "Xiaoou Tang"
        ],
        "abstract": "Clustering is indispensable for data analysis in many scientific disciplines. Detecting clusters from heavy noise remains challenging, particularly for high-dimensional sparse data. Based on graph-theoretic framework, the present paper proposes a novel algorithm to address this issue. The locally asymmetric geometries of neighborhoods between data points result in a directed similarity graph to model the structural connectivity of data points. Performing similarity propagation on this directed graph simply by its adjacency matrix powers leads to an interesting discovery, in the sense that if the in-degrees are ordered by the corresponding sorted out-degrees, they will be self-organized to be homophilic layers according to the different distributions of cluster densities, which is dubbed the Homophilic In-degree figure (the HI figure). With the HI figure, we can easily single out all cores of clusters, identify the boundary between cluster and noise, and visualize the intrinsic structures of clusters. Based on the in-degree homophily, we also develop a simple efficient algorithm of linear space complexity to cluster noisy data. Extensive experiments on toy and real-world scientific data validate the effectiveness of our algorithms.\n    ",
        "submission_date": "2014-07-05T00:00:00",
        "last_modified_date": "2014-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1490",
        "title": "Large-scale Supervised Hierarchical Feature Learning for Face Recognition",
        "authors": [
            "Jianguo Li",
            "Yurong Chen"
        ],
        "abstract": "This paper proposes a novel face recognition algorithm based on large-scale supervised hierarchical feature learning. The approach consists of two parts: hierarchical feature learning and large-scale model learning. The hierarchical feature learning searches feature in three levels of granularity in a supervised way. First, face images are modeled by receptive field theory, and the representation is an image with many channels of Gaussian receptive maps. We activate a few most distinguish channels by supervised learning. Second, the face image is further represented by patches of picked channels, and we search from the over-complete patch pool to activate only those most discriminant patches. Third, the feature descriptor of each patch is further projected to lower dimension subspace with discriminant subspace analysis.\n",
        "submission_date": "2014-07-06T00:00:00",
        "last_modified_date": "2014-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1610",
        "title": "Analyzing the Performance of Multilayer Neural Networks for Object Recognition",
        "authors": [
            "Pulkit Agrawal",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1785",
        "title": "Novel methods for multilinear data completion and de-noising based on tensor-SVD",
        "authors": [
            "Zemin Zhang",
            "Gregory Ely",
            "Shuchin Aeron",
            "Ning Hao",
            "Misha Kilmer"
        ],
        "abstract": "In this paper we propose novel methods for completion (from limited samples) and de-noising of multilinear (tensor) data and as an application consider 3-D and 4- D (color) video data completion and de-noising. We exploit the recently proposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the notion of multilinear rank and a related tensor nuclear norm was proposed in [11] to characterize informational and structural complexity of multilinear data. We first show that videos with linear camera motion can be represented more efficiently using t-SVD compared to the approaches based on vectorizing or flattening of the tensors. Since efficiency in representation implies efficiency in recovery, we outline a tensor nuclear norm penalized algorithm for video completion from missing entries. Application of the proposed algorithm for video recovery from missing entries is shown to yield a superior performance over existing methods. We also consider the problem of tensor robust Principal Component Analysis (PCA) for de-noising 3-D video data from sparse random corruptions. We show superior performance of our method compared to the matrix robust PCA adapted to this setting as proposed in [4].\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1808",
        "title": "Simultaneous Detection and Segmentation",
        "authors": [
            "Bharath Hariharan",
            "Pablo Arbel\u00e1ez",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top- down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1957",
        "title": "Regression-Based Image Alignment for General Object Categories",
        "authors": [
            "Hilton Bristow",
            "Simon Lucey"
        ],
        "abstract": "Gradient-descent methods have exhibited fast and reliable performance for image alignment in the facial domain, but have largely been ignored by the broader vision community. They require the image function be smooth and (numerically) differentiable -- properties that hold for pixel-based representations obeying natural image statistics, but not for more general classes of non-linear feature transforms. We show that transforms such as Dense SIFT can be incorporated into a Lucas Kanade alignment framework by predicting descent directions via regression. This enables robust matching of instances from general object categories whilst maintaining desirable properties of Lucas Kanade such as the capacity to handle high-dimensional warp parametrizations and a fast rate of convergence. We present alignment results on a number of objects from ImageNet, and an extension of the method to unsupervised joint alignment of objects from a corpus of images.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1974",
        "title": "Learning Discriminative Stein Kernel for SPD Matrices and Its Applications",
        "authors": [
            "Jianjia Zhang",
            "Lei Wang",
            "Luping Zhou",
            "Wanqing Li"
        ],
        "abstract": "Stein kernel has recently shown promising performance on classifying images represented by symmetric positive definite (SPD) matrices. It evaluates the similarity between two SPD matrices through their eigenvalues. In this paper, we argue that directly using the original eigenvalues may be problematic because: i) Eigenvalue estimation becomes biased when the number of samples is inadequate, which may lead to unreliable kernel evaluation; ii) More importantly, eigenvalues only reflect the property of an individual SPD matrix. They are not necessarily optimal for computing Stein kernel when the goal is to discriminate different sets of SPD matrices. To address the two issues in one shot, we propose a discriminative Stein kernel, in which an extra parameter vector is defined to adjust the eigenvalues of the input SPD matrices. The optimal parameter values are sought by optimizing a proxy of classification performance. To show the generality of the proposed method, three different kernel learning criteria that are commonly used in the literature are employed respectively as a proxy. A comprehensive experimental study is conducted on a variety of image classification tasks to compare our proposed discriminative Stein kernel with the original Stein kernel and other commonly used methods for evaluating the similarity between SPD matrices. The experimental results demonstrate that, the discriminative Stein kernel can attain greater discrimination and better align with classification tasks by altering the eigenvalues. This makes it produce higher classification performance than the original Stein kernel and other commonly used methods.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2044",
        "title": "Tracking Individual Targets in High Density Crowd Scenes Analysis of a Video Recording in Hajj 2009",
        "authors": [
            "Mohamed H. Dridi"
        ],
        "abstract": "In this paper we present a number of methods (manual, semi-automatic and automatic) for tracking individual targets in high density crowd scenes where thousand of people are gathered. The necessary data about the motion of individuals and a lot of other physical information can be extracted from consecutive image sequences in different ways, including optical flow and block motion estimation. One of the famous methods for tracking moving objects is the block matching method. This way to estimate subject motion requires the specification of a comparison window which determines the scale of the estimate. In this work we present a real-time method for pedestrian recognition and tracking in sequences of high resolution images obtained by a stationary (high definition) camera located in different places on the Haram mosque in Mecca. The objective is to estimate pedestrian velocities as a function of the local ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2170",
        "title": "Orientation covariant aggregation of local descriptors with embeddings",
        "authors": [
            "Giorgos Tolias",
            "Teddy Furon",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "Image search systems based on local descriptors typically achieve orientation invariance by aligning the patches on their dominant orientations. Albeit successful, this choice introduces too much invariance because it does not guarantee that the patches are rotated consistently. This paper introduces an aggregation strategy of local descriptors that achieves this covariance property by jointly encoding the angle in the aggregation stage in a continuous manner. It is combined with an efficient monomial embedding to provide a codebook-free method to aggregate local descriptors into a single vector representation. Our strategy is also compatible and employed with several popular encoding methods, in particular bag-of-words, VLAD and the Fisher vector. Our geometric-aware aggregation strategy is effective for image search, as shown by experiments performed on standard benchmarks for image and particular object retrieval, namely Holidays and Oxford buildings.\n    ",
        "submission_date": "2014-07-08T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2343",
        "title": "Fast Separable Non-Local Means",
        "authors": [
            "S. Ghosh",
            "K. N. Chaudhury"
        ],
        "abstract": "We propose a simple and fast algorithm called PatchLift for computing distances between patches (contiguous block of samples) extracted from a given one-dimensional signal. PatchLift is based on the observation that the patch distances can be efficiently computed from a matrix that is derived from the one-dimensional signal using lifting; importantly, the number of operations required to compute the patch distances using this approach does not scale with the patch length. We next demonstrate how PatchLift can be used for patch-based denoising of images corrupted with Gaussian noise. In particular, we propose a separable formulation of the classical Non-Local Means (NLM) algorithm that can be implemented using PatchLift. We demonstrate that the PatchLift-based implementation of separable NLM is few orders faster than standard NLM, and is competitive with existing fast implementations of NLM. Moreover, its denoising performance is shown to be consistently superior to that of NLM and some of its variants, both in terms of PSNR/SSIM and visual quality.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2017-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2390",
        "title": "Online Stroke and Akshara Recognition GUI in Assamese Language Using Hidden Markov Model",
        "authors": [
            "SRM Prasanna",
            "Rituparna Devi",
            "Deepjoy Das",
            "Subhankar Ghosh",
            "Krishna Naik"
        ],
        "abstract": "The work describes the development of Online Assamese Stroke & Akshara Recognizer based on a set of language rules. In handwriting literature strokes are composed of two coordinate trace in between pen down and pen up labels. The Assamese aksharas are combination of a number of strokes, the maximum number of strokes taken to make a combination being eight. Based on these combinations eight language rule models have been made which are used to test if a set of strokes form a valid akshara. A Hidden Markov Model is used to train 181 different stroke patterns which generates a model used during stroke level testing. Akshara level testing is performed by integrating a GUI (provided by CDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and the language rules using a dynamic linked library (dll). We have got a stroke level performance of 94.14% and akshara level performance of 84.2%.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2572",
        "title": "Classifiers fusion method to recognize handwritten persian numerals",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Iraj Mogharreb",
            "Shahram Jamali"
        ],
        "abstract": "Recognition of Persian handwritten characters has been considered as a significant field of research for the last few years under pattern analysing technique. In this paper, a new approach for robust handwritten Persian numerals recognition using strong feature set and a classifier fusion method is scrutinized to increase the recognition percentage. For implementing the classifier fusion technique, we have considered k nearest neighbour (KNN), linear classifier (LC) and support vector machine (SVM) classifiers. The innovation of this tactic is to attain better precision with few features using classifier fusion method. For evaluation of the proposed method we considered a Persian numerals database with 20,000 handwritten samples. Spending 15,000 samples for training stage, we verified our technique on other 5,000 samples, and the correct recognition ratio achieved approximately 99.90%. Additional, we got 99.97% exactness using four-fold cross validation procedure on 20,000 databases.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2630",
        "title": "A Statistical Modeling Approach to Computer-Aided Quantification of Dental Biofilm",
        "authors": [
            "Awais Mansoor",
            "Valery Patsekin",
            "Dale Scherl",
            "J. Paul Robinson",
            "Bartlomiej Rajwa"
        ],
        "abstract": "Biofilm is a formation of microbial material on tooth substrata. Several methods to quantify dental biofilm coverage have recently been reported in the literature, but at best they provide a semi-automated approach to quantification with significant input from a human grader that comes with the graders bias of what are foreground, background, biofilm, and tooth. Additionally, human assessment indices limit the resolution of the quantification scale; most commercial scales use five levels of quantification for biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, current state-of-the-art techniques in automatic plaque quantification fail to make their way into practical applications owing to their inability to incorporate human input to handle misclassifications. This paper proposes a new interactive method for biofilm quantification in Quantitative light-induced fluorescence (QLF) images of canine teeth that is independent of the perceptual bias of the grader. The method partitions a QLF image into segments of uniform texture and intensity called superpixels; every superpixel is statistically modeled as a realization of a single 2D Gaussian Markov random field (GMRF) whose parameters are estimated; the superpixel is then assigned to one of three classes (background, biofilm, tooth substratum) based on the training set of data. The quantification results show a high degree of consistency and precision. At the same time, the proposed method gives pathologists full control to post-process the automatic quantification by flipping misclassified superpixels to a different state (background, tooth, biofilm) with a single click, providing greater usability than simply marking the boundaries of biofilm and tooth as done by current state-of-the-art methods.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2649",
        "title": "Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform",
        "authors": [
            "Alican Bozkurt",
            "Pinar Duygulu",
            "A. Enis Cetin"
        ],
        "abstract": "Recognizing fonts has become an important task in document analysis, due to the increasing number of available digital documents in different fonts and emphases. A generic font-recognition system independent of language, script and content is desirable for processing various types of documents. At the same time, categorizing calligraphy styles in handwritten manuscripts is important for palaeographic analysis, but has not been studied sufficiently in the literature. We address the font-recognition problem as analysis and categorization of textures. We extract features using complex wavelet transform and use support vector machines for classification. Extensive experimental evaluations on different datasets in four languages and comparisons with state-of-the-art studies show that our proposed method achieves higher recognition accuracy while being computationally simpler. Furthermore, on a new dataset generated from Ottoman manuscripts, we show that the proposed method can also be used for categorizing Ottoman calligraphy with high accuracy.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2700",
        "title": "Offline handwritten signature identification using adaptive window positioning techniques",
        "authors": [
            "Ghazali Sulong",
            "Anwar Yahy Ebrahim",
            "Muhammad Jehanzeb"
        ],
        "abstract": "The paper presents to address this challenge, we have proposed the use of Adaptive Window Positioning technique which focuses on not just the meaning of the handwritten signature but also on the individuality of the writer. This innovative technique divides the handwritten signature into 13 small windows of size nxn(13x13).This size should be large enough to contain ample information about the style of the author and small enough to ensure a good identification ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2721",
        "title": "ARTOS -- Adaptive Real-Time Object Detection System",
        "authors": [
            "Bj\u00f6rn Barz",
            "Erik Rodner",
            "Joachim Denzler"
        ],
        "abstract": "ARTOS is all about creating, tuning, and applying object detection models with just a few clicks. In particular, ARTOS facilitates learning of models for visual object detection by eliminating the burden of having to collect and annotate a large set of positive and negative samples manually and in addition it implements a fast learning technique to reduce the time needed for the learning step.\n",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2776",
        "title": "What you need to know about the state-of-the-art computational models of object-vision: A tour through the models",
        "authors": [
            "Seyed-Mahdi Khaligh-Razavi"
        ],
        "abstract": "Models of object vision have been of great interest in computer vision and visual neuroscience. During the last decades, several models have been developed to extract visual features from images for object recognition tasks. Some of these were inspired by the hierarchical structure of primate visual system, and some others were engineered models. The models are varied in several aspects: models that are trained by supervision, models trained without supervision, and models (e.g. feature extractors) that are fully hard-wired and do not need training. Some of the models come with a deep hierarchical structure consisting of several layers, and some others are shallow and come with only one or two layers of processing. More recently, new models have been developed that are not hand-tuned but trained using millions of images, through which they learn how to extract informative task-related features. Here I will survey all these different models and provide the reader with an intuitive, as well as a more detailed, understanding of the underlying computations in each of the models.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2961",
        "title": "On the Convergence of the Mean Shift Algorithm in the One-Dimensional Space",
        "authors": [
            "Youness Aliyari Ghassabeh"
        ],
        "abstract": "The mean shift algorithm is a non-parametric and iterative technique that has been used for finding modes of an estimated probability density function. It has been successfully employed in many applications in specific areas of machine vision, pattern recognition, and image processing. Although the mean shift algorithm has been used in many applications, a rigorous proof of its convergence is still missing in the literature. In this paper we address the convergence of the mean shift algorithm in the one-dimensional space and prove that the sequence generated by the mean shift algorithm is a monotone and convergent sequence.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2987",
        "title": "FAME: Face Association through Model Evolution",
        "authors": [
            "Eren Golge",
            "Pinar Duygulu"
        ],
        "abstract": "We attack the problem of learning face models for public faces from weakly-labelled images collected from web through querying a name. The data is very noisy even after face detection, with several irrelevant faces corresponding to other people. We propose a novel method, Face Association through Model Evolution (FAME), that is able to prune the data in an iterative way, for the face models associated to a name to evolve. The idea is based on capturing discriminativeness and representativeness of each instance and eliminating the outliers. The final models are used to classify faces on novel datasets with possibly different characteristics. On benchmark datasets, our results are comparable to or better than state-of-the-art studies for the task of face identification.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3068",
        "title": "Deep Networks with Internal Selective Attention through Feedback Connections",
        "authors": [
            "Marijn Stollenga",
            "Jonathan Masci",
            "Faustino Gomez",
            "Juergen Schmidhuber"
        ],
        "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3176",
        "title": "CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation of Lungs from CT Scans",
        "authors": [
            "Awais Mansoor",
            "Ulas Bagci",
            "Brent Foster",
            "Ziyue Xu",
            "Deborah Douglas",
            "Jeffrey M. Solomon",
            "Jayaram K. Udupa",
            "Daniel J. Mollura"
        ],
        "abstract": "Accurate and fast extraction of lung volumes from computed tomography (CT) scans remains in a great demand in the clinical environment because the available methods fail to provide a generic solution due to wide anatomical variations of lungs and existence of pathologies. Manual annotation, current gold standard, is time consuming and often subject to human bias. On the other hand, current state-of-the-art fully automated lung segmentation methods fail to make their way into the clinical practice due to their inability to efficiently incorporate human input for handling misclassifications and praxis. This paper presents a lung annotation tool for CT images that is interactive, efficient, and robust. The proposed annotation tool produces an \"as accurate as possible\" initial annotation based on the fuzzy-connectedness image segmentation, followed by efficient manual fixation of the initial extraction if deemed necessary by the practitioner. To provide maximum flexibility to the users, our annotation tool is supported in three major operating systems (Windows, Linux, and the Mac OS X). The quantitative results comparing our free software with commercially available lung segmentation tools show higher degree of consistency and precision of our software with a considerable potential to enhance the performance of routine clinical tasks.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3179",
        "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation",
        "authors": [
            "Awais Mansoor",
            "Ulas Bagci",
            "Daniel J. Mollura"
        ],
        "abstract": "Accurate delineation of pathological lungs from computed tomography (CT) images remains mostly unsolved because available methods fail to provide a reliable generic solution due to high variability of abnormality appearance. Local descriptor-based classification methods have shown to work well in annotating pathologies; however, these methods are usually computationally intensive which restricts their widespread use in real-time or near-real-time clinical applications. In this paper, we present a novel approach for fast, accurate, reliable segmentation of pathological lungs from CT scans by combining region-based segmentation method with local descriptor classification that is performed on an optimized sampling grid. Our method works in two stages; during stage one, we adapted the fuzzy connectedness (FC) image segmentation algorithm to perform initial lung parenchyma extraction. In the second stage, texture-based local descriptors are utilized to segment abnormal imaging patterns using a near optimal keypoint analysis by employing centroid of supervoxel as grid points. The quantitative results show that our pathological lung segmentation method is fast, robust, and improves on current standards and has potential to enhance the performance of routine clinical tasks.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3193",
        "title": "Optimally Stabilized PET Image Denoising Using Trilateral Filtering",
        "authors": [
            "Awais Mansoor",
            "Ulas Bagci",
            "Daniel J. Mollura"
        ],
        "abstract": "Low-resolution and signal-dependent noise distribution in positron emission tomography (PET) images makes denoising process an inevitable step prior to qualitative and quantitative image analysis tasks. Conventional PET denoising methods either over-smooth small-sized structures due to resolution limitation or make incorrect assumptions about the noise characteristics. Therefore, clinically important quantitative information may be corrupted. To address these challenges, we introduced a novel approach to remove signal-dependent noise in the PET images where the noise distribution was considered as Poisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation (GAT) was used to stabilize varying nature of the PET noise. Other than noise stabilization, it is also desirable for the noise removal filter to preserve the boundaries of the structures while smoothing the noisy regions. Indeed, it is important to avoid significant loss of quantitative information such as standard uptake value (SUV)-based metrics as well as metabolic lesion volume. To satisfy all these properties, we extended bilateral filtering method into trilateral filtering through multiscaling and optimal Gaussianization process. The proposed method was tested on more than 50 PET-CT images from various patients having different cancers and achieved the superior performance compared to the widely used denoising techniques in the literature.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3399",
        "title": "Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations",
        "authors": [
            "Xianjie Chen",
            "Alan Yuille"
        ],
        "abstract": "We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.\n    ",
        "submission_date": "2014-07-12T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3535",
        "title": "Optimizing Auto-correlation for Fast Target Search in Large Search Space",
        "authors": [
            "Arif Mahmood",
            "Ajmal Mian",
            "Robyn Owens"
        ],
        "abstract": "In remote sensing image-blurring is induced by many sources such as atmospheric scatter, optical aberration, spatial and temporal sensor integration. The natural blurring can be exploited to speed up target search by fast template matching. In this paper, we synthetically induce additional non-uniform blurring to further increase the speed of the matching process. To avoid loss of accuracy, the amount of synthetic blurring is varied spatially over the image according to the underlying content. We extend transitive algorithm for fast template matching by incorporating controlled image blur. To this end we propose an Efficient Group Size (EGS) algorithm which minimizes the number of similarity computations for a particular search image. A larger efficient group size guarantees less computations and more speedup. EGS algorithm is used as a component in our proposed Optimizing auto-correlation (OptA) algorithm. In OptA a search image is iteratively non-uniformly blurred while ensuring no accuracy degradation at any image location. In each iteration efficient group size and overall computations are estimated by using the proposed EGS algorithm. The OptA algorithm stops when the number of computations cannot be further decreased without accuracy degradation. The proposed algorithm is compared with six existing state of the art exhaustive accuracy techniques using correlation coefficient as the similarity measure. Experiments on satellite and aerial image datasets demonstrate the effectiveness of the proposed algorithm.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3540",
        "title": "Measuring Atmospheric Scattering from Digital Images of Urban Scenery using Temporal Polarization-Based Vision",
        "authors": [
            "Tarek El-Gaaly",
            "Joshua Gluckman"
        ],
        "abstract": "Particulate Matter (PM) is a form of air pollution that visually degrades urban scenery and is hazardous to human health and the environment. Current monitoring devices are limited in measuring average PM over large areas. Quantifying the visual effects of haze in digital images of urban scenery and correlating these effects to PM levels is a vital step in more practically monitoring our environment. Current image haze extraction algorithms remove haze from the scene for the sole purpose of enhancing vision. We present two algorithms which bridge the gap between image haze extraction and environmental monitoring. We provide a means of measuring atmospheric scattering from images of urban scenery by incorporating temporal knowledge. In doing so, we also present a method of recovering an accurate depthmap of the scene and recovering the scene without the visual effects of haze. We compare our algorithm to three known haze removal methods. The algorithms are composed of an optimization over a model of haze formation in images and an optimization using a constraint of constant depth over a sequence of images taken over time. These algorithms not only measure atmospheric scattering, but also recover a more accurate depthmap and dehazed image. The measurements of atmospheric scattering this research produces, can be directly correlated to PM levels and therefore pave the way to monitoring the health of the environment by visual means. Accurate atmospheric sensing from digital images is a challenging and under-researched problem. This work provides an important step towards a more practical and accurate visual means of measuring PM from digital images.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3664",
        "title": "An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image",
        "authors": [
            "Mohammed M. Abdelsamea"
        ],
        "abstract": "A good segmentation result depends on a set of \"correct\" choice for the seeds. When the input images are noisy, the seeds may fall on atypical pixels that are not representative of the region statistics. This can lead to erroneous segmentation results. In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from in the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3673",
        "title": "Enhanced EZW Technique for Compression of Image by Setting Detail Retaining Pass Number",
        "authors": [
            "Isha Tyagi",
            "Ashish Nautiyal",
            "Vishwanath Bijalwan",
            "Meenu Balodhi"
        ],
        "abstract": "This submission has been withdrawn by arXiv administrators because it contains excessive and unattributed reuse of content from other authors.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2016-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3675",
        "title": "A New Approach for Super resolution by Using Web Images and FFT Based Image Registration",
        "authors": [
            "Archana Vijayan",
            "Vincy Salam"
        ],
        "abstract": "Preserving accuracy is a challenging issue in super resolution images. In this paper, we propose a new FFT based image registration algorithm and a sparse based super resolution algorithm to improve the accuracy of super resolution image. Given a low resolution image, our approach initially extracts the local descriptors from the input and then the local descriptors from the whole correlated images using the SIFT algorithm. Once this is completed, it will compare the local descriptors on the basis of a threshold value. The retrieved images could be having different focal length, illumination, inclination and size. To overcome the above differences of the retrieved images, we propose a new FFT based image registration algorithm. After the registration stage, we apply a sparse based super resolution on the images for recreating images with better resolution compared to the input. Based on the PSSNR calculation and SSIM comparison, we can see that the new methodology creates a better image than the traditional methods.\n    ",
        "submission_date": "2014-07-05T00:00:00",
        "last_modified_date": "2014-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3686",
        "title": "Spatiotemporal Stacked Sequential Learning for Pedestrian Detection",
        "authors": [
            "Alejandro Gonz\u00e1lez",
            "Sebastian Ramos",
            "David V\u00e1zquez",
            "Antonio M. L\u00f3pez",
            "Jaume Amores"
        ],
        "abstract": "Pedestrian classifiers decide which image windows contain a pedestrian. In practice, such classifiers provide a relatively high response at neighbor windows overlapping a pedestrian, while the responses around potential false positives are expected to be lower. An analogous reasoning applies for image sequences. If there is a pedestrian located within a frame, the same pedestrian is expected to appear close to the same location in neighbor frames. Therefore, such a location has chances of receiving high classification scores during several frames, while false positives are expected to be more spurious. In this paper we propose to exploit such correlations for improving the accuracy of base pedestrian classifiers. In particular, we propose to use two-stage classifiers which not only rely on the image descriptors required by the base classifiers but also on the response of such base classifiers in a given spatiotemporal neighborhood. More specifically, we train pedestrian classifiers using a stacked sequential learning (SSL) paradigm. We use a new pedestrian dataset we have acquired from a car to evaluate our proposal at different frame rates. We also test on a well known dataset: Caltech. The obtained results show that our SSL proposal boosts detection accuracy significantly with a minimal impact on the computational cost. Interestingly, SSL improves more the accuracy at the most dangerous situations, i.e. when a pedestrian is close to the camera.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3695",
        "title": "Recovery of Images with Missing Pixels using a Gradient Compressive Sensing Algorithm",
        "authors": [
            "Isidora Stankovi\u0107"
        ],
        "abstract": "This paper investigates the possibility of reconstruction of images considering that they are sparse in the DCT transformation domain. Two approaches are considered. One when the image is pre-processed in the DCT domain, using 8x8 blocks. The image is made sparse by setting the smallest DCT coefficients to zero. In the other case the original image is considered without pre-processing, assuming the sparsity as intrinsic property of the analyzed image. A gradient based algorithm is used to recover a large number of missing pixels in the image. The case of a salt-and-paper noise affecting a large number of pixels is easily reduced to the case of missing pixels and considered within the same framework. The reconstruction of images affected with salt-and-paper impulsive is compared with the images filtered using a median filter. The same algorithm can be used considering transformation of the whole image. Reconstructions of black and white and colour images are considered.\n    ",
        "submission_date": "2014-06-22T00:00:00",
        "last_modified_date": "2014-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3840",
        "title": "Depth Reconstruction from Sparse Samples: Representation, Algorithm, and Sampling",
        "authors": [
            "Lee-Kang Liu",
            "Stanley H. Chan",
            "Truong Q. Nguyen"
        ],
        "abstract": "The rapid development of 3D technology and computer vision applications have motivated a thrust of methodologies for depth acquisition and estimation. However, most existing hardware and software methods have limited performance due to poor depth precision, low resolution and high computational cost. In this paper, we present a computationally efficient method to recover dense depth maps from sparse measurements. We make three contributions. First, we provide empirical evidence that depth maps can be encoded much more sparsely than natural images by using common dictionaries such as wavelets and contourlets. We also show that a combined wavelet-contourlet dictionary achieves better performance than using either dictionary alone. Second, we propose an alternating direction method of multipliers (ADMM) to achieve fast reconstruction. A multi-scale warm start procedure is proposed to speed up the convergence. Third, we propose a two-stage randomized sampling scheme to optimally choose the sampling locations, thus maximizing the reconstruction performance for any given sampling budget. Experimental results show that the proposed method produces high quality dense depth estimates, and is robust to noisy measurements. Applications to real data in stereo matching are demonstrated.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3867",
        "title": "Part-based R-CNNs for Fine-grained Category Detection",
        "authors": [
            "Ning Zhang",
            "Jeff Donahue",
            "Ross Girshick",
            "Trevor Darrell"
        ],
        "abstract": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3956",
        "title": "Globally Optimal Joint Image Segmentation and Shape Matching Based on Wasserstein Modes",
        "authors": [
            "Bernhard Schmitzer",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "A functional for joint variational object segmentation and shape matching is developed. The formulation is based on optimal transport w.r.t. geometric distance and local feature similarity. Geometric invariance and modelling of object-typical statistical variations is achieved by introducing degrees of freedom that describe transformations and deformations of the shape template. The shape model is mathematically equivalent to contour-based approaches but inference can be performed without conversion between the contour and region representations, allowing combination with other convex segmentation approaches and simplifying optimization. While the overall functional is non-convex, non-convexity is confined to a low-dimensional variable. We propose a locally optimal alternating optimization scheme and a globally optimal branch and bound scheme, based on adaptive convex relaxation. Combining both methods allows to eliminate the delicate initialization problem inherent to many contour based approaches while remaining computationally practical. The properties of the functional, its ability to adapt to a wide range of input data structures and the different optimization schemes are illustrated and compared by numerical experiments.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3969",
        "title": "An iterative approach to Hough transform without re-voting",
        "authors": [
            "Giorgio Ricca",
            "Mauro C. Beltrametti",
            "Anna Maria Massone"
        ],
        "abstract": "Many bone shapes in the human skeleton are characterized by profiles that can be associated to equations of algebraic curves. Fixing the parameters in the curve equation, by means of a classical pattern recognition procedure like the Hough transform technique, it is then possible to associate an equation to a specific bone profile. However, most skeleton districts are more accurately described by piecewise defined curves. This paper utilizes an iterative approach of the Hough transform without re-voting, to provide an efficient procedure for describing the profile of a bone in the human skeleton as a collection of different but continuously attached curves.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3986",
        "title": "Image Fusion Using LEP Filtering and Bilinear Interpolation",
        "authors": [
            "Haritha Raveendran",
            "Deepa Thomas"
        ],
        "abstract": "Image Fusion is the process in which core information from a set of component images is merged to form a single image, which is more informative and complete than the component input images in quality and appearance. This paper presents a fast and effective image fusion method for creating high quality fused images by merging component images. In the proposed method, the input image is broken down to a two-scale image representation with a base layer having large scale variations in intensity, and a detail layer containing small scale details. Here fusion of the base and detail layers is implemented by means of a Local Edge preserving filtering based technique. The proposed method is an efficient image fusion technique in which the noise component is very low and quality of the resultant image is high so that it can be used for applications like medical image processing, requiring very accurate edge preserved images. Performance is tested by calculating PSNR and SSIM of images. The benefit of the proposed method is that it removes noise without altering the underlying structures of the image. This paper also presents an image zooming technique using bilinear interpolation in which a portion of the input image is cropped and bilinear interpolation is applied. Experimental results showed that the when PSNR value is calculated, the noise is found to be very low for the resultant image portion.\n    ",
        "submission_date": "2014-07-05T00:00:00",
        "last_modified_date": "2014-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4023",
        "title": "Aggregate channel features for multi-view face detection",
        "authors": [
            "Bin Yang",
            "Junjie Yan",
            "Zhen Lei",
            "Stan Z. Li"
        ],
        "abstract": "Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multi-scale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in Viola-Jones framework, the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB testsets, while runs at 42 FPS on VGA images.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4206",
        "title": "Mobile Camera Array Calibration for Light Field Acquisition",
        "authors": [
            "Yichao Xu",
            "Kazuki Maeno",
            "Hajime Nagahara",
            "Rin-ichiro Taniguchi"
        ],
        "abstract": "The light field camera is useful for computer graphics and vision applications. Calibration is an essential step for these applications. After calibration, we can rectify the captured image by using the calibrated camera parameters. However, the large camera array calibration method, which assumes that all cameras are on the same plane, ignores the orientation and intrinsic parameters. The multi-camera calibration technique usually assumes that the working volume and viewpoints are fixed. In this paper, we describe a calibration algorithm suitable for a mobile camera array based light field acquisition system. The algorithm performs in Zhang's style by moving a checkerboard, and computes the initial parameters in closed form. Global optimization is then applied to refine all the parameters simultaneously. Our implementation is rather flexible in that users can assign the number of viewpoints and refinement of intrinsic parameters is optional. Experiments on both simulated data and real data acquired by a commercial product show that our method yields good results. Digital refocusing application shows the calibrated light field can well focus to the target object we desired.\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2014-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4420",
        "title": "Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral Images",
        "authors": [
            "Fei Zhu",
            "Paul Honeine",
            "Maya Kallas"
        ],
        "abstract": "The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of the factorization matrices, which is a fallout of the curse of the pre-image. In this paper, we propose a novel kernel-based model for the NMF that does not suffer from the pre-image problem, by investigating the estimation of the factorization matrices directly in the input space. For different kernel functions, we describe two schemes for iterative algorithms: an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the Lee and Seung algorithm. Within the proposed framework, we develop several extensions to incorporate constraints, including sparseness, smoothness, and spatial regularization with a total-variation-like penalty. The effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images, using well-known real images and results with state-of-the-art techniques.\n    ",
        "submission_date": "2014-07-16T00:00:00",
        "last_modified_date": "2016-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4739",
        "title": "An landcover fuzzy logic classification by maximumlikelihood",
        "authors": [
            "T.Sarath",
            "G.Nagalakshmi"
        ],
        "abstract": "In present days remote sensing is most used application in many sectors. This remote sensing uses different images like multispectral, hyper spectral or ultra spectral. The remote sensing image classification is one of the significant method to classify image. In this state we classify the maximum likelihood classification with fuzzy logic. In this we experimenting fuzzy logic like spatial, spectral texture methods in that different sub methods to be used for image classification.\n    ",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2014-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4764",
        "title": "Efficient On-the-fly Category Retrieval using ConvNets and GPUs",
        "authors": [
            "Ken Chatfield",
            "Karen Simonyan",
            "Andrew Zisserman"
        ],
        "abstract": "We investigate the gains in precision and speed, that can be obtained by using Convolutional Networks (ConvNets) for on-the-fly retrieval - where classifiers are learnt at run time for a textual query from downloaded images, and used to rank large image or video datasets.\n",
        "submission_date": "2014-07-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4867",
        "title": "Analysis of Gait Pattern to Recognize the Human Activities",
        "authors": [
            "Jay Prakash Gupta",
            "Pushkar Dixit",
            "Nishant Singh",
            "Vijay Bhaskar Semwal"
        ],
        "abstract": "Human activity recognition based on the computer vision is the process of labelling image sequences with action labels. Accurate systems for this problem are applied in areas such as visual surveillance, human computer interaction and video retrieval.\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4874",
        "title": "Affine Subspace Representation for Feature Description",
        "authors": [
            "Zhenhua Wang",
            "Bin Fan",
            "Fuchao Wu"
        ],
        "abstract": "This paper proposes a novel Affine Subspace Representation (ASR) descriptor to deal with affine distortions induced by viewpoint changes. Unlike the traditional local descriptors such as SIFT, ASR inherently encodes local information of multi-view patches, making it robust to affine distortions while maintaining a high discriminative ability. To this end, PCA is used to represent affine-warped patches as PCA-patch vectors for its compactness and efficiency. Then according to the subspace assumption, which implies that the PCA-patch vectors of various affine-warped patches of the same keypoint can be represented by a low-dimensional linear subspace, the ASR descriptor is obtained by using a simple subspace-to-point mapping. Such a linear subspace representation could accurately capture the underlying information of a keypoint (local structure) under multiple views without sacrificing its distinctiveness. To accelerate the computation of ASR descriptor, a fast approximate algorithm is proposed by moving the most computational part (ie, warp patch under various affine transformations) to an offline training stage. Experimental results show that ASR is not only better than the state-of-the-art descriptors under various image transformations, but also performs well without a dedicated affine invariant detector when dealing with viewpoint changes.\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4898",
        "title": "Hand Pointing Detection Using Live Histogram Template of Forehead Skin",
        "authors": [
            "Ghassem Tofighi",
            "Nasser Ali Afarin",
            "Kamraan Raahemifar",
            "Anastasios N. Venetsanopoulos"
        ],
        "abstract": "Hand pointing detection has multiple applications in many fields such as virtual reality and control devices in smart homes. In this paper, we proposed a novel approach to detect pointing vector in 2D space of a room. After background subtraction, face and forehead is detected. In the second step, forehead skin H-S plane histograms in HSV space is calculated. By using these histogram templates of users skin, and back projection method, skin areas are detected. The contours of hand are extracted using Freeman chain code algorithm. Next step is finding fingertips. Points in hand contour which are candidates for the fingertip can be found in convex defects of convex hull and contour. We introduced a novel method for finding the fingertip based on the special points on the contour and their relationships. Our approach detects hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4979",
        "title": "Deep Metric Learning for Practical Person Re-Identification",
        "authors": [
            "Dong Yi",
            "Zhen Lei",
            "Stan Z. Li"
        ],
        "abstract": "Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a \"siamese\" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers.\n",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5035",
        "title": "LSDA: Large Scale Detection Through Adaptation",
        "authors": [
            "Judy Hoffman",
            "Sergio Guadarrama",
            "Eric Tzeng",
            "Ronghang Hu",
            "Jeff Donahue",
            "Ross Girshick",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5055",
        "title": "Adaptive Image Denoising by Targeted Databases",
        "authors": [
            "Enming Luo",
            "Stanley H. Chan",
            "Truong Q. Nguyen"
        ],
        "abstract": "We propose a data-dependent denoising procedure to restore noisy images. Different from existing denoising algorithms which search for patches from either the noisy image or a generic database, the new algorithm finds patches from a database that contains only relevant patches. We formulate the denoising problem as an optimal filter design problem and make two contributions. First, we determine the basis function of the denoising filter by solving a group sparsity minimization problem. The optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance. Improvement methods are proposed to enhance the patch search process. Second, we determine the spectral coefficients of the denoising filter by considering a localized Bayesian prior. The localized prior leverages the similarity of the targeted database, alleviates the intensive Bayesian computation, and links the new method to the classical linear minimum mean squared error estimation. We demonstrate applications of the proposed method in a variety of scenarios, including text images, multiview images and face images. Experimental results show the superiority of the new algorithm over existing methods.\n    ",
        "submission_date": "2014-06-30T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5242",
        "title": "Object Proposal Generation using Two-Stage Cascade SVMs",
        "authors": [
            "Ziming Zhang",
            "Philip H.S. Torr"
        ],
        "abstract": "Object proposal algorithms have shown great promise as a first step for object recognition and detection. Good object proposal generation algorithms require high object recall rate as well as low computational cost, because generating object proposals is usually utilized as a preprocessing step. The problem of how to accelerate the object proposal generation and evaluation process without decreasing recall is thus of great interest. In this paper, we propose a new object proposal generation method using two-stage cascade SVMs, where in the first stage linear filters are learned for predefined quantized scales/aspect-ratios independently, and in the second stage a global linear classifier is learned across all the quantized scales/aspect-ratios for calibration, so that all the proposals can be compared properly. The proposals with highest scores are our final output. Specifically, we explain our scale/aspect-ratio quantization scheme, and investigate the effects of combinations of $\\ell_1$ and $\\ell_2$ regularizers in cascade SVMs with/without ranking constraints in learning. Comprehensive experiments on VOC2007 dataset are conducted, and our results achieve the state-of-the-art performance with high object recall rate and high computational efficiency. Besides, our method has been demonstrated to be suitable for not only class-specific but also generic object proposal generation.\n    ",
        "submission_date": "2014-07-20T00:00:00",
        "last_modified_date": "2014-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5245",
        "title": "Feature and Region Selection for Visual Learning",
        "authors": [
            "Ji Zhao",
            "Liantao Wang",
            "Ricardo Cabral",
            "Fernando De la Torre"
        ],
        "abstract": "Visual learning problems such as object classification and action recognition are typically approached using extensions of the popular bag-of-words (BoW) model. Despite its great success, it is unclear what visual features the BoW model is learning: Which regions in the image or video are used to discriminate among classes? Which are the most discriminative visual words? Answering these questions is fundamental for understanding existing BoW models and inspiring better models for visual recognition.\n",
        "submission_date": "2014-07-20T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5324",
        "title": "Optimized Method for Iranian Road Signs Detection and recognition system",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Iman Tavakoli Kazerooni"
        ],
        "abstract": "Road sign recognition is one of the core technologies in Intelligent Transport Systems. In the current study, a robust and real-time method is presented to identify and detect the roads speed signs in road image in different situations. In our proposed method, first, the connected components are created in the main image using the edge detection and mathematical morphology and the location of the road signs extracted by the geometric and color data; then the letters are segmented and recognized by Multiclass Support Vector Machine (SVMs) classifiers. Regarding that the geometric and color features ate properly used in detection the location of the road signs, so it is not sensitive to the distance and noise and has higher speed and efficiency. In the result part, the proposed approach is applied on Iranian road speed sign database and the detection and recognition accuracy rate achieved 98.66% and 100% respectively.\n    ",
        "submission_date": "2014-07-20T00:00:00",
        "last_modified_date": "2014-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5367",
        "title": "Certifying the Existence of Epipolar Matrices",
        "authors": [
            "Sameer Agarwal",
            "Hon-leung Lee",
            "Bernd Sturmfels",
            "Rekha R. Thomas"
        ],
        "abstract": "Given a set of point correspondences in two images, the existence of a fundamental matrix is a necessary condition for the points to be the images of a 3-dimensional scene imaged with two pinhole cameras. If the camera calibration is known then one requires the existence of an essential matrix.\n",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5736",
        "title": "Learning Rich Features from RGB-D Images for Object Detection and Segmentation",
        "authors": [
            "Saurabh Gupta",
            "Ross Girshick",
            "Pablo Arbel\u00e1ez",
            "Jitendra Malik"
        ],
        "abstract": "In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5759",
        "title": "Aggregation of local parametric candidates with exemplar-based occlusion handling for optical flow",
        "authors": [
            "Denis Fortun",
            "Patrick Bouthemy",
            "Charles Kervrann"
        ],
        "abstract": "Handling all together large displacements, motion details and occlusions remains an open issue for reliable computation of optical flow in a video sequence. We propose a two-step aggregation paradigm to address this problem. The idea is to supply local motion candidates at every pixel in a first step, and then to combine them to determine the global optical flow field in a second step. We exploit local parametric estimations combined with patch correspondences and we experimentally demonstrate that they are sufficient to produce highly accurate motion candidates. The aggregation step is designed as the discrete optimization of a global regularized energy. The occlusion map is estimated jointly with the flow field throughout the two steps. We propose a generic exemplar-based approach for occlusion filling with motion vectors. We achieve state-of-the-art results in computer vision benchmarks, with particularly significant improvements in the case of large displacements and occlusions.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5976",
        "title": "Detection of Sclerotic Spine Metastases via Random Aggregation of Deep Convolutional Neural Network Classifications",
        "authors": [
            "Holger R. Roth",
            "Jianhua Yao",
            "Le Lu",
            "James Stieger",
            "Joseph E. Burns",
            "Ronald M. Summers"
        ],
        "abstract": "Automated detection of sclerotic metastases (bone lesions) in Computed Tomography (CT) images has potential to be an important tool in clinical practice and research. State-of-the-art methods show performance of 79% sensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume. We design a two-tiered coarse-to-fine cascade framework to first operate a highly sensitive candidate generation system at a maximum sensitivity of ~92% but with high FP level (~50 per patient). Regions of interest (ROI) for lesion candidates are generated in this step and function as input for the second tier. In the second tier we generate N 2D views, via scale, random translations, and rotations with respect to each ROI centroid coordinates. These random views are used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign individual probabilities for a new set of N random views that are averaged at each ROI to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. We validate the approach on CT images of 59 patients (49 with sclerotic metastases and 10 normal controls). The proposed method reduces the number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing a sensitivity rates of 60%, 70%, and 80% respectively in testing. The Area-Under-the-Curve (AUC) is 0.834. The results show marked improvement upon previous work.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6082",
        "title": "Joint Energy-based Detection and Classificationon of Multilingual Text Lines",
        "authors": [
            "Igor Milevskiy",
            "Yuri Boykov"
        ],
        "abstract": "This paper proposes a new hierarchical MDL-based model for a joint detection and classification of multilingual text lines in im- ages taken by hand-held cameras. The majority of related text detec- tion methods assume alphabet-based writing in a single language, e.g. in Latin. They use simple clustering heuristics specific to such texts: prox- imity between letters within one line, larger distance between separate lines, etc. We are interested in a significantly more ambiguous problem where images combine alphabet and logographic characters from multiple languages and typographic rules vary a lot (e.g. English, Korean, and Chinese). Complexity of detecting and classifying text lines in multiple languages calls for a more principled approach based on information- theoretic principles. Our new MDL model includes data costs combining geometric errors with classification likelihoods and a hierarchical sparsity term based on label costs. This energy model can be efficiently minimized by fusion moves. We demonstrate robustness of the proposed algorithm on a large new database of multilingual text images collected in the pub- lic transit system of Seoul.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6174",
        "title": "Visual Word Selection without Re-Coding and Re-Pooling",
        "authors": [
            "Fatih Cakir",
            "Stan Sclaroff"
        ],
        "abstract": "The Bag-of-Words (BoW) representation is widely used in computer vision. The size of the codebook impacts the time and space complexity of the applications that use BoW. Thus, given a training set for a particular computer vision task, a key problem is pruning a large codebook to select only a subset of visual words. Evaluating possible selections of words to be included in the pruned codebook can be computationally prohibitive; in a brute-force scheme, evaluating each pruned codebook requires re-coding of all features extracted from training images to words in the candidate codebook and then re-pooling the words to obtain a representation of each image, e.g., histogram of visual word frequencies. In this paper, a method is proposed that selects and evaluates a subset of words from an initially large codebook, without the need for re-coding or re-pooling. Formulations are proposed for two commonly-used schemes: hard and soft (kernel) coding of visual words with average-pooling. The effectiveness of these formulations is evaluated on the 15 Scenes and Caltech 10 benchmarks.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6251",
        "title": "FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation",
        "authors": [
            "Philip Lenz",
            "Andreas Geiger",
            "Raquel Urtasun"
        ],
        "abstract": "One of the most popular approaches to multi-target tracking is tracking-by-detection. Current min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm which solves the data association problem optimally while reusing computation, resulting in significantly faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrarily length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6318",
        "title": "A robust and adaptable method for face detection based on Color Probabilistic Estimation Technique",
        "authors": [
            "Reza Azad",
            "Fatemeh Davami"
        ],
        "abstract": "Human face perception is currently an active research area in the computer vision community. Skin detection is one of the most important and primary stages for this purpose. So far, many approaches are proposed to done this case. Near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as RGB, HSI or YCBCR. Results show that these methods cannot provide an accurate approach for every kind of skin. In this paper, an approach is proposed to solve this problem using a color probabilistic estimation technique. This approach is including two stages. In the first one, the skin intensity distribution is estimated using some train photos of pure skin, and at the second stage, the skin pixels are detected using Gaussian model and optimal threshold tuning. Then from the skin region facial features have been extracted to get the face from the skin region. In the results section, the proposed approach is applied on FEI database and the accuracy rate reached 99.25%. The proposed approach can be used for all kinds of skin using train stage which is the main advantage among the other advantages, such as Low noise sensitivity and low computational complexity.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6321",
        "title": "Novel and Automatic Parking Inventory System Based on Pattern Recognition and Directional Chain Code",
        "authors": [
            "Reza Azad",
            "Majid Nazari"
        ],
        "abstract": "The objective of this paper is to design an efficient vehicle license plate recognition System and to implement it for automatic parking inventory system. The system detects the vehicle first and then captures the image of the front view of the vehicle. Vehicle license plate is localized and characters are segmented. For finding the place of plate, a novel and real time method is expressed. A new and robust technique based on directional chain code is used for character recognition. The resulting vehicle number is then compared with the available database of all the vehicles so as to come up with information about the vehicle type and to charge entrance cost accordingly. The system is then allowed to open parking barrier for the vehicle and generate entrance cost receipt. The vehicle information (such as entrance time, date, and cost amount) is also stored in the database to maintain the record. The hardware and software integrated system is implemented and a working prototype model is developed. Under the available database, the average accuracy of locating vehicle license plate obtained 100%. Using 70% samples of character for training, we tested our scheme on whole samples and obtained 100% correct recognition rate. Further we tested our character recognition stage on Persian vehicle data set and we achieved 99% correct recognition.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6423",
        "title": "Performance evaluation of wavelet scattering network in image texture classification in various color spaces",
        "authors": [
            "Jiasong Wu",
            "Longyu Jiang",
            "Xu Han",
            "Lotfi Senhadji",
            "Huazhong Shu"
        ],
        "abstract": "Texture plays an important role in many image analysis applications. In this paper, we give a performance evaluation of color texture classification by performing wavelet scattering network in various color spaces. Experimental results on the KTH_TIPS_COL database show that opponent RGB based wavelet scattering network outperforms other color spaces. Therefore, when dealing with the problem of color texture classification, opponent RGB based wavelet scattering network is recommended.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6492",
        "title": "Recognition of Handwritten Persian/Arabic Numerals Based on Robust Feature Set and K-NN Classifier",
        "authors": [
            "Reza Azad",
            "Fatemeh Davami",
            "Hamid Reza Shayegh"
        ],
        "abstract": "This paper has been withdrawn by the author due to a crucial sign error in equation 2 and some mistake in Table 1 information. please let me for changing this information and updating this paper.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6496",
        "title": "Novel and Fast Algorithm for Extracting License Plate Location Based on Edge Analysis",
        "authors": [
            "Reza Azad",
            "Mohammad Baghdadi"
        ],
        "abstract": "Nowadays in developing or developed countries, the Intelligent Transportation System (ITS) technology has attracted so much attention to itself. License Plate Recognition (LPR) systems have many applications in ITSs, such as the payment of parking fee, controlling the traffic volume, traffic data collection, etc. This paper presents a new and fast method for license plate extraction based on edge analysis. our proposed method consist of four stage, which are edge detection, non-useable edge and noise removing, edge analysis and morphology-based license plate extraction. In the result part, the proposed algorithm is applied on vehicle database and the accuracy rate reached 98%. From the experimental results it is shown that the proposed method gives fairly acceptable level of accuracy for practical license plate recognition system.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6498",
        "title": "Real-Time and Efficient Method for Accuracy Enhancement of Edge Based License Plate Recognition System",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Hamid Reza Shayegh"
        ],
        "abstract": "License Plate Recognition plays an important role on the traffic monitoring and parking management. Administration and restriction of those transportation tools for their better service becomes very essential. In this paper, a fast and real time method has an appropriate application to find plates that the plat has tilt and the picture quality is poor. In the proposed method, at the beginning, the image is converted into binary mode with use of adaptive threshold. And with use of edge detection and morphology operation, plate number location has been specified and if the plat has tilt; its tilt is removed away. Then its characters are distinguished using image processing techniques. Finally, K Nearest Neighbour (KNN) classifier was used for character recognition. This method has been tested on available data set that has different images of the background, considering distance, and angel of view so that the correct extraction rate of plate reached at 98% and character recognition rate achieved at 99.12%. Further we tested our character recognition stage on Persian vehicle data set and we achieved 99% correct recognition rate.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6506",
        "title": "Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space and Color Statistical Features",
        "authors": [
            "Reza Azad",
            "Hamid Reza Shayegh"
        ],
        "abstract": "Skin detection is one of the most important and primary stages in some of image processing applications such as face detection and human tracking. So far, many approaches are proposed to done this case. Near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as RGB, CMYK or YCbCr. Results show these methods cannot provide an accurate approach for every kinds of skin. In this paper, an approach is proposed to solve this problem using statistical features technique. This approach is including two stages. In the first one, from pure skin statistical features were extracted and at the second stage, the skin pixels are detected using HSV and YCbCr color spaces. In the result part, the proposed approach is applied on FEI database and the accuracy rate reached 99.25 + 0.2. Further proposed method is applied on complex background database and accuracy rate obtained 95.40+0.31%. The proposed approach can be used for all kinds of skin using train stage which is the main advantages of it. Low noise sensitivity and low computational complexity are some of other advantages.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6510",
        "title": "New Method for Optimization of License Plate Recognition system with Use of Edge Detection and Connected Component",
        "authors": [
            "Reza Azad",
            "Hamid Reza Shayegh"
        ],
        "abstract": "License Plate recognition plays an important role on the traffic monitoring and parking management systems. In this paper, a fast and real time method has been proposed which has an appropriate application to find tilt and poor quality plates. In the proposed method, at the beginning, the image is converted into binary mode using adaptive threshold. Then, by using some edge detection and morphology operations, plate number location has been specified. Finally, if the plat has tilt, its tilt is removed away. This method has been tested on another paper data set that has different images of the background, considering distance, and angel of view so that the correct extraction rate of plate reached at 98.66%.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6705",
        "title": "A Robust and Efficient Method for Improving Accuracy of License Plate Characters Recognition",
        "authors": [
            "Reza Azad",
            "Hamid Reza Shayegh",
            "Hamed Amiri"
        ],
        "abstract": "License Plate Recognition (LPR) plays an important role on the traffic monitoring and parking management. A robust and efficient method for enhancing accuracy of license plate characters recognition based on K Nearest Neighbours (K-NN) classifier is presented in this paper. The system first prepares a contour form of the extracted character, then the angle and distance feature information about the character is extracted and finally K-NN classifier is used to character recognition. Angle and distance features of a character have been computed based on distribution of points on the bitmap image of character. In K-NN method, the Euclidean distance between testing point and reference points is calculated in order to find the k-nearest neighbours. We evaluated our method on the available dataset that contain 1200 sample. Using 70% samples for training, we tested our method on whole samples and obtained 99% correct recognition ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6748",
        "title": "Enhancing the Accuracy of Biometric Feature Extraction Fusion Using Gabor Filter and Mahalanobis Distance Algorithm",
        "authors": [
            "Ayodeji S. Makinde",
            "Yaw Nkansah-Gyekye",
            "Loserian S. Laizer"
        ],
        "abstract": "Biometric recognition systems have advanced significantly in the last decade and their use in specific applications will increase in the near future. The ability to conduct meaningful comparisons and assessments will be crucial to successful deployment and increasing biometric adoption. The best modality used as unimodal biometric systems are unable to fully address the problem of higher recognition rate. Multimodal biometric systems are able to mitigate some of the limitations encountered in unimodal biometric systems, such as non-universality, distinctiveness, non-acceptability, noisy sensor data, spoof attacks, and performance. More reliable recognition accuracy and performance are achievable as different modalities were being combined together and different algorithms or techniques were being used. The work presented in this paper focuses on a bimodal biometric system using face and fingerprint. An image enhancement technique (histogram equalization) is used to enhance the face and fingerprint images. Salient features of the face and fingerprint were extracted using the Gabor filter technique. A dimensionality reduction technique was carried out on both images extracted features using a principal component analysis technique. A feature level fusion algorithm (Mahalanobis distance technique) is used to combine each unimodal feature together. The performance of the proposed approach is validated and is effective.\n    ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7317",
        "title": "A unified framework for thermal face recognition",
        "authors": [
            "Reza Shoja Ghiass",
            "Ognjen Arandjelovic",
            "Hakim Bendada",
            "Xavier Maldague"
        ],
        "abstract": "The reduction of the cost of infrared (IR) cameras in recent years has made IR imaging a highly viable modality for face recognition in practice. A particularly attractive advantage of IR-based over conventional, visible spectrum-based face recognition stems from its invariance to visible illumination. In this paper we argue that the main limitation of previous work on face recognition using IR lies in its ad hoc approach to treating different nuisance factors which affect appearance, prohibiting a unified approach that is capable of handling concurrent changes in multiple (or indeed all) major extrinsic sources of variability, which is needed in practice. We describe the first approach that attempts to achieve this - the framework we propose achieves outstanding recognition performance in the presence of variable (i) pose, (ii) facial expression, (iii) physiological state, (iv) partial occlusion due to eye-wear, and (v) quasi-occlusion due to facial hair growth.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7330",
        "title": "Discovering Discriminative Cell Attributes for HEp-2 Specimen Image Classification",
        "authors": [
            "Arnold Wiliem",
            "Peter Hobson",
            "Brian C. Lovell"
        ],
        "abstract": "Recently, there has been a growing interest in developing Computer Aided Diagnostic (CAD) systems for improving the reliability and consistency of pathology test results. This paper describes a novel CAD system for the Anti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol on Human Epithelial Type 2 (HEp-2) cells. While prior works have primarily focused on classifying cell images extracted from ANA specimen images, this work takes a further step by focussing on the specimen image classification problem itself. Our system is able to efficiently classify specimen images as well as producing meaningful descriptions of ANA pattern class which helps physicians to understand the differences between various ANA patterns. We achieve this goal by designing a specimen-level image descriptor that: (1) is highly discriminative; (2) has small descriptor length and (3) is semantically meaningful at the cell level. In our work, a specimen image descriptor is represented by its overall cell attribute descriptors. As such, we propose two max-margin based learning schemes to discover cell attributes whilst still maintaining the discrimination of the specimen image descriptor. Our learning schemes differ from the existing discriminative attribute learning approaches as they primarily focus on discovering image-level attributes. Comparative evaluations were undertaken to contrast the proposed approach to various state-of-the-art approaches on a novel HEp-2 cell dataset which was specifically proposed for the specimen-level classification. Finally, we showcase the ability of the proposed approach to provide textual descriptions to explain ANA patterns.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7390",
        "title": "A discussion on the validation tests employed to compare human action recognition methods using the MSR Action3D dataset",
        "authors": [
            "Jos\u00e9 Ram\u00f3n Padilla-L\u00f3pez",
            "Alexandros Andr\u00e9 Chaaraoui",
            "Francisco Fl\u00f3rez-Revuelta"
        ],
        "abstract": "This paper aims to determine which is the best human action recognition method based on features extracted from RGB-D devices, such as the Microsoft Kinect. A review of all the papers that make reference to MSR Action3D, the most used dataset that includes depth information acquired from a RGB-D device, has been performed. We found that the validation method used by each work differs from the others. So, a direct comparison among works cannot be made. However, almost all the works present their results comparing them without taking into account this issue. Therefore, we present different rankings according to the methodology used for the validation in orden to clarify the existing confusion.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7504",
        "title": "A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction",
        "authors": [
            "Lluis Gomez",
            "Dimosthenis Karatzas"
        ],
        "abstract": "Typography and layout lead to the hierarchical organisation of text in words, text lines, paragraphs. This inherent structure is a key property of text in any script and language, which has nonetheless been minimally leveraged by existing text detection methods. This paper addresses the problem of text segmentation in natural scenes from a hierarchical perspective. Contrary to existing methods, we make explicit use of text structure, aiming directly to the detection of region groupings corresponding to text within a hierarchy produced by an agglomerative similarity clustering process over individual regions. We propose an optimal way to construct such an hierarchy introducing a feature space designed to produce text group hypotheses with high recall and a novel stopping rule combining a discriminative classifier and a probabilistic measure of group meaningfulness based in perceptual organization. Results obtained over four standard datasets, covering text in variable orientations and different languages, demonstrate that our algorithm, while being trained in a single mixed dataset, outperforms state of the art methods in unconstrained scenarios.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7556",
        "title": "Entropic one-class classifiers",
        "authors": [
            "Lorenzo Livi",
            "Alireza Sadeghian",
            "Witold Pedrycz"
        ],
        "abstract": "The one-class classification problem is a well-known research endeavor in pattern recognition. The problem is also known under different names, such as outlier and novelty/anomaly detection. The core of the problem consists in modeling and recognizing patterns belonging only to a so-called target class. All other patterns are termed non-target, and therefore they should be recognized as such. In this paper, we propose a novel one-class classification system that is based on an interplay of different techniques. Primarily, we follow a dissimilarity representation based approach; we embed the input data into the dissimilarity space by means of an appropriate parametric dissimilarity measure. This step allows us to process virtually any type of data. The dissimilarity vectors are then represented through a weighted Euclidean graphs, which we use to (i) determine the entropy of the data distribution in the dissimilarity space, and at the same time (ii) derive effective decision regions that are modeled as clusters of vertices. Since the dissimilarity measure for the input data is parametric, we optimize its parameters by means of a global optimization scheme, which considers both mesoscopic and structural characteristics of the data represented through the graphs. The proposed one-class classifier is designed to provide both hard (Boolean) and soft decisions about the recognition of test patterns, allowing an accurate description of the classification process. We evaluate the performance of the system on different benchmarking datasets, containing either feature-based or structured patterns. Experimental results demonstrate the effectiveness of the proposed technique.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2015-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7626",
        "title": "A Survey on Two Dimensional Cellular Automata and Its Application in Image Processing",
        "authors": [
            "Deepak Ranjan Nayak",
            "Prashanta Kumar Patra",
            "Amitav Mahapatra"
        ],
        "abstract": "Parallel algorithms for solving any image processing task is a highly demanded approach in the modern world. Cellular Automata (CA) are the most common and simple models of parallel computation. So, CA has been successfully used in the domain of image processing for the last couple of years. This paper provides a survey of available literatures of some methodologies employed by different researchers to utilize the cellular automata for solving some important problems of image processing. The survey includes some important image processing tasks such as rotation, zooming, translation, segmentation, edge detection, compression and noise reduction of images. Finally, the experimental results of some methodologies are presented.\n    ",
        "submission_date": "2014-07-29T00:00:00",
        "last_modified_date": "2014-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7686",
        "title": "Hyperspectral Imaging and Analysis for Sparse Reconstruction and Recognition",
        "authors": [
            "Zohaib Khan"
        ],
        "abstract": "This thesis proposes spatio-spectral techniques for hyperspectral image analysis. Adaptive spatio-spectral support and variable exposure hyperspectral imaging is demonstrated to improve spectral reflectance recovery from hyperspectral images. Novel spectral dimensionality reduction techniques have been proposed from the perspective of spectral only and spatio-spectral information preservation. It was found that the joint sparse and joint group sparse hyperspectral image models achieve lower reconstruction error and higher recognition accuracy using only a small subset of bands. Hyperspectral image databases have been developed and made publicly available for further research in compressed hyperspectral imaging, forensic document analysis and spectral reflectance recovery.\n    ",
        "submission_date": "2014-07-29T00:00:00",
        "last_modified_date": "2014-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8121",
        "title": "Clustering Approach Towards Image Segmentation: An Analytical Study",
        "authors": [
            "Dibya Jyoti Bora",
            "Anil Kumar Gupta"
        ],
        "abstract": "Image processing is an important research area in computer vision. Image segmentation plays the vital rule in image processing research. There exist so many methods for image segmentation. Clustering is an unsupervised study. Clustering can also be used for image segmentation. In this paper, an in-depth study is done on different clustering techniques that can be used for image segmentation with their pros and cons. An experiment for color image segmentation based on clustering with K-Means algorithm is performed to observe the accuracy of clustering technique for the segmentation purpose.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8123",
        "title": "Merging and Shifting of Images with Prominence Coefficient for Predictive Analysis using Combined Image",
        "authors": [
            "T.R. Gopalakrishnan Nair",
            "Richa Sharma"
        ],
        "abstract": "Shifting of objects in an image and merging many images after appropriate shifting is being used in several engineering and scientific applications which require complex perception development. A method has been presented here which could be used in precision engineering and biological applications where more precise prediction is required of a combined phenomenon with varying prominence of each phenomenon. Accurate merging of intended pixels can be achieved in high quality using frequency domain techniques even though initial properties of the original pixels are lost in this process. This paper introduces a technique to shift and merge various images with varying prominence of each image. A coefficient named prominence coefficient has been introduced which is capable of making some of the images transparent and highlighting the rest as per requirement of merging process which can be used as a simple but effective technique for overlapped view of a set of images.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8176",
        "title": "Accurate merging of images for predictive analysis using combined image",
        "authors": [
            "T.R. Gopalakrishnan Nair",
            "Richa Sharma"
        ],
        "abstract": "Several Scientific and engineering applications require merging of sampled images for complex perception development. In most cases, for such requirements, images are merged at intensity level. Even though it gives fairly good perception of combined scenario of objects and scenes, it is found that they are not sufficient enough to analyze certain engineering cases. The main problem is incoherent modulation of intensity arising out of phase properties being lost. In order to compensate these losses, combined phase and amplitude merge is demanded. We present here a method which could be used in precision engineering and biological applications where more precise prediction is required of a combined phenomenon. When pixels are added, its original property is lost but accurate merging of intended pixels can be achieved in high quality using frequency domain properties of an image. This paper introduces a technique to merge various images which can be used as a simple but effective technique for overlapped view of a set of images and producing reduced dataset for review purposes.\n    ",
        "submission_date": "2014-07-30T00:00:00",
        "last_modified_date": "2014-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8497",
        "title": "A Bottom-Up Approach for Automatic Pancreas Segmentation in Abdominal CT Scans",
        "authors": [
            "Amal Farag",
            "Le Lu",
            "Evrim Turkbey",
            "Jiamin Liu",
            "Ronald M. Summers"
        ],
        "abstract": "Organ segmentation is a prerequisite for a computer-aided diagnosis (CAD) system to detect pathologies and perform quantitative analysis. For anatomically high-variability abdominal organs such as the pancreas, previous segmentation works report low accuracies when comparing to organs like the heart or liver. In this paper, a fully-automated bottom-up method is presented for pancreas segmentation, using abdominal computed tomography (CT) scans. The method is based on a hierarchical two-tiered information propagation by classifying image patches. It labels superpixels as pancreas or not via pooling patch-level confidences on 2D CT slices over-segmented by the Simple Linear Iterative Clustering approach. A supervised random forest (RF) classifier is trained on the patch level and a two-level cascade of RFs is applied at the superpixel level, coupled with multi-channel feature extraction, respectively. On six-fold cross-validation using 80 patient CT volumes, we achieved 68.8% Dice coefficient and 57.2% Jaccard Index, comparable to or slightly better than published state-of-the-art methods.\n    ",
        "submission_date": "2014-07-31T00:00:00",
        "last_modified_date": "2014-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8518",
        "title": "Beyond KernelBoost",
        "authors": [
            "Roberto Rigamonti",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "In this Technical Report we propose a set of improvements with respect to the KernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with a scheme inspired by Auto-Context, but that is suitable in situations where the lack of large training sets poses a potential problem of overfitting. The aim is to capture the interactions between neighboring image pixels to better regularize the boundaries of segmented regions. As in Auto-Context [Tu et al., PAMI 2009] the segmentation process is iterative and, at each iteration, the segmentation results for the previous iterations are taken into account in conjunction with the image itself. However, unlike in [Tu et al., PAMI 2009], we organize our recursion so that the classifiers can progressively focus on difficult-to-classify locations. This lets us exploit the power of the decision-tree paradigm while avoiding over-fitting. In the context of this architecture, KernelBoost represents a powerful building block due to its ability to learn on the score maps coming from previous iterations. We first introduce two important mechanisms to empower the KernelBoost classifier, namely pooling and the clustering of positive samples based on the appearance of the corresponding ground-truth. These operations significantly contribute to increase the effectiveness of the system on biomedical images, where texture plays a major role in the recognition of the different image components. We then present some other techniques that can be easily integrated in the KernelBoost framework to further improve the accuracy of the final segmentation. We show extensive results on different medical image datasets, including some multi-label tasks, on which our method is shown to outperform state-of-the-art approaches. The resulting segmentations display high accuracy, neat contours, and reduced noise.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0173",
        "title": "Variational Depth from Focus Reconstruction",
        "authors": [
            "Michael Moeller",
            "Martin Benning",
            "Carola Sch\u00f6nlieb",
            "Daniel Cremers"
        ],
        "abstract": "This paper deals with the problem of reconstructing a depth map from a sequence of differently focused images, also known as depth from focus or shape from focus. We propose to state the depth from focus problem as a variational problem including a smooth but nonconvex data fidelity term, and a convex nonsmooth regularization, which makes the method robust to noise and leads to more realistic depth maps. Additionally, we propose to solve the nonconvex minimization problem with a linearized alternating directions method of multipliers (ADMM), allowing to minimize the energy very efficiently. A numerical comparison to classical methods on simulated as well as on real data is presented.\n    ",
        "submission_date": "2014-08-01T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0452",
        "title": "Methodology For Detection of QRS Pattern Using Secondary Wavelets",
        "authors": [
            "T.R. Gopalakrishnan Nair",
            "A.P. Geetha",
            "Asharani"
        ],
        "abstract": "Applications of wavelet transform to the field of health care signals have paved the way for implementing revolutionary approaches in detecting the presence of certain abnormalities in human health patterns. There were extensive studies carried out using primary wavelets in various signals like Electrocardiogram (ECG), sonogram etc. with a certain amount of success. On the other hand analysis using secondary wavelets which inherits the characteristics of a set of variations available in signals like ECG can be a promise to detect diseases with ease. Here a method to create a generalized adapted wavelet is presented which contains the information of QRS pattern collected from an anomaly sample space. The method has been tested and found to be successful in locating the position of R peak in noise embedded ECG signal.\n    ",
        "submission_date": "2014-08-03T00:00:00",
        "last_modified_date": "2014-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0453",
        "title": "Adaptive Wavelet Based Identification and Extraction of PQRST Combination in Randomly Stretching ECG Sequence",
        "authors": [
            "T.R. Gopalakrishnan Nair",
            "A.P. Geetha",
            "M. Asharani"
        ],
        "abstract": "Cardiovascular system study using ECG signals have evolved tremendously in the domain of electronics and signal processing. However, there are certain floating challenges unresolved in the analysis and detection of abnormal performances of cardiovascular system. As the medical field is moving towards more automated and intelligent systems, wrong detection or wrong interpretations of ECG waveform of abnormal conditions can be quite fatal. Since the PQRST signals vary their positions randomly, the process of locating, identifying and classifying each feature can be cumbersome and it is prone to errors. Here we present an automated scheme using adaptive wavelet to detect prominent R-peak with extreme accuracy and algorithmically tag and mark the coexisting peaks P, Q, S, and T with almost same accuracy. The adaptive wavelet approach used in this scheme is capable of detecting R-peak in ECG with 99.99% accuracy along with the rest of the waveforms.\n    ",
        "submission_date": "2014-08-03T00:00:00",
        "last_modified_date": "2014-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0680",
        "title": "A Pattern Recognition System for Detecting Use of Mobile Phones While Driving",
        "authors": [
            "Rafael A. Berri",
            "Alexandre G. Silva",
            "Rafael S. Parpinelli",
            "Elaine Girardi",
            "Rangel Arthur"
        ],
        "abstract": "It is estimated that 80% of crashes and 65% of near collisions involved drivers inattentive to traffic for three seconds before the event. This paper develops an algorithm for extracting characteristics allowing the cell phones identification used during driving a vehicle. Experiments were performed on sets of images with 100 positive images (with phone) and the other 100 negative images (no phone), containing frontal images of the driver. Support Vector Machine (SVM) with Polynomial kernel is the most advantageous classification system to the features provided by the algorithm, obtaining a success rate of 91.57% for the vision system. Tests done on videos show that it is possible to use the image datasets for training classifiers in real situations. Periods of 3 seconds were correctly classified at 87.43% of cases.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0814",
        "title": "Object Detection Through Exploration With A Foveated Visual Field",
        "authors": [
            "Emre Akbas",
            "Miguel P. Eckstein"
        ],
        "abstract": "We present a foveated object detector (FOD) as a biologically-inspired alternative to the sliding window (SW) approach which is the dominant method of search in computer vision object detection. Similar to the human visual system, the FOD has higher resolution at the fovea and lower resolution at the visual periphery. Consequently, more computational resources are allocated at the fovea and relatively fewer at the periphery. The FOD processes the entire scene, uses retino-specific object detection classifiers to guide eye movements, aligns its fovea with regions of interest in the input image and integrates observations across multiple fixations. Our approach combines modern object detectors from computer vision with a recent model of peripheral pooling regions found at the V1 layer of the human visual system. We assessed various eye movement strategies on the PASCAL VOC 2007 dataset and show that the FOD performs on par with the SW detector while bringing significant computational cost savings.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2017-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0872",
        "title": "Open-set Person Re-identification",
        "authors": [
            "Shengcai Liao",
            "Zhipeng Mo",
            "Jianqing Zhu",
            "Yang Hu",
            "Stan Z. Li"
        ],
        "abstract": "Person re-identification is becoming a hot research for developing both machine learning algorithms and video surveillance applications. The task of person re-identification is to determine which person in a gallery has the same identity to a probe image. This task basically assumes that the subject of the probe image belongs to the gallery, that is, the gallery contains this person. However, in practical applications such as searching a suspect in a video, this assumption is usually not true. In this paper, we consider the open-set person re-identification problem, which includes two sub-tasks, detection and identification. The detection sub-task is to determine the presence of the probe subject in the gallery, and the identification sub-task is to determine which person in the gallery has the same identity as the accepted probe. We present a database collected from a video surveillance setting of 6 cameras, with 200 persons and 7,413 images segmented. Based on this database, we develop a benchmark protocol for evaluating the performance under the open-set person re-identification scenario. Several popular metric learning algorithms for person re-identification have been evaluated as baselines. From the baseline performance, we observe that the open-set person re-identification problem is still largely unresolved, thus further attention and effort is needed.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1135",
        "title": "It is hard to see a needle in a haystack: Modeling contrast masking effect in a numerical observer",
        "authors": [
            "Ali R. N. Avanaki",
            "Kathryn S. Espig",
            "Albert Xthona",
            "Tom R. L. Kimpe",
            "Predrag R. Bakic",
            "Andrew D. A. Maidment"
        ],
        "abstract": "Within the framework of a virtual clinical trial for breast imaging, we aim to develop numerical observers that follow the same detection performance trends as those of a typical human observer. In our prior work, we showed that by including spatiotemporal contrast sensitivity function (stCSF) of human visual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), we can correctly predict trends of a typical human observer performance with the viewing parameters of browsing speed, viewing distance and contrast. In this work we further improve our numerical observer by modeling contrast masking. After stCSF, contrast masking is the second most prominent property of HVS and it refers to the fact that the presence of one signal affects the visibility threshold for another signal. Our results indicate that the improved numerical observer better predicts changes in detection performance with background complexity.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1292",
        "title": "Scalable Greedy Algorithms for Transfer Learning",
        "authors": [
            "Ilja Kuzborskij",
            "Francesco Orabona",
            "Barbara Caputo"
        ],
        "abstract": "In this paper we consider the binary transfer learning problem, focusing on how to select and combine sources from a large pool to yield a good performance on a target task. Constraining our scenario to real world, we do not assume the direct access to the source data, but rather we employ the source hypotheses trained from them. We propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously, building on the literature on the best subset selection problem. Our algorithm achieves state-of-the-art results on three computer vision datasets, substantially outperforming both transfer learning and popular feature selection baselines in a small-sample setting. We also present a randomized variant that achieves the same results with the computational cost independent from the number of source hypotheses and feature dimensions. Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples.\n    ",
        "submission_date": "2014-08-06T00:00:00",
        "last_modified_date": "2016-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1549",
        "title": "Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Nabil Belhaj Khalifa",
            "Shahram Jamali"
        ],
        "abstract": "At the present time, hand gestures recognition system could be used as a more expected and useable approach for human computer interaction. Automatic hand gesture recognition system provides us a new tactic for interactive with the virtual environment. In this paper, a face and hand gesture recognition system which is able to control computer media player is offered. Hand gesture and human face are the key element to interact with the smart system. We used the face recognition scheme for viewer verification and the hand gesture recognition in mechanism of computer media player, for instance, volume down/up, next music and etc. In the proposed technique, first, the hand gesture and face location is extracted from the main image by combination of skin and cascade detector and then is sent to recognition stage. In recognition stage, first, the threshold condition is inspected then the extracted face and gesture will be recognized. In the result stage, the proposed technique is applied on the video dataset and the high precision ratio acquired. Additional the recommended hand gesture recognition method is applied on static American Sign Language (ASL) database and the correctness rate achieved nearby 99.40%. also the planned method could be used in gesture based computer games and virtual reality.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1656",
        "title": "A Fast and Accurate Unconstrained Face Detector",
        "authors": [
            "Shengcai Liao",
            "Anil K. Jain",
            "Stan Z. Li"
        ],
        "abstract": "We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.\n    ",
        "submission_date": "2014-08-06T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1688",
        "title": "Low-rank SIFT: An Affine Invariant Feature for Place Recognition",
        "authors": [
            "Chao Yang",
            "Shengnan Caih",
            "Jingdong Wang",
            "Long Quan"
        ],
        "abstract": "In this paper, we present a novel affine-invariant feature based on SIFT, leveraging the regular appearance of man-made objects. The feature achieves full affine invariance without needing to simulate over affine parameter space. Low-rank SIFT, as we name the feature, is based on our observation that local tilt, which are caused by changes of camera axis orientation, could be normalized by converting local patches to standard low-rank forms. Rotation, translation and scaling invariance could be achieved in ways similar to SIFT. As an extension of SIFT, our method seeks to add prior to solve the ill-posed affine parameter estimation problem and normalizes them directly, and is applicable to objects with regular structures. Furthermore, owing to recent breakthrough in convex optimization, such parameter could be computed efficiently. We will demonstrate its effectiveness in place recognition as our major application. As extra contributions, we also describe our pipeline of constructing geotagged building database from the ground up, as well as an efficient scheme for automatic feature selection.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1759",
        "title": "Real-Time and Robust Method for Hand Gesture Recognition System Based on Cross-Correlation Coefficient",
        "authors": [
            "Reza Azad",
            "Babak Azad",
            "Iman Tavakoli Kazerooni"
        ],
        "abstract": "Hand gesture recognition possesses extensive applications in virtual reality, sign language recognition, and computer games. The direct interface of hand gestures provides us a new way for communicating with the virtual environment. In this paper a novel and real-time approach for hand gesture recognition system is presented. In the suggested method, first, the hand gesture is extracted from the main image by the image segmentation and morphological operation and then is sent to feature extraction stage. In feature extraction stage the Cross-correlation coefficient is applied on the gesture to recognize it. In the result part, the proposed approach is applied on American Sign Language (ASL) database and the accuracy rate obtained 98.34%.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1984",
        "title": "Neighborhood Rank Order Coding for Robust Texture Analysis and Feature Extraction",
        "authors": [
            "C. Mayr",
            "R. Sch\u00fcffny"
        ],
        "abstract": "Research into the visual cortex and general neural information processing has led to various attempts to integrate pulse computation schemes in image analysis systems. Of interest is especially the robustness of representing an analogue signal in the phase or duration of a pulsed, quasi-digital signal, as well as the possibility of direct digital interaction, i.e. computation, among these signals. Such a computation can also achieve information compaction for subsequent processing stages. By using a pulse order encoding scheme motivated by dendritic pulse interaction, we will show that a powerful low-level feature and texture extraction operator, called Pulsed Local Orientation Coding (PLOC), can be implemented. Feature extraction results are being presented, and a possible VLSI implementation is detailed.\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1986",
        "title": "Gabor-like Image Filtering using a Neural Microcircuit",
        "authors": [
            "C. Mayr",
            "A. Heittmann",
            "R. Sch\u00fcffny"
        ],
        "abstract": "In this letter, we present an implementation of a neural microcircuit for image processing employing Hebbian-adaptive learning. The neuronal circuit utilizes only excitatory synapses to correlate action potentials, extracting the uncorrelated ones, which contain significant image information. This circuit is capable of approximating Gabor-like image filtering and other image processing functions\n    ",
        "submission_date": "2014-08-08T00:00:00",
        "last_modified_date": "2014-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2015",
        "title": "Automatic Removal of Marginal Annotations in Printed Text Document",
        "authors": [
            "Abdessamad Elboushaki",
            "Rachida Hannane",
            "P. Nagabhushan",
            "Mohammed Javed"
        ],
        "abstract": "Recovering the original printed texts from a document with added handwritten annotations in the marginal area is one of the challenging problems, especially when the original document is not available. Therefore, this paper aims at salvaging automatically the original document from the annotated document by detecting and removing any handwritten annotations that appear in the marginal area of the document without any loss of information. Here a two stage algorithm is proposed, where in the first stage due to approximate marginal boundary detection with horizontal and vertical projection profiles, all of the marginal annotations along with some part of the original printed text that may appear very close to the marginal boundary are removed. Therefore as a second stage, using the connected components, a strategy is applied to bring back the printed text components cropped during the first stage. The proposed method is validated using a dataset of 50 documents having complex handwritten annotations, which gives an overall accuracy of 89.01% in removing the marginal annotations and 97.74% in case of retrieving the original printed text document.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2289",
        "title": "Physical Computing With No Clock to Implement the Gaussian Pyramid of SIFT Algorithm",
        "authors": [
            "Yi Li",
            "Qi Wei",
            "Fei Qiao",
            "Huazhong Yang"
        ],
        "abstract": "Physical computing is a technology utilizing the nature of electronic devices and circuit topology to cope with computing tasks. In this paper, we propose an active circuit network to implement multi-scale Gaussian filter, which is also called Gaussian Pyramid in image preprocessing. Various kinds of methods have been tried to accelerate the key stage in image feature extracting algorithm these years. Compared with existing technologies, GPU parallel computing and FPGA accelerating technology, physical computing has great advantage on processing speed as well as power consumption. We have verified that processing time to implement the Gaussian pyramid of the SIFT algorithm stands on nanosecond level through the physical computing technology, while other existing methods all need at least hundreds of millisecond. With an estimate on the stray capacitance of the circuit, the power consumption is around 670pJ to filter a 256x256 image. To the best of our knowledge, this is the most fast processing technology to accelerate the SIFT algorithm, and it is also a rather energy-efficient method, thanks to the proposed physical computing technology.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2313",
        "title": "Bags of Affine Subspaces for Robust Object Tracking",
        "authors": [
            "Sareh Shirazi",
            "Conrad Sanderson",
            "Chris McCool",
            "Mehrtash T. Harandi"
        ],
        "abstract": "We propose an adaptive tracking algorithm where the object is modelled as a continuously updated bag of affine subspaces, with each subspace constructed from the object's appearance over several consecutive frames. In contrast to linear subspaces, affine subspaces explicitly model the origin of subspaces. Furthermore, instead of using a brittle point-to-subspace distance during the search for the object in a new frame, we propose to use a subspace-to-subspace distance by representing candidate image areas also as affine subspaces. Distances between subspaces are then obtained by exploiting the non-Euclidean geometry of Grassmann manifolds. Experiments on challenging videos (containing object occlusions, deformations, as well as variations in pose and illumination) indicate that the proposed method achieves higher tracking accuracy than several recent discriminative trackers.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2380",
        "title": "Video Face Editing Using Temporal-Spatial-Smooth Warping",
        "authors": [
            "Xiaoyan Li",
            "Dacheng Tao"
        ],
        "abstract": "Editing faces in videos is a popular yet challenging aspect of computer vision and graphics, which encompasses several applications including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos (or even two consecutive frames representing the same face in one video). Therefore, high performance face editing usually requires significant manual manipulation. In this paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm to effectively exploit the temporal information in two consecutive frames, as well as the spatial smoothness within each frame. TSSW precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame, by minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. Corresponding warping surfaces then precisely map source frames to the target frames. Experimental testing on facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry, skin detail, identity, and expression, which outperform the existing face editing methods. In particular, TSSW is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2478",
        "title": "Learning to see like children: proof of concept",
        "authors": [
            "Marco Gori",
            "Marco Lippi",
            "Marco Maggini",
            "Stefano Melacci"
        ],
        "abstract": "In the last few years we have seen a growing interest in machine learning approaches to computer vision and, especially, to semantic labeling. Nowadays state of the art systems use deep learning on millions of labeled images with very successful results on benchmarks, though it is unlikely to expect similar results in unrestricted visual environments. Most learning schemes essentially ignore the inherent sequential structure of videos: this might be a critical issue, since any visual recognition process is remarkably more complex when shuffling video frames. Based on this remark, we propose a re-foundation of the communication protocol between visual agents and the environment, which is referred to as learning to see like children. Like for human interaction, visual concepts are acquired by the agents solely by processing their own visual stream along with human supervisions on selected pixels. We give a proof of concept that remarkable semantic labeling can emerge within this protocol by using only a few supervised examples. This is made possible by exploiting a constraint of motion coherent labeling that virtually offers tons of supervisions. Additional visual constraints, including those associated with object supervisions, are used within the context of learning from constraints. The framework is extended in the direction of lifelong learning, so as our visual agents live in their own visual environment without distinguishing learning and test set. Learning takes place in deep architectures under a progressive developmental scheme. In order to evaluate our Developmental Visual Agents (DVAs), in addition to classic benchmarks, we open the doors of our lab, allowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanism might result in a paradigm shift in methodologies and algorithms for computer vision, encouraging truly novel solutions within the proposed framework.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2590",
        "title": "Multidimensional Digital Filters for Point-Target Detection in Cluttered Infrared Scenes",
        "authors": [
            "Hugh L. Kennedy"
        ],
        "abstract": "A 3-D spatiotemporal prediction-error filter (PEF), is used to enhance foreground/background contrast in (real and simulated) sensor image sequences. Relative velocity is utilized to extract point-targets that would otherwise be indistinguishable on spatial frequency alone. An optical-flow field is generated using local estimates of the 3-D autocorrelation function via the application of the fast Fourier transform (FFT) and inverse FFT. Velocity estimates are then used to tune in a background-whitening PEF that is matched to the motion and texture of the local background. Finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression for the frequency response of velocity-tuned FIR filters, of odd or even dimension, with an arbitrary delay in each dimension, is derived.\n    ",
        "submission_date": "2014-08-12T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2810",
        "title": "Spectral Unmixing of Hyperspectral Imagery using Multilayer NMF",
        "authors": [
            "Roozbeh Rajabi",
            "Hassan Ghassemian"
        ],
        "abstract": "Hyperspectral images contain mixed pixels due to low spatial resolution of hyperspectral sensors. Spectral unmixing problem refers to decomposing mixed pixels into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fractions, nonnegative matrix factorization (NMF) methods have been widely used for solving spectral unmixing problem. In this letter we proposed using multilayer NMF (MLNMF) for the purpose of hyperspectral unmixing. In this approach, spectral signature matrix can be modeled as a product of sparse matrices. In fact MLNMF decomposes the observation matrix iteratively in a number of layers. In each layer, we applied sparseness constraint on spectral signature matrix as well as on abundance fractions matrix. In this way signatures matrix can be sparsely decomposed despite the fact that it is not generally a sparse matrix. The proposed algorithm is applied on synthetic and real datasets. Synthetic data is generated based on endmembers from USGS spectral library. AVIRIS Cuprite dataset has been used as a real dataset for evaluation of proposed method. Results of experiments are quantified based on SAD and AAD measures. Results in comparison with previously proposed methods show that the multilayer approach can unmix data more effectively.\n    ",
        "submission_date": "2014-08-12T00:00:00",
        "last_modified_date": "2014-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2938",
        "title": "Learning Multi-Scale Representations for Material Classification",
        "authors": [
            "Wenbin Li",
            "Mario Fritz"
        ],
        "abstract": "The recent progress in sparse coding and deep learning has made unsupervised feature learning methods a strong competitor to hand-crafted descriptors. In computer vision, success stories of learned features have been predominantly reported for object recognition tasks. In this paper, we investigate if and how feature learning can be used for material recognition. We propose two strategies to incorporate scale information into the learning procedure resulting in a novel multi-scale coding procedure. Our results show that our learned features for material recognition outperform hand-crafted descriptors on the FMD and the KTH-TIPS2 material classification benchmarks.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2997",
        "title": "An Improved Approach for Contrast Enhancement of Spinal Cord Images based on Multiscale Retinex Algorithm",
        "authors": [
            "Sreenivasa Setty",
            "N. K Srinath",
            "M. C Hanumantharaju"
        ],
        "abstract": "This paper presents a new approach for contrast enhancement of spinal cord medical images based on multirate scheme incorporated into multiscale retinex algorithm. The proposed work here uses HSV color space, since HSV color space separates color details from intensity. The enhancement of medical image is achieved by down sampling the original image into five versions, namely, tiny, small, medium, fine, and normal scale. This is due to the fact that the each versions of the image when independently enhanced and reconstructed results in enormous improvement in the visual quality. Further, the contrast stretching and MultiScale Retinex (MSR) techniques are exploited in order to enhance each of the scaled version of the image. Finally, the enhanced image is obtained by combining each of these scales in an efficient way to obtain the composite enhanced image. The efficiency of the proposed algorithm is validated by using a wavelet energy metric in the wavelet domain. Reconstructed image using proposed method highlights the details (edges and tissues), reduces image noise (Gaussian and Speckle) and improves the overall contrast. The proposed algorithm also enhances sharp edges of the tissue surrounding the spinal cord regions which is useful for diagnosis of spinal cord lesions. Elaborated experiments are conducted on several medical images and results presented show that the enhanced medical pictures are of good quality and is found to be better compared with other researcher methods.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3139",
        "title": "Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering",
        "authors": [
            "Hossein Hosseini",
            "Farzad Hessar",
            "Farokh Marvasti"
        ],
        "abstract": "In this paper, we propose a method for real-time high density impulse noise suppression from images. In our method, we first apply an impulse detector to identify the corrupted pixels and then employ an innovative weighted-average filter to restore them. The filter takes the nearest neighboring interpolated image as the initial image and computes the weights according to the relative positions of the corrupted and uncorrupted pixels. Experimental results show that the proposed method outperforms the best existing methods in both PSNR measure and visual quality and is quite suitable for real-time applications.\n    ",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3218",
        "title": "Toward Automated Discovery of Artistic Influence",
        "authors": [
            "Babak Saleh",
            "Kanako Abe",
            "Ravneet Singh Arora",
            "Ahmed Elgammal"
        ],
        "abstract": "Considering the huge amount of art pieces that exist, there is valuable information to be discovered. Examining a painting, an expert can determine its style, genre, and the time period that the painting belongs. One important task for art historians is to find influences and connections between artists. Is influence a task that a computer can measure? The contribution of this paper is in exploring the problem of computer-automated suggestion of influences between artists, a problem that was not addressed before in a general setting. We first present a comparative study of different classification methodologies for the task of fine-art style classification. A two-level comparative study is performed for this classification problem. The first level reviews the performance of discriminative vs. generative models, while the second level touches the features aspect of the paintings and compares semantic-level features vs. low-level and intermediate-level features present in the painting. Then, we investigate the question \"Who influenced this artist?\" by looking at his masterpieces and comparing them to others. We pose this interesting question as a knowledge discovery problem. For this purpose, we investigated several painting-similarity and artist-similarity measures. As a result, we provide a visualization of artists (Map of Artists) based on the similarity between their works\n    ",
        "submission_date": "2014-08-14T00:00:00",
        "last_modified_date": "2014-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3264",
        "title": "A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet)",
        "authors": [
            "Mohammad Ali Keyvanrad",
            "Mohammad Mehdi Homayounpour"
        ],
        "abstract": "Nowadays, this is very popular to use the deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. DBNs have many ability like feature extraction and classification that are used in many applications like image processing, speech processing and etc. This paper introduces a new object oriented MATLAB toolbox with most of abilities needed for the implementation of DBNs. In the new version, the toolbox can be used in Octave. According to the results of the experiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups (text) datasets, it was shown that the toolbox can learn automatically a good representation of the input from unlabeled data with better discrimination between different classes. Also on all datasets, the obtained classification errors are comparable to those of state of the art classifiers. In addition, the toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and our new FEPCD method), different sparsity methods (quadratic, rate distortion and our new normal method), different RBM types (generative and discriminative), using GPU, etc. The toolbox is a user-friendly open source software and is freely available on the website ",
        "submission_date": "2014-08-14T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3300",
        "title": "Gradient Distribution Priors for Biomedical Image Processing",
        "authors": [
            "Yuanhao Gong",
            "Ivo F. Sbalzarini"
        ],
        "abstract": "Ill-posed inverse problems are commonplace in biomedical image processing. Their solution typically requires imposing prior knowledge about the latent ground truth. While this regularizes the problem to an extent where it can be solved, it also biases the result toward the expected. With inappropriate priors harming more than they use, it remains unclear what prior to use for a given practical problem. Priors are hence mostly chosen in an {\\em ad hoc} or empirical fashion. We argue here that the gradient distribution of natural-scene images may provide a versatile and well-founded prior for biomedical images. We provide motivation for this choice from different points of view, and we fully validate the resulting prior for use on biomedical images by showing its stability and correlation with image quality. We then provide a set of simple parametric models for the resulting prior, leading to straightforward (quasi-)convex optimization problems for which we provide efficient solver algorithms. We illustrate the use of the present models and solvers in a variety of common image-processing tasks, including contrast enhancement, noise level estimation, denoising, blind deconvolution, zooming/up-sampling, and dehazing. In all cases we show that the present method leads to results that are comparable to or better than the state of the art; always using the same, simple prior. We conclude by discussing the limitations and possible interpretations of the prior.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3304",
        "title": "On Pairwise Costs for Network Flow Multi-Object Tracking",
        "authors": [
            "Visesh Chari",
            "Simon Lacoste-Julien",
            "Ivan Laptev",
            "Josef Sivic"
        ],
        "abstract": "Multi-object tracking has been recently approached with the min-cost network flow optimization techniques. Such methods simultaneously resolve multiple object tracks in a video and enable modeling of dependencies among tracks. Min-cost network flow methods also fit well within the \"tracking-by-detection\" paradigm where object trajectories are obtained by connecting per-frame outputs of an object detector. Object detectors, however, often fail due to occlusions and clutter in the video. To cope with such situations, we propose to add pairwise costs to the min-cost network flow framework. While integer solutions to such a problem become NP-hard, we design a convex relaxation solution with an efficient rounding heuristic which empirically gives certificates of small suboptimality. We evaluate two particular types of pairwise costs and demonstrate improvements over recent tracking methods in real-world video sequences.\n    ",
        "submission_date": "2014-08-14T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3337",
        "title": "2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy of Linear Classifiers",
        "authors": [
            "Ari Seff",
            "Le Lu",
            "Kevin M. Cherry",
            "Holger Roth",
            "Jiamin Liu",
            "Shijun Wang",
            "Joanne Hoffman",
            "Evrim B. Turkbey",
            "Ronald M. Summers"
        ],
        "abstract": "Enlarged lymph nodes (LNs) can provide important information for cancer diagnosis, staging, and measuring treatment reactions, making automated detection a highly sought goal. In this paper, we propose a new algorithm representation of decomposing the LN detection problem into a set of 2D object detection subtasks on sampled CT slices, largely alleviating the curse of dimensionality issue. Our 2D detection can be effectively formulated as linear classification on a single image feature type of Histogram of Oriented Gradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We exploit both simple pooling and sparse linear fusion schemes to aggregate these 2D detection scores for the final 3D LN detection. In this manner, detection is more tractable and does not need to perform perfectly at instance level (as weak hypotheses) since our aggregation process will robustly harness collective information for LN detection. Two datasets (90 patients with 389 mediastinal LNs and 86 patients with 595 abdominal LNs) are used for validation. Cross-validation demonstrates 78.0% sensitivity at 6 false positives/volume (FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10 FP/vol.), for the mediastinal and abdominal datasets respectively. Our results compare favorably to previous state-of-the-art methods.\n    ",
        "submission_date": "2014-08-14T00:00:00",
        "last_modified_date": "2014-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3526",
        "title": "Parallel software implementation of recursive multidimensional digital filters for point-target detection in cluttered infrared scenes",
        "authors": [
            "Hugh L. Kennedy"
        ],
        "abstract": "A technique for the enhancement of point targets in clutter is described. The local 3-D spectrum at each pixel is estimated recursively. An optical flow-field for the textured background is then generated using the 3-D autocorrelation function and the local velocity estimates are used to apply high-pass velocity-selective spatiotemporal filters, with finite impulse responses (FIRs), to subtract the background clutter signal, leaving the foreground target signal, plus noise. Parallel software implementations using a multicore central processing unit (CPU) and a graphical processing unit (GPU) are investigated.\n    ",
        "submission_date": "2014-08-15T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3573",
        "title": "Turkish Presidential Elections TRT Publicity Speech Facial Expression Analysis",
        "authors": [
            "H. Emrah Tasli",
            "Paul Ivan"
        ],
        "abstract": "In this paper, facial expressions of the three Turkish presidential candidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzed during the publicity speeches featured at TRT (Turkish Radio and Television) on 03.08.2014. FaceReader is used for the analysis where 3D modeling of the face is achieved using the active appearance models (AAM). Over 500 landmark points are tracked and analyzed for obtaining the facial expressions during the whole speech. All source videos and the data are publicly available for research purposes.\n    ",
        "submission_date": "2014-08-15T00:00:00",
        "last_modified_date": "2014-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3686",
        "title": "Motion Deblurring for Plenoptic Images",
        "authors": [
            "Paramanand Chandramouli",
            "Paolo Favaro",
            "Daniele Perrone"
        ],
        "abstract": "We address for the first time the issue of motion blur in light field images captured from plenoptic cameras. We propose a solution to the estimation of a sharp high resolution scene radiance given a blurry light field image, when the motion blur point spread function is unknown, i.e., the so-called blind deconvolution problem. In a plenoptic camera, the spatial sampling in each view is not only decimated but also defocused. Consequently, current blind deconvolution approaches for traditional cameras are not applicable. Due to the complexity of the imaging model, we investigate first the case of uniform (shift-invariant) blur of Lambertian objects, i.e., when objects are sufficiently far away from the camera to be approximately invariant to depth changes and their reflectance does not vary with the viewing direction. We introduce a highly parallelizable model for light field motion blur that is computationally and memory efficient. We then adapt a regularized blind deconvolution approach to our model and demonstrate its performance on both synthetic and real light field data. Our method handles practical issues in real cameras such as radial distortion correction and alignment within an energy minimization framework.\n    ",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3709",
        "title": "Robust 3D face recognition in presence of pose and partial occlusions or missing parts",
        "authors": [
            "Parama Bagchi",
            "Debotosh Bhattacharjee",
            "Mita Nasipuri"
        ],
        "abstract": "In this paper, we propose a robust 3D face recognition system which can handle pose as well as occlusions in real world. The system at first takes as input, a 3D range image, simultaneously registers it using ICP(Iterative Closest Point) algorithm. ICP used in this work, registers facial surfaces to a common model by minimizing distances between a probe model and a gallery model. However the performance of ICP relies heavily on the initial conditions. Hence, it is necessary to provide an initial registration, which will be improved iteratively and finally converge to the best alignment possible. Once the faces are registered, the occlusions are automatically extracted by thresholding the depth map values of the 3D image. After the occluded regions are detected, restoration is done by Principal Component Analysis (PCA). The restored images, after the removal of occlusions, are then fed to the recognition system for classification purpose. Features are extracted from the reconstructed non-occluded face images in the form of face normals. The experimental results which were obtained on the occluded facial images from the Bosphorus 3D face database, illustrate that our occlusion compensation scheme has attained a recognition accuracy of 91.30%.\n    ",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2014-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3740",
        "title": "A fast patch-dictionary method for whole image recovery",
        "authors": [
            "Yangyang Xu",
            "Wotao Yin"
        ],
        "abstract": "Various algorithms have been proposed for dictionary learning. Among those for image processing, many use image patches to form dictionaries. This paper focuses on whole-image recovery from corrupted linear measurements. We address the open issue of representing an image by overlapping patches: the overlapping leads to an excessive number of dictionary coefficients to determine. With very few exceptions, this issue has limited the applications of image-patch methods to the local kind of tasks such as denoising, inpainting, cartoon-texture decomposition, super-resolution, and image deblurring, for which one can process a few patches at a time. Our focus is global imaging tasks such as compressive sensing and medical image recovery, where the whole image is encoded together, making it either impossible or very ineffective to update a few patches at a time.\n",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2014-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3750",
        "title": "Real-time emotion recognition for gaming using deep convolutional network features",
        "authors": [
            "S\u00e9bastien Ouellet"
        ],
        "abstract": "The goal of the present study is to explore the application of deep convolutional network features to emotion recognition. Results indicate that they perform similarly to other published models at a best recognition rate of 94.4%, and do so with a single still image rather than a video stream. An implementation of an affective feedback game is also described, where a classifier using these features tracks the facial expressions of a player in real-time.\n    ",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2014-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3772",
        "title": "Highly Accurate Multispectral Palmprint Recognition Using Statistical and Wavelet Features",
        "authors": [
            "Shervin Minaee",
            "AmirAli Abdolrashidi"
        ],
        "abstract": "Palmprint is one of the most useful physiological biometrics that can be used as a powerful means in personal recognition systems. The major features of the palmprints are palm lines, wrinkles and ridges, and many approaches use them in different ways towards solving the palmprint recognition problem. Here we have proposed to use a set of statistical and wavelet-based features; statistical to capture the general characteristics of palmprints; and wavelet-based to find those information not evident in the spatial domain. Also we use two different classification approaches, minimum distance classifier scheme and weighted majority voting algorithm, to perform palmprint matching. The proposed method is tested on a well-known palmprint dataset of 6000 samples and has shown an impressive accuracy rate of 99.65\\%-100\\% for most scenarios.\n    ",
        "submission_date": "2014-08-16T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3809",
        "title": "HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition",
        "authors": [
            "Hossein Rahmani",
            "Arif Mahmood",
            "Du Q. Huynh",
            "Ajmal Mian"
        ],
        "abstract": "Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which change significantly with viewpoint. In contrast, we directly process the pointclouds and propose a new technique for action recognition which is more robust to noise, action speed and viewpoint variations. Our technique consists of a novel descriptor and keypoint detection algorithm. The proposed descriptor is extracted at a point by encoding the Histogram of Oriented Principal Components (HOPC) within an adaptive spatio-temporal support volume around that point. Based on this descriptor, we present a novel method to detect Spatio-Temporal Key-Points (STKPs) in 3D pointcloud sequences. Experimental results show that the proposed descriptor and STKP detector outperform state-of-the-art algorithms on three benchmark human activity datasets. We also introduce a new multiview public dataset and show the robustness of our proposed method to viewpoint variations.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3810",
        "title": "Action Classification with Locality-constrained Linear Coding",
        "authors": [
            "Hossein Rahmani",
            "Arif Mahmood",
            "Du Huynh",
            "Ajmal Mian"
        ],
        "abstract": "We propose an action classification algorithm which uses Locality-constrained Linear Coding (LLC) to capture discriminative information of human body variations in each spatiotemporal subsequence of a video sequence. Our proposed method divides the input video into equally spaced overlapping spatiotemporal subsequences, each of which is decomposed into blocks and then cells. We use the Histogram of Oriented Gradient (HOG3D) feature to encode the information in each cell. We justify the use of LLC for encoding the block descriptor by demonstrating its superiority over Sparse Coding (SC). Our sequence descriptor is obtained via a logistic regression classifier with L2 regularization. We evaluate and compare our algorithm with ten state-of-the-art algorithms on five benchmark datasets. Experimental results show that, on average, our algorithm gives better accuracy than these ten algorithms.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3814",
        "title": "Robust Statistical Approach for Extraction of Moving Human Silhouettes from Videos",
        "authors": [
            "Oinam Binarani Devi",
            "Nissi S. Paul",
            "Y. Jayanta Singh"
        ],
        "abstract": "Human pose estimation is one of the key problems in computer vision that has been studied in the recent years. The significance of human pose estimation is in the higher level tasks of understanding human actions applications such as recognition of anomalous actions present in videos and many other related applications. The human poses can be estimated by extracting silhouettes of humans as silhouettes are robust to variations and it gives the shape information of the human body. Some common challenges include illumination changes, variation in environments, and variation in human appearances. Thus there is a need for a robust method for human pose estimation. This paper presents a study and analysis of approaches existing for silhouette extraction and proposes a robust technique for extracting human silhouettes in video sequences. Gaussian Mixture Model (GMM) A statistical approach is combined with HSV (Hue, Saturation and Value) color space model for a robust background model that is used for background subtraction to produce foreground blobs, called human silhouettes. Morphological operations are then performed on foreground blobs from background subtraction. The silhouettes obtained from this work can be used in further tasks associated with human action interpretation and activity processes like human action classification, human pose estimation and action recognition or action interpretation.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2014-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3873",
        "title": "Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome",
        "authors": [
            "Lorenzo Livi",
            "Antonello Rizzi",
            "Alireza Sadeghian"
        ],
        "abstract": "We evaluate a version of the recently-proposed classification system named Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space of sequences of generic objects. The ODSE system has been originally presented as a classification system for patterns represented as labeled graphs. However, since ODSE is founded on the dissimilarity space representation of the input data, the classifier can be easily adapted to any input domain where it is possible to define a meaningful dissimilarity measure. Here we demonstrate the effectiveness of the ODSE classifier for sequences by considering an application dealing with the recognition of the solubility degree of the Escherichia coli proteome. Solubility, or analogously aggregation propensity, is an important property of protein molecules, which is intimately related to the mechanisms underlying the chemico-physical process of folding. Each protein of our dataset is initially associated with a solubility degree and it is represented as a sequence of symbols, denoting the 20 amino acid residues. The herein obtained computational results, which we stress that have been achieved with no context-dependent tuning of the ODSE system, confirm the validity and generality of the ODSE-based approach for structured data classification.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2015-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3967",
        "title": "Learning Deep Representation for Face Alignment with Auxiliary Attributes",
        "authors": [
            "Zhanpeng Zhang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.\n    ",
        "submission_date": "2014-08-18T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3985",
        "title": "Offline Signature-Based Fuzzy Vault (OSFV: Review and New Results",
        "authors": [
            "George S. Eskander",
            "Robert Sabourin",
            "Eric Granger"
        ],
        "abstract": "An offline signature-based fuzzy vault (OSFV) is a bio-cryptographic implementation that uses handwritten signature images as biometrics instead of traditional passwords to secure private cryptographic keys. Having a reliable OSFV implementation is the first step towards automating financial and legal authentication processes, as it provides greater security of confidential documents by means of the embedded handwritten signatures. The authors have recently proposed the first OSFV implementation which is reviewed in this paper. In this system, a machine learning approach based on the dissimilarity representation concept is employed to select a reliable feature representation adapted for the fuzzy vault scheme. Some variants of this system are proposed for enhanced accuracy and security. In particular, a new method that adapts user key size is presented. Performance of proposed methods are compared using the Brazilian PUCPR and GPDS signature databases and results indicate that the key-size adaptation method achieves a good compromise between security and accuracy. While average system entropy is increased from 45-bits to about 51-bits, the AER (average error rate) is decreased by about 21%.\n    ",
        "submission_date": "2014-08-18T00:00:00",
        "last_modified_date": "2014-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4002",
        "title": "The Filament Sensor for Near Real-Time Detection of Cytoskeletal Fiber Structures",
        "authors": [
            "Benjamin Eltzner",
            "Carina Wollnik",
            "Carsten Gottschlich",
            "Stephan Huckemann",
            "Florian Rehfeldt"
        ],
        "abstract": "A reliable extraction of filament data from microscopic images is of high interest in the analysis of acto-myosin structures as early morphological markers in mechanically guided differentiation of human mesenchymal stem cells and the understanding of the underlying fiber arrangement processes. In this paper, we propose the filament sensor (FS), a fast and robust processing sequence which detects and records location, orientation, length and width for each single filament of an image, and thus allows for the above described analysis. The extraction of these features has previously not been possible with existing methods. We evaluate the performance of the proposed FS in terms of accuracy and speed in comparison to three existing methods with respect to their limited output. Further, we provide a benchmark dataset of real cell images along with filaments manually marked by a human expert as well as simulated benchmark images. The FS clearly outperforms existing methods in terms of computational runtime and filament extraction accuracy. The implementation of the FS and the benchmark database are available as open source.\n    ",
        "submission_date": "2014-08-18T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4143",
        "title": "Self Organization Map based Texture Feature Extraction for Efficient Medical Image Categorization",
        "authors": [
            "Marghny H. Mohamed",
            "Mohammed M. Abdelsamea"
        ],
        "abstract": "Texture is one of the most important properties of visual surface that helps in discriminating one object from another or an object from background. The self-organizing map (SOM) is an excellent tool in exploratory phase of data mining. It projects its input space on prototypes of a low-dimensional regular grid that can be effectively utilized to visualize and explore properties of the data. This paper proposes an enhancement extraction method for accurate extracting features for efficient image representation it based on SOM neural network. In this approach, we apply three different partitioning approaches as a region of interested (ROI) selection methods for extracting different accurate textural features from medical image as a primary step of our extraction method. Fisherfaces feature selection is used, for selecting discriminated features form extracted textural features. Experimental result showed the high accuracy of medical image categorization with our proposed extraction method. Experiments held on Mammographic Image Analysis Society (MIAS) dataset.\n    ",
        "submission_date": "2014-07-14T00:00:00",
        "last_modified_date": "2014-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4325",
        "title": "What makes an Image Iconic? A Fine-Grained Case Study",
        "authors": [
            "Yangmuzi Zhang",
            "Diane Larlus",
            "Florent Perronnin"
        ],
        "abstract": "A natural approach to teaching a visual concept, e.g. a bird species, is to show relevant images. However, not all relevant images represent a concept equally well. In other words, they are not necessarily iconic. This observation raises three questions. Is iconicity a subjective property? If not, can we predict iconicity? And what exactly makes an image iconic? We provide answers to these questions through an extensive experimental study on a challenging fine-grained dataset of birds. We first show that iconicity ratings are consistent across individuals, even when they are not domain experts, thus demonstrating that iconicity is not purely subjective. We then consider an exhaustive list of properties that are intuitively related to iconicity and measure their correlation with these iconicity ratings. We combine them to predict iconicity of new unseen images. We also propose a direct iconicity predictor that is discriminatively trained with iconicity ratings. By combining both systems, we get an iconicity prediction that approaches human performance.\n    ",
        "submission_date": "2014-08-19T00:00:00",
        "last_modified_date": "2014-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4363",
        "title": "Object Segmentation in Images using EEG Signals",
        "authors": [
            "Eva Mohedano",
            "Graham Healy",
            "Kevin McGuinness",
            "Xavier Giro-i-Nieto",
            "Noel E. O'Connor",
            "Alan F. Smeaton"
        ],
        "abstract": "This paper explores the potential of brain-computer interfaces in segmenting objects from images. Our approach is centered around designing an effective method for displaying the image parts to the users such that they generate measurable brain reactions. When an image region, specifically a block of pixels, is displayed we estimate the probability of the block containing the object of interest using a score based on EEG activity. After several such blocks are displayed, the resulting probability map is binarized and combined with the GrabCut algorithm to segment the image into object and background regions. This study shows that BCI and simple EEG analysis are useful in locating object boundaries in images.\n    ",
        "submission_date": "2014-08-19T00:00:00",
        "last_modified_date": "2014-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4504",
        "title": "Unsupervised Parallel Extraction based Texture for Efficient Image Representation",
        "authors": [
            "Mohammed M. Abdelsamea"
        ],
        "abstract": "SOM is a type of unsupervised learning where the goal is to discover some underlying structure of the data. In this paper, a new extraction method based on the main idea of Concurrent Self-Organizing Maps (CSOM), representing a winner-takes-all collection of small SOM networks is proposed. Each SOM of the system is trained individually to provide best results for one class only. The experiments confirm that the proposed features based CSOM is capable to represent image content better than extracted features based on a single big SOM and these proposed features improve the final decision of the CAD. Experiments held on Mammographic Image Analysis Society (MIAS) dataset.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4692",
        "title": "Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods",
        "authors": [
            "Alexander Freytag",
            "Johannes R\u00fchle",
            "Paul Bodesheim",
            "Erik Rodner",
            "Joachim Denzler"
        ],
        "abstract": "Vector-quantized local features frequently used in bag-of-visual-words approaches are the backbone of popular visual recognition systems due to both their simplicity and their performance. Despite their success, bag-of-words-histograms basically contain low-level image statistics (e.g., number of edges of different orientations). The question remains how much visual information is \"lost in quantization\" when mapping visual features to code words? To answer this question, we present an in-depth analysis of the effect of local feature quantization on human recognition performance. Our analysis is based on recovering the visual information by inverting quantized local features and presenting these visualizations with different codebook sizes to human observers. Although feature inversion techniques are around for quite a while, to the best of our knowledge, our technique is the first visualizing especially the effect of feature quantization. Thereby, we are now able to compare single steps in common image classification pipelines to human counterparts.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4703",
        "title": "GIMP and Wavelets for Medical Image Processing: Enhancing Images of the Fundus of the Eye",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "The visual analysis of retina and of its vascular characteristics is important in the diagnosis and monitoring of diseases of visual perception. In the related medical diagnoses, the digital processing of the fundus images is used to obtain the segmentation of retinal vessels. However, an image segmentation is often requiring methods based on peculiar or complex algorithms: in this paper we will show some alternative approaches obtained by applying freely available tools to enhance, without a specific segmentation, the images of the fundus of the eye. We will see in particular, that combining the use of GIMP, the GNU Image Manipulation Program, with the wavelet filter of Iris, a program well-known for processing astronomical images, the result is giving images which can be alternative of those obtained from segmentation.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4712",
        "title": "Bi-l0-l2-Norm Regularization for Blind Motion Deblurring",
        "authors": [
            "Wen-Ze Shao",
            "Hai-Bo Li",
            "Michael Elad"
        ],
        "abstract": "In blind motion deblurring, leading methods today tend towards highly non-convex approximations of the l0-norm, especially in the image regularization term. In this paper, we propose a simple, effective and fast approach for the estimation of the motion blur-kernel, through a bi-l0-l2-norm regularization imposed on both the intermediate sharp image and the blur-kernel. Compared with existing methods, the proposed regularization is shown to be more effective and robust, leading to a more accurate motion blur-kernel and a better final restored image. A fast numerical scheme is deployed for alternatingly computing the sharp image and the blur-kernel, by coupling the operator splitting and augmented Lagrangian methods. Experimental results on both a benchmark image dataset and real-world motion blurred images show that the proposed approach is highly competitive with state-of-the- art methods in both deblurring effectiveness and computational efficiency.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4721",
        "title": "Code Generation for High-Level Synthesis of Multiresolution Applications on FPGAs",
        "authors": [
            "Moritz Schmid",
            "Oliver Reiche",
            "Christian Schmitt",
            "Frank Hannig",
            "J\u00fcrgen Teich"
        ],
        "abstract": "Multiresolution Analysis (MRA) is a mathematical method that is based on working on a problem at different scales. One of its applications is medical imaging where processing at multiple scales, based on the concept of Gaussian and Laplacian image pyramids, is a well-known technique. It is often applied to reduce noise while preserving image detail on different levels of granularity without modifying the filter kernel. In scientific computing, multigrid methods are a popular choice, as they are asymptotically optimal solvers for elliptic Partial Differential Equations (PDEs). As such algorithms have a very high computational complexity that would overwhelm CPUs in the presence of real-time constraints, application-specific processors come into consideration for implementation. Despite of huge advancements in leveraging productivity in the respective fields, designers are still required to have detailed knowledge about coding techniques and the targeted architecture to achieve efficient solutions. Recently, the HIPAcc framework was proposed as a means for automatic code generation of image processing algorithms, based on a Domain-Specific Language (DSL). From the same code base, it is possible to generate code for efficient implementations on several accelerator technologies including different types of Graphics Processing Units (GPUs) as well as reconfigurable logic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generate code for the implementation of multiresolution applications on FPGAs and embedded GPUs.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5093",
        "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
        "authors": [
            "Yangqing Jia",
            "Evan Shelhamer",
            "Jeff Donahue",
            "Sergey Karayev",
            "Jonathan Long",
            "Ross Girshick",
            "Sergio Guadarrama",
            "Trevor Darrell"
        ],
        "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2014-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5275",
        "title": "Unsupervised Spike Sorting Based on Discriminative Subspace Learning",
        "authors": [
            "Mohammad Reza Keshtkaran",
            "Zhi Yang"
        ],
        "abstract": "Spike sorting is a fundamental preprocessing step for many neuroscience studies which rely on the analysis of spike trains. In this paper, we present two unsupervised spike sorting algorithms based on discriminative subspace learning. The first algorithm simultaneously learns the discriminative feature subspace and performs clustering. It uses histogram of features in the most discriminative projection to detect the number of neurons. The second algorithm performs hierarchical divisive clustering that learns a discriminative 1-dimensional subspace for clustering in each level of the hierarchy until achieving almost unimodal distribution in the subspace. The algorithms are tested on synthetic and in-vivo data, and are compared against two widely used spike sorting methods. The comparative results demonstrate that our spike sorting methods can achieve substantially higher accuracy in lower dimensional feature space, and they are highly robust to noise. Moreover, they provide significantly better cluster separability in the learned subspace than in the subspace obtained by principal component analysis or wavelet transform.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5286",
        "title": "Designing labeled graph classifiers by exploiting the R\u00e9nyi entropy of the dissimilarity representation",
        "authors": [
            "Lorenzo Livi"
        ],
        "abstract": "Representing patterns as labeled graphs is becoming increasingly common in the broad field of computational intelligence. Accordingly, a wide repertoire of pattern recognition tools, such as classifiers and knowledge discovery procedures, are nowadays available and tested for various datasets of labeled graphs. However, the design of effective learning procedures operating in the space of labeled graphs is still a challenging problem, especially from the computational complexity viewpoint. In this paper, we present a major improvement of a general-purpose classifier for graphs, which is conceived on an interplay between dissimilarity representation, clustering, information-theoretic techniques, and evolutionary optimization algorithms. The improvement focuses on a specific key subroutine devised to compress the input data. We prove different theorems which are fundamental to the setting of the parameters controlling such a compression operation. We demonstrate the effectiveness of the resulting classifier by benchmarking the developed variants on well-known datasets of labeled graphs, considering as distinct performance indicators the classification accuracy, computing time, and parsimony in terms of structural complexity of the synthesized classification models. The results show state-of-the-art standards in terms of test set accuracy and a considerable speed-up for what concerns the computing time.\n    ",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2017-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5400",
        "title": "Hierarchical Adaptive Structural SVM for Domain Adaptation",
        "authors": [
            "Jiaolong Xu",
            "Sebastian Ramos",
            "David Vazquez",
            "Antonio M. Lopez"
        ],
        "abstract": "A key topic in classification is the accuracy loss produced when the data distribution in the training (source) domain differs from that in the testing (target) domain. This is being recognized as a very relevant problem for many computer vision tasks such as image classification, object detection, and object category recognition. In this paper, we present a novel domain adaptation method that leverages multiple target domains (or sub-domains) in a hierarchical adaptation tree. The core idea is to exploit the commonalities and differences of the jointly considered target domains.\n",
        "submission_date": "2014-08-22T00:00:00",
        "last_modified_date": "2014-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5418",
        "title": "Hierarchical Saliency Detection on Extended CSSD",
        "authors": [
            "Jianping Shi",
            "Qiong Yan",
            "Li Xu",
            "Jiaya Jia"
        ],
        "abstract": "Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure region-based scales. The final saliency values are inferred optimally combining all the saliency cues in different scales using hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex but general natural images.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5516",
        "title": "Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation",
        "authors": [
            "Sanja Fidler",
            "Marko Boben",
            "Ales Leonardis"
        ],
        "abstract": "Hierarchies allow feature sharing between objects at multiple levels of representation, can code exponential variability in a very compact way and enable fast inference. This makes them potentially suitable for learning and recognizing a higher number of object classes. However, the success of the hierarchical approaches so far has been hindered by the use of hand-crafted features or predetermined grouping rules. This paper presents a novel framework for learning a hierarchical compositional shape vocabulary for representing multiple object classes. The approach takes simple contour fragments and learns their frequent spatial configurations. These are recursively combined into increasingly more complex and class-specific shape compositions, each exerting a high degree of shape variability. At the top-level of the vocabulary, the compositions are sufficiently large and complex to represent the whole shapes of the objects. We learn the vocabulary layer after layer, by gradually increasing the size of the window of analysis and reducing the spatial resolution at which the shape configurations are learned. The lower layers are learned jointly on images of all classes, whereas the higher layers of the vocabulary are learned incrementally, by presenting the algorithm with one object class after another. The experimental results show that the learned multi-class object representation scales favorably with the number of object classes and achieves a state-of-the-art detection performance at both, faster inference as well as shorter training times.\n    ",
        "submission_date": "2014-08-23T00:00:00",
        "last_modified_date": "2014-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5552",
        "title": "Fuzzy and entropy facial recognition",
        "authors": [
            "Jaejun Lee",
            "Taeseon Yun"
        ],
        "abstract": "This paper suggests an effective method for facial recognition using fuzzy theory and Shannon entropy. Combination of fuzzy theory and Shannon entropy eliminates the complication of other methods. Shannon entropy calculates the ratio of an element between faces, and fuzzy theory calculates the member ship of the entropy with 1. More details will be mentioned in Section 3. The learning performance is better than others as it is very simple, and only need two data per learning. By using factors that don't usually change during the life, the method will have a high accuracy.\n    ",
        "submission_date": "2014-08-24T00:00:00",
        "last_modified_date": "2014-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5601",
        "title": "Learn Convolutional Neural Network for Face Anti-Spoofing",
        "authors": [
            "Jianwei Yang",
            "Zhen Lei",
            "Stan Z. Li"
        ],
        "abstract": "Though having achieved some progresses, the hand-crafted texture features, e.g., LBP [23], LBP-TOP [11] are still unable to capture the most discriminative cues between genuine and fake faces. In this paper, instead of designing feature by ourselves, we rely on the deep convolutional neural network (CNN) to learn features of high discriminative ability in a supervised manner. Combined with some data pre-processing, the face anti-spoofing performance improves drastically. In the experiments, over 70% relative decrease of Half Total Error Rate (HTER) is achieved on two challenging datasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art. Meanwhile, the experimental results from inter-tests between two datasets indicates CNN can obtain features with better generalization ability. Moreover, the nets trained using combined data from two datasets have less biases between two datasets.\n    ",
        "submission_date": "2014-08-24T00:00:00",
        "last_modified_date": "2014-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5667",
        "title": "Dependent Nonparametric Bayesian Group Dictionary Learning for online reconstruction of Dynamic MR images",
        "authors": [
            "Dornoosh Zonoobi",
            "Shahrooz Faghih Roohi",
            "Ashraf A. Kassim"
        ],
        "abstract": "In this paper, we introduce a dictionary learning based approach applied to the problem of real-time reconstruction of MR image sequences that are highly undersampled in k-space. Unlike traditional dictionary learning, our method integrates both global and patch-wise (local) sparsity information and incorporates some priori information into the reconstruction process. Moreover, we use a Dependent Hierarchical Beta-process as the prior for the group-based dictionary learning, which adaptively infers the dictionary size and the sparsity of each patch; and also ensures that similar patches are manifested in terms of similar dictionary atoms. An efficient numerical algorithm based on the alternating direction method of multipliers (ADMM) is also presented. Through extensive experimental results we show that our proposed method achieves superior reconstruction quality, compared to the other state-of-the- art DL-based methods.\n    ",
        "submission_date": "2014-08-25T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6257",
        "title": "Sparse Graph-based Transduction for Image Classification",
        "authors": [
            "Sheng Huang",
            "Dan Yang",
            "Jia Zhou",
            "Luwen Huangfu",
            "Xiaohong Zhang"
        ],
        "abstract": "Motivated by the remarkable successes of Graph-based Transduction (GT) and Sparse Representation (SR), we present a novel Classifier named Sparse Graph-based Classifier (SGC) for image classification. In SGC, SR is leveraged to measure the correlation (similarity) of each two samples and a graph is constructed for encoding these correlations. Then the Laplacian eigenmapping is adopted for deriving the graph Laplacian of the graph. Finally, SGC can be obtained by plugging the graph Laplacian into the conventional GT framework. In the image classification procedure, SGC utilizes the correlations, which are encoded in the learned graph Laplacian, to infer the labels of unlabeled images. SGC inherits the merits of both GT and SR. Compared to SR, SGC improves the robustness and the discriminating power of GT. Compared to GT, SGC sufficiently exploits the whole data. Therefore it alleviates the undercomplete dictionary issue suffered by SR. Four popular image databases are employed for evaluation. The results demonstrate that SGC can achieve a promising performance in comparison with the state-of-the-art classifiers, particularly in the small training sample size case and the noisy sample case.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6335",
        "title": "Compression, Restoration, Re-sampling, Compressive Sensing: Fast Transforms in Digital Imaging",
        "authors": [
            "Leonid Yaroslavsky"
        ],
        "abstract": "Transform image processing methods are methods that work in domains of image transforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. They are the basic tool in image compression, in image restoration, in image re-sampling and geometrical transformations and can be traced back to early 1970-ths. The paper presents a review of these methods with emphasis on their comparison and relationships, from the very first steps of transform image compression methods to adaptive and local adaptive transform domain filters for image restoration, to methods of precise image re-sampling and image reconstruction from sparse samples and up to \"compressive sensing\" approach that has gained popularity in last few years. The review has a tutorial character and purpose.\n    ",
        "submission_date": "2014-08-27T00:00:00",
        "last_modified_date": "2014-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6418",
        "title": "Video In Sentences Out",
        "authors": [
            "Andrei Barbu",
            "Alexander Bridge",
            "Zachary Burchill",
            "Dan Coroian",
            "Sven Dickinson",
            "Sanja Fidler",
            "Aaron Michaux",
            "Sam Mussman",
            "Siddharth Narayanaswamy",
            "Dhaval Salvi",
            "Lara Schmidt",
            "Jiangnan Shangguan",
            "Jeffrey Mark Siskind",
            "Jarrell Waggoner",
            "Song Wang",
            "Jinlian Wei",
            "Yifan Yin",
            "Zhiqi Zhang"
        ],
        "abstract": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto-role assignments, and changing body posture.\n    ",
        "submission_date": "2014-08-09T00:00:00",
        "last_modified_date": "2014-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6615",
        "title": "Multispectral Palmprint Recognition Using Textural Features",
        "authors": [
            "Shervin Minaee",
            "AmirAli Abdolrashidi"
        ],
        "abstract": "In order to utilize identification to the best extent, we need robust and fast algorithms and systems to process the data. Having palmprint as a reliable and unique characteristic of every person, we extract and use its features based on its geometry, lines and angles. There are countless ways to define measures for the recognition task. To analyze a new point of view, we extracted textural features and used them for palmprint recognition. Co-occurrence matrix can be used for textural feature extraction. As classifiers, we have used the minimum distance classifier (MDC) and the weighted majority voting system (WMV). The proposed method is tested on a well-known multispectral palmprint dataset of 6000 samples and an accuracy rate of 99.96-100% is obtained for most scenarios which outperforms all previous works in multispectral palmprint recognition.\n    ",
        "submission_date": "2014-08-28T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6911",
        "title": "Text Line Identification in Tagore's Manuscript",
        "authors": [
            "Chandranath Adak",
            "Bidyut B. Chaudhuri"
        ],
        "abstract": "In this paper, a text line identification method is proposed. The text lines of printed document are easy to segment due to uniform straightness of the lines and sufficient gap between the lines. But in handwritten documents, the line is non-uniform and interline gaps are variable. We take Rabindranath Tagore's manuscript as it is one of the most difficult manuscripts that contain doodles. Our method consists of a pre-processing stage to clean the document image. Then we separate doodles from the manuscript to get the textual region. After that we identify the text lines on the manuscript. For text line identification, we use window examination, black run-length smearing, horizontal histogram and connected component analysis.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2016-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6915",
        "title": "Binary matrices of optimal autocorrelations as alignment marks",
        "authors": [
            "Scott A. Skirlo",
            "Ling Lu",
            "Marin Solja\u010di\u0107"
        ],
        "abstract": "We define a new class of binary matrices by maximizing the peak-sidelobe distances in the aperiodic autocorrelations. These matrices can be used as robust position marks for in-plane spatial alignment. The optimal square matrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices of 8 by 8 and 9 by 9 were found by exhaustive searches.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6963",
        "title": "Comment on \"Ensemble Projection for Semi-supervised Image Classification\"",
        "authors": [
            "Xavier Boix",
            "Gemma Roig",
            "Luc Van Gool"
        ],
        "abstract": "In a series of papers by Dai and colleagues [1,2], a feature map (or kernel) was introduced for semi- and unsupervised learning. This feature map is build from the output of an ensemble of classifiers trained without using the ground-truth class labels. In this critique, we analyze the latest version of this series of papers, which is called Ensemble Projections [2]. We show that the results reported in [2] were not well conducted, and that Ensemble Projections performs poorly for semi-supervised learning.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.7071",
        "title": "Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for Action Recognition",
        "authors": [
            "Zhenzhong Lan",
            "Xuanchong Li",
            "Alexandar G. Hauptmann"
        ],
        "abstract": "Historically, researchers in the field have spent a great deal of effort to create image representations that have scale invariance and retain spatial location information. This paper proposes to encode equivalent temporal characteristics in video representations for action recognition. To achieve temporal scale invariance, we develop a method called temporal scale pyramid (TSP). To encode temporal information, we present and compare two methods called temporal extension descriptor (TED) and temporal division pyramid (TDP) . Our purpose is to suggest solutions for matching complex actions that have large variation in velocity and appearance, which is missing from most current action representations. The experimental results on four benchmark datasets, UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and significantly outperform state-of-the-art methods. Most noticeably, we achieve 65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51 and Hollywood2 datasets which constitutes an absolute improvement over the state-of-the-art by 7.8% and 3.9%, respectively.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0083",
        "title": "Sparse Coding on Symmetric Positive Definite Manifolds using Bregman Divergences",
        "authors": [
            "Mehrtash Harandi",
            "Richard Hartley",
            "Brian Lovell",
            "Conrad Sanderson"
        ],
        "abstract": "This paper introduces sparse coding and dictionary learning for Symmetric Positive Definite (SPD) matrices, which are often used in machine learning, computer vision and related areas. Unlike traditional sparse coding schemes that work in vector spaces, in this paper we discuss how SPD matrices can be described by sparse combination of dictionary atoms, where the atoms are also SPD matrices. We propose to seek sparse coding by embedding the space of SPD matrices into Hilbert spaces through two types of Bregman matrix divergences. This not only leads to an efficient way of performing sparse coding, but also an online and iterative scheme for dictionary learning. We apply the proposed methods to several computer vision tasks where images are represented by region covariance matrices. Our proposed algorithms outperform state-of-the-art methods on a wide range of classification tasks, including face recognition, action recognition, material classification and texture categorization.\n    ",
        "submission_date": "2014-08-30T00:00:00",
        "last_modified_date": "2014-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0084",
        "title": "Kernel Coding: General Formulation and Special Cases",
        "authors": [
            "Mehrtash Harandi",
            "Mathieu Salzmann"
        ],
        "abstract": "Representing images by compact codes has proven beneficial for many visual recognition tasks. Most existing techniques, however, perform this coding step directly in image feature space, where the distributions of the different classes are typically entangled. In contrast, here, we study the problem of performing coding in a high-dimensional Hilbert space, where the classes are expected to be more easily separable. To this end, we introduce a general coding formulation that englobes the most popular techniques, such as bag of words, sparse coding and locality-based coding, and show how this formulation and its special cases can be kernelized. Importantly, we address several aspects of learning in our general formulation, such as kernel learning, dictionary learning and supervised kernel coding. Our experimental evaluation on several visual recognition tasks demonstrates the benefits of performing coding in Hilbert space, and in particular of jointly learning the kernel, the dictionary and the classifier.\n    ",
        "submission_date": "2014-08-30T00:00:00",
        "last_modified_date": "2014-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0347",
        "title": "Multi-tensor Completion for Estimating Missing Values in Video Data",
        "authors": [
            "Chao Li",
            "Lili Guo",
            "Andrzej Cichocki"
        ],
        "abstract": "Many tensor-based data completion methods aim to solve image and video in-painting problems. But, all methods were only developed for a single dataset. In most of real applications, we can usually obtain more than one dataset to reflect one phenomenon, and all the datasets are mutually related in some sense. Thus one question raised whether such the relationship can improve the performance of data completion or not? In the paper, we proposed a novel and efficient method by exploiting the relationship among datasets for multi-video data completion. Numerical results show that the proposed method significantly improve the performance of video in-painting, particularly in the case of very high missing percentage.\n    ",
        "submission_date": "2014-09-01T00:00:00",
        "last_modified_date": "2014-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0575",
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "authors": [
            "Olga Russakovsky",
            "Jia Deng",
            "Hao Su",
            "Jonathan Krause",
            "Sanjeev Satheesh",
            "Sean Ma",
            "Zhiheng Huang",
            "Andrej Karpathy",
            "Aditya Khosla",
            "Michael Bernstein",
            "Alexander C. Berg",
            "Li Fei-Fei"
        ],
        "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\n",
        "submission_date": "2014-09-01T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0602",
        "title": "Transferring Landmark Annotations for Cross-Dataset Face Alignment",
        "authors": [
            "Shizhan Zhu",
            "Cheng Li",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Dataset bias is a well known problem in object recognition domain. This issue, nonetheless, is rarely explored in face alignment research. In this study, we show that dataset plays an integral part of face alignment performance. Specifically, owing to face alignment dataset bias, training on one database and testing on another or unseen domain would lead to poor performance. Creating an unbiased dataset through combining various existing databases, however, is non-trivial as one has to exhaustively re-label the landmarks for standardisation. In this work, we propose a simple and yet effective method to bridge the disparate annotation spaces between databases, making datasets fusion possible. We show extensive results on combining various popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and unseen data alignment.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0685",
        "title": "Effective Spectral Unmixing via Robust Representation and Learning-based Sparsity",
        "authors": [
            "Feiyun Zhu",
            "Ying Wang",
            "Bin Fan",
            "Gaofeng Meng",
            "Chunhong Pan"
        ],
        "abstract": "Hyperspectral unmixing (HU) plays a fundamental role in a wide range of hyperspectral applications. It is still challenging due to the common presence of outlier channels and the large solution space. To address the above two issues, we propose a novel model by emphasizing both robust representation and learning-based sparsity. Specifically, we apply the $\\ell_{2,1}$-norm to measure the representation error, preventing outlier channels from dominating our objective. In this way, the side effects of outlier channels are greatly relieved. Besides, we observe that the mixed level of each pixel varies over image grids. Based on this observation, we exploit a learning-based sparsity method to simultaneously learn the HU results and a sparse guidance map. Via this guidance map, the sparsity constraint in the $\\ell_{p}\\!\\left(\\!0\\!<\\! p\\!\\leq\\!1\\right)$-norm is adaptively imposed according to the learnt mixed level of each pixel. Compared with state-of-the-art methods, our model is better suited to the real situation, thus expected to achieve better HU results. The resulted objective is highly non-convex and non-smooth, and so it is hard to optimize. As a profound theoretical contribution, we propose an efficient algorithm to solve it. Meanwhile, the convergence proof and the computational complexity analysis are systematically provided. Extensive evaluations verify that our method is highly promising for the HU task---it achieves very accurate guidance maps and much better HU results compared with state-of-the-art methods.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2017-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0814",
        "title": "CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein Tertiary Structure Retrieval",
        "authors": [
            "Rezaul Karim",
            "Mohd. Momin Al Aziz",
            "Swakkhar Shatabda",
            "M. Sohel Rahman",
            "Md. Abul Kashem Mia",
            "Farhana Zaman",
            "Salman Rakin"
        ],
        "abstract": "Due to the advancements in technology number of entries in the structural database of proteins are increasing day by day. Methods for retrieving protein tertiary structures from this large database is the key to comparative analysis of structures which plays an important role to understand proteins and their function. In this paper, we present fast and accurate methods for the retrieval of proteins from a large database with tertiary structures similar to a query protein. Our proposed methods borrow ideas from the field of computer vision. The speed and accuracy of our methods comes from the two newly introduced features, the co-occurrence matrix of the oriented gradient and pyramid histogram of oriented gradient and from the use of Euclidean distance as the distance measure. Experimental results clearly indicate the superiority of our approach in both running time and accuracy. Our method is readily available for use from this website: ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0908",
        "title": "Action Recognition in the Frequency Domain",
        "authors": [
            "Anh Tran",
            "Jinyan Guan",
            "Thanima Pilantanakitti",
            "Paul Cohen"
        ],
        "abstract": "In this paper, we describe a simple strategy for mitigating variability in temporal data series by shifting focus onto long-term, frequency domain features that are less susceptible to variability. We apply this method to the human action recognition task and demonstrate how working in the frequency domain can yield good recognition features for commonly used optical flow and articulated pose features, which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion and other sources of variability. We show how these frequency-based features can be used in combination with a simple forest classifier to achieve good and robust results on the popular KTH Actions dataset.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0924",
        "title": "Visual Passwords Using Automatic Lip Reading",
        "authors": [
            "Ahmad Basheer Hassanat"
        ],
        "abstract": "This paper presents a visual passwords system to increase security. The system depends mainly on recognizing the speaker using the visual speech signal alone. The proposed scheme works in two stages: setting the visual password stage and the verification stage. At the setting stage the visual passwords system request the user to utter a selected password, a video recording of the user face is captured, and processed by a special words-based VSR system which extracts a sequence of feature vectors. In the verification stage, the same procedure is executed, the features will be sent to be compared with the stored visual password. The proposed scheme has been evaluated using a video database of 20 different speakers (10 females and 10 males), and 15 more males in another video database with different experiment sets. The evaluation has proved the system feasibility, with average error rate in the range of 7.63% to 20.51% at the worst tested scenario, and therefore, has potential to be a practical approach with the support of other conventional authentication methods such as the use of usernames and passwords.\n    ",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0925",
        "title": "Bypassing Captcha By Machine A Proof For Passing The Turing Test",
        "authors": [
            "Ahmad B. A. Hassanat"
        ],
        "abstract": "For the last ten years, CAPTCHAs have been widely used by websites to prevent their data being automatically updated by machines. By supposedly allowing only humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT), knowing that humans are more intelligent than machines. Generally, CAPTCHAs have defeated machines, but things are changing rapidly as technology improves. Hence, advanced research into optical character recognition (OCR) is overtaking attempts to strengthen CAPTCHAs against machine-based attacks. This paper investigates the immunity of CAPTCHA, which was built on the failure of the TT. We show that some CAPTCHAs are easily broken using a simple OCR machine built for the purpose of this study. By reviewing other techniques, we show that even more difficult CAPTCHAs can be broken using advanced OCR machines. Current advances in OCR should enable machines to pass the TT in the image recognition domain, which is exactly where machines are seeking to overcome CAPTCHAs. We enhance traditional CAPTCHAs by employing not only characters, but also natural language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs might be able to hold out against machines, at least until the advent of a machine that passes the TT completely.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0964",
        "title": "Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive Features",
        "authors": [
            "Liansheng Zhuang",
            "Shenghua Gao",
            "Jinhui Tang",
            "Jingjing Wang",
            "Zhouchen Lin",
            "Yi Ma"
        ],
        "abstract": "This paper aims at constructing a good graph for discovering intrinsic data structures in a semi-supervised learning setting. Firstly, we propose to build a non-negative low-rank and sparse (referred to as NNLRS) graph for the given data representation. Specifically, the weights of edges in the graph are obtained by seeking a nonnegative low-rank and sparse matrix that represents each data sample as a linear combination of others. The so-obtained NNLRS-graph can capture both the global mixture of subspaces structure (by the low rankness) and the locally linear structure (by the sparseness) of the data, hence is both generative and discriminative. Secondly, as good features are extremely important for constructing a good graph, we propose to learn the data embedding matrix and construct the graph jointly within one framework, which is termed as NNLRS with embedded features (referred to as NNLRS-EF). Extensive experiments on three publicly available datasets demonstrate that the proposed method outperforms the state-of-the-art graph construction method by a large margin for both semi-supervised classification and discriminative analysis, which verifies the effectiveness of our proposed method.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1411",
        "title": "Visual Speech Recognition",
        "authors": [
            "Ahmad B. A. Hassanat"
        ],
        "abstract": "Lip reading is used to understand or interpret speech without hearing it, a technique especially mastered by people with hearing difficulties. The ability to lip read enables a person with a hearing impairment to communicate with others and to engage in social activities, which otherwise would be difficult. Recent advances in the fields of computer vision, pattern recognition, and signal processing has led to a growing interest in automating this challenging task of lip reading. Indeed, automating the human ability to lip read, a process referred to as visual speech recognition (VSR) (or sometimes speech reading), could open the door for other novel related applications. VSR has received a great deal of attention in the last decade for its potential use in applications such as human-computer interaction (HCI), audio-visual speech recognition (AVSR), speaker recognition, talking heads, sign language recognition and video surveillance. Its main aim is to recognise spoken word(s) by using only the visual signal that is produced during speech. Hence, VSR deals with the visual domain of speech and involves image processing, artificial intelligence, object detection, pattern recognition, statistical modelling, etc.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1484",
        "title": "The Evolution of First Person Vision Methods: A Survey",
        "authors": [
            "Alejandro Betancourt",
            "Pietro Morerio",
            "Carlo S. Regazzoni",
            "Matthias Rauterberg"
        ],
        "abstract": "The emergence of new wearable technologies such as action cameras and smart-glasses has increased the interest of computer vision scientists in the First Person perspective. Nowadays, this field is attracting attention and investments of companies aiming to develop commercial devices with First Person Vision recording capabilities. Due to this interest, an increasing demand of methods to process these videos, possibly in real-time, is expected. Current approaches present a particular combinations of different image features and quantitative methods to accomplish specific objectives like object detection, activity recognition, user machine interaction and so on. This paper summarizes the evolution of the state of the art in First Person Vision video analysis between 1997 and 2014, highlighting, among others, most commonly used features, methods, challenges and opportunities within the field.\n    ",
        "submission_date": "2014-09-04T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1556",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "authors": [
            "Karen Simonyan",
            "Andrew Zisserman"
        ],
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n    ",
        "submission_date": "2014-09-04T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1789",
        "title": "Identifying Synapses Using Deep and Wide Multiscale Recursive Networks",
        "authors": [
            "Gary B. Huang",
            "Stephen Plaza"
        ],
        "abstract": "In this work, we propose a learning framework for identifying synapses using a deep and wide multi-scale recursive (DAWMR) network, previously considered in image segmentation applications. We apply this approach on electron microscopy data from invertebrate fly brain tissue. By learning features directly from the data, we are able to achieve considerable improvements over existing techniques that rely on a small set of hand-designed features. We show that this system can reduce the amount of manual annotation required, in both acquisition of training data as well as verification of inferred detections.\n    ",
        "submission_date": "2014-09-05T00:00:00",
        "last_modified_date": "2014-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2050",
        "title": "Depth image hand tracking from an overhead perspective using partially labeled, unbalanced data: Development and real-world testing",
        "authors": [
            "Stephen Czarnuch",
            "Alex Mihailidis"
        ],
        "abstract": "We present the development and evaluation of a hand tracking algorithm based on single depth images captured from an overhead perspective for use in the COACH prompting system. We train a random decision forest body part classifier using approximately 5,000 manually labeled, unbalanced, partially labeled training images. The classifier represents a random subset of pixels in each depth image with a learned probability density function across all trained body parts. A local mode-find approach is used to search for clusters present in the underlying feature space sampled by the classified pixels. In each frame, body part positions are chosen as the mode with the highest confidence. User hand positions are translated into hand washing task actions based on proximity to environmental objects. We validate the performance of the classifier and task action proposals on a large set of approximately 24,000 manually labeled images.\n    ",
        "submission_date": "2014-09-06T00:00:00",
        "last_modified_date": "2014-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2104",
        "title": "A Computational Model of the Short-Cut Rule for 2D Shape Decomposition",
        "authors": [
            "Lei Luo",
            "Chunhua Shen",
            "Xinwang Liu",
            "Chunyuan Zhang"
        ],
        "abstract": "We propose a new 2D shape decomposition method based on the short-cut rule. The short-cut rule originates from cognition research, and states that the human visual system prefers to partition an object into parts using the shortest possible cuts. We propose and implement a computational model for the short-cut rule and apply it to the problem of shape decomposition. The model we proposed generates a set of cut hypotheses passing through the points on the silhouette which represent the negative minima of curvature. We then show that most part-cut hypotheses can be eliminated by analysis of local properties of each. Finally, the remaining hypotheses are evaluated in ascending length order, which guarantees that of any pair of conflicting cuts only the shortest will be accepted. We demonstrate that, compared with state-of-the-art shape decomposition methods, the proposed approach achieves decomposition results which better correspond to human intuition as revealed in psychological experiments.\n    ",
        "submission_date": "2014-09-07T00:00:00",
        "last_modified_date": "2014-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2232",
        "title": "When coding meets ranking: A joint framework based on local learning",
        "authors": [
            "Jim Jing-Yan Wang",
            "Xuefeng Cui",
            "Ge Yu",
            "Lili Guo",
            "Xin Gao"
        ],
        "abstract": "Sparse coding, which represents a data point as a sparse reconstruction code with regard to a dictionary, has been a popular data representation method. Meanwhile, in database retrieval problems, learning the ranking scores from data points plays an important role. Up to now, these two problems have always been considered separately, assuming that data coding and ranking are two independent and irrelevant problems. However, is there any internal relationship between sparse coding and ranking score learning? If yes, how to explore and make use of this internal relationship? In this paper, we try to answer these questions by developing the first joint sparse coding and ranking score learning algorithm. To explore the local distribution in the sparse code space, and also to bridge coding and ranking problems, we assume that in the neighborhood of each data point, the ranking scores can be approximated from the corresponding sparse codes by a local linear function. By considering the local approximation error of ranking scores, the reconstruction error and sparsity of sparse coding, and the query information provided by the user, we construct a unified objective function for learning of sparse codes, the dictionary and ranking scores. We further develop an iterative algorithm to solve this optimization problem.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2016-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2413",
        "title": "Image processing",
        "authors": [
            "Franco Rino"
        ],
        "abstract": "Gabor filters can extract multi-orientation and multiscale features from face images. Researchers have designed different ways to use the magnitude of the filtered results for face recognition: Gabor Fisher classifier exploited only the magnitude information of Gabor magnitude pictures (GMPs); Local Gabor Binary Pattern uses only the gradient information. In this paper, we regard GMPs as smooth surfaces. By completely describing the shape of GMPs, we get a face representation method called Gabor Surface Feature (GSF). First, we compute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and transform them into decimal values. Finally we construct joint histograms and use subspace methods for classification. Experiments on FERET, ORL and FRGC 1.0.4 database show the effectiveness of GSF.\n    ",
        "submission_date": "2014-08-25T00:00:00",
        "last_modified_date": "2014-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2465",
        "title": "Comparing Feature Detectors: A bias in the repeatability criteria, and how to correct it",
        "authors": [
            "Ives Rey-Otero",
            "Mauricio Delbracio",
            "Jean-Michel Morel"
        ],
        "abstract": "Most computer vision application rely on algorithms finding local correspondences between different images. These algorithms detect and compare stable local invariant descriptors centered at scale-invariant keypoints. Because of the importance of the problem, new keypoint detectors and descriptors are constantly being proposed, each one claiming to perform better (or to be complementary) to the preceding ones. This raises the question of a fair comparison between very diverse methods. This evaluation has been mainly based on a repeatability criterion of the keypoints under a series of image perturbations (blur, illumination, noise, rotations, homotheties, homographies, etc). In this paper, we argue that the classic repeatability criterion is biased towards algorithms producing redundant overlapped detections. To compensate this bias, we propose a variant of the repeatability rate taking into account the descriptors overlap. We apply this variant to revisit the popular benchmark by Mikolajczyk et al., on classic and new feature detectors. Experimental evidence shows that the hierarchy of these feature detectors is severely disrupted by the amended comparator.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2702",
        "title": "F-formation Detection: Individuating Free-standing Conversational Groups in Images",
        "authors": [
            "Francesco Setti",
            "Chris Russell",
            "Chiara Bassetti",
            "Marco Cristani"
        ],
        "abstract": "Detection of groups of interacting people is a very interesting and useful task in many modern technologies, with application fields spanning from video-surveillance to social robotics. In this paper we first furnish a rigorous definition of group considering the background of the social sciences: this allows us to specify many kinds of group, so far neglected in the Computer Vision literature. On top of this taxonomy, we present a detailed state of the art on the group detection algorithms. Then, as a main contribution, we present a brand new method for the automatic detection of groups in still images, which is based on a graph-cuts framework for clustering individuals; in particular we are able to codify in a computational sense the sociological definition of F-formation, that is very useful to encode a group having only proxemic information: position and orientation of people. We call the proposed method Graph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all the state of the art methods in terms of different accuracy measures (some of them are brand new), demonstrating also a strong robustness to noise and versatility in recognizing groups of various cardinality.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2800",
        "title": "Enforcing Label and Intensity Consistency for IR Target Detection",
        "authors": [
            "Toufiq Parag"
        ],
        "abstract": "This study formulates the IR target detection as a binary classification problem of each pixel. Each pixel is associated with a label which indicates whether it is a target or background pixel. The optimal label set for all the pixels of an image maximizes aposteriori distribution of label configuration given the pixel intensities. The posterior probability is factored into (or proportional to) a conditional likelihood of the intensity values and a prior probability of label configuration. Each of these two probabilities are computed assuming a Markov Random Field (MRF) on both pixel intensities and their labels. In particular, this study enforces neighborhood dependency on both intensity values, by a Simultaneous Auto Regressive (SAR) model, and on labels, by an Auto-Logistic model. The parameters of these MRF models are learned from labeled examples. During testing, an MRF inference technique, namely Iterated Conditional Mode (ICM), produces the optimal label for each pixel. The detection performance is further improved by incorporating temporal information through background subtraction. High performances on benchmark datasets demonstrate effectiveness of this method for IR target detection.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2918",
        "title": "Quantum Edge Detection for Image Segmentation in Optical Environments",
        "authors": [
            "Mario Mastriani"
        ],
        "abstract": "A quantum edge detector for image segmentation in optical environments is presented in this work. A Boolean version of the same detector is presented too. The quantum version of the new edge detector works with computational basis states, exclusively. This way, we can easily avoid the problem of quantum measurement retrieving the result of applying the new detector on the image. Besides, a new criterion and logic based on projections onto vertical axis of Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a simpler development of logic quantum operations, where they will closer to those used in the classical logic operations, 2) building simple and robust classical-to-quantum and quantum-to-classical interfaces. Said so far is extended to quantum algorithms outside image processing too. In a special section on metric and simulations, a new metric based on the comparison between the classical and quantum versions algorithms for edge detection of images is presented. Notable differences between the results of classical and quantum versions of such algorithms (outside and inside of quantum computer, respectively) show the existence of implementation problems involved in the experiment, and that they have not been properly modeled for optical environments. However, although they are different, the quantum results are equally valid. The latter is clearly seen in the computer simulations\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3024",
        "title": "One-Dimensional Vector based Pattern Matching",
        "authors": [
            "Y. M. Fouda"
        ],
        "abstract": "Template matching is a basic method in image analysis to extract useful information from images. In this paper, we suggest a new method for pattern matching. Our method transform the template image from two dimensional image into one dimensional vector. Also all sub-windows (same size of template) in the reference image will transform into one dimensional vectors. The three similarity measures SAD, SSD, and Euclidean are used to compute the likeness between template and all sub-windows in the reference image to find the best match. The experimental results show the superior performance of the proposed method over the conventional methods on various template of different sizes.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3505",
        "title": "DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection",
        "authors": [
            "Wanli Ouyang",
            "Ping Luo",
            "Xingyu Zeng",
            "Shi Qiu",
            "Yonglong Tian",
            "Hongsheng Li",
            "Shuo Yang",
            "Zhe Wang",
            "Yuanjun Xiong",
            "Chen Qian",
            "Zhenyao Zhu",
            "Ruohui Wang",
            "Chen-Change Loy",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we propose multi-stage and deformable deep convolutional neural networks for object detection. This new deep learning object detection diagram has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. With the proposed multi-stage training strategy, multiple classifiers are jointly optimized to process samples at different difficulty levels. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of modeling averaging. The proposed approach ranked \\#2 in ILSVRC 2014. It improves the mean averaged precision obtained by RCNN, which is the state-of-the-art of object detection, from $31\\%$ to $45\\%$. Detailed component-wise analysis is also provided through extensive experimental evaluation.\n    ",
        "submission_date": "2014-09-11T00:00:00",
        "last_modified_date": "2014-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3879",
        "title": "Unsupervised learning of clutter-resistant visual representations from natural videos",
        "authors": [
            "Qianli Liao",
            "Joel Z. Leibo",
            "Tomaso Poggio"
        ],
        "abstract": "Populations of neurons in inferotemporal cortex (IT) maintain an explicit code for object identity that also tolerates transformations of object appearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learning rules are not known, recent results [4, 5, 6] suggest the operation of an unsupervised temporal-association-based method e.g., Foldiak's trace rule [7]. Such methods exploit the temporal continuity of the visual world by assuming that visual experience over short timescales will tend to have invariant identity content. Thus, by associating representations of frames from nearby times, a representation that tolerates whatever transformations occurred in the video may be achieved. Many previous studies verified that such rules can work in simple situations without background clutter, but the presence of visual clutter has remained problematic for this approach. Here we show that temporal association based on large class-specific filters (templates) avoids the problem of clutter. Our system learns in an unsupervised way from natural videos gathered from the internet, and is able to perform a difficult unconstrained face recognition task on natural images: Labeled Faces in the Wild [8].\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3906",
        "title": "Structure Preserving Large Imagery Reconstruction",
        "authors": [
            "Ju Shen",
            "Jianjun Yang",
            "Sami Taha-abusneineh",
            "Bryson Payne",
            "Markus Hitz"
        ],
        "abstract": "With the explosive growth of web-based cameras and mobile devices, billions of photographs are uploaded to the internet. We can trivially collect a huge number of photo streams for various goals, such as image clustering, 3D scene reconstruction, and other big data applications. However, such tasks are not easy due to the fact the retrieved photos can have large variations in their view perspectives, resolutions, lighting, noises, and distortions. Fur-thermore, with the occlusion of unexpected objects like people, vehicles, it is even more challenging to find feature correspondences and reconstruct re-alistic scenes. In this paper, we propose a structure-based image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture. We use an edge matching technique to infer the potential structure of the unknown region. Driven by the estimated structure, texture synthesis is performed automatically along the estimated curves. We evaluate the proposed method on different types of images: from highly structured indoor environment to natural scenes. Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing, such as image localization, object retrieval, and scene reconstruction. Our experiments show that this approach achieves favorable results that outperform existing state-of-the-art techniques.\n    ",
        "submission_date": "2014-09-13T00:00:00",
        "last_modified_date": "2014-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3913",
        "title": "Concurrent Tracking of Inliers and Outliers",
        "authors": [
            "Jae-Yeong Lee",
            "Wonpil Yu"
        ],
        "abstract": "In object tracking, outlier is one of primary factors which degrade performance of image-based tracking algorithms. In this respect, therefore, most of the existing methods simply discard detected outliers and pay little or no attention to employing them as an important source of information for motion estimation. We consider outliers as important as inliers for object tracking and propose a motion estimation algorithm based on concurrent tracking of inliers and outliers. Our tracker makes use of pyramidal implementation of the Lucas-Kanade tracker to estimate motion flows of inliers and outliers and final target motion is estimated robustly based on both of these information. Experimental results from challenging benchmark video sequences confirm enhanced tracking performance, showing highly stable target tracking under severe occlusion compared with state-of-the-art algorithms. The proposed algorithm runs at more than 100 frames per second even without using a hardware accelerator, which makes the proposed method more practical and portable.\n    ",
        "submission_date": "2014-09-13T00:00:00",
        "last_modified_date": "2014-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3964",
        "title": "Self-taught Object Localization with Deep Networks",
        "authors": [
            "Loris Bazzani",
            "Alessandro Bergamo",
            "Dragomir Anguelov",
            "Lorenzo Torresani"
        ],
        "abstract": "This paper introduces self-taught object localization, a novel approach that leverages deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision, i.e., without using any ground-truth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artificially masking out different regions of the image. The masking out of a region that includes the object typically causes a significant drop in recognition score. This idea is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. Our object localization scheme outperforms existing proposal methods in both precision and recall for small number of subwindow proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that the annotations automatically-generated by our method can be used to train object detectors yielding recognition results remarkably close to those obtained by training on manually-annotated bounding boxes.\n    ",
        "submission_date": "2014-09-13T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3970",
        "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data",
        "authors": [
            "Yin Zheng",
            "Yu-Jin Zhang",
            "Hugo Larochelle"
        ],
        "abstract": "Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.\n    ",
        "submission_date": "2014-09-13T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4014",
        "title": "Mining Mid-level Features for Action Recognition Based on Effective Skeleton Representation",
        "authors": [
            "Pichao Wang",
            "Wanqing Li",
            "Philip Ogunbona",
            "Zhimin Gao",
            "Hanling Zhang"
        ],
        "abstract": "Recently, mid-level features have shown promising performance in computer vision. Mid-level features learned by incorporating class-level information are potentially more discriminative than traditional low-level local features. In this paper, an effective method is proposed to extract mid-level features from Kinect skeletons for 3D human action recognition. Firstly, the orientations of limbs connected by two skeleton joints are computed and each orientation is encoded into one of the 27 states indicating the spatial relationship of the joints. Secondly, limbs are combined into parts and the limb's states are mapped into part states. Finally, frequent pattern mining is employed to mine the most frequent and relevant (discriminative, representative and non-redundant) states of parts in continuous several frames. These parts are referred to as Frequent Local Parts or FLPs. The FLPs allow us to build powerful bag-of-FLP-based action representation. This new representation yields state-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D.\n    ",
        "submission_date": "2014-09-14T00:00:00",
        "last_modified_date": "2014-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4046",
        "title": "A New Framework for Retinex based Color Image Enhancement using Particle Swarm Optimization",
        "authors": [
            "M. C Hanumantharaju",
            "M. Ravishankar",
            "D. R Rameshbabu",
            "V. N Manjunath Aradhya"
        ],
        "abstract": "A new approach for tuning the parameters of MultiScale Retinex (MSR) based color image enhancement algorithm using a popular optimization method, namely, Particle Swarm Optimization (PSO) is presented in this paper. The image enhancement using MSR scheme heavily depends on parameters such as Gaussian surround space constant, number of scales, gain and offset etc. Selection of these parameters, empirically and its application to MSR scheme to produce inevitable results are the major blemishes. The method presented here results in huge savings of computation time as well as improvement in the visual quality of an image, since the PSO exploited maximizes the MSR parameters. The objective of PSO is to validate the visual quality of the enhanced image iteratively using an effective objective criterion based on entropy and edge information of an image. The PSO method of parameter optimization of MSR scheme achieves a very good quality of reconstructed images, far better than that possible with the other existing methods. Finally, the quality of the enhanced color images obtained by the proposed method are evaluated using novel metric, namely, Wavelet Energy (WE). The experimental results presented show that color images enhanced using the proposed scheme are clearer, more vivid and efficient.\n    ",
        "submission_date": "2014-09-14T00:00:00",
        "last_modified_date": "2014-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4095",
        "title": "Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects",
        "authors": [
            "Jonathan Balzer",
            "Daniel Acevedo-Feliz",
            "Stefano Soatto",
            "Sebastian H\u00f6fer",
            "Markus Hadwiger",
            "J\u00fcrgen Beyerer"
        ],
        "abstract": "We introduce a method based on the deflectometry principle for the reconstruction of specular objects exhibiting significant size and geometric complexity. A key feature of our approach is the deployment of an Automatic Virtual Environment (CAVE) as pattern generator. To unfold the full power of this extraordinary experimental setup, an optical encoding scheme is developed which accounts for the distinctive topology of the CAVE. Furthermore, we devise an algorithm for detecting the object of interest in raw deflectometric images. The segmented foreground is used for single-view reconstruction, the background for estimation of the camera pose, necessary for calibrating the sensor system. Experiments suggest a significant gain of coverage in single measurements compared to previous methods. To facilitate research on specular surface reconstruction, we will make our data set publicly available.\n    ",
        "submission_date": "2014-09-14T00:00:00",
        "last_modified_date": "2014-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4127",
        "title": "Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network",
        "authors": [
            "Yu-Chuan Su",
            "Tzu-Hsuan Chiu",
            "Chun-Yen Yeh",
            "Hsin-Fu Huang",
            "Winston H. Hsu"
        ],
        "abstract": "Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4205",
        "title": "Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers",
        "authors": [
            "B. Conejo",
            "N. Komodakis",
            "S. Leprince",
            "J.P. Avouac"
        ],
        "abstract": "We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach relies on a multi-scale pruning scheme that is able to progressively reduce the solution space by use of a novel strategy based on a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4326",
        "title": "Computing the Stereo Matching Cost with a Convolutional Neural Network",
        "authors": [
            "Jure \u017dbontar",
            "Yann LeCun"
        ],
        "abstract": "We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4327",
        "title": "Zero Shot Recognition with Unreliable Attributes",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like \\emph{striped} and \\emph{four-legged}, one can construct a classifier for the zebra category by enumerating which properties it possesses---even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4349",
        "title": "On the optimality of shape and data representation in the spectral domain",
        "authors": [
            "Yonathan Aflalo",
            "Haim Brezis",
            "Ron Kimmel"
        ],
        "abstract": "A proof of the optimality of the eigenfunctions of the Laplace-Beltrami operator (LBO)   in representing smooth functions on surfaces is provided and adapted to the   field of applied shape and data analysis. It is based on the Courant-Fischer min-max principle adapted to our case. % The theorem we present supports the new trend in geometry processing of   treating geometric structures by using their projection onto the leading eigenfunctions   of the decomposition of the LBO. Utilisation of this result can be used for constructing numerically efficient algorithms   to process shapes in their spectrum. We review a couple of applications as possible practical usage cases   of the proposed optimality criteria. % We refer to a scale invariant metric, which is also invariant to bending of the manifold. This novel pseudo-metric allows constructing an LBO by which a scale   invariant eigenspace on the surface is defined. We demonstrate the efficiency of an intermediate metric, defined as an interpolation   between the scale invariant and the regular one, in representing geometric   structures while capturing both coarse and fine details. Next, we review a numerical acceleration technique for classical scaling, a member   of a family of flattening methods known as multidimensional scaling (MDS). There, the optimality is exploited   to efficiently approximate all geodesic distances between pairs of points on a given   surface, and thereby match and compare between almost isometric surfaces. Finally, we revisit the classical principal component analysis (PCA) definition   by coupling its variational form with a Dirichlet energy on the data manifold. By pairing the PCA with the LBO we can handle cases that go beyond the   scope defined by the observation set that is handled by regular PCA.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4481",
        "title": "Real-time Crowd Tracking using Parameter Optimized Mixture of Motion Models",
        "authors": [
            "Aniket Bera",
            "David Wolinski",
            "Julien Pettr\u00e9",
            "Dinesh Manocha"
        ],
        "abstract": "We present a novel, real-time algorithm to track the trajectory of each pedestrian in moderately dense crowded scenes. Our formulation is based on an adaptive particle-filtering scheme that uses a combination of various multi-agent heterogeneous pedestrian simulation models. We automatically compute the optimal parameters for each of these different models based on prior tracked data and use the best model as motion prior for our particle-filter based tracking algorithm. We also use our \"mixture of motion models\" for adaptive particle selection and accelerate the performance of the online tracking algorithm. The motion model parameter estimation is formulated as an optimization problem, and we use an approach that solves this combinatorial optimization problem in a model independent manner and hence scalable to any multi-agent pedestrian motion model. We evaluate the performance of our approach on different crowd video datasets and highlight the improvement in accuracy over homogeneous motion models and a baseline mean-shift based tracker. In practice, our formulation can compute trajectories of tens of pedestrians on a multi-core desktop CPU in in real time and offer higher accuracy as compared to prior real time pedestrian tracking algorithms.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4559",
        "title": "A Combined Method Of Fractal And GLCM Features For MRI And CT Scan Images Classification",
        "authors": [
            "Redouan Korchiyne",
            "Sidi Mohamed Farssi",
            "Abderrahmane Sbihi",
            "Rajaa Touahni",
            "Mustapha Tahiri Alaoui"
        ],
        "abstract": "Fractal analysis has been shown to be useful in image processing for characterizing shape and gray-scale complexity. The fractal feature is a compact descriptor used to give a numerical measure of the degree of irregularity of the medical images. This descriptor property does not give ownership of the local image structure. In this paper, we present a combination of this parameter based on Box Counting with GLCM Features. This powerful combination has proved good results especially in classification of medical texture from MRI and CT Scan images of trabecular bone. This method has the potential to improve clinical diagnostics tests for osteoporosis pathologies.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4689",
        "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering",
        "authors": [
            "Johannes Lederer",
            "Sergio Guadarrama"
        ],
        "abstract": "Sparse Filtering is a popular feature learning algorithm for image classification pipelines. In this paper, we connect the performance of Sparse Filtering with spectral properties of the corresponding feature matrices. This connection provides new insights into Sparse Filtering; in particular, it suggests early stopping of Sparse Filtering. We therefore introduce the Optimal Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We show that this stopping criterion is related with pre-processing procedures such as Statistical Whitening and demonstrate that it can make image classification with Sparse Filtering considerably faster and more accurate.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2015-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4842",
        "title": "Going Deeper with Convolutions",
        "authors": [
            "Christian Szegedy",
            "Wei Liu",
            "Yangqing Jia",
            "Pierre Sermanet",
            "Scott Reed",
            "Dragomir Anguelov",
            "Dumitru Erhan",
            "Vincent Vanhoucke",
            "Andrew Rabinovich"
        ],
        "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4995",
        "title": "Adaptive Tag Selection for Image Annotation",
        "authors": [
            "Xixi He",
            "Xirong Li",
            "Gang Yang",
            "Jieping Xu",
            "Qin Jin"
        ],
        "abstract": "Not all tags are relevant to an image, and the number of relevant tags is image-dependent. Although many methods have been proposed for image auto-annotation, the question of how to determine the number of tags to be selected per image remains open. The main challenge is that for a large tag vocabulary, there is often a lack of ground truth data for acquiring optimal cutoff thresholds per tag. In contrast to previous works that pre-specify the number of tags to be selected, we propose in this paper adaptive tag selection. The key insight is to divide the vocabulary into two disjoint subsets, namely a seen set consisting of tags having ground truth available for optimizing their thresholds and a novel set consisting of tags without any ground truth. Such a division allows us to estimate how many tags shall be selected from the novel set according to the tags that have been selected from the seen set. The effectiveness of the proposed method is justified by our participation in the ImageCLEF 2014 image annotation task. On a set of 2,065 test images with ground truth available for 207 tags, the benchmark evaluation shows that compared to the popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tag selection achieves a higher F-score of 0.223. Moreover, by treating the underlying image annotation system as a black box, the new method can be used as an easy plug-in to boost the performance of existing systems.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5114",
        "title": "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution",
        "authors": [
            "Shuxin Ouyang",
            "Timothy Hospedales",
            "Yi-Zhe Song",
            "Xueming Li"
        ],
        "abstract": "Heterogeneous face recognition (HFR) refers to matching face imagery across different domains. It has received much interest from the research community as a result of its profound implications in law enforcement. A wide variety of new invariant features, cross-modality matching models and heterogeneous datasets being established in recent years. This survey provides a comprehensive review of established techniques and recent developments in HFR. Moreover, we offer a detailed account of datasets and benchmarks commonly used for evaluation. We finish by assessing the state of the field and discussing promising directions for future research.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5188",
        "title": "Fingerprint Classification Based on Depth Neural Network",
        "authors": [
            "Ruxin Wang",
            "Congying Han",
            "Yanping Wu",
            "Tiande Guo"
        ],
        "abstract": "Fingerprint classification is an effective technique for reducing the candidate numbers of fingerprints in the stage of matching in automatic fingerprint identification system (AFIS). In recent years, deep learning is an emerging technology which has achieved great success in many fields, such as image processing, natural language processing and so on. In this paper, we only choose the orientation field as the input feature and adopt a new method (stacked sparse autoencoders) based on depth neural network for fingerprint classification. For the four-class problem, we achieve a classification of 93.1 percent using the depth network structure which has three hidden layers (with 1.8% rejection) in the NIST-DB4 database. And then we propose a novel method using two classification probabilities for fuzzy classification which can effectively enhance the accuracy of classification. By only adjusting the probability threshold, we get the accuracy of classification is 96.1% (setting threshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting threshold is 0.95). Using the fuzzy method, we obtain higher accuracy than other methods.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5209",
        "title": "Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning.\n",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5230",
        "title": "Deep Regression for Face Alignment",
        "authors": [
            "Baoguang Shi",
            "Xiang Bai",
            "Wenyu Liu",
            "Jingdong Wang"
        ],
        "abstract": "In this paper, we present a deep regression approach for face alignment. The deep architecture consists of a global layer and multi-stage local layers. We apply the back-propagation algorithm with the dropout strategy to jointly optimize the regression parameters. We show that the resulting deep regressor gradually and evenly approaches the true facial landmarks stage by stage, avoiding the tendency to yield over-strong early stage regressors while over-weak later stage regressors. Experimental results show that our approach achieves the state-of-the-art\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5241",
        "title": "Subspace Alignment For Domain Adaptation",
        "authors": [
            "Basura Fernando",
            "Amaury Habrard",
            "Marc Sebban",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces spanned by eigenvectors. Our method seeks a domain invariant feature space by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We present two approaches to determine the only hyper-parameter in our method corresponding to the size of the subspaces. In the first approach we tune the size of subspaces using a theoretical bound on the stability of the obtained result. In the second approach, we use maximum likelihood estimation to determine the subspace size, which is particularly useful for high dimensional data. Apart from PCA, we propose a subspace creation method that outperform partial least squares (PLS) and linear discriminant analysis (LDA) in domain adaptation. We test our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5400",
        "title": "Visual Landmark Recognition from Internet Photo Collections: A Large-Scale Evaluation",
        "authors": [
            "Tobias Weyand",
            "Bastian Leibe"
        ],
        "abstract": "The task of a visual landmark recognition system is to identify photographed buildings or objects in query photos and to provide the user with relevant information on them. With their increasing coverage of the world's landmark buildings and objects, Internet photo collections are now being used as a source for building such systems in a fully automatic fashion. This process typically consists of three steps: clustering large amounts of images by the objects they depict; determining object names from user-provided tags; and building a robust, compact, and efficient recognition index. To this date, however, there is little empirical information on how well current approaches for those steps perform in a large-scale open-set mining and recognition task. Furthermore, there is little empirical information on how recognition performance varies for different types of landmark objects and where there is still potential for improvement. With this paper, we intend to fill these gaps. Using a dataset of 500k images from Paris, we analyze each component of the landmark recognition pipeline in order to answer the following questions: How many and what kinds of objects can be discovered automatically? How can we best use the resulting image clusters to recognize the object in a query? How can the object be efficiently represented in memory for recognition? How reliably can semantic information be extracted? And finally: What are the limiting factors in the resulting pipeline from query to semantics? We evaluate how different choices of methods and parameters for the individual pipeline steps affect overall system performance and examine their effects for different query categories such as buildings, paintings or sculptures.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5403",
        "title": "Deformable Part Models are Convolutional Neural Networks",
        "authors": [
            "Ross Girshick",
            "Forrest Iandola",
            "Trevor Darrell",
            "Jitendra Malik"
        ],
        "abstract": "Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are \"black-box\" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) CNN layer. From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running an order of magnitude faster.\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5729",
        "title": "Hyperspectral and Multispectral Image Fusion based on a Sparse Representation",
        "authors": [
            "Qi Wei",
            "Jos\u00e9 Bioucas-Dias",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "abstract": "This paper presents a variational based approach to fusing hyperspectral and multispectral images. The fusion process is formulated as an inverse problem whose solution is the target image assumed to live in a much lower dimensional subspace. A sparse regularization term is carefully designed, relying on a decomposition of the scene on a set of dictionaries. The dictionary atoms and the corresponding supports of active coding coefficients are learned from the observed images. Then, conditionally on these dictionaries and supports, the fusion problem is solved via alternating optimization with respect to the target image (using the alternating direction method of multipliers) and the coding coefficients. Simulation results demonstrate the efficiency of the proposed algorithm when compared with the state-of-the-art fusion methods.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5763",
        "title": "Active Dictionary Learning in Sparse Representation Based Classification",
        "authors": [
            "Jin Xu",
            "Haibo He",
            "Hong Man"
        ],
        "abstract": "Sparse representation, which uses dictionary atoms to reconstruct input vectors, has been studied intensively in recent years. A proper dictionary is a key for the success of sparse representation. In this paper, an active dictionary learning (ADL) method is introduced, in which classification error and reconstruction error are considered as the active learning criteria in selection of the atoms for dictionary construction. The learned dictionaries are caculated in sparse representation based classification (SRC). The classification accuracy and reconstruction error are used to evaluate the proposed dictionary learning method. The performance of the proposed dictionary learning method is compared with other methods, including unsupervised dictionary learning and whole-training-data dictionary. The experimental results based on the UCI data sets and face data set demonstrate the efficiency of the proposed method.\n    ",
        "submission_date": "2014-09-19T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5786",
        "title": "Fast Low-rank Representation based Spatial Pyramid Matching for Image Classification",
        "authors": [
            "Xi Peng",
            "Rui Yan",
            "Bo Zhao",
            "Huajin Tang",
            "Zhang Yi"
        ],
        "abstract": "Spatial Pyramid Matching (SPM) and its variants have achieved a lot of success in image classification. The main difference among them is their encoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of Vector Quantization (VQ) into the framework of SPM. Although the methods achieve a higher recognition rate than the traditional SPM, they consume more time to encode the local descriptors extracted from the image. In this paper, we propose using Low Rank Representation (LRR) to encode the descriptors under the framework of SPM. Different from SC, LRR considers the group effect among data points instead of sparsity. Benefiting from this property, the proposed method (i.e., LrrSPM) can offer a better performance. To further improve the generalizability and robustness, we reformulate the rank-minimization problem as a truncated projection problem. Extensive experimental studies show that LrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving competitive recognition rates on nine image data sets.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5957",
        "title": "A Global Approach for Solving Edge-Matching Puzzles",
        "authors": [
            "Shahar Z. Kovalsky",
            "Daniel Glasner",
            "Ronen Basri"
        ],
        "abstract": "We consider apictorial edge-matching puzzles, in which the goal is to arrange a collection of puzzle pieces with colored edges so that the colors match along the edges of adjacent pieces. We devise an algebraic representation for this problem and provide conditions under which it exactly characterizes a puzzle. Using the new representation, we recast the combinatorial, discrete problem of solving puzzles as a global, polynomial system of equations with continuous variables. We further propose new algorithms for generating approximate solutions to the continuous problem by solving a sequence of convex relaxations.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6041",
        "title": "Domain Adaptive Neural Networks for Object Recognition",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang"
        ],
        "abstract": "We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2014-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6070",
        "title": "Spatially-sparse convolutional neural networks",
        "authors": [
            "Benjamin Graham"
        ],
        "abstract": "Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.\n",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6080",
        "title": "Temporally Coherent Bayesian Models for Entity Discovery in Videos by Tracklet Clustering",
        "authors": [
            "Adway Mitra",
            "Soma Biswas",
            "Chiranjib Bhattacharyya"
        ],
        "abstract": "A video can be represented as a sequence of tracklets, each spanning 10-20 frames, and associated with one entity (eg. a person). The task of \\emph{Entity Discovery} in videos can be naturally posed as tracklet clustering. We approach this task by leveraging \\emph{Temporal Coherence}(TC): the fundamental property of videos that each tracklet is likely to be associated with the same entity as its temporal neighbors. Our major contributions are the first Bayesian nonparametric models for TC at tracklet-level. We extend Chinese Restaurant Process (CRP) to propose TC-CRP, and further to Temporally Coherent Chinese Restaurant Franchise (TC-CRF) to jointly model short temporal segments. On the task of discovering persons in TV serial videos without meta-data like scripts, these methods show considerable improvement in cluster purity and person coverage compared to state-of-the-art approaches to tracklet clustering. We represent entities with mixture components, and tracklets with vectors of very generic features, which can work for any type of entity (not necessarily person). The proposed methods can perform online tracklet clustering on streaming videos with little performance deterioration unlike existing approaches, and can automatically reject tracklets resulting from false detections. Finally we discuss entity-driven video summarization- where some temporal segments of the video are selected automatically based on the discovered entities.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6155",
        "title": "1-HKUST: Object Detection in ILSVRC 2014",
        "authors": [
            "Cewu Lu",
            "Hao Chen",
            "Qifeng Chen",
            "Hei Law",
            "Yao Xiao",
            "Chi-Keung Tang"
        ],
        "abstract": "The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one of the most important big data challenges to date. We participated in the object detection track of ILSVRC 2014 and received the fourth place among the 38 teams. We introduce in our object detection system a number of novel techniques in localization and recognition. For localization, initial candidate proposals are generated using selective search, and a novel bounding boxes regression method is used for better object localization. For recognition, to represent a candidate proposal, we adopt three features, namely, RCNN feature, IFV feature, and DPM feature. Given these features, category-specific combination functions are learned to improve the object recognition rate. In addition, object context in the form of background priors and object interaction priors are learned and applied in our system. Our ILSVRC 2014 results are reported alongside with the results of other participating teams.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2014-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6235",
        "title": "Detecting People in Cubist Art",
        "authors": [
            "Shiry Ginosar",
            "Daniel Haas",
            "Timothy Brown",
            "Jitendra Malik"
        ],
        "abstract": "Although the human visual system is surprisingly robust to extreme distortion when recognizing objects, most evaluations of computer object detection methods focus only on robustness to natural form deformations such as people's pose changes. To determine whether algorithms truly mirror the flexibility of human vision, they must be compared against human vision at its limits. For example, in Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings. Our results demonstrate that while human perception significantly outperforms current methods, human perception and part-based models exhibit a similarly graceful degradation in object detection performance as the objects become increasingly abstract and fragmented, corroborating the theory of part-based object representation in the brain.\n    ",
        "submission_date": "2014-09-22T00:00:00",
        "last_modified_date": "2014-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6440",
        "title": "A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy",
        "authors": [
            "Rashid Khogali"
        ],
        "abstract": "A fast Non-linear and non-iterative learning and classification algorithm is synthesized and validated. This algorithm named the \"Reverse Ripple Effect(R.R.E)\", achieves 100% learning accuracy but is computationally expensive upon classification. The R.R.E is a (deterministic) algorithm that super imposes Gaussian weighted functions on training points. In this work, the R.R.E algorithm is compared against known learning and classification techniques/algorithms such as: the Perceptron Criterion algorithm, Linear Support Vector machines, the Linear Fisher Discriminant and a simple Neural Network. The classification accuracy of the R.R.E algorithm is evaluated using simulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed under linearly and non-linearly separable data sets. For the comparison with the Neural Network, the classical XOR problem is considered.\n    ",
        "submission_date": "2014-09-23T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6448",
        "title": "HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection",
        "authors": [
            "Bo Han",
            "Bo He",
            "Tingting Sun",
            "Mengmeng Ma",
            "Amaury Lendasse"
        ],
        "abstract": "In this paper, we propose a novel method for fast face recognition called L1/2 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing L1/2 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gabor-feature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.\n    ",
        "submission_date": "2014-09-23T00:00:00",
        "last_modified_date": "2014-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6498",
        "title": "Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT Images",
        "authors": [
            "Moo K. Chung",
            "Anqi Qiu",
            "Seongho Seo",
            "Houri K. Vorperian"
        ],
        "abstract": "We present a novel kernel regression framework for smoothing scalar surface data using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel constructed from the eigenfunctions, we formulate a new bivariate kernel regression framework as a weighted eigenfunction expansion with the heat kernel as the weights. The new kernel regression is mathematically equivalent to isotropic heat diffusion, kernel smoothing and recently popular diffusion wavelets. Unlike many previous partial differential equation based approaches involving diffusion, our approach represents the solution of diffusion analytically, reducing numerical inaccuracy and slow convergence. The numerical implementation is validated on a unit sphere using spherical harmonics. As an illustration, we have applied the method in characterizing the localized growth pattern of mandible surfaces obtained in CT images from subjects between ages 0 and 20 years by regressing the length of displacement vectors with respect to the template surface.\n    ",
        "submission_date": "2014-09-23T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6689",
        "title": "Visual Words for Automatic Lip-Reading",
        "authors": [
            "Ahmad Basheer Hassanat"
        ],
        "abstract": "Lip reading is used to understand or interpret speech without hearing it, a technique especially mastered by people with hearing difficulties. The ability to lip read enables a person with a hearing impairment to communicate with others and to engage in social activities, which otherwise would be difficult. Recent advances in the fields of computer vision, pattern recognition, and signal processing has led to a growing interest in automating this challenging task of lip reading. Indeed, automating the human ability to lip read, a process referred to as visual speech recognition, could open the door for other novel applications. This thesis investigates various issues faced by an automated lip-reading system and proposes a novel \"visual words\" based approach to automatic lip reading. The proposed approach includes a novel automatic face localisation scheme and a lip localisation method.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6745",
        "title": "A Concept Learning Approach to Multisensory Object Perception",
        "authors": [
            "Ifeoma Nwogu",
            "Goker Erdogan",
            "Ilker Yildirim",
            "Robert Jacobs"
        ],
        "abstract": "This paper presents a computational model of concept learning using Bayesian inference for a grammatically structured hypothesis space, and test the model on multisensory (visual and haptics) recognition of 3D objects. The study is performed on a set of artificially generated 3D objects known as fribbles, which are complex, multipart objects with categorical structures. The goal of this work is to develop a working multisensory representational model that integrates major themes on concepts and concepts learning from the cognitive science literature. The model combines the representational power of a probabilistic generative grammar with the inferential power of Bayesian induction.\n    ",
        "submission_date": "2014-09-23T00:00:00",
        "last_modified_date": "2014-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6813",
        "title": "Histogram of Oriented Principal Components for Cross-View Action Recognition",
        "authors": [
            "Hossein Rahmani",
            "Arif Mahmood",
            "Du Huynh",
            "Ajmal Mian"
        ],
        "abstract": "Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which are viewpoint dependent. In contrast, we directly process pointclouds for cross-view action recognition from unknown and unseen views. We propose the Histogram of Oriented Principal Components (HOPC) descriptor that is robust to noise, viewpoint, scale and action speed variations. At a 3D point, HOPC is computed by projecting the three scaled eigenvectors of the pointcloud within its local spatio-temporal support volume onto the vertices of a regular dodecahedron. HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D pointcloud sequences so that view-invariant STK descriptors (or Local HOPC descriptors) at these key locations only are used for action recognition. We also propose a global descriptor computed from the normalized spatio-temporal distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the performance of our proposed descriptors against nine existing techniques on two cross-view and three single-view human action recognition datasets. The Experimental results show that our techniques provide significant improvement over state-of-the-art methods.\n    ",
        "submission_date": "2014-09-24T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6838",
        "title": "Recent Progress in Image Deblurring",
        "authors": [
            "Ruxin Wang",
            "Dacheng Tao"
        ],
        "abstract": "This paper comprehensively reviews the recent development of image deblurring, including non-blind/blind, spatially invariant/variant deblurring techniques. Indeed, these techniques share the same objective of inferring a latent sharp image from one or several corresponding blurry images, while the blind deblurring techniques are also required to derive an accurate blur kernel. Considering the critical role of image restoration in modern imaging systems to provide high-quality images under complex environments such as motion, undesirable lighting conditions, and imperfect system components, image deblurring has attracted growing attention in recent years. From the viewpoint of how to handle the ill-posedness which is a crucial issue in deblurring tasks, existing methods can be grouped into five categories: Bayesian inference framework, variational methods, sparse representation-based methods, homography-based modeling, and region-based methods. In spite of achieving a certain level of development, image deblurring, especially the blind case, is limited in its success by complex application conditions which make the blur kernel hard to obtain and be spatially variant. We provide a holistic understanding and deep insight into image deblurring in this review. An analysis of the empirical evidence for representative methods, practical issues, as well as a discussion of promising future directions are also presented.\n    ",
        "submission_date": "2014-09-24T00:00:00",
        "last_modified_date": "2014-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6911",
        "title": "Do More Dropouts in Pool5 Feature Maps for Better Object Detection",
        "authors": [
            "Zhiqiang Shen",
            "Xiangyang Xue"
        ],
        "abstract": "Deep Convolutional Neural Networks (CNNs) have gained great success in image classification and object detection. In these fields, the outputs of all layers of CNNs are usually considered as a high dimensional feature vector extracted from an input image and the correspondence between finer level feature vectors and concepts that the input image contains is all-important. However, fewer studies focus on this deserving issue. On considering the correspondence, we propose a novel approach which generates an edited version for each original CNN feature vector by applying the maximum entropy principle to abandon particular vectors. These selected vectors correspond to the unfriendly concepts in each image category. The classifier trained from merged feature sets can significantly improve model generalization of individual categories when training data is limited. The experimental results for classification-based object detection on canonical datasets including VOC 2007 (60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average precision (mAP) with simple linear support vector machines.\n    ",
        "submission_date": "2014-09-24T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7164",
        "title": "Deep Learning Representation using Autoencoder for 3D Shape Retrieval",
        "authors": [
            "Zhuotun Zhu",
            "Xinggang Wang",
            "Song Bai",
            "Cong Yao",
            "Xiang Bai"
        ],
        "abstract": "We study the problem of how to build a deep learning representation for 3D shape. Deep learning has shown to be very effective in variety of visual applications, such as image classification and object detection. However, it has not been successfully applied to 3D shape recognition. This is because 3D shape has complex structure in 3D space and there are limited number of 3D shapes for feature learning. To address these problems, we project 3D shapes into 2D space and use autoencoder for feature learning on the 2D images. High accuracy 3D shape retrieval performance is obtained by aggregating the features learned on 2D images. In addition, we show the proposed deep learning feature is complementary to conventional local image descriptors. By combing the global deep learning representation and the local descriptor representation, our method can obtain the state-of-the-art performance on 3D shape retrieval benchmarks.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7307",
        "title": "Image Classification with A Deep Network Model based on Compressive Sensing",
        "authors": [
            "Yufei Gan",
            "Tong Zhuo",
            "Chu He"
        ],
        "abstract": "To simplify the parameter of the deep learning network, a cascaded compressive sensing model \"CSNet\" is implemented for image classification. Firstly, we use cascaded compressive sensing network to learn feature from the data. Secondly, CSNet generates the feature by binary hashing and block-wise histograms. Finally, a linear SVM classifier is used to classify these features. The experiments on the MNIST dataset indicate that higher classification accuracy can be obtained by this algorithm.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7313",
        "title": "A Deep Graph Embedding Network Model for Face Recognition",
        "authors": [
            "Yufei Gan",
            "Teng Yang",
            "Chu He"
        ],
        "abstract": "In this paper, we propose a new deep learning network \"GENet\", it combines the multi-layer network architec- ture and graph embedding framework. Firstly, we use simplest unsupervised learning PCA/LDA as first layer to generate the low- level feature. Secondly, many cascaded dimensionality reduction layers based on graph embedding framework are applied to GENet. Finally, a linear SVM classifier is used to classify dimension-reduced features. The experiments indicate that higher classification accuracy can be obtained by this algorithm on the CMU-PIE, ORL, Extended Yale B dataset.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7474",
        "title": "Extracting man-made objects from remote sensing images via fast level set evolutions",
        "authors": [
            "Zhongbin Li",
            "Wenzhong Shi",
            "Qunming Wang",
            "Zelang Miao"
        ],
        "abstract": "Object extraction from remote sensing images has long been an intensive research topic in the field of surveying and mapping. Most existing methods are devoted to handling just one type of object and little attention has been paid to improving the computational efficiency. In recent years, level set evolution (LSE) has been shown to be very promising for object extraction in the community of image processing and computer vision because it can handle topological changes automatically while achieving high accuracy. However, the application of state-of-the-art LSEs is compromised by laborious parameter tuning and expensive computation. In this paper, we proposed two fast LSEs for man-made object extraction from high spatial resolution remote sensing images. The traditional mean curvature-based regularization term is replaced by a Gaussian kernel and it is mathematically sound to do that. Thus a larger time step can be used in the numerical scheme to expedite the proposed LSEs. In contrast to existing methods, the proposed LSEs are significantly faster. Most importantly, they involve much fewer parameters while achieving better performance. The advantages of the proposed LSEs over other state-of-the-art approaches have been verified by a range of experiments.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7556",
        "title": "Location Recognition Over Large Time Lags",
        "authors": [
            "Basura Fernando",
            "Tatiana Tommasi",
            "Tinne Tuytelaars"
        ],
        "abstract": "Would it be possible to automatically associate ancient pictures to modern ones and create fancy cultural heritage city maps? We introduce here the task of recognizing the location depicted in an old photo given modern annotated images collected from the Internet. We present an extensive analysis on different features, looking for the most discriminative and most robust to the image variability induced by large time lags. Moreover, we show that the described task benefits from domain adaptation.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7618",
        "title": "Multiple Object Tracking: A Literature Review",
        "authors": [
            "Wenhan Luo",
            "Junliang Xing",
            "Anton Milan",
            "Xiaoqin Zhang",
            "Wei Liu",
            "Tae-Kyun Kim"
        ],
        "abstract": "Multiple Object Tracking (MOT) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an MOT system, including formulation, categorization, key principles, evaluation of MOT are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of MOT research, as well as some interesting directions which will become potential research effort in the future.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2022-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7686",
        "title": "How close are we to understanding image-based saliency?",
        "authors": [
            "Matthias K\u00fcmmerer",
            "Thomas Wallis",
            "Matthias Bethge"
        ],
        "abstract": "Within the set of the many complex factors driving gaze placement, the properities of an image that are associated with fixations under free viewing conditions have been studied extensively. There is a general impression that the field is close to understanding this particular association. Here we frame saliency models probabilistically as point processes, allowing the calculation of log-likelihoods and bringing saliency evaluation into the domain of information. We compared the information gain of state-of-the-art models to a gold standard and find that only one third of the explainable spatial information is captured. We additionally provide a principled method to show where and how models fail to capture information in the fixations. Thus, contrary to previous assertions, purely spatial saliency remains a significant challenge.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2014-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7818",
        "title": "On The Power of Joint Wavelet-DCT Features for Multispectral Palmprint Recognition",
        "authors": [
            "Shervin Minaee",
            "AmirAli Abdolrashidi"
        ],
        "abstract": "Biometric-based identification has drawn a lot of attention in the recent years. Among all biometrics, palmprint is known to possess a rich set of features. In this paper we have proposed to use DCT-based features in parallel with wavelet-based ones for palmprint identification. PCA is applied to the features to reduce their dimensionality and the majority voting algorithm is used to perform classification. The features introduced here result in a near-perfectly accurate identification. This method is tested on a well-known multispectral palmprint database and an accuracy rate of 99.97-100\\% is achieved, outperforming all previous methods in similar conditions.\n    ",
        "submission_date": "2014-09-27T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7963",
        "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation",
        "authors": [
            "Arjun Jain",
            "Jonathan Tompson",
            "Yann LeCun",
            "Christoph Bregler"
        ],
        "abstract": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.\n    ",
        "submission_date": "2014-09-28T00:00:00",
        "last_modified_date": "2014-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8230",
        "title": "RENOIR - A Dataset for Real Low-Light Image Noise Reduction",
        "authors": [
            "Josue Anaya",
            "Adrian Barbu"
        ],
        "abstract": "Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset.\n    ",
        "submission_date": "2014-09-29T00:00:00",
        "last_modified_date": "2017-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8403",
        "title": "Evaluation of Output Embeddings for Fine-Grained Image Classification",
        "authors": [
            "Zeynep Akata",
            "Scott Reed",
            "Daniel Walter",
            "Honglak Lee",
            "Bernt Schiele"
        ],
        "abstract": "Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0117",
        "title": "Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape Estimation from Monocular Image Sequences",
        "authors": [
            "Atul Kanaujia"
        ],
        "abstract": "Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused on acquiring behavioral information of the targets and their activities. Continuous evolution of intelligence being gathered of the human centric activities has put increased focus on the humans, especially inferring their innate characteristics - size, shapes and physiology. These bio-signatures extracted from the surveillance sensors can be used to deduce age, ethnicity, gender and actions, and further characterize human actions in unseen scenarios. However, recovery of pose and shape of humans in such monocular videos is inherently an ill-posed problem, marked by frequent depth and view based ambiguities due to self-occlusion, foreshortening and misalignment. The likelihood function often yields a highly multimodal posterior that is difficult to propagate even using the most advanced particle filtering(PF) algorithms. Motivated by the recent success of the discriminative approaches to efficiently predict 3D poses directly from the 2D images, we present several principled approaches to integrate predictive cues using learned regression models to sustain multimodality of the posterior during tracking. Additionally, these learned priors can be actively adapted to the test data using a likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D human shape model to each frame independently for inferring anthropometric bio-signatures. The proposed system is fully automated, robust to noisy test data and has ability to swiftly recover from tracking failures even after confronting with significant errors. We evaluate the system on a large number of monocular human motion sequences.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0226",
        "title": "Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and Photographic Imagery of Forests",
        "authors": [
            "Juheon Lee",
            "Xiaohao Cai",
            "Carola-Bibiane Schonlieb",
            "David Coomes"
        ],
        "abstract": "There is much current interest in using multi-sensor airborne remote sensing to monitor the structure and biodiversity of forests. This paper addresses the application of non-parametric image registration techniques to precisely align images obtained from multimodal imaging, which is critical for the successful identification of individual trees using object recognition approaches. Non-parametric image registration, in particular the technique of optimizing one objective function containing data fidelity and regularization terms, provides flexible algorithms for image registration. Using a survey of woodlands in southern Spain as an example, we show that non-parametric image registration can be successful at fusing datasets when there is little prior knowledge about how the datasets are interrelated (i.e. in the absence of ground control points). The validity of non-parametric registration methods in airborne remote sensing is demonstrated by a series of experiments. Precise data fusion is a prerequisite to accurate recognition of objects within airborne imagery, so non-parametric image registration could make a valuable contribution to the analysis pipeline.\n    ",
        "submission_date": "2014-07-28T00:00:00",
        "last_modified_date": "2014-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0243",
        "title": "Pattern Encoding on the Poincare Sphere",
        "authors": [
            "Aleksandra Pizurica"
        ],
        "abstract": "This paper presents a convenient graphical tool for encoding visual patterns (such as image patches and image atoms) as point constellations in a space spanned by perceptual features and with a clear geometrical interpretation. General theory and a practical pattern encoding scheme are presented, inspired by encoding polarization states of a light wave on the Poincare sphere. This new pattern encoding scheme can be useful for many applications in image processing and computer vision. Here, three possible applications are illustrated, in clustering perceptually similar patterns, visualizing properties of learned dictionaries of image atoms and generating new dictionaries of image atoms from spherical codes.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0311",
        "title": "$\\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous Update",
        "authors": [
            "Subhadip Mukherjee",
            "Rupam Basu",
            "Chandra Sekhar Seelamantula"
        ],
        "abstract": "We develop a dictionary learning algorithm by minimizing the $\\ell_1$ distortion metric on the data term, which is known to be robust for non-Gaussian noise contamination. The proposed algorithm exploits the idea of iterative minimization of weighted $\\ell_2$ error. We refer to this algorithm as $\\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparse coefficients are simultaneously updated to minimize the $\\ell_1$ objective, resulting in noise-robustness. We demonstrate through experiments that the $\\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the K-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al., both in Gaussian and non-Gaussian noise conditions. We also show that, for fixed values of sparsity, number of dictionary atoms, and data-dimension, the $\\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the training set available is small. We apply the proposed algorithm for denoising natural images corrupted by additive Gaussian and Laplacian noise. The images denoised using $\\ell_1$-K-SVD are observed to have slightly higher peak signal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but the improvement in structural similarity index (SSIM) is significant (approximately $0.1$) for lower values of input PSNR, indicating the efficacy of the $\\ell_1$ metric.\n    ",
        "submission_date": "2014-08-26T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0371",
        "title": "Real Time Fabric Defect Detection System on an Embedded DSP Platform",
        "authors": [
            "J. L. Raheja",
            "B. Ajay",
            "Ankit Chaudhary"
        ],
        "abstract": "In industrial fabric productions, automated real time systems are needed to find out the minor defects. It will save the cost by not transporting defected products and also would help in making compmay image of quality fabrics by sending out only undefected products. A real time fabric defect detection system (FDDS), implementd on an embedded DSP platform is presented here. Textural features of fabric image are extracted based on gray level co-occurrence matrix (GLCM). A sliding window technique is used for defect detection where window moves over the whole image computing a textural energy from the GLCM of the fabric image. The energy values are compared to a reference and the deviations beyond a threshold are reported as defects and also visually represented by a window. The implementation is carried out on a TI TMS320DM642 platform and programmed using code composer studio software. The real time output of this implementation was shown on a monitor.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0389",
        "title": "Learning to Transfer Privileged Information",
        "authors": [
            "Viktoriia Sharmanska",
            "Novi Quadrianto",
            "Christoph H. Lampert"
        ],
        "abstract": "We introduce a learning framework called learning using privileged information (LUPI) to the computer vision field. We focus on the prototypical computer vision problem of teaching computers to recognize objects in images. We want the computers to be able to learn faster at the expense of providing extra information during training time. As additional information about the image data, we look at several scenarios that have been studied in computer vision before: attributes, bounding boxes and image tags. The information is privileged as it is available at training time but not at test time. We explore two maximum-margin techniques that are able to make use of this additional source of information, for binary and multiclass object classification. We interpret these methods as learning easiness and hardness of the objects in the privileged space and then transferring this knowledge to train a better classifier in the original space. We provide a thorough analysis and comparison of information transfer from privileged to the original data spaces for both LUPI methods. Our experiments show that incorporating privileged information can improve the classification accuracy. Finally, we conduct user studies to understand which samples are easy and which are hard for human learning, and explore how this information is related to easy and hard samples when learning a classifier.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0478",
        "title": "Recognition of Handwritten Bangla Basic Characters and Digits using Convex Hull based Feature Set",
        "authors": [
            "Nibaran Das",
            "Sandip Pramanik",
            "Subhadip Basu",
            "Punam Kumar Saha",
            "Ram Sarkar",
            "Mahantapas Kundu",
            "Mita Nasipuri"
        ],
        "abstract": "In dealing with the problem of recognition of handwritten character patterns of varying shapes and sizes, selection of a proper feature set is important to achieve high recognition performance. The current research aims to evaluate the performance of the convex hull based feature set, i.e. 125 features in all computed over different bays attributes of the convex hull of a pattern, for effective recognition of isolated handwritten Bangla basic characters and digits. On experimentation with a database of 10000 samples, the maximum recognition rate of 76.86% is observed for handwritten Bangla characters. For Bangla numerals the maximum success rate of 99.45%. is achieved on a database of 12000 sample. The current work validates the usefulness of a new kind of feature set for recognition of handwritten Bangla basic characters and numerals.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2014-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0582",
        "title": "Multidimensional Digital Smoothing Filters for Target Detection",
        "authors": [
            "Hugh L. Kennedy"
        ],
        "abstract": "Recursive, causal and non-causal, multidimensional digital filters, with infinite impulse responses and maximally flat magnitude and delay responses in the low-frequency region, are designed to negate correlated clutter and interference in the background and to accumulate power due to dim targets in the foreground of a surveillance sensor. Expressions relating mean impulse-response duration, frequency selectivity and group delay, to low-order linear-difference-equation coefficients are derived using discrete Laguerre polynomials and discounted least-squares regression, then verified through simulation.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0736",
        "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition",
        "authors": [
            "Zhicheng Yan",
            "Hao Zhang",
            "Robinson Piramuthu",
            "Vignesh Jagadeesh",
            "Dennis DeCoste",
            "Wei Di",
            "Yizhou Yu"
        ],
        "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0745",
        "title": "Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer Depth Camera and Synthetic Data",
        "authors": [
            "Qiaosong Wang",
            "Vignesh Jagadeesh",
            "Bryan Ressler",
            "Robinson Piramuthu"
        ],
        "abstract": "Recent advances in consumer depth sensors have created many opportunities for human body measurement and modeling. Estimation of 3D body shape is particularly useful for fashion e-commerce applications such as virtual try-on or fit personalization. In this paper, we propose a method for capturing accurate human body shape and anthropometrics from a single consumer grade depth sensor. We first generate a large dataset of synthetic 3D human body models using real-world body size distributions. Next, we estimate key body measurements from a single monocular depth image. We combine body measurement estimates with local geometry features around key joint positions to form a robust multi-dimensional feature vector. This allows us to conduct a fast nearest-neighbor search to every sample in the dataset and return the closest one. Compared to existing methods, our approach is able to predict accurate full body parameters from a partial view using measurement parameters learned from the synthetic dataset. Furthermore, our system is capable of generating 3D human mesh models in real-time, which is significantly faster than methods which attempt to model shape and pose deformations. To validate the efficiency and applicability of our system, we collected a dataset that contains frontal and back scans of 83 clothed people with ground truth height and weight. Experiments on real-world dataset show that the proposed method can achieve real-time performance with competing results achieving an average error of 1.9 cm in estimated measurements.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0818",
        "title": "Feature Learning from Incomplete EEG with Denoising Autoencoder",
        "authors": [
            "Junhua Li",
            "Zbigniew Struzik",
            "Liqing Zhang",
            "Andrzej Cichocki"
        ],
        "abstract": "An alternative pathway for the human brain to communicate with the outside world is by means of a brain computer interface (BCI). A BCI can decode electroencephalogram (EEG) signals of brain activities, and then send a command or an intent to an external interactive device, such as a wheelchair. The effectiveness of the BCI depends on the performance in decoding the EEG. Usually, the EEG is contaminated by different kinds of artefacts (e.g., electromyogram (EMG), background activity), which leads to a low decoding performance. A number of filtering methods can be utilized to remove or weaken the effects of artefacts, but they generally fail when the EEG contains extreme artefacts. In such cases, the most common approach is to discard the whole data segment containing extreme artefacts. This causes the fatal drawback that the BCI cannot output decoding results during that time. In order to solve this problem, we employ the Lomb-Scargle periodogram to estimate the spectral power from incomplete EEG (after removing only parts contaminated by artefacts), and Denoising Autoencoder (DAE) for learning. The proposed method is evaluated with motor imagery EEG data. The results show that our method can successfully decode incomplete EEG to good effect.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2014-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0925",
        "title": "A Framework for the Volumetric Integration of Depth Images",
        "authors": [
            "Victor Adrian Prisacariu",
            "Olaf K\u00e4hler",
            "Ming Ming Cheng",
            "Carl Yuheng Ren",
            "Julien Valentin",
            "Philip H.S. Torr",
            "Ian D. Reid",
            "David W. Murray"
        ],
        "abstract": "Volumetric models have become a popular representation for 3D scenes in recent years. One of the breakthroughs leading to their popularity was KinectFusion, where the focus is on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a truncated signed distance function leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is also memory-intensive and limits the applicability to small scale reconstructions. Several avenues have been explored for overcoming this limitation. With the aim of summarizing them and providing for a fast and flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The core idea is that individual steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the needs of the user. Along with the framework we also provide a set of components for scalable reconstruction: two implementations of camera trackers, based on RGB data and on depth data, two representations of the 3D volumetric data, a dense volume and one based on hashes of subblocks, and an optional module for swapping subblocks in and out of the typically limited GPU memory.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0969",
        "title": "A Model of Plant Identification System Using GLCM, Lacunarity And Shen Features",
        "authors": [
            "Abdul Kadir"
        ],
        "abstract": "Recently, many approaches have been introduced by several researchers to identify plants. Now, applications of texture, shape, color and vein features are common practices. However, there are many possibilities of methods can be developed to improve the performance of such identification systems. Therefore, several experiments had been conducted in this research. As a result, a new novel approach by using combination of Gray-Level Co-occurrence Matrix, lacunarity and Shen features and a Bayesian classifier gives a better result compared to other plant identification systems. For comparison, this research used two kinds of several datasets that were usually used for testing the performance of each plant identification system. The results show that the system gives an accuracy rate of 97.19% when using the Flavia dataset and 95.00% when using the Foliage dataset and outperforms other approaches.\n    ",
        "submission_date": "2014-08-27T00:00:00",
        "last_modified_date": "2014-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1035",
        "title": "Learning Invariant Color Features for Person Re-Identification",
        "authors": [
            "Rahul Rama Varior",
            "Gang Wang",
            "Jiwen Lu"
        ],
        "abstract": "Matching people across multiple camera views known as person re-identification, is a challenging problem due to the change in visual appearance caused by varying lighting conditions. The perceived color of the subject appears to be different with respect to illumination. Previous works use color as it is or address these challenges by designing color spaces focusing on a specific cue. In this paper, we propose a data driven approach for learning color patterns from pixels sampled from images across two camera views. The intuition behind this work is that, even though pixel values of same color would be different across views, they should be encoded with the same values. We model color feature generation as a learning problem by jointly learning a linear transformation and a dictionary to encode pixel values. We also analyze different photometric invariant color spaces. Using color as the only cue, we compare our approach with all the photometric invariant color spaces and show superior performance over all of them. Combining with other learned low-level and high-level features, we obtain promising results in ViPER, Person Re-ID 2011 and CAVIAR4REID datasets.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1037",
        "title": "Facial Feature Point Detection: A Comprehensive Survey",
        "authors": [
            "Nannan Wang",
            "Xinbo Gao",
            "Dacheng Tao",
            "Xuelong Li"
        ],
        "abstract": "This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods can be categorized into the following four groups: constrained local model (CLM)-based, active appearance model (AAM)-based, regression-based, and other methods. CLM-based methods consist of a shape model and a number of local experts, each of which is utilized to detect a facial feature point. AAM-based methods fit a shape model to an image by minimizing texture synthesis errors. Regression-based methods directly learn a mapping function from facial image appearance to facial feature points. Besides the above three major categories of methods, there are also minor categories of methods which we classify into other methods: graphical model-based methods, joint face alignment methods, independent facial feature point detectors, and deep learning-based methods. Though significant progress has been made, facial feature point detection is limited in its success by wild and real-world conditions: variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provide us a holistic understanding and deep insight into facial feature point detection, which also motivates us to explore promising future directions.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1090",
        "title": "Explain Images with Multimodal Recurrent Neural Networks",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.\n    ",
        "submission_date": "2014-10-04T00:00:00",
        "last_modified_date": "2014-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1267",
        "title": "Memristive Threshold Logic Circuit Design of Fast Moving Object Detection",
        "authors": [
            "Akshay Kumar Maan",
            "Dinesh Sasi Kumar",
            "Sherin Sugathan",
            "Alex Pappachen James"
        ],
        "abstract": "Real-time detection of moving objects involves memorisation of features in the template image and their comparison with those in the test image. At high sampling rates, such techniques face the problems of high algorithmic complexity and component delays. We present a new resistive switching based threshold logic cell which encodes the pixels of a template image. The cell comprises a voltage divider circuit that programs the resistances of the memristors arranged in a single node threshold logic network and the output is encoded as a binary value using a CMOS inverter gate. When a test image is applied to the template-programmed cell, a mismatch in the respective pixels is seen as a change in the output voltage of the cell. The proposed cell when compared with CMOS equivalent implementation shows improved performance in area, leakage power, power dissipation and delay.\n    ",
        "submission_date": "2014-10-06T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1606",
        "title": "Hierarchical Sparse and Collaborative Low-Rank Representation for Emotion Recognition",
        "authors": [
            "Xiang Xiang",
            "Minh Dao",
            "Gregory D. Hager",
            "Trac D. Tran"
        ],
        "abstract": "In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank (C-HiSLR) model that is natural for recognizing human emotion in visual data. Previous attempts require explicit expression components, which are often unavailable and difficult to recover. Instead, our model exploits the lowrank property over expressive facial frames and rescue inexact sparse representations by incorporating group sparsity. For the CK+ dataset, C-HiSLR on raw expressive faces performs as competitive as the Sparse Representation based Classification (SRC) applied on manually prepared emotions. C-HiSLR performs even better than SRC in terms of true positive rate.\n    ",
        "submission_date": "2014-10-07T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1980",
        "title": "Deep Representations for Iris, Face, and Fingerprint Spoofing Detection",
        "authors": [
            "David Menotti",
            "Giovani Chiachia",
            "Allan Pinto",
            "William Robson Schwartz",
            "Helio Pedrini",
            "Alexandre Xavier Falcao",
            "Anderson Rocha"
        ],
        "abstract": "Biometrics systems have significantly improved person identification and authentication, playing an important role in personal, national, and global security. However, these systems might be deceived (or \"spoofed\") and, despite the recent advances in spoofing detection, current solutions often rely on domain knowledge, specific biometric reading systems, and attack types. We assume a very limited knowledge about biometric spoofing at the sensor to derive outstanding spoofing detection systems for iris, face, and fingerprint modalities based on two deep learning approaches. The first approach consists of learning suitable convolutional network architectures for each domain, while the second approach focuses on learning the weights of the network via back-propagation. We consider nine biometric spoofing benchmarks --- each one containing real and fake samples of a given biometric modality and attack type --- and learn deep representations for each benchmark by combining and contrasting the two learning approaches. This strategy not only provides better comprehension of how these approaches interplay, but also creates systems that exceed the best known results in eight out of the nine benchmarks. The results strongly indicate that spoofing detection systems based on convolutional networks can be robust to attacks already known and possibly adapted, with little effort, to image-based attacks that are yet to come.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2173",
        "title": "Face Detection Using Radial Basis Functions Neural Networks With Fixed Spread",
        "authors": [
            "K.A.A. Aziz",
            "S.S. Abdullah"
        ],
        "abstract": "This paper presented a face detection system using Radial Basis Function Neural Networks With Fixed Spread Value. Face detection is the first step in face recognition system. The purpose is to localize and extract the face region from the background that will be fed into the face recognition system for identification. General preprocessing approach was used for normalizing the image and Radial Basis Function (RBF) Neural Network was used to distinguish between face and non-face. RBF Neural Networks offer several advantages compared to other neural network architecture such as they can be trained using fast two stages training algorithm and the network possesses the property of best approximation. The output of the network can be optimized by setting suitable value of center and spread of the RBF. In this paper, fixed spread value will be used. The Radial Basis Function Neural Network (RBFNN) used to distinguish faces and non-faces and the evaluation of the system will be the performance of detection, False Acceptance Rate (FAR), False Rejection Rate (FRR) and the discriminative properties.\n    ",
        "submission_date": "2014-08-07T00:00:00",
        "last_modified_date": "2014-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2175",
        "title": "Image Denoising using New Adaptive Based Median Filters",
        "authors": [
            "Suman Shrestha"
        ],
        "abstract": "Noise is a major issue while transferring images through all kinds of electronic communication. One of the most common noise in electronic communication is an impulse noise which is caused by unstable voltage. In this paper, the comparison of known image denoising techniques is discussed and a new technique using the decision based approach has been used for the removal of impulse noise. All these methods can primarily preserve image details while suppressing impulsive noise. The principle of these techniques is at first introduced and then analysed with various simulation results using MATLAB. Most of the previously known techniques are applicable for the denoising of images corrupted with less noise density. Here a new decision based technique has been presented which shows better performances than those already being used. The comparisons are made based on visual appreciation and further quantitatively by Mean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of different filtered images..\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2188",
        "title": "An Aerial Image Recognition Framework using Discrimination and Redundancy Quality Measure",
        "authors": [
            "Yuxin Hu",
            "Luming Zhang"
        ],
        "abstract": "Aerial image categorization plays an indispensable role in remote sensing and artificial intelligence. In this paper, we propose a new aerial image categorization framework, focusing on organizing the local patches of each aerial image into multiple discriminative subgraphs. The subgraphs reflect both the geometric property and the color distribution of an aerial image. First, each aerial image is decomposed into a collection of regions in terms of their color intensities. Thereby region connected graph (RCG), which models the connection between the spatial neighboring regions, is constructed to encode the spatial context of an aerial image. Second, a subgraph mining technique is adopted to discover the frequent structures in the RCGs constructed from the training aerial images. Thereafter, a set of refined structures are selected among the frequent ones toward being highly discriminative and low redundant. Lastly, given a new aerial image, its sub-RCGs corresponding to the refined structures are extracted. They are further quantized into a discriminative vector for SVM classification. Thorough experimental results validate the effectiveness of the proposed method. In addition, the visualized mined subgraphs show that the discriminative topologies of each aerial image are discovered.\n    ",
        "submission_date": "2014-10-06T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2381",
        "title": "Recognition of cDNA microarray image Using Feedforward artificial neural network",
        "authors": [
            "R. M. Farouk",
            "S. Badr",
            "M. Sayed Elahl"
        ],
        "abstract": "The complementary DNA (cDNA) sequence is considered to be the magic biometric technique for personal identification. In this paper, we present a new method for cDNA recognition based on the artificial neural network (ANN). Microarray imaging is used for the concurrent identification of thousands of genes. We have segmented the location of the spots in a cDNA microarray. Thus, a precise localization and segmenting of a spot are essential to obtain a more accurate intensity measurement, leading to a more precise expression measurement of a gene. The segmented cDNA microarray image is resized and it is used as an input for the proposed artificial neural network. For matching and recognition, we have trained the artificial neural network. Recognition results are given for the galleries of cDNA sequences . The numerical results show that, the proposed matching technique is an effective in the cDNA sequences process. We also compare our results with previous results and find out that, the proposed technique is an effective matching performance.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2386",
        "title": "Bayesian Robust Tensor Factorization for Incomplete Multiway Data",
        "authors": [
            "Qibin Zhao",
            "Guoxu Zhou",
            "Liqing Zhang",
            "Andrzej Cichocki",
            "Shun-ichi Amari"
        ],
        "abstract": "We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The low-CP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-$t$ distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2474",
        "title": "Genetic Stereo Matching Algorithm with Fuzzy Fitness",
        "authors": [
            "Haythem Ghazouani"
        ],
        "abstract": "This paper presents a genetic stereo matching algorithm with fuzzy evaluation function. The proposed algorithm presents a new encoding scheme in which a chromosome is represented by a disparity matrix. Evolution is controlled by a fuzzy fitness function able to deal with noise and uncertain camera measurements, and uses classical evolutionary operators. The result of the algorithm is accurate dense disparity maps obtained in a reasonable computational time suitable for real-time applications as shown in experimental results.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2488",
        "title": "Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science",
        "authors": [
            "Emily L. Spratt",
            "Ahmed Elgammal"
        ],
        "abstract": "In part one of the Critique of Judgment, Immanuel Kant wrote that \"the judgment of taste...is not a cognitive judgment, and so not logical, but is aesthetic.\"\\cite{Kant} While the condition of aesthetic discernment has long been the subject of philosophical discourse, the role of the arbiters of that judgment has more often been assumed than questioned. The art historian, critic, connoisseur, and curator have long held the esteemed position of the aesthetic judge, their training, instinct, and eye part of the inimitable subjective processes that Kant described as occurring upon artistic evaluation. Although the concept of intangible knowledge in regard to aesthetic theory has been much explored, little discussion has arisen in response to the development of new types of artificial intelligence as a challenge to the seemingly ineffable abilities of the human observer. This paper examines the developments in the field of computer vision analysis of paintings from canonical movements with the history of Western art and the reaction of art historians to the application of this technology in the field. Through an investigation of the ethical consequences of this innovative technology, the unquestioned authority of the art expert is challenged and the subjective nature of aesthetic judgment is brought to philosophical scrutiny once again.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2014-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2535",
        "title": "A unified approach for multi-object triangulation, tracking and camera calibration",
        "authors": [
            "Jeremie Houssineau",
            "Daniel Clark",
            "Spela Ivekovic",
            "Chee Sing Lee",
            "Jose Franco"
        ],
        "abstract": "Object triangulation, 3-D object tracking, feature correspondence, and camera calibration are key problems for estimation from camera networks. This paper addresses these problems within a unified Bayesian framework for joint multi-object tracking and sensor registration. Given that using standard filtering approaches for state estimation from cameras is problematic, an alternative parametrisation is exploited, called disparity space. The disparity space-based approach for triangulation and object tracking is shown to be more effective than non-linear versions of the Kalman filter and particle filtering for non-rectified cameras. The approach for feature correspondence is based on the Probability Hypothesis Density (PHD) filter, and hence inherits the ability to update without explicit measurement association, to initiate new targets, and to discriminate between target and clutter. The PHD filtering approach then forms the basis of a camera calibration method from static or moving objects. Results are shown on simulated data.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2663",
        "title": "Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet marginals",
        "authors": [
            "Florian Yger"
        ],
        "abstract": "This short memo aims at explaining our approach for the challenge IEEE-ISBI on Bone Texture Characterization. In this work, we focus on the use of covariance matrices and wavelet marginals in an SVM classifier.\n    ",
        "submission_date": "2014-10-10T00:00:00",
        "last_modified_date": "2014-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2959",
        "title": "Direct Processing of Document Images in Compressed Domain",
        "authors": [
            "Mohammed Javed",
            "P. Nagabhushan",
            "B.B. Chaudhuri"
        ],
        "abstract": "With the rapid increase in the volume of Big data of this digital era, fax documents, invoices, receipts, etc are traditionally subjected to compression for the efficiency of data storage and transfer. However, in order to process these documents, they need to undergo the stage of decompression which indents additional computing resources. This limitation induces the motivation to research on the possibility of directly processing of compressed images. In this research paper, we summarize the research work carried out to perform different operations straight from run-length compressed documents without going through the stage of decompression. The different operations demonstrated are feature extraction; text-line, word and character segmentation; document block segmentation; and font size detection, all carried out in the compressed version of the document. Feature extraction methods demonstrate how to extract the conventionally defined features such as projection profile, run-histogram and entropy, directly from the compressed document data. Document segmentation involves the extraction of compressed segments of text-lines, words and characters using the vertical and horizontal projection profile features. Further an attempt is made to segment randomly a block of interest from the compressed document and subsequently facilitate absolute and relative characterization of the segmented block which finds real time applications in automatic processing of Bank Cheques, Challans, etc, in compressed domain. Finally an application to detect font size at text line level is also investigated. All the proposed algorithms are validated experimentally with sufficient data set of compressed documents.\n    ",
        "submission_date": "2014-10-11T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3080",
        "title": "Tree-Structure Bayesian Compressive Sensing for Video",
        "authors": [
            "Xin Yuan",
            "Patrick Llull",
            "David J. Brady",
            "Lawrence Carin"
        ],
        "abstract": "A Bayesian compressive sensing framework is developed for video reconstruction based on the color coded aperture compressive temporal imaging (CACTI) system. By exploiting the three dimension (3D) tree structure of the wavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian compressive sensing inversion algorithm is derived to reconstruct (up to 22) color video frames from a single monochromatic compressive measurement. Both simulated and real datasets are adopted to verify the performance of the proposed algorithm.\n    ",
        "submission_date": "2014-10-12T00:00:00",
        "last_modified_date": "2014-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3349",
        "title": "Single Image Super Resolution via Manifold Approximation",
        "authors": [
            "Chinh Dang",
            "Hayder Radha"
        ],
        "abstract": "Image super-resolution remains an important research topic to overcome the limitations of physical acquisition systems, and to support the development of high resolution displays. Previous example-based super-resolution approaches mainly focus on analyzing the co-occurrence properties of low resolution and high-resolution patches. Recently, we proposed a novel single image super-resolution approach based on linear manifold approximation of the high-resolution image-patch space [1]. The image super-resolution problem is then formulated as an optimization problem of searching for the best matched high resolution patch in the manifold for a given low-resolution patch. We developed a novel technique based on the l1 norm sparse graph to learn a set of low dimensional affine spaces or tangent subspaces of the high-resolution patch manifold. The optimization problem is then solved based on the learned set of tangent subspaces. In this paper, we build on our recent work as follows. First, we consider and analyze each tangent subspace as one point in a Grassmann manifold, which helps to compute geodesic pairwise distances among these tangent subspaces. Second, we develop a min-max algorithm to select an optimal subset of tangent subspaces. This optimal subset reduces the computational cost while still preserving the quality of the reconstructed high-resolution image. Third, and to further achieve lower computational complexity, we perform hierarchical clustering on the optimal subset based on Grassmann manifold distances. Finally, we analytically prove the validity of the proposed Grassmann-distance based clustering. A comparison of the obtained results with other state-of-the-art methods clearly indicates the viability of the new proposed framework.\n    ",
        "submission_date": "2014-10-13T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3699",
        "title": "A graph Laplacian regularization for hyperspectral data unmixing",
        "authors": [
            "Rita Ammanouil",
            "Andr\u00e9 Ferrari",
            "C\u00e9dric Richard"
        ],
        "abstract": "This paper introduces a graph Laplacian regularization in the hyperspectral unmixing formulation. The proposed regularization relies upon the construction of a graph representation of the hyperspectral image. Each node in the graph represents a pixel's spectrum, and edges connect spectrally and spatially similar pixels. The proposed graph framework promotes smoothness in the estimated abundance maps and collaborative estimation between homogeneous areas of the image. The resulting convex optimization problem is solved using the Alternating Direction Method of Multipliers (ADMM). A special attention is given to the computational complexity of the algorithm, and Graph-cut methods are proposed in order to reduce the computational burden. Finally, simulations conducted on synthetic data illustrate the effectiveness of the graph Laplacian regularization with respect to other classical regularizations for hyperspectral unmixing.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3726",
        "title": "Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding",
        "authors": [
            "Chern Hong Lim",
            "Anhar Risnumawan",
            "Chee Seng Chan"
        ],
        "abstract": "Ambiguity or uncertainty is a pervasive element of many real world decision making processes. Variation in decisions is a norm in this situation when the same problem is posed to different subjects. Psychological and metaphysical research had proven that decision making by human is subjective. It is influenced by many factors such as experience, age, background, etc. Scene understanding is one of the computer vision problems that fall into this category. Conventional methods relax this problem by assuming scene images are mutually exclusive; and therefore, focus on developing different approaches to perform the binary classification tasks. In this paper, we show that scene images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC provides a ranking interpretation instead of binary decision. Evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3744",
        "title": "Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking",
        "authors": [
            "Mei Kuan Lim",
            "Chee Seng Chan",
            "Dorothy Monekosso",
            "Paolo Remagnino"
        ],
        "abstract": "Conventional tracking solutions are not feasible in handling abrupt motion as they are based on smooth motion assumption or an accurate motion model. Abrupt motion is not subject to motion continuity and smoothness. To assuage this, we deem tracking as an optimisation problem and propose a novel abrupt motion tracker that based on swarm intelligence - the SwaTrack. Unlike existing swarm-based filtering methods, we first of all introduce an optimised swarm-based sampling strategy to tradeoff between the exploration and exploitation of the search space in search for the optimal proposal distribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow on the fly tuning of the best mean and variance of the distribution for sampling. Such innovating idea of combining these strategies in an ingenious way in the PSO framework to handle the abrupt motion, which so far no existing works are found. Experimental results in both quantitative and qualitative had shown the effectiveness of the proposed method in tracking abrupt motions.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3748",
        "title": "Zero-Shot Object Recognition System based on Topic Model",
        "authors": [
            "Wai Lam Hoo",
            "Chee Seng Chan"
        ],
        "abstract": "Object recognition systems usually require fully complete manually labeled training data to train the classifier. In this paper, we study the problem of object recognition where the training samples are missing during the classifier learning stage, a task also known as zero-shot learning. We propose a novel zero-shot learning strategy that utilizes the topic model and hierarchical class concept. Our proposed method advanced where cumbersome human annotation stage (i.e. attribute-based classification) is eliminated. We achieve comparable performance with state-of-the-art algorithms in four public datasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), and Animals with Attributes (49.65%) when unseen classes exist in the classification task.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3751",
        "title": "A Fusion Approach for Efficient Human Skin Detection",
        "authors": [
            "Wei Ren Tan",
            "Chee Seng Chan",
            "Pratheepan Yogarajah",
            "Joan Condell"
        ],
        "abstract": "A reliable human skin detection method that is adaptable to different human skin colours and illu- mination conditions is essential for better human skin segmentation. Even though different human skin colour detection solutions have been successfully applied, they are prone to false skin detection and are not able to cope with the variety of human skin colours across different ethnic. Moreover, existing methods require high computational cost. In this paper, we propose a novel human skin de- tection approach that combines a smoothed 2D histogram and Gaussian model, for automatic human skin detection in colour image(s). In our approach an eye detector is used to refine the skin model for a specific person. The proposed approach reduces computational costs as no training is required; and it improves the accuracy of skin detection despite wide variation in ethnicity and illumination. To the best of our knowledge, this is the first method to employ fusion strategy for this purpose. Qualitative and quantitative results on three standard public datasets and a comparison with state-of-the-art methods have shown the effectiveness and robustness of the proposed approach.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3752",
        "title": "Enhanced Random Forest with Image/Patch-Level Learning for Image Understanding",
        "authors": [
            "Wai Lam Hoo",
            "Tae-Kyun Kim",
            "Yuru Pei",
            "Chee Seng Chan"
        ],
        "abstract": "Image understanding is an important research domain in the computer vision due to its wide real-world applications. For an image understanding framework that uses the Bag-of-Words model representation, the visual codebook is an essential part. Random forest (RF) as a tree-structure discriminative codebook has been a popular choice. However, the performance of the RF can be degraded if the local patch labels are poorly assigned. In this paper, we tackle this problem by a novel way to update the RF codebook learning for a more discriminative codebook with the introduction of the soft class labels, estimated from the pLSA model based on a feedback scheme. The feedback scheme is performed on both the image and patch levels respectively, which is in contrast to the state- of-the-art RF codebook learning that focused on either image or patch level only. Experiments on 15-Scene and C-Pascal datasets had shown the effectiveness of the proposed method in image understanding task.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3756",
        "title": "Crowd Saliency Detection via Global Similarity Structure",
        "authors": [
            "Mei Kuan Lim",
            "Ven Jyn Kok",
            "Chen Change Loy",
            "Chee Seng Chan"
        ],
        "abstract": "It is common for CCTV operators to overlook inter- esting events taking place within the crowd due to large number of people in the crowded scene (i.e. marathon, rally). Thus, there is a dire need to automate the detection of salient crowd regions acquiring immediate attention for a more effective and proactive surveillance. This paper proposes a novel framework to identify and localize salient regions in a crowd scene, by transforming low-level features extracted from crowd motion field into a global similarity structure. The global similarity structure representation allows the discovery of the intrinsic manifold of the motion dynamics, which could not be captured by the low-level representation. Ranking is then performed on the global similarity structure to identify a set of extrema. The proposed approach is unsupervised so learning stage is eliminated. Experimental results on public datasets demonstrates the effectiveness of exploiting such extrema in identifying salient regions in various crowd scenarios that exhibit crowding, local irregular motion, and unique motion areas such as sources and sinks.\n    ",
        "submission_date": "2014-10-14T00:00:00",
        "last_modified_date": "2014-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3905",
        "title": "Efficient Image Categorization with Sparse Fisher Vector",
        "authors": [
            "Xiankai Lu",
            "Zheng Fang",
            "Tao Xu",
            "Haiting Zhang",
            "Hongya Tuo"
        ],
        "abstract": "In object recognition, Fisher vector (FV) representation is one of the state-of-art image representations ways at the expense of dense, high dimensional features and increased computation time. A simplification of FV is attractive, so we propose Sparse Fisher vector (SFV). By incorporating locality strategy, we can accelerate the Fisher coding step in image categorization which is implemented from a collective of local descriptors. Combining with pooling step, we explore the relationship between coding step and pooling step to give a theoretical explanation about SFV. Experiments on benchmark datasets have shown that SFV leads to a speedup of several-fold of magnitude compares with FV, while maintaining the categorization performance. In addition, we demonstrate how SFV preserves the consistence in representation of similar local features.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3910",
        "title": "High Order Structure Descriptors for Scene Images",
        "authors": [
            "Wenya Zhu",
            "Xiankai Lu",
            "Tao Xu",
            "Ziyi Zhao"
        ],
        "abstract": "Structure information is ubiquitous in natural scene images and it plays an important role in scene representation. In this paper, third order structure statistics (TOSS) and fourth order structure statistics (FOSS) are exploited to encode higher order structure information. Afterwards, based on the radial and normal slice of TOSS and FOSS, we propose the high order structure feature: third order structure feature (TOSF) and fourth order structure feature (FOSF). It is well known that scene images are well characterized by particular arrangements of their local structures, we divide the scene image into the non-overlapping sub-regions and compute the proposed higher order structural features among them. Then a scene classification is performed by using SVM classifier with these higher order structure features. The experimental results show that higher order structure statistics can deliver image structure information well and its spatial envelope has strong discriminative ability.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3932",
        "title": "Detection of Salient Regions in Crowded Scenes",
        "authors": [
            "Mei Kuan Lim",
            "Chee Seng Chan",
            "Dorothy Monekosso",
            "Paolo Remagnino"
        ],
        "abstract": "The increasing number of cameras and a handful of human operators to monitor the video inputs from hundreds of cameras leave the system ill equipped to fulfil the task of detecting anomalies. Thus, there is a dire need to automatically detect regions that require immediate attention for a more effective and proactive surveillance. We propose a framework that utilises the temporal variations in the flow field of a crowd scene to automatically detect salient regions, while eliminating the need to have prior knowledge of the scene or training. We deem the flow fields to be a dynamic system and adopt the stability theory of dynamical systems, to determine the motion dynamics within a given area. In the context of this work, salient regions refer to areas with high motion dynamics, where points in a particular region are unstable. Experimental results on public, crowd scenes have shown the effectiveness of the proposed method in detecting salient regions which correspond to unstable flow, occlusions, bottlenecks, entries and exits.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3970",
        "title": "Shape and Color Object Tracking for Real-Time Robotic Navigation",
        "authors": [
            "Haythem Ghazouani"
        ],
        "abstract": "This paper presents a real-time approach for single-colored ball detection and tracking. The approach consists of two main phases. In a first offline calibration phase, the intrinsic parameters of the camera and the radial distortion are estimated, and a classification of colors is learned from a sample image of colored balls. The second phase consists of four main steps: (1) color segmentation of the input image into several regions based on the offline classification, (2) robust estimation of the circle parameters (3) refinement of the circle parameters, and (4) ball tracking. The experimental results showed that the approach presents a good compromise between suitability for real-time navigation and robustness to occlusions, background congestion and colors interference in the scene.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4012",
        "title": "Online interpretation of numeric sign language using 2-d skeletal model",
        "authors": [
            "Subhadip Basu",
            "S. Dey",
            "K. Mukherjee",
            "T. S. Jana"
        ],
        "abstract": "Gesturing is one of the natural modes of human communication. Signs produced by gestures can have a basic meaning coupled with additional information that is layered over the basic meaning of the sign. Sign language is an important example of communicative gestures that are highly structured and well accepted across the world as a communication medium for deaf and dumb. In this paper, an online recognition scheme is proposed to interpret the standard numeric sign language comprising of 10 basic hand symbols. A web camera is used to capture the real time hand movements as input to the system. The basic meaning of the hand gesture is extracted from the input data frame by analysing the shape of the hand, considering its orientation, movement and location to be fixed. The input hand shape is processed to identify the palm structure, fingertips and their relative positions and the presence of the extended thumb. A 2-dimensional skeletal model is generated from the acquired shape information to represent and subsequently interpret the basic meaning of the hand gesture.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4013",
        "title": "A two-pass fuzzy-geno approach to pattern classification",
        "authors": [
            "Subhadip Basu",
            "Mahantapas Kundu",
            "Mita Nasipuri",
            "Dipak Kumar Basu"
        ],
        "abstract": "The work presents an extension of the fuzzy approach to 2-D shape recognition [1] through refinement of initial or coarse classification decisions under a two pass approach. In this approach, an unknown pattern is classified by refining possible classification decisions obtained through coarse classification of the same. To build a fuzzy model of a pattern class horizontal and vertical fuzzy partitions on the sample images of the class are optimized using genetic algorithm. To make coarse classification decisions about an unknown pattern, the fuzzy representation of the pattern is compared with models of all pattern classes through a specially designed similarity measure. Coarse classification decisions are refined in the second pass to obtain the final classification decision of the unknown pattern. To do so, optimized horizontal and vertical fuzzy partitions are again created on certain regions of the image frame, specific to each group of similar type of pattern classes. It is observed through experiments that the technique improves the overall recognition rate from 86.2%, in the first pass, to 90.4% after the second pass, with 500 training samples of handwritten digits.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4017",
        "title": "Online Tracking of Skin Colour Regions Against a Complex Background",
        "authors": [
            "Subhadip Basu",
            "S. Chakraborty",
            "K. Mukherjee",
            "S. K. Pandit"
        ],
        "abstract": "Online tracking of human activity against a complex background is a challenging task for many applications. In this paper, we have developed a robust technique for localizing skin colour regions from unconstrained image frames. A simple and fast segmentation algorithm is used to train a multiplayer perceptron (MLP) for detection of skin colours. Stepper motors are synchronized with the MLP to track the movement of the skin colour regions.\n    ",
        "submission_date": "2014-10-15T00:00:00",
        "last_modified_date": "2014-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4393",
        "title": "The HAWKwood Database",
        "authors": [
            "Christopher Herbon"
        ],
        "abstract": "We present a database consisting of wood pile images, which can be used as a benchmark to evaluate the performance of wood pile detection and surveying algorithms. We distinguish six database cate- gories which can be used for different types of algorithms. Images of real and synthetic scenes are provided, which consist of 7655 images divided into 354 data sets. Depending on the category the data sets either include ground truth data or forestry specific measurements with which algorithms may be compared.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4441",
        "title": "Improve CAPTCHA's Security Using Gaussian Blur Filter",
        "authors": [
            "Ariyan Zarei"
        ],
        "abstract": "Providing security for webservers against unwanted and automated registrations has become a big concern. To prevent these kinds of false registrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based or visual CAPTCHAs are very common. Actually visual CAPTCHA is an image containing a sequence of characters. So far most of visual CAPTCHAs, in order to resist against OCR programs, use some common implementations such as wrapping the characters, random placement and rotations of characters, etc. In this paper we applied Gaussian Blur filter, which is an image transformation, to visual CAPTCHAs to reduce their readability by OCR programs. We concluded that this technique made CAPTCHAs almost unreadable for OCR programs but, their readability by human users still remained high.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4470",
        "title": "MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex Optimization",
        "authors": [
            "Raviteja Vemulapalli",
            "Vinay Praneeth Boda",
            "Rama Chellappa"
        ],
        "abstract": "In the recent past, automatic selection or combination of kernels (or features) based on multiple kernel learning (MKL) approaches has been receiving significant attention from various research communities. Though MKL has been extensively studied in the context of support vector machines (SVM), it is relatively less explored for ratio-trace problems. In this paper, we show that MKL can be formulated as a convex optimization problem for a general class of ratio-trace problems that encompasses many popular algorithms used in various computer vision applications. We also provide an optimization procedure that is guaranteed to converge to the global optimum of the proposed optimization problem. We experimentally demonstrate that the proposed MKL approach, which we refer to as MKL-RT, can be successfully used to select features for discriminative dimensionality reduction and cross-modal retrieval. We also show that the proposed convex MKL-RT approach performs better than the recently proposed non-convex MKL-DR approach.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4485",
        "title": "A Gesture Recognition System for Detecting Behavioral Patterns of ADHD",
        "authors": [
            "Miguel \u00c1ngel Bautista",
            "Antonio Hern\u00e1ndez-Vela",
            "Sergio Escalera",
            "Laura Igual",
            "Oriol Pujol",
            "Josep Moya",
            "Ver\u00f3nica Violant",
            "Mar\u00eda Teresa Anguera"
        ],
        "abstract": "We present an application of gesture recognition using an extension of Dynamic Time Warping (DTW) to recognize behavioural patterns of Attention Deficit Hyperactivity Disorder (ADHD). We propose an extension of DTW using one-class classifiers in order to be able to encode the variability of a gesture category, and thus, perform an alignment between a gesture sample and a gesture class. We model the set of gesture samples of a certain gesture category using either GMMs or an approximation of Convex Hulls. Thus, we add a theoretical contribution to classical warping path in DTW by including local modeling of intra-class gesture variability. This methodology is applied in a clinical context, detecting a group of ADHD behavioural patterns defined by experts in psychology/psychiatry, to provide support to clinicians in the diagnose procedure. The proposed methodology is tested on a novel multi-modal dataset (RGB plus Depth) of ADHD children recordings with behavioural patterns. We obtain satisfying results when compared to standard state-of-the-art approaches in the DTW context.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4521",
        "title": "Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling",
        "authors": [
            "Michael Maire",
            "Stella X. Yu",
            "Pietro Perona"
        ],
        "abstract": "We frame the task of predicting a semantic labeling as a sparse reconstruction procedure that applies a target-specific learned transfer function to a generic deep sparse code representation of an image. This strategy partitions training into two distinct stages. First, in an unsupervised manner, we learn a set of generic dictionaries optimized for sparse coding of image patches. We train a multilayer representation via recursive sparse dictionary learning on pooled codes output by earlier layers. Second, we encode all training images with the generic dictionaries and learn a transfer function that optimizes reconstruction of patches extracted from annotated ground-truth given the sparse codes of their corresponding image patches. At test time, we encode a novel image using the generic dictionaries and then reconstruct using the transfer function. The output reconstruction is a semantic labeling of the test image.\n",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4627",
        "title": "Learning visual biases from human imagination",
        "authors": [
            "Carl Vondrick",
            "Hamed Pirsiavash",
            "Aude Oliva",
            "Antonio Torralba"
        ],
        "abstract": "Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4650",
        "title": "Randomized Structural Sparsity via Constrained Block Subsampling for Improved Sensitivity of Discriminative Voxel Identification",
        "authors": [
            "Yilun Wang",
            "Junjie Zheng",
            "Sheng Zhang",
            "Xujun Duan",
            "Huafu Chen"
        ],
        "abstract": "In this paper, we consider voxel selection for functional Magnetic Resonance Imaging (fMRI) brain data with the aim of finding a more complete set of probably correlated discriminative voxels, thus improving interpretation of the discovered potential biomarkers. The main difficulty in doing this is an extremely high dimensional voxel space and few training samples, resulting in unreliable feature selection. In order to deal with the difficulty, stability selection has received a great deal of attention lately, especially due to its finite sample control of false discoveries and transparent principle for choosing a proper amount of regularization. However, it fails to make explicit use of the correlation property or structural information of these discriminative features and leads to large false negative rates. In other words, many relevant but probably correlated discriminative voxels are missed. Thus, we propose a new variant on stability selection \"randomized structural sparsity\", which incorporates the idea of structural sparsity. Numerical experiments demonstrate that our method can be superior in controlling for false negatives while also keeping the control of false positives inherited from stability selection.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4673",
        "title": "KCRC-LCD: Discriminative Kernel Collaborative Representation with Locality Constrained Dictionary for Visual Categorization",
        "authors": [
            "Weiyang Liu",
            "Zhiding Yu",
            "Lijia Lu",
            "Yandong Wen",
            "Hui Li",
            "Yuexian Zou"
        ],
        "abstract": "We consider the image classification problem via kernel collaborative representation classification with locality constrained dictionary (KCRC-LCD). Specifically, we propose a kernel collaborative representation classification (KCRC) approach in which kernel method is used to improve the discrimination ability of collaborative representation classification (CRC). We then measure the similarities between the query and atoms in the global dictionary in order to construct a locality constrained dictionary (LCD) for KCRC. In addition, we discuss several similarity measure approaches in LCD and further present a simple yet effective unified similarity measure whose superiority is validated in experiments. There are several appealing aspects associated with LCD. First, LCD can be nicely incorporated under the framework of KCRC. The LCD similarity measure can be kernelized under KCRC, which theoretically links CRC and LCD under the kernel method. Second, KCRC-LCD becomes more scalable to both the training set size and the feature dimension. Example shows that KCRC is able to perfectly classify data with certain distribution, while conventional CRC fails completely. Comprehensive experiments on many public datasets also show that KCRC-LCD is a robust discriminative classifier with both excellent performance and good scalability, being comparable or outperforming many other state-of-the-art approaches.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2014-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4688",
        "title": "Large Vocabulary Arabic Online Handwriting Recognition System",
        "authors": [
            "Ibrahim Abdelaziz",
            "Sherif Abdou",
            "Hassanin Al-Barhamtoshy"
        ],
        "abstract": "Arabic handwriting is a consonantal and cursive writing. The analysis of Arabic script is further complicated due to obligatory dots/strokes that are placed above or below most letters and usually written delayed in order. Due to ambiguities and diversities of writing styles, recognition systems are generally based on a set of possible words called lexicon. When the lexicon is small, recognition accuracy is more important as the recognition time is minimal. On the other hand, recognition speed as well as the accuracy are both critical when handling large lexicons. Arabic is rich in morphology and syntax which makes its lexicon large. Therefore, a practical online handwriting recognition system should be able to handle a large lexicon with reasonable performance in terms of both accuracy and time. In this paper, we introduce a fully-fledged Hidden Markov Model (HMM) based system for Arabic online handwriting recognition that provides solutions for most of the difficulties inherent in recognizing the Arabic script. A new preprocessing technique for handling the delayed strokes is introduced. We use advanced modeling techniques for building our recognition system from the training data to provide more detailed representation for the differences between the writing units, minimize the variances between writers in the training data and have a better representation for the features space. System results are enhanced using an additional post-processing step with a higher order language model and cross-word HMM models. The system performance is evaluated using two different databases covering small and large lexicons. Our system outperforms the state-of-art systems for the small lexicon database. Furthermore, it shows promising results (accuracy and time) when supporting large lexicon with the possibility for adapting the models for specific writers to get even better results.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5058",
        "title": "Dense 3D Face Correspondence",
        "authors": [
            "Syed Zulqarnain Gilani",
            "Ajmal Mian",
            "Faisal Shafait",
            "Ian Reid"
        ],
        "abstract": "We present an algorithm that automatically establishes dense correspondences between a large number of 3D faces. Starting from automatically detected sparse correspondences on the outer boundary of 3D faces, the algorithm triangulates existing correspondences and expands them iteratively by matching points of distinctive surface curvature along the triangle edges. After exhausting keypoint matches, further correspondences are established by generating evenly distributed points within triangles by evolving level set geodesic curves from the centroids of large triangles. A deformable model (K3DM) is constructed from the dense corresponded faces and an algorithm is proposed for morphing the K3DM to fit unseen faces. This algorithm iterates between rigid alignment of an unseen face followed by regularized morphing of the deformable model. We have extensively evaluated the proposed algorithms on synthetic data and real 3D faces from the FRGCv2, Bosphorus, BU3DFE and UND Ear databases using quantitative and qualitative benchmarks. Our algorithm achieved dense correspondences with a mean localisation error of 1.28mm on synthetic faces and detected $14$ anthropometric landmarks on unseen real faces from the FRGCv2 database with 3mm precision. Furthermore, our deformable model fitting algorithm achieved 98.5% face recognition accuracy on the FRGCv2 and 98.6% on Bosphorus database. Our dense model is also able to generalize to unseen datasets.\n    ",
        "submission_date": "2014-10-19T00:00:00",
        "last_modified_date": "2019-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5224",
        "title": "Supervised mid-level features for word image representation",
        "authors": [
            "Albert Gordo"
        ],
        "abstract": "This paper addresses the problem of learning word image representations: given the cropped image of a word, we are interested in finding a descriptive, robust, and compact fixed-length representation. Machine learning techniques can then be supplied with these representations to produce models useful for word retrieval or recognition tasks. Although many works have focused on the machine learning aspect once a global representation has been produced, little work has been devoted to the construction of those base image representations: most works use standard coding and aggregation techniques directly on top of standard computer vision features such as SIFT or HOG.\n",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5263",
        "title": "Building pattern recognition applications with the SPARE library",
        "authors": [
            "Lorenzo Livi",
            "Guido Del Vescovo",
            "Antonello Rizzi",
            "Fabio Massimo Frattale Mascioli"
        ],
        "abstract": "This paper presents the SPARE C++ library, an open source software tool conceived to build pattern recognition and soft computing systems. The library follows the requirement of the generality: most of the implemented algorithms are able to process user-defined input data types transparently, such as labeled graphs and sequences of objects, as well as standard numeric vectors. Here we present a high-level picture of the SPARE library characteristics, focusing instead on the specific practical possibility of constructing pattern recognition systems for different input data types. In particular, as a proof of concept, we discuss two application instances involving clustering of real-valued multidimensional sequences and classification of labeled graphs.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5358",
        "title": "Remote sensing image classification exploiting multiple kernel learning",
        "authors": [
            "Claudio Cusano",
            "Paolo Napoletano",
            "Raimondo Schettini"
        ],
        "abstract": "We propose a strategy for land use classification which exploits Multiple Kernel Learning (MKL) to automatically determine a suitable combination of a set of features without requiring any heuristic knowledge about the classification task. We present a novel procedure that allows MKL to achieve good performance in the case of small training sets. Experimental results on publicly available datasets demonstrate the feasibility of the proposed approach.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5524",
        "title": "Learning to Rank Binary Codes",
        "authors": [
            "Jie Feng",
            "Wei Liu",
            "Yan Wang"
        ],
        "abstract": "Binary codes have been widely used in vision problems as a compact feature representation to achieve both space and time advantages. Various methods have been proposed to learn data-dependent hash functions which map a feature vector to a binary code. However, considerable data information is inevitably lost during the binarization step which also causes ambiguity in measuring sample similarity using Hamming distance. Besides, the learned hash functions cannot be changed after training, which makes them incapable of adapting to new data outside the training data set. To address both issues, in this paper we propose a flexible bitwise weight learning framework based on the binary codes obtained by state-of-the-art hashing methods, and incorporate the learned weights into the weighted Hamming distance computation. We then formulate the proposed framework as a ranking problem and leverage the Ranking SVM model to offline tackle the weight learning. The framework is further extended to an online mode which updates the weights at each time new data comes, thereby making it scalable to large and dynamic data sets. Extensive experimental results demonstrate significant performance gains of using binary codes with bitwise weighting in image retrieval tasks. It is appealing that the online weight learning leads to comparable accuracy with its offline counterpart, which thus makes our approach practical for realistic applications.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5600",
        "title": "Mobility Enhancement for Elderly",
        "authors": [
            "Ramviyas Parasuraman"
        ],
        "abstract": "Loss of Mobility is a common handicap to senior citizens. It denies them the ease of movement they would like to have like outdoor visits, movement in hospitals, social outgoings, but more seriously in the day to day in-house routine functions necessary for living etc. Trying to overcome this handicap by means of servant or domestic help and simple wheel chairs is not only costly in the long run, but forces the senior citizen to be at the mercy of sincerity of domestic helps and also the consequent loss of dignity. In order to give a dignified life, the mobility obtained must be at the complete discretion, will and control of the senior citizen. This can be provided only by a reasonably sophisticated and versatile wheel chair, giving enhanced ability of vision, hearing through man-machine interface, and sensor aided navigation and control. More often than not senior people have poor vision which makes it difficult for them to maker visual judgement and so calls for the use of Artificial Intelligence in visual image analysis and guided navigation systems.\n",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5605",
        "title": "Attentive monitoring of multiple video streams driven by a Bayesian foraging strategy",
        "authors": [
            "Paolo Napoletano",
            "Giuseppe Boccignone",
            "Francesco Tisato"
        ],
        "abstract": "In this paper we shall consider the problem of deploying attention to subsets of the video streams for collating the most relevant data and information of interest related to a given task. We formalize this monitoring problem as a foraging problem. We propose a probabilistic framework to model observer's attentive behavior as the behavior of a forager. The forager, moment to moment, focuses its attention on the most informative stream/camera, detects interesting objects or activities, or switches to a more profitable stream. The approach proposed here is suitable to be exploited for multi-stream video summarization. Meanwhile, it can serve as a preliminary step for more sophisticated video surveillance, e.g. activity and behavior analysis. Experimental results achieved on the UCR Videoweb Activities Dataset, a publicly available dataset, are presented to illustrate the utility of the proposed technique.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5861",
        "title": "Compositional Structure Learning for Action Understanding",
        "authors": [
            "Ran Xu",
            "Gang Chen",
            "Caiming Xiong",
            "Wei Chen",
            "Jason J. Corso"
        ],
        "abstract": "The focus of the action understanding literature has predominately been classification, how- ever, there are many applications demanding richer action understanding such as mobile robotics and video search, with solutions to classification, localization and detection. In this paper, we propose a compositional model that leverages a new mid-level representation called compositional trajectories and a locally articulated spatiotemporal deformable parts model (LALSDPM) for fully action understanding. Our methods is advantageous in capturing the variable structure of dynamic human activity over a long range. First, the compositional trajectories capture long-ranging, frequently co-occurring groups of trajectories in space time and represent them in discriminative hierarchies, where human motion is largely separated from camera motion; second, LASTDPM learns a structured model with multi-layer deformable parts to capture multiple levels of articulated motion. We implement our methods and demonstrate state of the art performance on all three problems: action detection, localization, and recognition.\n    ",
        "submission_date": "2014-10-21T00:00:00",
        "last_modified_date": "2014-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5894",
        "title": "Vehicle Detection and Tracking Techniques: A Concise Review",
        "authors": [
            "Raad Ahmed Hadi",
            "Ghazali Sulong",
            "Loay Edwar George"
        ],
        "abstract": "Vehicle detection and tracking applications play an important role for civilian and military applications such as in highway traffic surveillance control, management and urban traffic planning. Vehicle detection process on road are used for vehicle tracking, counts, average speed of each individual vehicle, traffic analysis and vehicle categorizing objectives and may be implemented under different environments changes. In this review, we present a concise overview of image processing methods and analysis tools which used in building these previous mentioned applications that involved developing traffic surveillance systems. More precisely and in contrast with other reviews, we classified the processing methods under three categories for more clarification to explain the traffic systems.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2014-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.5926",
        "title": "Salient Object Detection: A Discriminative Regional Feature Integration Approach",
        "authors": [
            "Huaizu Jiang",
            "Zejian Yuan",
            "Ming-Ming Cheng",
            "Yihong Gong",
            "Nanning Zheng",
            "Jingdong Wang"
        ],
        "abstract": "Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. Saliency scores across multiple levels are finally fused to produce the saliency map. The contributions lie in two-fold. One is that we propose a discriminate regional feature integration approach for salient object detection. Compared with existing heuristic models, our proposed method is able to automatically integrate high-dimensional regional saliency features and choose discriminative ones. The other is that by investigating standard generic region properties as well as two widely studied concepts for salient object detection, i.e., regional contrast and backgroundness, our approach significantly outperforms state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate that our method runs as fast as most existing algorithms.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2014-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6126",
        "title": "Motion Estimation via Robust Decomposition with Constrained Rank",
        "authors": [
            "German Ros",
            "Jose Alvarez",
            "Julio Guerrero"
        ],
        "abstract": "In this work, we address the problem of outlier detection for robust motion estimation by using modern sparse-low-rank decompositions, i.e., Robust PCA-like methods, to impose global rank constraints. Robust decompositions have shown to be good at splitting a corrupted matrix into an uncorrupted low-rank matrix and a sparse matrix, containing outliers. However, this process only works when matrices have relatively low rank with respect to their ambient space, a property not met in motion estimation problems. As a solution, we propose to exploit the partial information present in the decomposition to decide which matches are outliers. We provide evidences showing that even when it is not possible to recover an uncorrupted low-rank matrix, the resulting information can be exploited for outlier detection. To this end we propose the Robust Decomposition with Constrained Rank (RD-CR), a proximal gradient based method that enforces the rank constraints inherent to motion estimation. We also present a general framework to perform robust estimation for stereo Visual Odometry, based on our RD-CR and a simple but effective compressed optimization method that achieves high performance. Our evaluation on synthetic data and on the KITTI dataset demonstrates the applicability of our approach in complex scenarios and it yields state-of-the-art performance.\n    ",
        "submission_date": "2014-10-22T00:00:00",
        "last_modified_date": "2014-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6264",
        "title": "Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features",
        "authors": [
            "Alessandro Perina",
            "Nebojsa Jojic"
        ],
        "abstract": "In recent scene recognition research images or large image regions are often represented as disorganized \"bags\" of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. However, image feature counts are likely to be constrained in different ways than word counts in text. For example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky Fig. 1, some feature counts in the image drop while others rise -- only to drop again giving way to features found more often at higher elevations. The space of all possible feature count combinations is constrained both by the properties of the larger scene and the size and the location of the window into it. To capture such variation, in this paper we propose the use of the counting grid model. This generative model is based on a grid of feature counts, considerably larger than any of the modeled images, and considerably smaller than the real estate needed to tile the images next to each other tightly. Each modeled image is assumed to have a representative window in the grid in which the feature counts mimic the feature distribution in the image. We provide a learning procedure that jointly maps all images in the training set to the counting grid and estimates the appropriate local counts in it. Experimentally, we demonstrate that the resulting representation captures the space of feature count combinations more accurately than the traditional models, not only when the input images come from a panning camera, but even when modeling images of different scenes from the same category.\n    ",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6313",
        "title": "Canonical Polyadic Decomposition with Auxiliary Information for Brain Computer Interface",
        "authors": [
            "Junhua Li",
            "Chao Li",
            "Andrzej Cichocki"
        ],
        "abstract": "Physiological signals are often organized in the form of multiple dimensions (e.g., channel, time, task, and 3D voxel), so it is better to preserve original organization structure when processing. Unlike vector-based methods that destroy data structure, Canonical Polyadic Decomposition (CPD) aims to process physiological signals in the form of multi-way array, which considers relationships between dimensions and preserves structure information contained by the physiological signal. Nowadays, CPD is utilized as an unsupervised method for feature extraction in a classification problem. After that, a classifier, such as support vector machine, is required to classify those features. In this manner, classification task is achieved in two isolated steps. We proposed supervised Canonical Polyadic Decomposition by directly incorporating auxiliary label information during decomposition, by which a classification task can be achieved without an extra step of classifier training. The proposed method merges the decomposition and classifier learning together, so it reduces procedure of classification task compared with that of respective decomposition and classification. In order to evaluate the performance of the proposed method, three different kinds of signals, synthetic signal, EEG signal, and MEG signal, were used. The results based on evaluations of synthetic and real signals demonstrated that the proposed method is effective and efficient.\n    ",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6333",
        "title": "A Regularization Approach to Blind Deblurring and Denoising of QR Barcodes",
        "authors": [
            "Yves van Gennip",
            "Prashant Athavale",
            "J\u00e9r\u00f4me Gilles",
            "Rustum Choksi"
        ],
        "abstract": "QR bar codes are prototypical images for which part of the image is a priori known (required patterns). Open source bar code readers, such as ZBar, are readily available. We exploit both these facts to provide and assess purely regularization-based methods for blind deblurring of QR bar codes in the presence of noise.\n    ",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2017-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6447",
        "title": "Density-Based Region Search with Arbitrary Shape for Object Localization",
        "authors": [
            "Ji Zhao",
            "Deyu Meng",
            "Jiayi Ma"
        ],
        "abstract": "Region search is widely used for object localization. Typically, the region search methods project the score of a classifier into an image plane, and then search the region with the maximal score. The recently proposed region search methods, such as efficient subwindow search and efficient region search, %which localize objects from the score distribution on an image are much more efficient than sliding window search. However, for some classifiers and tasks, the projected scores are nearly all positive, and hence maximizing the score of a region results in localizing nearly the entire images as objects, which is meaningless.\n",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6472",
        "title": "Foreground-Background Segmentation Based on Codebook and Edge Detector",
        "authors": [
            "Mika\u00ebl A. Mousse",
            "Eug\u00e8ne C. Ezin",
            "Cina Motamed"
        ],
        "abstract": "Background modeling techniques are used for moving object detection in video. Many algorithms exist in the field of object detection with different purposes. In this paper, we propose an improvement of moving object detection based on codebook segmentation. We associate the original codebook algorithm with an edge detection algorithm. Our goal is to prove the efficiency of using an edge detection algorithm with a background modeling algorithm. Throughout our study, we compared the quality of the moving object detection when codebook segmentation algorithm is associated with some standard edge detectors. In each case, we use frame-based metrics for the evaluation of the detection. The different results are presented and analyzed.\n    ",
        "submission_date": "2014-10-23T00:00:00",
        "last_modified_date": "2014-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6532",
        "title": "A Novel Visual Word Co-occurrence Model for Person Re-identification",
        "authors": [
            "Ziming Zhang",
            "Yuting Chen",
            "Venkatesh Saligrama"
        ],
        "abstract": "Person re-identification aims to maintain the identity of an individual in diverse locations through different non-overlapping camera views. The problem is fundamentally challenging due to appearance variations resulting from differing poses, illumination and configurations of camera views. To deal with these difficulties, we propose a novel visual word co-occurrence model. We first map each pixel of an image to a visual word using a codebook, which is learned in an unsupervised manner. The appearance transformation between camera views is encoded by a co-occurrence matrix of visual word joint distributions in probe and gallery images. Our appearance model naturally accounts for spatial similarities and variations caused by pose, illumination & configuration change across camera views. Linear SVMs are then trained as classifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campus benchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on the Cumulative Match Characteristic (CMC) curves, and beats the state-of-the-art results by 10.44% and 22.27%.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6736",
        "title": "On The Effect of Hyperedge Weights On Hypergraph Learning",
        "authors": [
            "Sheng Huang",
            "Ahmed Elgammal",
            "Dan Yang"
        ],
        "abstract": "Hypergraph is a powerful representation in several computer vision, machine learning and pattern recognition problems. In the last decade, many researchers have been keen to develop different hypergraph models. In contrast, no much attention has been paid to the design of hyperedge weights. However, many studies on pairwise graphs show that the choice of edge weight can significantly influence the performances of such graph algorithms. We argue that this also applies to hypegraphs. In this paper, we empirically discuss the influence of hyperedge weight on hypegraph learning via proposing three novel hyperedge weights from the perspectives of geometry, multivariate statistical analysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE, Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar to graph learning, several representative hyperedge weighting schemes can be concluded by our experimental studies. Moreover, the experiments also demonstrate that the combinations of such weighting schemes and conventional hypergraph models can get very promising classification and clustering performances in comparison with some recent state-of-the-art algorithms.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6751",
        "title": "Detecting Figures and Part Labels in Patents: Competition-Based Development of Image Processing Algorithms",
        "authors": [
            "Christoph Riedl",
            "Richard Zanibbi",
            "Marti A. Hearst",
            "Siyu Zhu",
            "Michael Menietti",
            "Jason Crusan",
            "Ivan Metelsky",
            "Karim R. Lakhani"
        ],
        "abstract": "We report the findings of a month-long online competition in which participants developed algorithms for augmenting the digital version of patent documents published by the United States Patent and Trademark Office (USPTO). The goal was to detect figures and part labels in U.S. patent drawing pages. The challenge drew 232 teams of two, of which 70 teams (30%) submitted solutions. Collectively, teams submitted 1,797 solutions that were compiled on the competition servers. Participants reported spending an average of 63 hours developing their solutions, resulting in a total of 5,591 hours of development time. A manually labeled dataset of 306 patents was used for training, online system tests, and evaluation. The design and performance of the top-5 systems are presented, along with a system developed after the competition which illustrates that winning teams produced near state-of-the-art results under strict time and computation constraints. For the 1st place system, the harmonic mean of recall and precision (f-measure) was 88.57% for figure region detection, 78.81% for figure regions with correctly recognized figure titles, and 70.98% for part label detection and character recognition. Data and software from the competition are available through the online UCI Machine Learning repository to inspire follow-on work by the image processing community.\n    ",
        "submission_date": "2014-10-24T00:00:00",
        "last_modified_date": "2014-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6909",
        "title": "A Framework for On-Line Devanagari Handwritten Character Recognition",
        "authors": [
            "Sunil Kumar Kopparapu",
            "Lajish V. L"
        ],
        "abstract": "The main challenge in on-line handwritten character recognition in Indian lan- guage is the large size of the character set, larger similarity between different characters in the script and the huge variation in writing style. In this paper we propose a framework for on-line handwitten script recognition taking cues from speech signal processing literature. The framework is based on identify- ing strokes, which in turn lead to recognition of handwritten on-line characters rather that the conventional character identification. Though the framework is described for Devanagari script, the framework is general and can be applied to any language.\n",
        "submission_date": "2014-10-25T00:00:00",
        "last_modified_date": "2014-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7164",
        "title": "Directional Bilateral Filters",
        "authors": [
            "Manasij Venkatesh",
            "Chandra Sekhar Seelamantula"
        ],
        "abstract": "We propose a bilateral filter with a locally controlled domain kernel for directional edge-preserving smoothing. Traditional bilateral filters use a range kernel, which is responsible for edge preservation, and a fixed domain kernel that performs smoothing. Our intuition is that orientation and anisotropy of image structures should be incorporated into the domain kernel while smoothing. For this purpose, we employ an oriented Gaussian domain kernel locally controlled by a structure tensor. The oriented domain kernel combined with a range kernel forms the directional bilateral filter. The two kernels assist each other in effectively suppressing the influence of the outliers while smoothing. To find the optimal parameters of the directional bilateral filter, we propose the use of Stein's unbiased risk estimate (SURE). We test the capabilities of the kernels separately as well as together, first on synthetic images, and then on real endoscopic images. The directional bilateral filter has better denoising performance than the Gaussian bilateral filter at various noise levels in terms of peak signal-to-noise ratio (PSNR).\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7211",
        "title": "A method for context-based adaptive QRS clustering in real-time",
        "authors": [
            "Daniel Castro",
            "Paulo F\u00e9lix",
            "Jes\u00fas Presedo"
        ],
        "abstract": "Continuous follow-up of heart condition through long-term electrocardiogram monitoring is an invaluable tool for diagnosing some cardiac arrhythmias. In such context, providing tools for fast locating alterations of normal conduction patterns is mandatory and still remains an open issue. This work presents a real-time method for adaptive clustering QRS complexes from multilead ECG signals that provides the set of QRS morphologies that appear during an ECG recording. The method processes the QRS complexes sequentially, grouping them into a dynamic set of clusters based on the information content of the temporal context. The clusters are represented by templates which evolve over time and adapt to the QRS morphology changes. Rules to create, merge and remove clusters are defined along with techniques for noise detection in order to avoid their proliferation. To cope with beat misalignment, Derivative Dynamic Time Warping is used. The proposed method has been validated against the MIT-BIH Arrhythmia Database and the AHA ECG Database showing a global purity of 98.56% and 99.56%, respectively. Results show that our proposal not only provides better results than previous offline solutions but also fulfills real-time requirements.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7252",
        "title": "Iris Biometric System using a hybrid approach",
        "authors": [
            "Abhimanyu Sarin",
            "Dr. Jagadish Nayak"
        ],
        "abstract": "Iris Recognition Systems are ocular- based biometric devices used primarily for security reasons. The complexity and the randomness of the Iris, amongst various other factors, ensure that this biometric system is inarguably an exact and reliable method of identification. The algorithm is responsible for automatic localization and segmentation of boundaries using circular Hough Transform, noise reductions, image enhancement and feature extraction across numerous distinct images present in the database. This paper delves into the various kinds of techniques required to approximate the pupillary and limbic boundaries of the enrolled iris image, captured using a suitable image acquisition device and perform feature extraction on the normalized iris image with the help of Haar Wavelets to encode the input data into a binary string format. These techniques were validated using images from the CASIA database, and various other procedures were also tried and tested.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7265",
        "title": "An Unsupervised Ensemble-based Markov Random Field Approach to Microscope Cell Image Segmentation",
        "authors": [
            "Balint Antal",
            "Bence Remenyik",
            "Andras Hajdu"
        ],
        "abstract": "In this paper, we propose an approach to the unsupervised segmentation of images using Markov Random Field. The proposed approach is based on the idea of Bit Plane Slicing. We use the planes as initial labellings for an ensemble of segmentations. With pixelwise voting, a robust segmentation approach can be achieved, which we demonstrate on microscope cell images. We tested our approach on a publicly available database, where it proven to be competitive with other methods and manual segmentation.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7376",
        "title": "Visual Chunking: A List Prediction Framework for Region-Based Object Detection",
        "authors": [
            "Nicholas Rhinehart",
            "Jiaji Zhou",
            "Martial Hebert",
            "J. Andrew Bagnell"
        ],
        "abstract": "We consider detecting objects in an image by iteratively selecting from a set of arbitrarily shaped candidate regions. Our generic approach, which we term visual chunking, reasons about the locations of multiple object instances in an image while expressively describing object boundaries. We design an optimization criterion for measuring the performance of a list of such detections as a natural extension to a common per-instance metric. We present an efficient algorithm with provable performance for building a high-quality list of detections from any candidate set of region-based proposals. We also develop a simple class-specific algorithm to generate a candidate region instance in near-linear time in the number of low-level superpixels that outperforms other region generating methods. In order to make predictions on novel images at testing time without access to ground truth, we develop learning approaches to emulate these algorithms' behaviors. We demonstrate that our new approach outperforms sophisticated baselines on benchmark datasets.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7429",
        "title": "Higher-order MRFs based image super resolution: why not MAP?",
        "authors": [
            "Yunjin Chen"
        ],
        "abstract": "A trainable filter-based higher-order Markov Random Fields (MRFs) model - the so called Fields of Experts (FoE), has proved a highly effective image prior model for many classic image restoration problems. Generally, two options are available to incorporate the learned FoE prior in the inference procedure: (1) sampling-based minimum mean square error (MMSE) estimate, and (2) energy minimization-based maximum a posteriori (MAP) estimate. This letter is devoted to the FoE prior based single image super resolution (SR) problem, and we suggest to make use of the MAP estimate for inference based on two facts: (I) It is well-known that the MAP inference has a remarkable advantage of high computational efficiency, while the sampling-based MMSE estimate is very time consuming. (II) Practical SR experiment results demonstrate that the MAP estimate works equally well compared to the MMSE estimate with exactly the same FoE prior model. Moreover, it can lead to even further improvements by incorporating our discriminatively trained FoE prior model. In summary, we hold that for higher-order natural image prior based SR problem, it is better to employ the MAP estimate for inference.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7452",
        "title": "Consensus Message Passing for Layered Graphical Models",
        "authors": [
            "Varun Jampani",
            "S. M. Ali Eslami",
            "Daniel Tarlow",
            "Pushmeet Kohli",
            "John Winn"
        ],
        "abstract": "Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus' messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2015-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7454",
        "title": "Deep Structured learning for mass segmentation from Mammograms",
        "authors": [
            "Neeraj Dhungel",
            "Gustavo Carneiro",
            "Andrew P. Bradley"
        ],
        "abstract": "In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to as- sess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computa- tion of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7484",
        "title": "Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic Sampling",
        "authors": [
            "Tianfei Zhou",
            "Yao Lu",
            "Feng Lv",
            "Huijun Di",
            "Qingjie Zhao",
            "Jian Zhang"
        ],
        "abstract": "Stochastic sampling based trackers have shown good performance for abrupt motion tracking so that they have gained popularity in recent years. However, conventional methods tend to use a two-stage sampling paradigm, in which the search space needs to be uniformly explored with an inefficient preliminary sampling phase. In this paper, we propose a novel sampling-based method in the Bayesian filtering framework to address the problem. Within the framework, nearest neighbor field estimation is utilized to compute the importance proposal probabilities, which guide the Markov chain search towards promising regions and thus enhance the sampling efficiency; given the motion priors, a smoothing stochastic sampling Monte Carlo algorithm is proposed to approximate the posterior distribution through a smoothing weight-updating scheme. Moreover, to track the abrupt and the smooth motions simultaneously, we develop an abrupt-motion detection scheme which can discover the presence of abrupt motions during online tracking. Extensive experiments on challenging image sequences demonstrate the effectiveness and the robustness of our algorithm in handling the abrupt motions.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7580",
        "title": "Robust Piecewise-Constant Smoothing: M-Smoother Revisited",
        "authors": [
            "Linchao Bao",
            "Qingxiong Yang"
        ],
        "abstract": "A robust estimator, namely M-smoother, for piecewise-constant smoothing is revisited in this paper. Starting from its generalized formulation, we propose a numerical scheme/framework for solving it via a series of weighted-average filtering (e.g., box filtering, Gaussian filtering, bilateral filtering, and guided filtering). Because of the equivalence between M-smoother and local-histogram-based filters (such as median filter and mode filter), the proposed framework enables fast approximation of histogram filters via a number of box filtering or Gaussian filtering. In addition, high-quality piecewise-constant smoothing can be achieved via a number of bilateral filtering or guided filtering integrated in the proposed framework. Experiments on depth map denoising show the effectiveness of our framework.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2017-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7632",
        "title": "On the Covariance of ICP-based Scan-matching Techniques",
        "authors": [
            "Silv\u00e8re Bonnabel",
            "Martin Barczyk",
            "Fran\u00e7ois Goulette"
        ],
        "abstract": "This paper considers the problem of estimating the covariance of roto-translations computed by the Iterative Closest Point (ICP) algorithm. The problem is relevant for localization of mobile robots and vehicles equipped with depth-sensing cameras (e.g., Kinect) or Lidar (e.g., Velodyne). The closed-form formulas for covariance proposed in previous literature generally build upon the fact that the solution to ICP is obtained by minimizing a linear least-squares problem. In this paper, we show this approach needs caution because the rematching step of the algorithm is not explicitly accounted for, and applying it to the point-to-point version of ICP leads to completely erroneous covariances. We then provide a formal mathematical proof why the approach is valid in the point-to-plane version of ICP, which validates the intuition and experimental results of practitioners.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7679",
        "title": "Super-resolution method using sparse regularization for point-spread function recovery",
        "authors": [
            "Fred Maurice Ngol\u00e8 Mboula",
            "Jean-Luc Starck",
            "Samuel Ronayette",
            "Koryo Okumura",
            "J\u00e9r\u00f4me Amiaux"
        ],
        "abstract": "In large-scale spatial surveys, such as the forthcoming ESA Euclid mission, images may be undersampled due to the optical sensors sizes. Therefore, one may consider using a super-resolution (SR) method to recover aliased frequencies, prior to further analysis. This is particularly relevant for point-source images, which provide direct measurements of the instrument point-spread function (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse, which is an SR algorithm using a sparse analysis prior. We show that such a prior provides significant improvements over existing methods, especially on low SNR PSFs.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7730",
        "title": "New similarity index based on entropy and group theory",
        "authors": [
            "Yasel Garc\u00e9s",
            "Esley Torres",
            "Osvaldo Pereira",
            "Roberto Rodr\u00edguez"
        ],
        "abstract": "In this work, we propose a new similarity index for images considering the entropy function and group theory. This index considers an algebraic group of images, it is defined by an inner law that provides a novel approach for the subtraction of images. Through an equivalence relationship in the field of images, we prove the existence of the quotient group, on which the new similarity index is defined. We also present the main properties of the new index, and the immediate application thereof as a stopping criterion of the \"Mean Shift Iterative Algorithm\".\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7762",
        "title": "A hierarchical framework for object recognition",
        "authors": [
            "Reza Moazzezi"
        ],
        "abstract": "Object recognition in the presence of background clutter and distractors is a central problem both in neuroscience and in machine learning. However, the performance level of the models that are inspired by cortical mechanisms, including deep networks such as convolutional neural networks and deep belief networks, is shown to significantly decrease in the presence of noise and background objects [19, 24]. Here we develop a computational framework that is hierarchical, relies heavily on key properties of the visual cortex including mid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT) [4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18] and the prior knowledge that is built into cortical circuits (such as the emergence of edge detector neurons in primary visual cortex before the onset of the visual experience) [1, 21], and addresses the problem of object recognition in the presence of background noise and distractors. Our approach is specifically designed to address large deformations, allows flexible communication between different layers of representation and learns highly selective filters from a small number of training examples.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7876",
        "title": "Collaborative Multi-sensor Classification via Sparsity-based Representation",
        "authors": [
            "Minh Dao",
            "Nam H. Nguyen",
            "Nasser M. Nasrabadi",
            "Trac D. Tran"
        ],
        "abstract": "In this paper, we propose a general collaborative sparse representation framework for multi-sensor classification, which takes into account the correlations as well as complementary information between heterogeneous sensors simultaneously while considering joint sparsity within each sensor's observations. We also robustify our models to deal with the presence of sparse noise and low-rank interference signals. Specifically, we demonstrate that incorporating the noise or interference signal as a low-rank component in our models is essential in a multi-sensor classification problem when multiple co-located sources/sensors simultaneously record the same physical event. We further extend our frameworks to kernelized models which rely on sparsely representing a test sample in terms of all the training samples in a feature space induced by a kernel function. A fast and efficient algorithm based on alternative direction method is proposed where its convergence to an optimal solution is guaranteed. Extensive experiments are conducted on several real multi-sensor data sets and results are compared with the conventional classifiers to verify the effectiveness of the proposed methods.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7922",
        "title": "Extended Dynamic Programming and Fast Multidimensional Search Algorithm for Energy Minization in Stereo and Motion",
        "authors": [
            "Mikhail G. Mozerov"
        ],
        "abstract": "This paper presents a novel extended dynamic programming approach for energy minimization (EDP) to solve the correspondence problem for stereo and motion. A significant speedup is achieved using a recursive minimum search strategy (RMS). The mentioned speedup is particularly important if the disparity space is 2D as well as 3D. The proposed RMS can also be applied in the well-known dynamic programming (DP) approach for stereo and motion. In this case, the general 2D problem of the global discrete energy minimization is reduced to several mutually independent sub-problems of the one-dimensional minimization. The EDP method is used when the approximation of the general 2D discrete energy minimization problem is considered. Then the RMS algorithm is an essential part of the EDP method. Using the EDP algorithm we obtain a lower energy bound than the graph cuts (GC) expansion technique on stereo and motion problems. The proposed calculation scheme possesses natural parallelism and can be realized on graphics processing unit (GPU) platforms, and can be potentially restricted further by the number of scanlines in the image plane. Furthermore, the RMS and EDP methods can be used in any optimization problem where the objective function meets specific conditions in the smoothness term.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8151",
        "title": "A comparison of dense region detectors for image search and fine-grained classification",
        "authors": [
            "Ahmet Iscen",
            "Giorgos Tolias",
            "Philippe-Henri Gosselin",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "We consider a pipeline for image classification or search based on coding approaches like Bag of Words or Fisher vectors. In this context, the most common approach is to extract the image patches regularly in a dense manner on several scales. This paper proposes and evaluates alternative choices to extract patches densely. Beyond simple strategies derived from regular interest region detectors, we propose approaches based on super-pixels, edges, and a bank of Zernike filters used as detectors. The different approaches are evaluated on recent image retrieval and fine-grain classification benchmarks. Our results show that the regular dense detector is outperformed by other methods in most situations, leading us to improve the state of the art in comparable setups on standard retrieval and fined-grain benchmarks. As a byproduct of our study, we show that existing methods for blob and super-pixel extraction achieve high accuracy if the patches are extracted along the edges and not around the detected regions.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8546",
        "title": "A Solution for Multi-Alignment by Transformation Synchronisation",
        "authors": [
            "Florian Bernard",
            "Johan Thunberg",
            "Peter Gemmar",
            "Frank Hertel",
            "Andreas Husch",
            "Jorge Goncalves"
        ],
        "abstract": "The alignment of a set of objects by means of transformations plays an important role in computer vision. Whilst the case for only two objects can be solved globally, when multiple objects are considered usually iterative methods are used. In practice the iterative methods perform well if the relative transformations between any pair of objects are free of noise. However, if only noisy relative transformations are available (e.g. due to missing data or wrong correspondences) the iterative methods may fail.\n",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8576",
        "title": "An ensemble-based system for automatic screening of diabetic retinopathy",
        "authors": [
            "Balint Antal",
            "Andras Hajdu"
        ],
        "abstract": "In this paper, an ensemble-based method for the screening of diabetic retinopathy (DR) is proposed. This approach is based on features extracted from the output of several retinal image processing algorithms, such as image-level (quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms, exudates) and anatomical (macula, optic disc) components. The actual decision about the presence of the disease is then made by an ensemble of machine learning classifiers. We have tested our approach on the publicly available Messidor database, where 90% sensitivity, 91% specificity and 90% accuracy and 0.989 AUC are achieved in a disease/no-disease setting. These results are highly competitive in this field and suggest that retinal image processing is a valid approach for automatic DR screening.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8577",
        "title": "An Ensemble-based System for Microaneurysm Detection and Diabetic Retinopathy Grading",
        "authors": [
            "Balint Antal",
            "Andras Hajdu"
        ],
        "abstract": "Reliable microaneurysm detection in digital fundus images is still an open issue in medical image processing. We propose an ensemble-based framework to improve microaneurysm detection. Unlike the well-known approach of considering the output of multiple classifiers, we propose a combination of internal components of microaneurysm detectors, namely preprocessing methods and candidate extractors. We have evaluated our approach for microaneurysm detection in an online competition, where this algorithm is currently ranked as first and also on two other databases. Since microaneurysm detection is decisive in diabetic retinopathy grading, we also tested the proposed method for this task on the publicly available Messidor database, where a promising AUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification based on the presence or absence of the microaneurysms.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8586",
        "title": "DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks",
        "authors": [
            "Tao Chen",
            "Damian Borth",
            "Trevor Darrell",
            "Shih-Fu Chang"
        ],
        "abstract": "This paper introduces a visual sentiment concept classification method based on deep convolutional neural networks (CNNs). The visual sentiment concepts are adjective noun pairs (ANPs) automatically discovered from the tags of web photos, and can be utilized as effective statistical cues for detecting emotions depicted in the images. Nearly one million Flickr images tagged with these ANPs are downloaded to train the classifiers of the concepts. We adopt the popular model of deep convolutional neural networks which recently shows great performance improvement on classifying large-scale web-based image dataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a newly developed deep learning framework. To deal with the biased training data which only contains images with strong sentiment and to prevent overfitting, we initialize the model with the model weights trained from ImageNet. Performance evaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called DeepSentiBank) is significantly improved in both annotation accuracy and retrieval performance, compared to its predecessors which mainly use binary SVM classification models.\n    ",
        "submission_date": "2014-10-30T00:00:00",
        "last_modified_date": "2014-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8618",
        "title": "Symmetric low-rank representation for subspace clustering",
        "authors": [
            "Jie Chen",
            "Haixian Zhang",
            "Hua Mao",
            "Yongsheng Sang",
            "Zhang Yi"
        ],
        "abstract": "We propose a symmetric low-rank representation (SLRR) method for subspace clustering, which assumes that a data set is approximately drawn from the union of multiple subspaces. The proposed technique can reveal the membership of multiple subspaces through the self-expressiveness property of the data. In particular, the SLRR method considers a collaborative representation combined with low-rank matrix recovery techniques as a low-rank representation to learn a symmetric low-rank representation, which preserves the subspace structures of high-dimensional data. In contrast to performing iterative singular value decomposition in some existing low-rank representation based algorithms, the symmetric low-rank representation in the SLRR method can be calculated as a closed form solution by solving the symmetric low-rank optimization problem. By making use of the angular information of the principal directions of the symmetric low-rank representation, an affinity graph matrix is constructed for spectral clustering. Extensive experimental results show that it outperforms state-of-the-art subspace clustering algorithms.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8623",
        "title": "Addressing the non-functional requirements of computer vision systems: A case study",
        "authors": [
            "Shannon Fenn",
            "Alexandre Mendes",
            "David Budden"
        ],
        "abstract": "Computer vision plays a major role in the robotics industry, where vision data is frequently used for navigation and high-level decision making. Although there is significant research in algorithms and functional requirements, there is a comparative lack of emphasis on how best to map these abstract concepts onto an appropriate software architecture.\n",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0022",
        "title": "Generalized Adaptive Dictionary Learning via Domain Shift Minimization",
        "authors": [
            "Varun Panaganti"
        ],
        "abstract": "Visual data driven dictionaries have been successfully employed for various object recognition and classification tasks. However, the task becomes more challenging if the training and test data are from contrasting domains. In this paper, we propose a novel and generalized approach towards learning an adaptive and common dictionary for multiple domains. Precisely, we project the data from different domains onto a low dimensional space while preserving the intrinsic structure of data from each domain. We also minimize the domain-shift among the data from each pair of domains. Simultaneously, we learn a common adaptive dictionary. Our algorithm can also be modified to learn class-specific dictionaries which can be used for classification. We additionally propose a discriminative manifold regularization which imposes the intrinsic structure of class specific features onto the sparse coefficients. Experiments on image classification show that our approach fares better compared to the existing state-of-the-art methods.\n    ",
        "submission_date": "2014-10-31T00:00:00",
        "last_modified_date": "2014-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0085",
        "title": "Complex Events Recognition under Uncertainty in a Sensor Network",
        "authors": [
            "Atul Kanaujia",
            "Tae Eun Choe",
            "Hongli Deng"
        ],
        "abstract": "Automated extraction of semantic information from a network of sensors for cognitive analysis and human-like reasoning is a desired capability in future ground surveillance systems. We tackle the problem of complex decision making under uncertainty in network information environment, where lack of effective visual processing tools, incomplete domain knowledge frequently cause uncertainty in the visual primitives, leading to sub-optimal decisions. While state-of-the-art vision techniques exist in detecting visual entities (humans, vehicles and scene elements) in an image, a missing functionality is the ability to merge the information to reveal meaningful information for high level inference. In this work, we develop a probabilistic first order predicate logic(FOPL) based reasoning system for recognizing complex events in synchronized stream of videos, acquired from sensors with non-overlapping fields of view. We adopt Markov Logic Network(MLN) as a tool to model uncertainty in observations, and fuse information extracted from heterogeneous data in a probabilistically consistent way. MLN overcomes strong dependence on pure empirical learning by incorporating domain knowledge, in the form of user-defined rules and confidences associated with them. This work demonstrates that the MLN based decision control system can be made scalable to model statistical relations between a variety of entities and over long video sequences. Experiments with real-world data, under a variety of settings, illustrate the mathematical soundness and wide-ranging applicability of our approach.\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0126",
        "title": "Detection of texts in natural images",
        "authors": [
            "Gowtham Rangarajan Raman"
        ],
        "abstract": "A framework that makes use of Connected components and supervised Support machine to recognise texts is proposed. The image is preprocessed and and edge graph is calculated using a probabilistic framework to compensate for photometric noise. Connected components over the resultant image is calculated, which is bounded and then pruned using geometric constraints. Finally a Gabor Feature based SVM is used to classify the presence of text in the candidates. The proposed method was tested with ICDAR 10 dataset and few other images available on the internet. It resulted in a recall and precision metric of 0.72 and 0.88 comfortably better than the benchmark Eiphstein's algorithm. The proposed method recorded a 0.70 and 0.74 in natural images which is significantly better than current methods on natural images. The proposed method also scales almost linearly for high resolution, cluttered images.\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0130",
        "title": "A Two-phase Decision Support Framework for the Automatic Screening of Digital Fundus Images",
        "authors": [
            "Balint Antal",
            "Andras Hajdu",
            "Zsuzsanna Maros-Szabo",
            "Zsolt Torok",
            "Adrienne Csutak",
            "Tunde Peto"
        ],
        "abstract": "In this paper we give a brief review on the present status of automated detection systems describe for the screening of diabetic retinopathy. We further detail an enhanced detection procedure that consists of two steps. First, a pre-screening algorithm is considered to classify the input digital fundus images based on the severity of abnormalities. If an image is found to be seriously abnormal, it will not be analysed further with robust lesion detector algorithms. As a further improvement, we introduce a novel feature extraction approach based on clinical observations. The second step of the proposed method detects regions of interest with possible lesions on the images that previously passed the pre-screening step. These regions will serve as input to the specific lesion detectors for detailed analysis. This procedure can increase the computational performance of a screening system. Experimental results show that both two steps of the proposed approach are capable to efficiently exclude a large amount of data from further processing, thus, to decrease the computational burden of the automatic screening system.\n    ",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0326",
        "title": "High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging",
        "authors": [
            "Corneliu Florea",
            "Constantin Vertan",
            "Laura Florea"
        ],
        "abstract": "In this paper we emphasize a similarity between the Logarithmic-Type Image Processing (LTIP) model and the Naka-Rushton model of the Human Visual System (HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), which further replaces the logarithmic function with a ratio of polynomial functions. Based on this similarity, we show that it is possible to present an unifying framework for the High Dynamic Range (HDR) imaging problem, namely that performing exposure merging under the LTIP model is equivalent to standard irradiance map fusion. The resulting HDR algorithm is shown to provide high quality in both subjective and objective evaluations.\n    ",
        "submission_date": "2014-11-02T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0392",
        "title": "Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of Hyperspectral Data",
        "authors": [
            "Roozbeh Rajabi",
            "Hassan Ghassemian"
        ],
        "abstract": "Hyperspectral images contain mixed pixels due to low spatial resolution of hyperspectral sensors. Mixed pixels are pixels containing more than one distinct material called endmembers. The presence percentages of endmembers in mixed pixels are called abundance fractions. Spectral unmixing problem refers to decomposing these pixels into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fractions, nonnegative matrix factorization methods (NMF) have been widely used for solving spectral unmixing problem. In this paper we have used graph regularized NMF (GNMF) method combined with sparseness constraint to decompose mixed pixels in hyperspectral imagery. This method preserves the geometrical structure of data while representing it in low dimensional space. Adaptive regularization parameter based on temperature schedule in simulated annealing method also has been used in this paper for the sparseness term. Proposed algorithm is applied on synthetic and real datasets. Synthetic data is generated based on endmembers from USGS spectral library. AVIRIS Cuprite dataset is used as real dataset for evaluation of proposed method. Results are quantified based on spectral angle distance (SAD) and abundance angle distance (AAD) measures. Results in comparison with other methods show that the proposed method can unmix data more effectively. Specifically for the Cuprite dataset, performance of the proposed method is approximately 10% better than the VCA and Sparse NMF in terms of root mean square of SAD.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0442",
        "title": "Non Binary Local Gradient Contours for Face Recognition",
        "authors": [
            "Abdullah Gubbi",
            "Mohammad Fazle Azeem",
            "M Sharmila Kumari"
        ],
        "abstract": "As the features from the traditional Local Binary Patterns (LBP) and Local Directional Patterns (LDP) are found to be ineffective for face recognition, we have proposed a new approach derived on the basis of Information sets whereby the loss of information that occurs during the binarization is eliminated. The information sets expand the scope of fuzzy sets by connecting the attribute and the corresponding membership function value as a product. Since face is having smooth texture in a limited area, the extracted features must be highly discernible. To limit the number of features, we consider only the non overlapping windows. By the application of the information set theory we can reduce the number of feature of an image. The derived features are shown to work fairly well over eigenface, fisherface and LBP methods.\n    ",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0582",
        "title": "Affective Facial Expression Processing via Simulation: A Probabilistic Model",
        "authors": [
            "Jonathan Vitale",
            "Mary-Anne Williams",
            "Benjamin Johnston",
            "Giuseppe Boccignone"
        ],
        "abstract": "Understanding the mental state of other people is an important skill for intelligent agents and robots to operate within social environments. However, the mental processes involved in `mind-reading' are complex. One explanation of such processes is Simulation Theory - it is supported by a large body of neuropsychological research. Yet, determining the best computational model or theory to use in simulation-style emotion detection, is far from being understood.\n",
        "submission_date": "2014-11-03T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0740",
        "title": "State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis",
        "authors": [
            "Ahmadreza Baghaie",
            "Roshan M. D'souza",
            "Zeyun Yu"
        ],
        "abstract": "Optical Coherence Tomography (OCT) is one of the most emerging imaging modalities that has been used widely in the field of biomedical imaging. From its emergence in 1990's, plenty of hardware and software improvements have been made. Its applications range from ophthalmology to dermatology to coronary imaging etc. Here, the focus is on applications of OCT in ophthalmology and retinal imaging. OCT is able to non-invasively produce cross-sectional volume images of the tissues which are further used for analysis of the tissue structure and its properties. Due to the underlying physics, OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation, hence requiring specialized noise reduction techniques to remove the noise while preserving image details. Also, given the fact that OCT images are in the $\\mu m$ -level, further analysis in needed to distinguish between the different structures in the imaged volume. Therefore the use of different segmentation techniques are of high importance. The movement of the tissue under imaging or the progression of disease in the tissue also imposes further implications both on the quality and the proper interpretation of the acquired images. Thus, use of image registration techniques can be very helpful. In this work, an overview of such image analysis techniques will be given.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0791",
        "title": "A Robust Point Sets Matching Method",
        "authors": [
            "Xiao Liu",
            "Congying Han",
            "Tiande Guo"
        ],
        "abstract": "Point sets matching method is very important in computer vision, feature extraction, fingerprint matching, motion estimation and so on. This paper proposes a robust point sets matching method. We present an iterative algorithm that is robust to noise case. Firstly, we calculate all transformations between two points. Then similarity matrix are computed to measure the possibility that two transformation are both true. We iteratively update the matching score matrix by using the similarity matrix. By using matching algorithm on graph, we obtain the matching result. Experimental results obtained by our approach show robustness to outlier and jitter.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1045",
        "title": "Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet",
        "authors": [
            "Matthias K\u00fcmmerer",
            "Lucas Theis",
            "Matthias Bethge"
        ],
        "abstract": "Recent results suggest that state-of-the-art saliency models perform far from optimal in predicting fixations. This lack in performance has been attributed to an inability to model the influence of high-level image features such as objects. Recent seminal advances in applying deep neural networks to tasks like object recognition suggests that they are able to capture this kind of structure. However, the enormous amount of training data necessary to train these networks makes them difficult to apply directly to saliency prediction. We present a novel way of reusing existing neural networks that have been pretrained on the task of object recognition in models of fixation prediction. Using the well-known network of Krizhevsky et al. (2012), we come up with a new saliency model that significantly outperforms all state-of-the-art models on the MIT Saliency Benchmark. We show that the structure of this network allows new insights in the psychophysics of fixation selection and potentially their neural implementation. To train our network, we build on recent work on the modeling of saliency as point processes.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1091",
        "title": "Do Convnets Learn Correspondence?",
        "authors": [
            "Jonathan Long",
            "Ning Zhang",
            "Trevor Darrell"
        ],
        "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1171",
        "title": "Multilinear Principal Component Analysis Network for Tensor Object Classification",
        "authors": [
            "Rui Zeng",
            "Jiasong Wu",
            "Zhuhong Shao",
            "Lotfi Senhadji",
            "Huazhong Shu"
        ],
        "abstract": "The recently proposed principal component analysis network (PCANet) has been proved high performance for visual content classification. In this letter, we develop a tensorial extension of PCANet, namely, multilinear principal analysis component network (MPCANet), for tensor object classification. Compared to PCANet, the proposed MPCANet uses the spatial structure and the relationship between each dimension of tensor objects much more efficiently. Experiments were conducted on different visual content datasets including UCF sports action video sequences database and UCF11 database. The experimental results have revealed that the proposed MPCANet achieves higher classification accuracy than PCANet for tensor object classification.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1172",
        "title": "Tensor object classification via multilinear discriminant analysis network",
        "authors": [
            "Rui Zeng",
            "Jiasong Wu",
            "Lotfi Senhadji",
            "Huazhong Shu"
        ],
        "abstract": "This paper proposes a multilinear discriminant analysis network (MLDANet) for the recognition of multidimensional objects, known as tensor objects. The MLDANet is a variation of linear discriminant analysis network (LDANet) and principal component analysis network (PCANet), both of which are the recently proposed deep learning algorithms. The MLDANet consists of three parts: 1) The encoder learned by MLDA from tensor data. 2) Features maps ob-tained from decoder. 3) The use of binary hashing and histogram for feature pooling. A learning algorithm for MLDANet is described. Evaluations on UCF11 database indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA, and MLDA in terms of classification for tensor objects.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1297",
        "title": "Edge Detection based on Kernel Density Estimation",
        "authors": [
            "Osvaldo Pereira",
            "Esley Torre",
            "Yasel Garc\u00e9s",
            "Roberto Rodr\u00edguez"
        ],
        "abstract": "Edges of an image are considered a crucial type of information. These can be extracted by applying edge detectors with different methodology. Edge detection is a vital step in computer vision tasks, because it is an essential issue for pattern recognition and visual interpretation. In this paper, we propose a new method for edge detection in images, based on the estimation by kernel of the probability density function. In our algorithm, pixels in the image with minimum value of density function are labeled as edges. The boundary between two homogeneous regions is defined in two domains: the spatial/lattice domain and the range/color domain. Extensive experimental evaluations proved that our edge detection method is significantly a competitive algorithm.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1372",
        "title": "Online SLAM with Any-time Self-calibration and Automatic Change Detection",
        "authors": [
            "Nima Keivan",
            "Gabe Sibley"
        ],
        "abstract": "A framework for online simultaneous localization, mapping and self-calibration is presented which can detect and handle significant change in the calibration parameters. Estimates are computed in constant-time by factoring the problem and focusing on segments of the trajectory that are most informative for the purposes of calibration. A novel technique is presented to detect the probability that a significant change is present in the calibration parameters. The system is then able to re-calibrate. Maximum likelihood trajectory and map estimates are computed using an asynchronous and adaptive optimization. The system requires no prior information and is able to initialize without any special motions or routines, or in the case where observability over calibration parameters is delayed. The system is experimentally validated to calibrate camera intrinsic parameters for a nonlinear camera model on a monocular dataset featuring a significant zoom event partway through, and achieves high accuracy despite unknown initial calibration parameters. Self-calibration and re-calibration parameters are shown to closely match estimates computed using a calibration target. The accuracy of the system is demonstrated with SLAM results that achieve sub-1% distance-travel error even in the presence of significant re-calibration events.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1442",
        "title": "Optical Character Recognition, Using K-Nearest Neighbors",
        "authors": [
            "Wei Wang"
        ],
        "abstract": "The problem of optical character recognition, OCR, has been widely discussed in the literature. Having a hand-written text, the program aims at recognizing the text. Even though there are several approaches to this issue, it is still an open problem. In this paper we would like to propose an approach that uses K-nearest neighbors algorithm, and has the accuracy of more than 90%. The training and run time is also very short.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1446",
        "title": "Electrocardiography Separation of Mother and Baby",
        "authors": [
            "Wei Wang"
        ],
        "abstract": "Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is a challenging task, because one single device is used and it receives a mixture of multiple heart beats. In this paper, we would like to design a filter to separate the signals from each other.\n    ",
        "submission_date": "2014-11-05T00:00:00",
        "last_modified_date": "2014-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1509",
        "title": "Convolutional Neural Network-based Place Recognition",
        "authors": [
            "Zetao Chen",
            "Obadiah Lam",
            "Adam Jacobson",
            "Michael Milford"
        ],
        "abstract": "Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75% increase in recall at 100% precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1971",
        "title": "Power-Law Graph Cuts",
        "authors": [
            "Xiangyang Zhou",
            "Jiaxin Zhang",
            "Brian Kulis"
        ],
        "abstract": "Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2090",
        "title": "Parallax Effect Free Mosaicing of Underwater Video Sequence Based on Texture Features",
        "authors": [
            "Nagaraja S.",
            "Prabhakar C.J.",
            "Praveen Kumar P.U"
        ],
        "abstract": "In this paper, we present feature-based technique for construction of mosaic image from underwater video sequence, which suffers from parallax distortion due to propagation properties of light in the underwater environment. The most of the available mosaic tools and underwater image mosaicing techniques yields final result with some artifacts such as blurring, ghosting and seam due to presence of parallax in the input images. The removal of parallax from input images may not reduce its effects instead it must be corrected in successive steps of mosaicing. Thus, our approach minimizes the parallax effects by adopting an efficient local alignment technique after global registration. We extract texture features using Centre Symmetric Local Binary Pattern (CS-LBP) descriptor in order to find feature correspondences, which are used further for estimation of homography through RANSAC. In order to increase the accuracy of global registration, we perform preprocessing such as colour alignment between two selected frames based on colour distribution adjustment. Because of existence of 100% overlap in consecutive frames of underwater video, we select frames with minimum overlap based on mutual offset in order to reduce the computation cost during mosaicing. Our approach minimizes the parallax effects considerably in final mosaic constructed using our own underwater video sequences.\n    ",
        "submission_date": "2014-11-08T00:00:00",
        "last_modified_date": "2014-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2141",
        "title": "Fast Mesh-Based Medical Image Registration",
        "authors": [
            "Ahmadreza Baghaie",
            "Zeyun Yu",
            "Roshan M. D'souza"
        ],
        "abstract": "In this paper a fast triangular mesh based registration method is proposed. Having Template and Reference images as inputs, the template image is triangulated using a content adaptive mesh generation algorithm. Considering the pixel values at mesh nodes, interpolated using spline interpolation method for both of the images, the energy functional needed for image registration is minimized. The minimization process was achieved using a mesh based discretization of the distance measure and regularization term which resulted in a sparse system of linear equations, which due to the smaller size in comparison to the pixel-wise registration method, can be solved directly. Mean Squared Difference (MSD) is used as a metric for evaluating the results. Using the mesh based technique, higher speed was achieved compared to pixel-based curvature registration technique with fast DCT solver. The implementation was done in MATLAB without any specific optimization. Higher speeds can be achieved using C/C++ implementations.\n    ",
        "submission_date": "2014-11-08T00:00:00",
        "last_modified_date": "2014-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2173",
        "title": "Stacked Quantizers for Compositional Vector Compression",
        "authors": [
            "Julieta Martinez",
            "Holger H. Hoos",
            "James J. Little"
        ],
        "abstract": "Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a generalization of Product Quantization (PQ) where a non-independent set of codebooks is used to compress vectors into small binary codes. Unfortunately, under this scheme encoding cannot be done independently in each codebook, and optimal encoding is an NP-hard problem. In this paper, we observe that PQ and AQ are both compositional quantizers that lie on the extremes of the codebook dependence-independence assumption, and explore an intermediate approach that exploits a hierarchical structure in the codebooks. This results in a method that achieves quantization error on par with or lower than AQ, while being several orders of magnitude faster. We perform a complexity analysis of PQ, AQ and our method, and evaluate our approach on standard benchmarks of SIFT and GIST descriptors, as well as on new datasets of features obtained from state-of-the-art convolutional neural networks.\n    ",
        "submission_date": "2014-11-08T00:00:00",
        "last_modified_date": "2014-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2214",
        "title": "Abnormal Object Recognition: A Comprehensive Study",
        "authors": [
            "Babak Saleh",
            "Ali Farhadi",
            "Ahmed Elgammal"
        ],
        "abstract": "When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.\n    ",
        "submission_date": "2014-11-09T00:00:00",
        "last_modified_date": "2014-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2316",
        "title": "Zero-Aliasing Correlation Filters for Object Recognition",
        "authors": [
            "Joseph A. Fernandez",
            "Vishnu Naresh Boddeti",
            "Andres Rodriguez",
            "B. V. K. Vijaya Kumar"
        ],
        "abstract": "Correlation filters (CFs) are a class of classifiers that are attractive for object localization and tracking applications. Traditionally, CFs have been designed in the frequency domain using the discrete Fourier transform (DFT), where correlation is efficiently implemented. However, existing CF designs do not account for the fact that the multiplication of two DFTs in the frequency domain corresponds to a circular correlation in the time/spatial domain. Because this was previously unaccounted for, prior CF designs are not truly optimal, as their optimization criteria do not accurately quantify their optimization intention. In this paper, we introduce new zero-aliasing constraints that completely eliminate this aliasing problem by ensuring that the optimization criterion for a given CF corresponds to a linear correlation rather than a circular correlation. This means that previous CF designs can be significantly improved by this reformulation. We demonstrate the benefits of this new CF design approach with several important CFs. We present experimental results on diverse data sets and present solutions to the computational challenges associated with computing these CFs. Code for the CFs described in this paper and their respective zero-aliasing versions is available at ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2335",
        "title": "An Improved Tracking using IMU and Vision Fusion for Mobile Augmented Reality Applications",
        "authors": [
            "Kriti Kumar",
            "Ashley Varghese",
            "Pavan K Reddy",
            "N Narendra",
            "Prashanth Swamy",
            "M Girish Chandra",
            "P Balamuralidhar"
        ],
        "abstract": "Mobile Augmented Reality (MAR) is becoming an important cyber-physical system application given the ubiquitous availability of mobile phones. With the need to operate in unprepared environments, accurate and robust registration and tracking has become an important research problem to solve. In fact, when MAR is used for tele-interactive applications involving large distances, say from an accident site to insurance office, tracking at both the ends is desirable and further it is essential to appropriately fuse inertial and vision sensors data. In this paper, we present results and discuss some insights gained in marker-less tracking during the development of a prototype pertaining to an example use case related to breakdown or damage assessment of a vehicle. The novelty of this paper is in bringing together different components and modules with appropriate enhancements towards a complete working system.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2584",
        "title": "Applications of sampling Kantorovich operators to thermographic images for seismic engineering",
        "authors": [
            "Danilo Costarelli",
            "Federico Cluni",
            "Anna Maria Minotti",
            "Gianluca Vinti"
        ],
        "abstract": "In this paper, we present some applications of the multivariate sampling Kantorovich operators $S_w$ to seismic engineering. The mathematical theory of these operators, both in the space of continuous functions and in Orlicz spaces, show how it is possible to approximate/reconstruct multivariate signals, such as images. In particular, to obtain applications for thermographic images a mathematical algorithm is developed using MATLAB and matrix calculus. The setting of Orlicz spaces is important since allow us to reconstruct not necessarily continuous signals by means of $S_w$. The reconstruction of thermographic images of buildings by our sampling Kantorovich algorithm allow us to obtain models for the simulation of the behavior of structures under seismic action. We analyze a real world case study in term of structural analysis and we compare the behavior of the building under seismic action using various models.\n    ",
        "submission_date": "2014-11-09T00:00:00",
        "last_modified_date": "2014-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2861",
        "title": "Computational Baby Learning",
        "authors": [
            "Xiaodan Liang",
            "Si Liu",
            "Yunchao Wei",
            "Luoqi Liu",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for slightly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features from the pre-trained CNN. Simulating the baby's interaction with physical world, the well-designed tracking solution is then used to discover more diverse instances from the massive online unlabeled videos. Once a positive instance is detected/identified with high score in each video, more variable instances possibly from different view-angles and/or different distances are tracked and accumulated. Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 unlabeled videos.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2942",
        "title": "3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach",
        "authors": [
            "Xiaowei Zhou",
            "Spyridon Leonardos",
            "Xiaoyan Hu",
            "Kostas Daniilidis"
        ],
        "abstract": "We investigate the problem of estimating the 3D shape of an object, given a set of 2D landmarks in a single image. To alleviate the reconstruction ambiguity, a widely-used approach is to confine the unknown 3D shape within a shape space built upon existing shapes. While this approach has proven to be successful in various applications, a challenging issue remains, i.e., the joint estimation of shape parameters and camera-pose parameters requires to solve a nonconvex optimization problem. The existing methods often adopt an alternating minimization scheme to locally update the parameters, and consequently the solution is sensitive to initialization. In this paper, we propose a convex formulation to address this problem and develop an efficient algorithm to solve the proposed convex program. We demonstrate the exact recovery property of the proposed method, its merits compared to alternative methods, and the applicability in human pose and car shape estimation.\n    ",
        "submission_date": "2014-11-11T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3041",
        "title": "Collecting Image Description Datasets using Crowdsourcing",
        "authors": [
            "Ramakrishna Vedantam",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "We describe our two new datasets with images described by humans. Both the datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform. The two datasets contain significantly more descriptions per image than other existing datasets. One is based on a popular image description dataset called the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract Scenes dataset con- taining images made from clipart objects. In this paper we describe our interfaces, analyze some properties of and show example descriptions from our two datasets.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3159",
        "title": "Part Detector Discovery in Deep Convolutional Neural Networks",
        "authors": [
            "Marcel Simon",
            "Erik Rodner",
            "Joachim Denzler"
        ],
        "abstract": "Current fine-grained classification approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and efficient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called \"part detector discovery\" (PDD) is based on analyzing the gradient maps of the network outputs and finding activation centers spatially related to annotated semantic parts or bounding boxes.\n",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3169",
        "title": "On Coarse Graining of Information and Its Application to Pattern Recognition",
        "authors": [
            "Ali Ghaderi"
        ],
        "abstract": "We propose a method based on finite mixture models for classifying a set of observations into number of different categories. In order to demonstrate the method, we show how the component densities for the mixture model can be derived by using the maximum entropy method in conjunction with conservation of Pythagorean means. Several examples of distributions belonging to the Pythagorean family are derived. A discussion on estimation of model parameters and the number of categories is also given.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3229",
        "title": "Multi-modal Image Registration for Correlative Microscopy",
        "authors": [
            "Tian Cao",
            "Christopher Zach",
            "Shannon Modla",
            "Debbie Powell",
            "Kirk Czymmek",
            "Marc Niethammer"
        ],
        "abstract": "Correlative microscopy is a methodology combining the functionality of light microscopy with the high resolution of electron microscopy and other microscopy technologies. Image registration for correlative microscopy is quite challenging because it is a multi-modal, multi-scale and multi-dimensional registration problem. In this report, I introduce two methods of image registration for correlative microscopy. The first method is based on fiducials (beads). I generate landmarks from the fiducials and compute the similarity transformation matrix based on three pairs of nearest corresponding landmarks. A least-squares matching process is applied afterwards to further refine the registration. The second method is inspired by the image analogies approach. I introduce the sparse representation model into image analogies. I first train representative image patches (dictionaries) for pre-registered datasets from two different modalities, and then I use the sparse coding technique to transfer a given image to a predicted image from one modality to another based on the learned dictionaries. The final image registration is between the predicted image and the original image corresponding to the given image in the different modality. The method transforms a multi-modal registration problem to a mono-modal one. I test my approaches on Transmission Electron Microscopy (TEM) and confocal microscopy images. Experimental results of the methods are also shown in this report.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3230",
        "title": "Sparse Modeling for Image and Vision Processing",
        "authors": [
            "Julien Mairal",
            "Francis Bach",
            "Jean Ponce"
        ],
        "abstract": "In recent years, a large amount of multi-disciplinary research has been conducted on sparse models and their applications. In statistics and machine learning, the sparsity principle is used to perform model selection---that is, automatically selecting a simple model among a large collection of them. In signal processing, sparse coding consists of representing data with linear combinations of a few dictionary elements. Subsequently, the corresponding tools have been widely adopted by several scientific communities such as neuroscience, bioinformatics, or computer vision. The goal of this monograph is to offer a self-contained view of sparse modeling for visual recognition and image processing. More specifically, we focus on applications where the dictionary is learned and adapted to data, yielding a compact representation that has been successful in various contexts.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2014-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3285",
        "title": "Amoeba Techniques for Shape and Texture Analysis",
        "authors": [
            "Martin Welk"
        ],
        "abstract": "Morphological amoebas are image-adaptive structuring elements for morphological and other local image filters introduced by Lerallut et al. Their construction is based on combining spatial distance with contrast information into an image-dependent metric. Amoeba filters show interesting parallels to image filtering methods based on partial differential equations (PDEs), which can be confirmed by asymptotic equivalence results. In computing amoebas, graph structures are generated that hold information about local image texture. This paper reviews and summarises the work of the author and his coauthors on morphological amoebas, particularly their relations to PDE filters and texture analysis. It presents some extensions and points out directions for future investigation on the subject.\n    ",
        "submission_date": "2014-11-12T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3410",
        "title": "Person Re-identification Based on Color Histogram and Spatial Configuration of Dominant Color Regions",
        "authors": [
            "Kwangchol Jang",
            "Sokmin Han",
            "Insong Kim"
        ],
        "abstract": "There is a requirement to determine whether a given person of interest has already been observed over a network of cameras in video surveillance systems. A human appearance obtained in one camera is usually different from the ones obtained in another camera due to difference in illumination, pose and viewpoint, camera parameters. Being related to appearance-based approaches for person re-identification, we propose a novel method based on the dominant color histogram and spatial configuration of dominant color regions on human body parts. Dominant color histogram and spatial configuration of the dominant color regions based on dominant color descriptor(DCD) can be considered to be robust to illumination and pose, viewpoint changes. The proposed method is evaluated using benchmark video datasets. Experimental results using the cumulative matching characteristic(CMC) curve demonstrate the effectiveness of our approach for person re-identification.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3423",
        "title": "A Comparative Study of Techniques of Distant Reconstruction of Displacement Fields by using DISTRESS Simulator",
        "authors": [
            "Ghulam Mubashar Hassan",
            "Arcady V. Dyskin",
            "Cara K. MacNish"
        ],
        "abstract": "Reconstruction and monitoring of displacement and strain fields is an important problem in engineering. We analyze the remote and non-obtrusive methods of strain measurement based on photogrammetry and Digital Image Correlation (DIC). The method is based on covering the photographed surface with a pattern of speckles and comparing the images taken before and after the deformation. In this study, a comprehensive literature review and comparative analysis of photogrammetric solutions is presented. The analysis is based on a specially developed Digital Image Synthesizer To Reconstruct Strain in Solids (DISTRESS) Simulator to generate synthetic images of displacement and stress fields in order to investigate the intrinsic accuracy of the existing variants of DIC. We investigated the Basic DIC and a commercial software VIC 2D, both based on displacement field reconstruction with post processing strain determination based on numerical differentiation. We also investigated what we call the Extended DIC where the strain field is determined independently of the displacement field. While the Basic DIC and VIC 2D are faster, the Extended DIC delivers the best accuracy of strain reconstruction. The speckle pattern is found to be playing a critical role in achieving high accuracy for DIC. Increase in subset size for DIC does not significantly improves the accuracy, while the smallest subset size depends on the speckle pattern and speckle size. Increase in the overall image size provides more details but does not play significant role in improving the accuracy, while significantly increasing the computation cost.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3519",
        "title": "Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A Comparative Study on a Novel Dataset",
        "authors": [
            "Marwan Torki",
            "Mohamed E. Hussein",
            "Ahmed Elsallamy",
            "Mahmoud Fayyaz",
            "Shehab Yaser"
        ],
        "abstract": "This paper presents a comparative study for window-based descriptors on the application of Arabic handwritten alphabet recognition. We show a detailed experimental evaluation of different descriptors with several classifiers. The objective of the paper is to evaluate different window-based descriptors on the problem of Arabic letter recognition. Our experiments clearly show that they perform very well. Moreover, we introduce a novel spatial pyramid partitioning scheme that enhances the recognition accuracy for most descriptors. In addition, we introduce a novel dataset for Arabic handwritten isolated alphabet letters, which can serve as a benchmark for future research.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4005",
        "title": "A convex formulation for hyperspectral image superresolution via subspace-based regularization",
        "authors": [
            "Miguel Sim\u00f5es",
            "Jos\u00e9 Bioucas-Dias",
            "Luis B. Almeida",
            "Jocelyn Chanussot"
        ],
        "abstract": "Hyperspectral remote sensing images (HSIs) usually have high spectral resolution and low spatial resolution. Conversely, multispectral images (MSIs) usually have low spectral and high spatial resolutions. The problem of inferring images which combine the high spectral and high spatial resolutions of HSIs and MSIs, respectively, is a data fusion problem that has been the focus of recent active research due to the increasing availability of HSIs and MSIs retrieved from the same geographical area.\n",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4006",
        "title": "A Discriminative CNN Video Representation for Event Detection",
        "authors": [
            "Zhongwen Xu",
            "Yi Yang",
            "Alexander G. Hauptmann"
        ],
        "abstract": "In this paper, we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available. The focus of this paper is to effectively leverage deep Convolutional Neural Networks (CNNs) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkit. This paper makes two contributions to the inference of CNN video representation. First, while average pooling and max pooling have long been the standard approaches to aggregating frame level static features, we show that performance can be significantly improved by taking advantage of an appropriate encoding method. Second, we propose using a set of latent concept descriptors as the frame descriptor, which enriches visual information while keeping it computationally affordable. The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets. Compared to improved Dense Trajectories, which has been recognized as the best video representation for event detection, our new representation improves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID MEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset. This work is the core part of the winning solution of our CMU-Informedia team in TRECVID MED 2014 competition.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4033",
        "title": "Sparse And Low Rank Decomposition Based Batch Image Alignment for Speckle Reduction of retinal OCT Images",
        "authors": [
            "Ahmadreza Baghaie",
            "Roshan M. D'souza",
            "Zeyun Yu"
        ],
        "abstract": "Optical Coherence Tomography (OCT) is an emerging technique in the field of biomedical imaging, with applications in ophthalmology, dermatology, coronary imaging etc. Due to the underlying physics, OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation. Here, a sparse and low rank decomposition based method is used for speckle reduction in retinal OCT images. This technique works on input data that consists of several B-scans of the same location. The next step is the batch alignment of the images using a sparse and low-rank decomposition based technique. Finally the denoised image is created by median filtering of the low-rank component of the processed data. Simultaneous decomposition and alignment of the images result in better performance in comparison to simple registration-based methods that are used in the literature for noise reduction of OCT images.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4038",
        "title": "Fully Convolutional Networks for Semantic Segmentation",
        "authors": [
            "Jonathan Long",
            "Evan Shelhamer",
            "Trevor Darrell"
        ],
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2015-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4064",
        "title": "A Faster Method for Tracking and Scoring Videos Corresponding to Sentences",
        "authors": [
            "Haonan Yu",
            "Daniel P. Barrett",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "Prior work presented the sentence tracker, a method for scoring how well a sentence describes a video clip or alternatively how well a video clip depicts a sentence. We present an improved method for optimizing the same cost function employed by this prior work, reducing the space complexity from exponential in the sentence length to polynomial, as well as producing a qualitatively identical result in time polynomial in the sentence length instead of exponential. Since this new method is plug-compatible with the prior method, it can be used for the same applications: video retrieval with sentential queries, generating sentential descriptions of video clips, and focusing the attention of a tracker with a sentence, while allowing these applications to scale with significantly larger numbers of object detections, word meanings modeled with HMMs with significantly larger numbers of states, and significantly longer sentences, with no appreciable degradation in quality of results.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4098",
        "title": "GASP : Geometric Association with Surface Patches",
        "authors": [
            "Rahul Sawhney",
            "Fuxin Li",
            "Henrik I. Christensen"
        ],
        "abstract": "A fundamental challenge to sensory processing tasks in perception and robotics is the problem of obtaining data associations across views. We present a robust solution for ascertaining potentially dense surface patch (superpixel) associations, requiring just range information. Our approach involves decomposition of a view into regularized surface patches. We represent them as sequences expressing geometry invariantly over their superpixel neighborhoods, as uniquely consistent partial orderings. We match these representations through an optimal sequence comparison metric based on the Damerau-Levenshtein distance - enabling robust association with quadratic complexity (in contrast to hitherto employed joint matching formulations which are NP-complete). The approach is able to perform under wide baselines, heavy rotations, partial overlaps, significant occlusions and sensor noise.\n",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4102",
        "title": "Anisotropic Agglomerative Adaptive Mean-Shift",
        "authors": [
            "Rahul Sawhney",
            "Henrik I. Christensen",
            "Gary R. Bradski"
        ],
        "abstract": "Mean Shift today, is widely used for mode detection and clustering. The technique though, is challenged in practice due to assumptions of isotropicity and homoscedasticity. We present an adaptive Mean Shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. The bandwidth matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. The online methodology is practical and effecive for low-dimensional feature spaces, preserving better detail and clustering salience. Additionally, conventional Mean Shift either critically depends on a per instance choice of bandwidth, or relies on offline methods which are inflexible and/or again data instance specific. The presented approach, due to its adaptive design, also alleviates this issue - with a default form performing generally well. The methodology though, allows for effective tuning of results.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4199",
        "title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval",
        "authors": [
            "Ke Jiang",
            "Qichao Que",
            "Brian Kulis"
        ],
        "abstract": "We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH), a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space, and has several key theoretical and practical benefits. First, it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second, it yields the first formal retrieval performance bounds for KLSH. Third, our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets, and show that our analysis leads to improved recall performance of at least 12%, and sometimes much higher, over the standard KLSH method.\n    ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4229",
        "title": "Efficient and Accurate Approximations of Nonlinear Convolutional Networks",
        "authors": [
            "Xiangyu Zhang",
            "Jianhua Zou",
            "Xiang Ming",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the \"AlexNet\", but is 4.7% more accurate.\n    ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4280",
        "title": "Efficient Object Localization Using Convolutional Networks",
        "authors": [
            "Jonathan Tompson",
            "Ross Goroshin",
            "Arjun Jain",
            "Yann LeCun",
            "Christopher Bregler"
        ],
        "abstract": "Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.\n    ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4296",
        "title": "Combining contextual and local edges for line segment extraction in cluttered images",
        "authors": [
            "Rui F. C. Guerreiro"
        ],
        "abstract": "Automatic extraction methods typically assume that line segments are pronounced, thin, few and far between, do not cross each other, and are noise and clutter-free. Since these assumptions often fail in realistic scenarios, many line segments are not detected or are fragmented. In more severe cases, i.e., many who use the Hough Transform, extraction can fail entirely. In this paper, we propose a method that tackles these issues. Its key aspect is the combination of thresholded image derivatives obtained with filters of large and small footprints, which we denote as contextual and local edges, respectively. Contextual edges are robust to noise and we use them to select valid local edges, i.e., local edges that are of the same type as contextual ones: dark-to-bright transition of vice-versa. If the distance between valid local edges does not exceed a maximum distance threshold, we enforce connectivity by marking them and the pixels in between as edge points. This originates connected edge maps that are robust and well localized. We use a powerful two-sample statistical test to compute contextual edges, which we introduce briefly, as they are unfamiliar to the image processing community. Finally, we present experiments that illustrate, with synthetic and real images, how our method is efficient in extracting complete segments of all lengths and widths in several situations where current methods fail.\n    ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4304",
        "title": "Ten Years of Pedestrian Detection, What Have We Learned?",
        "authors": [
            "Rodrigo Benenson",
            "Mohamed Omran",
            "Jan Hosang",
            "Bernt Schiele"
        ],
        "abstract": "Paper-by-paper results make it easy to miss the forest for the ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4331",
        "title": "A Latent Clothing Attribute Approach for Human Pose Estimation",
        "authors": [
            "Weipeng Zhang",
            "Jie Shen",
            "Guangcan Liu",
            "Yong Yu"
        ],
        "abstract": "As a fundamental technique that concerns several vision tasks such as image parsing, action recognition and clothing retrieval, human pose estimation (HPE) has been extensively investigated in recent years. To achieve accurate and reliable estimation of the human pose, it is well-recognized that the clothing attributes are useful and should be utilized properly. Most previous approaches, however, require to manually annotate the clothing attributes and are therefore very costly. In this paper, we shall propose and explore a \\emph{latent} clothing attribute approach for HPE. Unlike previous approaches, our approach models the clothing attributes as latent variables and thus requires no explicit labeling for the clothing attributes. The inference of the latent variables are accomplished by utilizing the framework of latent structured support vector machines (LSSVM). We employ the strategy of \\emph{alternating direction} to train the LSSVM model: In each iteration, one kind of variables (e.g., human pose or clothing attribute) are fixed and the others are optimized. Our extensive experiments on two real-world benchmarks show the state-of-the-art performance of our proposed approach.\n    ",
        "submission_date": "2014-11-16T00:00:00",
        "last_modified_date": "2014-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4389",
        "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
        "authors": [
            "Jeff Donahue",
            "Lisa Anne Hendricks",
            "Marcus Rohrbach",
            "Subhashini Venugopalan",
            "Sergio Guadarrama",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4419",
        "title": "Automatic Subspace Learning via Principal Coefficients Embedding",
        "authors": [
            "Xi Peng",
            "Jiwen Lu",
            "Zhang Yi",
            "Rui Yan"
        ],
        "abstract": "In this paper, we address two challenging problems in unsupervised subspace learning: 1) how to automatically identify the feature dimension of the learned subspace (i.e., automatic subspace learning), and 2) how to learn the underlying subspace in the presence of Gaussian noise (i.e., robust subspace learning). We show that these two problems can be simultaneously solved by proposing a new method (called principal coefficients embedding, PCE). For a given data set $\\mathbf{D}\\in \\mathds{R}^{m\\times n}$, PCE recovers a clean data set $\\mathbf{D}_{0}\\in \\mathds{R}^{m\\times n}$ from $\\mathbf{D}$ and simultaneously learns a global reconstruction relation $\\mathbf{C}\\in \\mathbf{R}^{n\\times n}$ of $\\mathbf{D}_{0}$. By preserving $\\mathbf{C}$ into an $m^{\\prime}$-dimensional space, the proposed method obtains a projection matrix that can capture the latent manifold structure of $\\mathbf{D}_{0}$, where $m^{\\prime}\\ll m$ is automatically determined by the rank of $\\mathbf{C}$ with theoretical guarantees. PCE has three advantages: 1) it can automatically determine the feature dimension even though data are sampled from a union of multiple linear subspaces in presence of the Gaussian noise, 2) Although the objective function of PCE only considers the Gaussian noise, experimental results show that it is robust to the non-Gaussian noise (\\textit{e.g.}, random pixel corruption) and real disguises, 3) Our method has a closed-form solution and can be calculated very fast. Extensive experimental results show the superiority of PCE on a range of databases with respect to the classification accuracy, robustness and efficiency.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4423",
        "title": "A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis",
        "authors": [
            "Sotirios P. Chatzis"
        ],
        "abstract": "Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4464",
        "title": "Fully Convolutional Neural Networks for Crowd Segmentation",
        "authors": [
            "Kai Kang",
            "Xiaogang Wang"
        ],
        "abstract": "In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4491",
        "title": "Joint cross-domain classification and subspace learning for unsupervised adaptation",
        "authors": [
            "Basura Fernando",
            "Tatiana Tommasi",
            "Tinne Tuytelaars"
        ],
        "abstract": "Domain adaptation aims at adapting the knowledge acquired on a source domain to a new different but related target domain. Several approaches have beenproposed for classification tasks in the unsupervised scenario, where no labeled target data are available. Most of the attention has been dedicated to searching a new domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4555",
        "title": "Show and Tell: A Neural Image Caption Generator",
        "authors": [
            "Oriol Vinyals",
            "Alexander Toshev",
            "Samy Bengio",
            "Dumitru Erhan"
        ],
        "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4568",
        "title": "TILDE: A Temporally Invariant Learned DEtector",
        "authors": [
            "Yannick Verdie",
            "Kwang Moo Yi",
            "Pascal Fua",
            "Vincent Lepetit"
        ],
        "abstract": "We introduce a learning-based approach to detect repeatable keypoints under drastic imaging changes of weather and lighting conditions to which state-of-the-art keypoint detectors are surprisingly sensitive. We first identify good keypoint candidates in multiple training images taken from the same viewpoint. We then train a regressor to predict a score map whose maxima are those points so that they can be found by simple non-maximum suppression. As there are no standard datasets to test the influence of these kinds of changes, we created our own, which we will make publicly available. We will show that our method significantly outperforms the state-of-the-art methods in such challenging conditions, while still achieving state-of-the-art performance on the untrained standard Oxford dataset.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4670",
        "title": "AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline Arabic Handwriting Recognition",
        "authors": [
            "Mohamed E. Hussein",
            "Marwan Torki",
            "Ahmed Elsallamy",
            "Mahmoud Fayyaz"
        ],
        "abstract": "In this paper, we introduce the first phase of a new dataset for offline Arabic handwriting recognition. The aim is to collect a very large dataset of isolated Arabic words that covers all letters of the alphabet in all possible shapes using a small number of simple words. The end goal is to collect a very large dataset of segmented letter images, which can be used to build and evaluate Arabic handwriting recognition systems that are based on segmented letter recognition. The current version of the dataset contains $25114$ samples of $109$ unique Arabic words that cover all possible shapes of all alphabet letters. The samples were collected from $907$ writers. In its current form, the dataset can be used for the problem of closed-vocabulary word recognition. We evaluated a number of window-based descriptors and classifiers on this task and obtained an accuracy of $92.16\\%$ using a SIFT-based descriptor and ANN.\n    ",
        "submission_date": "2014-11-17T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4701",
        "title": "Structured Hough Voting for Vision-based Highway Border Detection",
        "authors": [
            "Zhiding Yu",
            "Wende Zhang",
            "B. V. K. Vijaya Kumar",
            "Dan Levi"
        ],
        "abstract": "We propose a vision-based highway border detection algorithm using structured Hough voting. Our approach takes advantage of the geometric relationship between highway road borders and highway lane markings. It uses a strategy where a number of trained road border and lane marking detectors are triggered, followed by Hough voting to generate corresponding detection of the border and lane marking. Since the initially triggered detectors usually result in large number of positives, conventional frame-wise Hough voting is not able to always generate robust border and lane marking results. Therefore, we formulate this problem as a joint detection-and-tracking problem under the structured Hough voting model, where tracking refers to exploiting inter-frame structural information to stabilize the detection results. Both qualitative and quantitative evaluations show the superiority of the proposed structured Hough voting model over a number of baseline methods.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4734",
        "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture",
        "authors": [
            "David Eigen",
            "Rob Fergus"
        ],
        "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4894",
        "title": "Low-level Vision by Consensus in a Spatial Hierarchy of Regions",
        "authors": [
            "Ayan Chakrabarti",
            "Ying Xiong",
            "Steven J. Gortler",
            "Todd Zickler"
        ],
        "abstract": "We introduce a multi-scale framework for low-level vision, where the goal is estimating physical scene values from image data---such as depth from stereo image pairs. The framework uses a dense, overlapping set of image regions at multiple scales and a \"local model,\" such as a slanted-plane model for stereo disparity, that is expected to be valid piecewise across the visual field. Estimation is cast as optimization over a dichotomous mixture of variables, simultaneously determining which regions are inliers with respect to the local model (binary variables) and the correct co-ordinates in the local model space for each inlying region (continuous variables). When the regions are organized into a multi-scale hierarchy, optimization can occur in an efficient and parallel architecture, where distributed computational units iteratively perform calculations and share information through sparse connections between parents and children. The framework performs well on a standard benchmark for binocular stereo, and it produces a distributional scene representation that is appropriate for combining with higher-level reasoning and other low-level cues.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4952",
        "title": "From Captions to Visual Concepts and Back",
        "authors": [
            "Hao Fang",
            "Saurabh Gupta",
            "Forrest Iandola",
            "Rupesh Srivastava",
            "Li Deng",
            "Piotr Doll\u00e1r",
            "Jianfeng Gao",
            "Xiaodong He",
            "Margaret Mitchell",
            "John C. Platt",
            "C. Lawrence Zitnick",
            "Geoffrey Zweig"
        ],
        "abstract": "  This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4958",
        "title": "Designing Deep Networks for Surface Normal Estimation",
        "authors": [
            "Xiaolong Wang",
            "David F. Fouhey",
            "Abhinav Gupta"
        ],
        "abstract": "In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations. In this paper, we use CNNs for the task of predicting surface normals from a single image. But what is the right architecture we should use? We propose to build upon the decades of hard work in 3D scene understanding, to design new CNN architecture for the task of surface normal estimation. We show by incorporating several constraints (man-made, manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation. We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5057",
        "title": "Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based Sparsity Reconstruction",
        "authors": [
            "Chen Chen",
            "Junzhou Huang",
            "Lei He",
            "Hongsheng Li"
        ],
        "abstract": "In this paper, we propose a novel algorithm for analysis-based sparsity reconstruction. It can solve the generalized problem by structured sparsity regularization with an orthogonal basis and total variation regularization. The proposed algorithm is based on the iterative reweighted least squares (IRLS) model, which is further accelerated by the preconditioned conjugate gradient method. The convergence rate of the proposed algorithm is almost the same as that of the traditional IRLS algorithms, that is, exponentially fast. Moreover, with the specifically devised preconditioner, the computational cost for each iteration is significantly less than that of traditional IRLS algorithms, which enables our approach to handle large scale problems. In addition to the fast convergence, it is straightforward to apply our method to standard sparsity, group sparsity, overlapping group sparsity and TV based problems. Experiments are conducted on a practical application: compressive sensing magnetic resonance imaging. Extensive results demonstrate that the proposed algorithm achieves superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational cost.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5065",
        "title": "SIRF: Simultaneous Image Registration and Fusion in A Unified Framework",
        "authors": [
            "Chen Chen",
            "Yeqing Li",
            "Wei Liu",
            "Junzhou Huang"
        ],
        "abstract": "In this paper, we propose a novel method for image fusion with a high-resolution panchromatic image and a low-resolution multispectral image at the same geographical location. The fusion is formulated as a convex optimization problem which minimizes a linear combination of a least-squares fitting term and a dynamic gradient sparsity regularizer. The former is to preserve accurate spectral information of the multispectral image, while the latter is to keep sharp edges of the high-resolution panchromatic image. We further propose to simultaneously register the two images during the fusing process, which is naturally achieved by virtue of the dynamic gradient sparsity property. An efficient algorithm is then devised to solve the optimization problem, accomplishing a linear computational complexity in the size of the output image in each iteration. We compare our method against seven state-of-the-art image fusion methods on multispectral image datasets from four satellites. Extensive experimental results demonstrate that the proposed method substantially outperforms the others in terms of both spatial and spectral qualities. We also show that our method can provide high-quality products from coarsely registered real-world datasets. Finally, a MATLAB implementation is provided to facilitate future research.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-01-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5140",
        "title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback",
        "authors": [
            "Qian Wang",
            "Jiaxing Zhang",
            "Sen Song",
            "Zheng Zhang"
        ],
        "abstract": "Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5190",
        "title": "A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Over the last two decades we have witnessed strong progress on modeling visual object classes, scenes and attributes that have significantly contributed to automated image understanding. On the other hand, surprisingly little progress has been made on incorporating a spatial representation and reasoning in the inference process. In this work, we propose a pooling interpretation of spatial relations and show how it improves image retrieval and annotations tasks involving spatial language. Due to the complexity of the spatial language, we argue for a learning-based approach that acquires a representation of spatial relations by learning parameters of the pooling operator. We show improvements on previous work on two datasets and two different tasks as well as provide additional insights on a new dataset with an explicit focus on spatial relations.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5268",
        "title": "Sparse distributed localized gradient fused features of objects",
        "authors": [
            "Swathikiran Sudhakarana",
            "Alex Pappachen James"
        ],
        "abstract": "The sparse, hierarchical, and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy. In this study, we report a sparse feature processing and encoding method, which improved the recognition performance of an automated object recognition system. Randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation, where we used a modular and hierarchical approach to detect the object features. These object features were combined with a minimum distance classifier, thereby obtaining object recognition system accuracies of 93% using the Amsterdam library of object images (ALOI) database, 92% using the Columbia object image library (COIL)-100 database, and 69% using the PASCAL visual object challenge 2007 database. The object recognition performance was shown to be robust to variations in noise, object scaling, and object shifts. Finally, a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10% with the PASCAL visual object challenge 2007 database.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5309",
        "title": "End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression",
        "authors": [
            "Li Wan",
            "David Eigen",
            "Rob Fergus"
        ],
        "abstract": "Deformable Parts Models and Convolutional Networks each have achieved notable performance in object detection. Yet these two approaches find their strengths in complementary areas: DPMs are well-versed in object composition, modeling fine-grained spatial relationships between parts; likewise, ConvNets are adept at producing powerful image features, having been discriminatively trained directly on the pixels. In this paper, we propose a new model that combines these two approaches, obtaining the advantages of each. We train this model using a new structured loss function that considers all bounding boxes within an image, rather than isolated object instances. This enables the non-maximal suppression (NMS) operation, previously treated as a separate post-processing stage, to be integrated into the model. This allows for discriminative training of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results on both benchmarks.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5319",
        "title": "Fashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors",
        "authors": [
            "Kota Hara",
            "Vignesh Jagadeesh",
            "Robinson Piramuthu"
        ],
        "abstract": "In this work, we propose and address a new computer vision task, which we call fashion item detection, where the aim is to detect various fashion items a person in the image is wearing or carrying. The types of fashion items we consider in this work include hat, glasses, bag, pants, shoes and so on. The detection of fashion items can be an important first step of various e-commerce applications for fashion industry. Our method is based on state-of-the-art object detection method pipeline which combines object proposal methods with a Deep Convolutional Neural Network. Since the locations of fashion items are in strong correlation with the locations of body joints positions, we incorporate contextual information from body poses in order to improve the detection performance. Through the experiments, we demonstrate the effectiveness of the proposed method.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2016-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5328",
        "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections",
        "authors": [
            "Bolei Zhou",
            "Vignesh Jagadeesh",
            "Robinson Piramuthu"
        ],
        "abstract": "Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition system, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5331",
        "title": "Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations",
        "authors": [
            "Michelle R. Greene",
            "Abraham P. Botros",
            "Diane M. Beck",
            "Li Fei-Fei"
        ],
        "abstract": "Our perceptions are guided both by the bottom-up information entering our eyes, as well as our top-down expectations of what we will see. Although bottom-up visual processing has been extensively studied, comparatively little is known about top-down signals. Here, we describe REVEAL (Representations Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's internal representation of a complex, real-world scene, allowing us to, for the first time, visualize the top-down information in an observer's mind. REVEAL rests on two innovations for solving this high dimensional problem: visual noise that samples from natural image statistics, and a computer algorithm that collaborates with human observers to efficiently obtain a solution. In this work, we visualize observers' internal representations of a visual scene category (street) using an experiment in which the observer views the naturalistic visual noise and collaborates with the algorithm to externalize his internal representation. As no scene information was presented, observers had to use their internal knowledge of the target, matching it with the visual features in the noise. We matched reconstructed images with images of real-world street scenes to enhance visualization. Critically, we show that the visualized mental images can be used to predict rapid scene detection performance, as each observer had faster and more accurate responses to detecting real-world images that were the most similar to his reconstructed street templates. These results show that it is possible to visualize previously unobservable mental representations of real world stimuli. More broadly, REVEAL provides a general method for objectively examining the content of previously private, subjective mental experiences.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5555",
        "title": "Maximum Likelihood Directed Enumeration Method in Piecewise-Regular Object Recognition",
        "authors": [
            "Andrey Savchenko"
        ],
        "abstract": "We explore the problems of classification of composite object (images, speech signals) with low number of models per class. We study the question of improving recognition performance for medium-sized database (thousands of classes). The key issue of fast approximate nearest-neighbor methods widely applied in this task is their heuristic nature. It is possible to strongly prove their efficiency by using the theory of algorithms only for simple similarity measures and artificially generated tasks. On the contrary, in this paper we propose an alternative, statistically optimal greedy algorithm. At each step of this algorithm joint density (likelihood) of distances to previously checked models is estimated for each class. The next model to check is selected from the class with the maximal likelihood. The latter is estimated based on the asymptotic properties of the Kullback-Leibler information discrimination and mathematical model of piecewise-regular object with distribution of each regular segment of exponential type. Experimental results in face recognition for FERET dataset prove that the proposed method is much more effective than not only brute force and the baseline (directed enumeration method) but also approximate nearest neighbor methods from FLANN and NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5654",
        "title": "Learning a Recurrent Visual Representation for Image Caption Generation",
        "authors": [
            "Xinlei Chen",
            "C. Lawrence Zitnick"
        ],
        "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5726",
        "title": "CIDEr: Consensus-based Image Description Evaluation",
        "authors": [
            "Ramakrishna Vedantam",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5731",
        "title": "Visual Sentiment Prediction with Deep Convolutional Neural Networks",
        "authors": [
            "Can Xu",
            "Suleyman Cetintas",
            "Kuang-Chih Lee",
            "Li-Jia Li"
        ],
        "abstract": "Images have become one of the most popular types of media through which users convey their emotions within online social networks. Although vast amount of research is devoted to sentiment analysis of textual data, there has been very limited work that focuses on analyzing sentiment of image data. In this work, we propose a novel visual sentiment prediction framework that performs image understanding with Deep Convolutional Neural Networks (CNN). Specifically, the proposed sentiment prediction framework performs transfer learning from a CNN with millions of parameters, which is pre-trained on large-scale data for object recognition. Experiments conducted on two real-world datasets from Twitter and Tumblr demonstrate the effectiveness of the proposed visual sentiment analysis framework.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5752",
        "title": "Hypercolumns for Object Segmentation and Fine-grained Localization",
        "authors": [
            "Bharath Hariharan",
            "Pablo Arbel\u00e1ez",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation[22], where we improve state-of-the-art from 49.7[22] mean AP^r to 60.0, keypoint localization, where we get a 3.3 point boost over[20] and part labeling, where we show a 6.6 point gain over a strong baseline.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2015-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5825",
        "title": "Assessment of algorithms for mitosis detection in breast cancer histopathology images",
        "authors": [
            "Mitko Veta",
            "Paul J. van Diest",
            "Stefan M. Willems",
            "Haibo Wang",
            "Anant Madabhushi",
            "Angel Cruz-Roa",
            "Fabio Gonzalez",
            "Anders B. L. Larsen",
            "Jacob S. Vestergaard",
            "Anders B. Dahl",
            "Dan C. Cire\u015fan",
            "J\u00fcrgen Schmidhuber",
            "Alessandro Giusti",
            "Luca M. Gambardella",
            "F. Boray Tek",
            "Thomas Walter",
            "Ching-Wei Wang",
            "Satoshi Kondo",
            "Bogdan J. Matuszewski",
            "Frederic Precioso",
            "Violet Snell",
            "Josef Kittler",
            "Teofilo E. de Campos",
            "Adnan M. Khan",
            "Nasir M. Rajpoot",
            "Evdokia Arkoumani",
            "Miangela M. Lacle",
            "Max A. Viergever",
            "Josien P.W. Pluim"
        ],
        "abstract": "The proliferative activity of breast tumors, which is routinely estimated by counting of mitotic figures in hematoxylin and eosin stained histology sections, is considered to be one of the most important prognostic markers. However, mitosis counting is laborious, subjective and may suffer from low inter-observer agreement. With the wider acceptance of whole slide images in pathology labs, automatic image analysis has been proposed as a potential solution for these issues. In this paper, the results from the Assessment of Mitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The challenge was based on a data set consisting of 12 training and 11 testing subjects, with more than one thousand annotated mitotic figures by multiple observers. Short descriptions and results from the evaluation of eleven methods are presented. The top performing method has an error rate that is comparable to the inter-observer agreement among pathologists.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5878",
        "title": "Salient Object Detection: A Survey",
        "authors": [
            "Ali Borji",
            "Ming-Ming Cheng",
            "Qibin Hou",
            "Huaizu Jiang",
            "Jia Li"
        ],
        "abstract": "Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2019-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5879",
        "title": "A Unified Semantic Embedding: Relating Taxonomies and Attributes",
        "authors": [
            "Sung Ju Hwang",
            "Leonid Sigal"
        ],
        "abstract": "We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be represented by a supercategory + sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5908",
        "title": "Understanding image representations by measuring their equivariance and equivalence",
        "authors": [
            "Karel Lenc",
            "Andrea Vedaldi"
        ],
        "abstract": "Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2015-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5928",
        "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks",
        "authors": [
            "Alexey Dosovitskiy",
            "Jost Tobias Springenberg",
            "Maxim Tatarchenko",
            "Thomas Brox"
        ],
        "abstract": "We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5935",
        "title": "Towards Scene Understanding with Detailed 3D Object Representations",
        "authors": [
            "M.Zeeshan Zia",
            "Michael Stark",
            "Konrad Schindler"
        ],
        "abstract": "Current approaches to semantic image and scene understanding typically employ rather simple object representations such as 2D or 3D bounding boxes. While such coarse models are robust and allow for reliable object detection, they discard much of the information about objects' 3D shape and pose, and thus do not lend themselves well to higher-level reasoning. Here, we propose to base scene understanding on a high-resolution object representation. An object class - in our case cars - is modeled as a deformable 3D wireframe, which enables fine-grained modeling at the level of individual vertices and faces. We augment that model to explicitly include vertex-level occlusion, and embed all instances in a common coordinate frame, in order to infer and exploit object-object interactions. Specifically, from a single view we jointly estimate the shapes and poses of multiple objects in a common 3D frame. A ground plane in that frame is estimated by consensus among different objects, which significantly stabilizes monocular 3D pose estimation. The fine-grained model, in conjunction with the explicit 3D scene model, further allows one to infer part-level occlusions between the modeled objects, as well as occlusions by other, unmodeled scene elements. To demonstrate the benefits of such detailed object class models in the context of scene understanding we systematically evaluate our approach on the challenging KITTI street scene dataset. The experiments show that the model's ability to utilize image evidence at the level of individual parts improves monocular 3D pose estimation w.r.t. both location and (continuous) viewpoint.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2014-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6031",
        "title": "Finding Action Tubes",
        "authors": [
            "Georgia Gkioxari",
            "Jitendra Malik"
        ],
        "abstract": "We address the problem of action detection in videos. Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues. We incorporate appearance and motion in two ways. First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action. This leads to a significant reduction in the number of regions being processed and allows for faster computations. Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks. We link our predictions to produce detections consistent in time, which we call action tubes. We show that our approach outperforms other techniques in the task of action detection.\n    ",
        "submission_date": "2014-11-21T00:00:00",
        "last_modified_date": "2014-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6067",
        "title": "Viewpoints and Keypoints",
        "authors": [
            "Shubham Tulsiani",
            "Jitendra Malik"
        ],
        "abstract": "We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings - the constrained setting with known bounding boxes and the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we analyze the error modes and effect of object characteristics on performance to guide future efforts towards this goal.\n    ",
        "submission_date": "2014-11-22T00:00:00",
        "last_modified_date": "2015-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6069",
        "title": "Category-Specific Object Reconstruction from a Single Image",
        "authors": [
            "Abhishek Kar",
            "Shubham Tulsiani",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "Object reconstruction from a single image -- in the wild -- is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.\n    ",
        "submission_date": "2014-11-22T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6091",
        "title": "Virtual View Networks for Object Reconstruction",
        "authors": [
            "Jo\u00e3o Carreira",
            "Abhishek Kar",
            "Shubham Tulsiani",
            "Jitendra Malik"
        ],
        "abstract": "All that structure from motion algorithms \"see\" are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by computing geodesics on novel networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of applications of rigid structure-from-motion techniques may be significantly extended.\n    ",
        "submission_date": "2014-11-22T00:00:00",
        "last_modified_date": "2014-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6206",
        "title": "Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for Dynamic 3D MRI reconstruction",
        "authors": [
            "Dornoosh Zonoobi",
            "Shahrooz Faghih Roohi",
            "Ashraf A. Kassim"
        ],
        "abstract": "It has been recently shown that incorporating priori knowledge significantly improves the performance of basic compressive sensing based approaches. We have managed to successfully exploit this idea for recovering a matrix as a summation of a Low-rank and a Sparse component from compressive measurements. When applied to the problem of construction of 4D Cardiac MR image sequences in real-time from highly under-sampled $k-$space data, our proposed method achieves superior reconstruction quality compared to the other state-of-the-art methods.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6228",
        "title": "From Image-level to Pixel-level Labeling with Convolutional Networks",
        "authors": [
            "Pedro O. Pinheiro",
            "Ronan Collobert"
        ],
        "abstract": "We are interested in inferring object segmentation by leveraging only object class information, and by considering only minimal priors on the object segmentation task. This problem could be viewed as a kind of weakly supervised segmentation task, and naturally fits the Multiple Instance Learning (MIL) framework: every training image is known to have (or not) at least one pixel corresponding to the image class label, and the segmentation task can be rewritten as inferring the pixels belonging to the class of the object (given one image, and its object class). We propose a Convolutional Neural Network-based model, which is constrained during training to put more weight on pixels which are important for classifying the image. We show that at test time, the model has learned to discriminate the right pixels well enough, such that it performs very well on an existing segmentation benchmark, by adding only few smoothing priors. Our system is trained using a subset of the Imagenet dataset and the segmentation experiments are performed on the challenging Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model beats the state of the art results in weakly supervised object segmentation task by a large margin. We also compare the performance of our model with state of the art fully-supervised segmentation approaches.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6275",
        "title": "Detection of Non-Stationary Photometric Perturbations on Projection Screens",
        "authors": [
            "Miguel Casta\u00f1eda-Garay",
            "Oscar Belmonte-Fern\u00e1ndez",
            "Hebert P\u00e9rez-Ros\u00e9s",
            "Antonio Diaz-Tula"
        ],
        "abstract": "Interfaces based on projection screens have become increasingly more popular in recent years, mainly due to the large screen size and resolution that they provide, as well as their stereo-vision capabilities. This work shows a local method for real-time detection of non-stationary photometric perturbations in projected images by means of computer vision techniques. The method is based on the computation of differences between the images in the projector's frame buffer and the corresponding images on the projection screen observed by the camera. It is robust under spatial variations in the intensity of light emitted by the projector on the projection surface and also robust under stationary photometric perturbations caused by external factors. Moreover, we describe the experiments carried out to show the reliability of the method.\n    ",
        "submission_date": "2014-11-23T00:00:00",
        "last_modified_date": "2014-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6340",
        "title": "Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors",
        "authors": [
            "Thalaiyasingam Ajanthan",
            "Richard Hartley",
            "Mathieu Salzmann",
            "Hongdong Li"
        ],
        "abstract": "While widely acknowledged as highly effective in computer vision, multi-label MRFs with non-convex priors are difficult to optimize. To tackle this, we introduce an algorithm that iteratively approximates the original energy with an appropriately weighted surrogate energy that is easier to minimize. Our algorithm guarantees that the original energy decreases at each iteration. In particular, we consider the scenario where the global minimizer of the weighted surrogate energy can be obtained by a multi-label graph cut algorithm, and show that our algorithm then lets us handle of large variety of non-convex priors. We demonstrate the benefits of our method over state-of-the-art MRF energy minimization techniques on stereo and inpainting problems.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6365",
        "title": "On the mathematic modeling of non-parametric curves based on cubic B\u00e9zier curves",
        "authors": [
            "Ha Jong Won",
            "Choe Chun Hwa",
            "Li Kum Song"
        ],
        "abstract": "B\u00e9zier splines are widely available in various systems with the curves and surface designs. In general, the B\u00e9zier spline can be specified with the B\u00e9zier curve segments and a B\u00e9zier curve segment can be fitted to any number of control points. The number of control points determines the degree of the B\u00e9zier polynomial. This paper presents a method which determines control points for B\u00e9zier curves approximating segments of obtained image outline(non-parametric curve) by using the properties of cubic B\u00e9zier curves. Proposed method is a technique to determine the control points that has generality and reduces the error of the B\u00e9zier curve approximation. Main advantage of proposed method is that it has higher accuracy and compression rate than previous methods. The cubic B\u00e9zier spline is obtained from cubic B\u00e9zier curve segments. To demonstrate the various performances of the proposed algorithm, experimental results are compared.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6369",
        "title": "Scale-Invariant Convolutional Neural Networks",
        "authors": [
            "Yichong Xu",
            "Tianjun Xiao",
            "Jiaxing Zhang",
            "Kuiyuan Yang",
            "Zheng Zhang"
        ],
        "abstract": "Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6382",
        "title": "Mid-level Deep Pattern Mining",
        "authors": [
            "Yao Li",
            "Lingqiao Liu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. In this work, we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). Specifically, we find that for an image patch, activations extracted from the first fully-connected layer of CNNs have two appealing properties which enable its seamless integration with pattern mining. Patterns are then discovered from a large number of CNN activations of image patches through the well-known association rule mining. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent. We apply our approach to scene and object classification tasks, and demonstrate that our approach outperforms all previous works on mid-level visual element discovery by a sizeable margin with far fewer elements being used. Our approach also outperforms or matches recent works using CNN for these tasks. Source code of the complete system is available online.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6387",
        "title": "Deep Convolutional Neural Fields for Depth Estimation from a Single Image",
        "authors": [
            "Fayao Liu",
            "Chunhua Shen",
            "Guosheng Lin"
        ],
        "abstract": "We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions, etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework.\n",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6406",
        "title": "Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors",
        "authors": [
            "Lingqiao Liu",
            "Chunhua Shen",
            "Lei Wang",
            "Anton van den Hengel",
            "Chao Wang"
        ],
        "abstract": "Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, % FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6447",
        "title": "The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification",
        "authors": [
            "Tianjun Xiao",
            "Yichong Xu",
            "Kuiyuan Yang",
            "Jiaxing Zhang",
            "Yuxin Peng",
            "Zheng Zhang"
        ],
        "abstract": "Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what).\n",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6509",
        "title": "Persistent Evidence of Local Image Properties in Generic ConvNets",
        "authors": [
            "Ali Sharif Razavian",
            "Hossein Azizpour",
            "Atsuto Maki",
            "Josephine Sullivan",
            "Carl Henrik Ek",
            "Stefan Carlsson"
        ],
        "abstract": "Supervised training of a convolutional network for object classification should make explicit any information related to the class of objects and disregard any auxiliary information associated with the capture of the image or the variation within the object class. Does this happen in practice? Although this seems to pertain to the very final layers in the network, if we look at earlier layers we find that this is not the case. Surprisingly, strong spatial information is implicit. This paper addresses this, in particular, exploiting the image representation at the first fully connected layer, i.e. the global image descriptor which has been recently shown to be most effective in a range of visual recognition tasks. We empirically demonstrate evidences for the finding in the contexts of four different tasks: 2d landmark detection, 2d object keypoints prediction, estimation of the RGB values of input image, and recovery of semantic label of each pixel. We base our investigation on a simple framework with ridge rigression commonly across these tasks, and show results which all support our insight. Such spatial information can be used for computing correspondence of landmarks to a good accuracy, but should potentially be useful for improving the training of the convolutional nets for classification purposes.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6660",
        "title": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition",
        "authors": [
            "Zhenzhong Lan",
            "Ming Lin",
            "Xuanchong Li",
            "Alexander G. Hauptmann",
            "Bhiksha Raj"
        ],
        "abstract": "Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6836",
        "title": "Deep convolutional filter banks for texture recognition and segmentation",
        "authors": [
            "Mircea Cimpoi",
            "Subhransu Maji",
            "Andrea Vedaldi"
        ],
        "abstract": "Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture at- tributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. D-CNN substantially improves the state-of-the-art in texture, mate- rial and scene recognition. Our approach achieves 82.3% accuracy on Flickr material dataset and 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. D-CNN easily trans- fers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at lo- calizing stuff categories and obtains state-of-the-art re- sults on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6850",
        "title": "Similarity- based approach for outlier detection",
        "authors": [
            "Amina Dik",
            "Khalid Jebari",
            "Abdelaziz Bouroumi",
            "Aziz Ettouhami"
        ],
        "abstract": "This paper presents a new approach for detecting outliers by introducing the notion of object's proximity. The main idea is that normal point has similar characteristics with several neighbors. So the point in not an outlier if it has a high degree of proximity and its neighbors are several. The performance of this approach is illustrated through real datasets\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6909",
        "title": "Image Classification and Retrieval from User-Supplied Tags",
        "authors": [
            "Hamid Izadinia",
            "Ali Farhadi",
            "Aaron Hertzmann",
            "Matthew D. Hoffman"
        ],
        "abstract": "This paper proposes direct learning of image classification from user-supplied tags, without filtering. Each tag is supplied by the user who shared the image online. Enormous numbers of these tags are freely available online, and they give insight about the image categories important to users and to image classification. Our approach is complementary to the conventional approach of manual annotation, which is extremely costly. We analyze of the Flickr 100 Million Image dataset, making several useful observations about the statistics of these tags. We introduce a large-scale robust classification algorithm, in order to handle the inherent noise in these tags, and a calibration procedure to better predict objective annotations. We show that freely available, user-supplied tags can obtain similar or superior results to large databases of costly manual annotations.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6970",
        "title": "Post-acquisition image based compensation for thickness variation in microscopy section series",
        "authors": [
            "Philipp Hanslovsky",
            "John A. Bogovic",
            "Stephan Saalfeld"
        ],
        "abstract": "Serial section Microscopy is an established method for volumetric anatomy reconstruction. Section series imaged with Electron Microscopy are currently vital for the reconstruction of the synaptic connectivity of entire animal brains such as that of Drosophila melanogaster. The process of removing ultrathin layers from a solid block containing the specimen, however, is a fragile procedure and has limited precision with respect to section thickness. We have developed a method to estimate the relative z-position of each individual section as a function of signal change across the section series. First experiments show promising results on both serial section Transmission Electron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) series. We made our solution available as Open Source plugins for the TrakEM2 software and the ImageJ distribution Fiji.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7113",
        "title": "Real time Detection of Lane Markers in Urban Streets",
        "authors": [
            "Mohamed Aly"
        ],
        "abstract": "We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7336",
        "title": "Edge direction matrixes-based local binar patterns descriptor for shape pattern recognition",
        "authors": [
            "Mohammed A. Talab",
            "Siti Norul Huda Sheikh Abdullah",
            "Bilal Bataineh"
        ],
        "abstract": "Shapes and texture image recognition usage is an essential branch of pattern recognition. It is made up of techniques that aim at extracting information from images via human knowledge and works. Local Binary Pattern (LBP) ensures encoding global and local information and scaling invariance by introducing a look-up table to reflect the uniformity structure of an object. However, edge direction matrixes (EDMS) only apply global invariant descriptor which employs first and secondary order relationships. The main idea behind this methodology is the need of improved recognition capabilities, a goal achieved by the combinative use of these descriptors. This collaboration aims to make use of the major advantages each one presents, by simultaneously complementing each other, in order to elevate their weak points. By using multiple classifier approaches such as random forest and multi-layer perceptron neural network, the proposed combinative descriptor are compared with the state of the art combinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS), LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS image, Enghlishfnt and Arabic calligraphy . The experiments have shown the superiority of the introduced descriptor over the GLCM with EDMS, LBP and moment invariants and other well-known descriptor such as Scale Invariant Feature Transform from the literature.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7399",
        "title": "Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation",
        "authors": [
            "Benjamin Klein",
            "Guy Lev",
            "Gil Sadeh",
            "Lior Wolf"
        ],
        "abstract": "In the traditional object recognition pipeline, descriptors are densely sampled over an image, pooled into a high dimensional non-linear representation and then passed to a classifier. In recent years, Fisher Vectors have proven empirically to be the leading representation for a large variety of applications. The Fisher Vector is typically taken as the gradients of the log-likelihood of descriptors, with respect to the parameters of a Gaussian Mixture Model (GMM). Motivated by the assumption that different distributions should be applied for different datasets, we present two other Mixture Models and derive their Expectation-Maximization and Fisher Vector expressions. The first is a Laplacian Mixture Model (LMM), which is based on the Laplacian distribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) which is based on a weighted geometric mean of the Gaussian and Laplacian distribution. An interesting property of the Expectation-Maximization algorithm for the latter is that in the maximization step, each dimension in each component is chosen to be either a Gaussian or a Laplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we achieve state-of-the-art results for both the image annotation and the image search by a sentence tasks.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2015-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7466",
        "title": "The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification",
        "authors": [
            "Lingqiao Liu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "A number of recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large dataset can be adopted as a universal image description which leads to astounding performance in many visual classification tasks. Most of these studies, if not all, adopt activations of the fully-connected layer of a DCNN as the image or region representation and it is believed that convolutional layer activations are less discriminative.\n",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7564",
        "title": "Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation and Applications",
        "authors": [
            "Peng Wang",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Philip H. S. Torr"
        ],
        "abstract": "In computer vision, many problems such as image segmentation, pixel labelling, and scene parsing can be formulated as binary quadratic programs (BQPs). For submodular problems, cuts based methods can be employed to efficiently solve large-scale problems. However, general nonsubmodular problems are significantly more challenging to solve. Finding a solution when the problem is of large size to be of practical interest, however, typically requires relaxation. Two standard relaxation methods are widely used for solving general BQPs--spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high, especially for large scale problems. In this work, we present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the dual problem. Both of them are significantly more efficiently than standard interior-point methods. In practice, the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. Our experiments on a few computer vision applications including clustering, image segmentation, co-segmentation and registration show the potential of our SDP formulation for solving large-scale BQPs.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7591",
        "title": "An Egocentric Look at Video Photographer Identity",
        "authors": [
            "Yedid Hoshen",
            "Shmuel Peleg"
        ],
        "abstract": "Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, reporting substantial increase in sales every year. As head-worn cameras do not capture the photographer, it may seem that the anonymity of the photographer is preserved even when the video is publicly distributed.\n",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7655",
        "title": "A statistical reduced-reference method for color image quality assessment",
        "authors": [
            "Mounir Omari",
            "Mohammed El Hassouni",
            "Abdelkaher Ait Abdelouahad",
            "Hocine Cherifi"
        ],
        "abstract": "Although color is a fundamental feature of human visual perception, it has been largely unexplored in the reduced-reference (RR) image quality assessment (IQA) schemes. In this paper, we propose a natural scene statistic (NSS) method, which efficiently uses this information. It is based on the statistical deviation between the steerable pyramid coefficients of the reference color image and the degraded one. We propose and analyze the multivariate generalized Gaussian distribution (MGGD) to model the underlying statistics. In order to quantify the degradation, we develop and evaluate two measures based respectively on the Geodesic distance between two MGGDs and on the closed-form of the Kullback Leibler divergence. We performed an extensive evaluation of both metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID 2008 benchmark and the FRTV Phase I validation process. Experimental results demonstrate the effectiveness of the proposed framework to achieve a good consistency with human visual perception. Furthermore, the best configuration is obtained with CIELAB color space associated to KLD deviation measure.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7676",
        "title": "Visual Representations: Defining Properties and Deep Approximations",
        "authors": [
            "Stefano Soatto",
            "Alessandro Chiuso"
        ],
        "abstract": "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7682",
        "title": "On color image quality assessment using natural image statistics",
        "authors": [
            "Mounir Omari",
            "Mohammed El Hassouni",
            "Hocine Cherifi",
            "Abdelkaher Ait Abdelouahad"
        ],
        "abstract": "Color distortion can introduce a significant damage in visual quality perception, however, most of existing reduced-reference quality measures are designed for grayscale images. In this paper, we consider a basic extension of well-known image-statistics based quality assessment measures to color images. In order to evaluate the impact of color information on the measures efficiency, two color spaces are investigated: RGB and CIELAB. Results of an extensive evaluation using TID 2013 benchmark demonstrates that significant improvement can be achieved for a great number of distortion type when the CIELAB color representation is used.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7714",
        "title": "Features in Concert: Discriminative Feature Selection meets Unsupervised Clustering",
        "authors": [
            "Marius Leordeanu",
            "Alexandra Radu",
            "Rahul Sukthankar"
        ],
        "abstract": "Feature selection is an essential problem in computer vision, important for category learning and recognition. Along with the rapid development of a wide variety of visual features and classifiers, there is a growing need for efficient feature selection and combination methods, to construct powerful classifiers for more complex and higher-level recognition tasks. We propose an algorithm that efficiently discovers sparse, compact representations of input features or classifiers, from a vast sea of candidates, with important optimality properties, low computational cost and excellent accuracy in practice. Different from boosting, we start with a discriminant linear classification formulation that encourages sparse solutions. Then we obtain an equivalent unsupervised clustering problem that jointly discovers ensembles of diverse features. They are independently valuable but even more powerful when united in a cluster of classifiers. We evaluate our method on the task of large-scale recognition in video and show that it significantly outperforms classical selection approaches, such as AdaBoost and greedy forward-backward selection, and powerful classifiers such as SVMs, in speed of training and performance, especially in the case of limited training data.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7715",
        "title": "Flying Objects Detection from a Single Moving Camera",
        "authors": [
            "Artem Rozantsev",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "We propose an approach to detect flying objects such as UAVs and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves.\n",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7766",
        "title": "Deep Learning Face Attributes in the Wild",
        "authors": [
            "Ziwei Liu",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7798",
        "title": "Cross-Modal Learning via Pairwise Constraints",
        "authors": [
            "Ran He",
            "Man Zhang",
            "Liang Wang",
            "Ye Ji",
            "Qiyue Yin"
        ],
        "abstract": "In multimedia applications, the text and image components in a web document form a pairwise constraint that potentially indicates the same semantic concept. This paper studies cross-modal learning via the pairwise constraint, and aims to find the common structure hidden in different modalities. We first propose a compound regularization framework to deal with the pairwise constraint, which can be used as a general platform for developing cross-modal algorithms. For unsupervised learning, we propose a cross-modal subspace clustering method to learn a common structure for different modalities. For supervised learning, to reduce the semantic gap and the outliers in pairwise constraints, we propose a cross-modal matching method based on compound ?21 regularization along with an iteratively reweighted algorithm to find the global optimum. Extensive experiments demonstrate the benefits of joint text and image modeling with semantically induced pairwise constraints, and show that the proposed cross-modal methods can further reduce the semantic gap between different modalities and improve the clustering/retrieval accuracy.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7855",
        "title": "V-variable image compression",
        "authors": [
            "Franklin Mendivil",
            "\u00d6rjan Stenflo"
        ],
        "abstract": "V-variable fractals, where $V$ is a positive integer, are intuitively fractals with at most $V$ different \"forms\" or \"shapes\" at all levels of magnification. In this paper we describe how V-variable fractals can be used for the purpose of image compression.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7883",
        "title": "Articulated motion discovery using pairs of trajectories",
        "authors": [
            "Luca Del Pero",
            "Susanna Ricco",
            "Rahul Sukthankar",
            "Vittorio Ferrari"
        ],
        "abstract": "We propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural, unscripted behaviors, such as tigers in the wild. We discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time, such that each trajectory is attached to a different moving part on the object. The pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajectories. Our method generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, and clusters them by type (e.g., running, turning head, drinking water). We present experiments on two datasets: dogs from YouTube-Objects and a new dataset of National Geographic tiger videos. Results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors (e.g., HOG and DTFs) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7911",
        "title": "On Rendering Synthetic Images for Training an Object Detector",
        "authors": [
            "Artem Rozantsev",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances.\n",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7923",
        "title": "Learning Face Representation from Scratch",
        "authors": [
            "Dong Yi",
            "Zhen Lei",
            "Shengcai Liao",
            "Stan Z. Li"
        ],
        "abstract": "Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7935",
        "title": "Multiple object tracking with context awareness",
        "authors": [
            "Laura Leal-Taix\u00e9"
        ],
        "abstract": "Multiple people tracking is a key problem for many applications such as surveillance, animation or car navigation, and a key input for tasks such as activity recognition. In crowded environments occlusions and false detections are common, and although there have been substantial advances in recent years, tracking is still a challenging task. Tracking is typically divided into two steps: detection, i.e., locating the pedestrians in the image, and data association, i.e., linking detections across frames to form complete trajectories.\n",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7964",
        "title": "Effective Face Frontalization in Unconstrained Images",
        "authors": [
            "Tal Hassner",
            "Shai Harel",
            "Eran Paz",
            "Roee Enbar"
        ],
        "abstract": "\"Frontalization\" is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation.\n    ",
        "submission_date": "2014-11-28T00:00:00",
        "last_modified_date": "2014-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0003",
        "title": "3D-Assisted Image Feature Synthesis for Novel Views of an Object",
        "authors": [
            "Hao Su",
            "Fan Wang",
            "Li Yi",
            "Leonidas Guibas"
        ],
        "abstract": "Comparing two images in a view-invariant way has been a challenging problem in computer vision for a long time, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize new features for other views of the same object. To accomplish this, we introduce an aligned set of 3D models in the same class as the input object image. Each 3D model is represented by a set of views, and we study the correlation of image patches between different views, seeking what we call surrogates --- patches in one view whose feature content predicts well the features of a patch in another view. In particular, for each patch in the novel desired view, we seek surrogates from the observed view of the given image. For a given surrogate, we predict that surrogate using linear combination of the corresponding patches of the 3D model views, learn the coefficients, and then transfer these coefficients on a per patch basis to synthesize the features of the patch in the novel view. In this way we can create feature sets for all views of the latent object, providing us a multi-view representation of the object. View-invariant object comparisons are achieved simply by computing the $L^2$ distances between the features of corresponding views. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the proposed view-agnostic distance (VAD) in fine-grained image retrieval (100 object classes) and classification tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than traditional image features in this respect.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0035",
        "title": "Understanding Deep Image Representations by Inverting Them",
        "authors": [
            "Aravindh Mahendran",
            "Andrea Vedaldi"
        ],
        "abstract": "Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0060",
        "title": "Egocentric Pose Recognition in Four Lines of Code",
        "authors": [
            "Gregory Rogez",
            "James S. Supancic III",
            "Deva Ramanan"
        ],
        "abstract": "We tackle the problem of estimating the 3D pose of an individual's upper limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider pose estimation during everyday interactions with objects. Past work shows that strong pose+viewpoint priors and depth-based features are crucial for robust performance. In egocentric views, hands and arms are observable within a well defined volume in front of the camera. We call this volume an egocentric workspace. A notable property is that hand appearance correlates with workspace location. To exploit this correlation, we classify arm+hand configurations in a global egocentric coordinate frame, rather than a local scanning window. This greatly simplify the architecture and improves performance. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera whose intrinsic parameters match our physical camera, 2) computes perspective-aware depth features on this entire volume and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. Our method provides state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0062",
        "title": "A Bayesian Framework for Sparse Representation-Based 3D Human Pose Estimation",
        "authors": [
            "Behnam Babagholami-Mohamadabadi",
            "Amin Jourabloo",
            "Ali Zarghami",
            "Shohreh Kasaei"
        ],
        "abstract": "A Bayesian framework for 3D human pose estimation from monocular images based on sparse representation (SR) is introduced. Our probabilistic approach aims at simultaneously learning two overcomplete dictionaries (one for the visual input space and the other for the pose space) with a shared sparse representation. Existing SR-based pose estimation approaches only offer a point estimation of the dictionary and the sparse codes. Therefore, they might be unreliable when the number of training examples is small. Our Bayesian framework estimates a posterior distribution for the sparse codes and the dictionaries from labeled training data. Hence, it is robust to overfitting on small-size training data. Experimental results on various human activities show that the proposed method is superior to the state of-the-art pose estimation algorithms.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0065",
        "title": "3D Hand Pose Detection in Egocentric RGB-D Images",
        "authors": [
            "Gregory Rogez",
            "James S. Supancic III",
            "Maryam Khademi",
            "Jose Maria Martinez Montiel",
            "Deva Ramanan"
        ],
        "abstract": "We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0069",
        "title": "Pedestrian Detection aided by Deep Learning Semantic Tasks",
        "authors": [
            "Yonglong Tian",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk' or `wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0100",
        "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images",
        "authors": [
            "Stefan Mathe",
            "Cristian Sminchisescu"
        ],
        "abstract": "State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0111",
        "title": "Color image quality assessment measure using multivariate generalized Gaussian distribution",
        "authors": [
            "Mounir Omari",
            "Abdelkaher Ait Abdelouahad",
            "Mohammed El Hassouni",
            "Hocine Cherifi"
        ],
        "abstract": "This paper deals with color image quality assessment in the reduced-reference framework based on natural scenes statistics. In this context, we propose to model the statistics of the steerable pyramid coefficients by a Multivariate Generalized Gaussian distribution (MGGD). This model allows taking into account the high correlation between the components of the RGB color space. For each selected scale and orientation, we extract a parameter matrix from the three color components subbands. In order to quantify the visual degradation, we use a closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using \"TID 2008\" benchmark, the proposed measure has been compared with the most influential methods according to the FRTV1 VQEG framework. Results demonstrates its effectiveness for a great variety of distortion type. Among other benefits this measure uses only very little information about the original image.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2014-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0165",
        "title": "Robust Camera Location Estimation by Convex Programming",
        "authors": [
            "Onur Ozyesil",
            "Amit Singer"
        ],
        "abstract": "$3$D structure recovery from a collection of $2$D images requires the estimation of the camera locations and orientations, i.e. the camera motion. For large, irregular collections of images, existing methods for the location estimation part, which can be formulated as the inverse problem of estimating $n$ locations $\\mathbf{t}_1, \\mathbf{t}_2, \\ldots, \\mathbf{t}_n$ in $\\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions $\\frac{\\mathbf{t}_i - \\mathbf{t}_j}{\\|\\mathbf{t}_i - \\mathbf{t}_j\\|}$, are sensitive to outliers in direction measurements. In this paper, we firstly provide a complete characterization of well-posed instances of the location estimation problem, by presenting its relation to the existing theory of parallel rigidity. For robust estimation of camera locations, we introduce a two-step approach, comprised of a pairwise direction estimation method robust to outliers in point correspondences between image pairs, and a convex program to maintain robustness to outlier directions. In the presence of partially corrupted measurements, we empirically demonstrate that our convex formulation can even recover the locations exactly. Lastly, we demonstrate the utility of our formulations through experiments on Internet photo collections.\n    ",
        "submission_date": "2014-11-29T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0251",
        "title": "A Clearer Picture of Blind Deconvolution",
        "authors": [
            "Daniele Perrone",
            "Paolo Favaro"
        ],
        "abstract": "Blind deconvolution is the problem of recovering a sharp image and a blur kernel from a noisy blurry image. Recently, there has been a significant effort on understanding the basic mechanisms to solve blind deconvolution. While this effort resulted in the deployment of effective algorithms, the theoretical findings generated contrasting views on why these approaches worked. On the one hand, one could observe experimentally that alternating energy minimization algorithms converge to the desired solution. On the other hand, it has been shown that such alternating minimization algorithms should fail to converge and one should instead use a so-called Variational Bayes approach. To clarify this conundrum, recent work showed that a good image and blur prior is instead what makes a blind deconvolution algorithm work. Unfortunately, this analysis did not apply to algorithms based on total variation regularization. In this manuscript, we provide both analysis and experiments to get a clearer picture of blind deconvolution. Our analysis reveals the very reason why an algorithm based on total variation works. We also introduce an implementation of this algorithm and show that, in spite of its extreme simplicity, it is very robust and achieves a performance comparable to the state of the art.\n    ",
        "submission_date": "2014-11-30T00:00:00",
        "last_modified_date": "2014-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0265",
        "title": "Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels",
        "authors": [
            "Sadeep Jayasumana",
            "Richard Hartley",
            "Mathieu Salzmann",
            "Hongdong Li",
            "Mehrtash Harandi"
        ],
        "abstract": "In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.\n    ",
        "submission_date": "2014-11-30T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0296",
        "title": "Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection",
        "authors": [
            "George Papandreou",
            "Iasonas Kokkinos",
            "Pierre-Andr\u00e9 Savalle"
        ],
        "abstract": "Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling' (MP) layers to extract deformation-invariant features, but we argue in favor of a more refined treatment. First, we introduce epitomic convolution as a building block alternative to the common convolution-MP cascade of DCNNs; while having identical complexity to MP, Epitomic Convolution allows for parameter sharing across different filters, resulting in faster convergence and better generalization. Second, we introduce a Multiple Instance Learning approach to explicitly accommodate global translation and scaling when training a DCNN exclusively with class labels. For this we rely on a `patchwork' data structure that efficiently lays out all image scales and positions as candidates to a DCNN. Factoring global and local deformations allows a DCNN to `focus its resources' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement. Third, further pursuing this idea, we develop an efficient DCNN sliding window object detector that employs explicit search over position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on the Pascal VOC 2007 benchmark.\n    ",
        "submission_date": "2014-11-30T00:00:00",
        "last_modified_date": "2014-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0439",
        "title": "Fuzzy human motion analysis: A review",
        "authors": [
            "Chern Hong Lim",
            "Ekta Vats",
            "Chee Seng Chan"
        ],
        "abstract": "Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0477",
        "title": "Recovering Spatiotemporal Correspondence between Deformable Objects by Exploiting Consistent Foreground Motion in Video",
        "authors": [
            "Luca Del Pero",
            "Susanna Ricco",
            "Rahul Sukthankar",
            "Vittorio Ferrari"
        ],
        "abstract": "Given unstructured videos of deformable objects, we automatically recover spatiotemporal correspondences to map one object to another (such as animals in the wild). While traditional methods based on appearance fail in such challenging conditions, we exploit consistency in object motion between instances. Our approach discovers pairs of short video intervals where the object moves in a consistent manner and uses these candidates as seeds for spatial alignment. We model the spatial correspondence between the point trajectories on the object in one interval to those in the other using a time-varying Thin Plate Spline deformation model. On a large dataset of tiger and horse videos, our method automatically aligns thousands of pairs of frames to a high accuracy, and outperforms the popular SIFT Flow algorithm.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2016-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0494",
        "title": "Orthogonal Matrix Retrieval in Cryo-Electron Microscopy",
        "authors": [
            "Tejal Bhamre",
            "Teng Zhang",
            "Amit Singer"
        ],
        "abstract": "In single particle reconstruction (SPR) from cryo-electron microscopy (cryo-EM), the 3D structure of a molecule needs to be determined from its 2D projection images taken at unknown viewing directions. Zvi Kam showed already in 1980 that the autocorrelation function of the 3D molecule over the rotation group SO(3) can be estimated from 2D projection images whose viewing directions are uniformly distributed over the sphere. The autocorrelation function determines the expansion coefficients of the 3D molecule in spherical harmonics up to an orthogonal matrix of size $(2l+1)\\times (2l+1)$ for each $l=0,1,2,...$. In this paper we show how techniques for solving the phase retrieval problem in X-ray crystallography can be modified for the cryo-EM setup for retrieving the missing orthogonal matrices. Specifically, we present two new approaches that we term Orthogonal Extension and Orthogonal Replacement, in which the main algorithmic components are the singular value decomposition and semidefinite programming. We demonstrate the utility of these approaches through numerical experiments on simulated data.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0623",
        "title": "Material Recognition in the Wild with the Materials in Context Database",
        "authors": [
            "Sean Bell",
            "Paul Upchurch",
            "Noah Snavely",
            "Kavita Bala"
        ],
        "abstract": "Recognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. In this paper, we introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild.\n",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0680",
        "title": "Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit",
        "authors": [
            "Ali Ayremlou",
            "Thomas Goldstein",
            "Ashok Veeraraghavan",
            "Richard Baraniuk"
        ],
        "abstract": "Sparse approximations using highly over-complete dictionaries is a state-of-the-art tool for many imaging applications including denoising, super-resolution, compressive sensing, light-field analysis, and object recognition. Unfortunately, the applicability of such methods is severely hampered by the computational burden of sparse approximation: these algorithms are linear or super-linear in both the data dimensionality and size of the dictionary. We propose a framework for learning the hierarchical structure of over-complete dictionaries that enables fast computation of sparse representations. Our method builds on tree-based strategies for nearest neighbor matching, and presents domain-specific enhancements that are highly efficient for the analysis of image patches. Contrary to most popular methods for building spatial data structures, out methods rely on shallow, balanced trees with relatively few layers. We show an extensive array of experiments on several applications such as image denoising/superresolution, compressive video/light-field sensing where we practically achieve 100-1000x speedup (with a less than 1dB loss in accuracy).\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2014-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0767",
        "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
        "authors": [
            "Du Tran",
            "Lubomir Bourdev",
            "Rob Fergus",
            "Lorenzo Torresani",
            "Manohar Paluri"
        ],
        "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0774",
        "title": "Feedforward semantic segmentation with zoom-out features",
        "authors": [
            "Mohammadreza Mostajabi",
            "Payman Yadollahpour",
            "Gregory Shakhnarovich"
        ],
        "abstract": "We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by \"zooming out\" from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves new state of the art performance in semantic segmentation, obtaining 64.4% average accuracy on the PASCAL VOC 2012 test set.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0781",
        "title": "Fast Steerable Principal Component Analysis",
        "authors": [
            "Zhizhen Zhao",
            "Yoel Shkolnisky",
            "Amit Singer"
        ],
        "abstract": "Cryo-electron microscopy nowadays often requires the analysis of hundreds of thousands of 2D images as large as a few hundred pixels in each direction. Here we introduce an algorithm that efficiently and accurately performs principal component analysis (PCA) for a large set of two-dimensional images, and, for each image, the set of its uniform rotations in the plane and their reflections. For a dataset consisting of $n$ images of size $L \\times L$ pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, while existing algorithms take $O(nL^4)$. The new algorithm computes the expansion coefficients of the images in a Fourier-Bessel basis efficiently using the non-uniform fast Fourier transform. We compare the accuracy and efficiency of the new algorithm with traditional PCA and existing algorithms for steerable PCA.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0801",
        "title": "Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation",
        "authors": [
            "Poorna Banerjee Dasgupta"
        ],
        "abstract": "Noise removal from images is a part of image restoration in which we try to reconstruct or recover an image that has been degraded by using apriori knowledge of the degradation phenomenon. Noises present in images can be of various types with their characteristic Probability Distribution Functions (PDF). Noise removal techniques depend on the kind of noise present in the image rather than on the image itself. This paper explores the effects of applying noise reduction filters having similar properties on noisy images with emphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the results.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0826",
        "title": "Hashing on Nonlinear Manifolds",
        "authors": [
            "Fumin Shen",
            "Chunhua Shen",
            "Qinfeng Shi",
            "Anton van den Hengel",
            "Zhenmin Tang",
            "Heng Tao Shen"
        ],
        "abstract": "Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties, an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved, for example, by minimizing the quantization error with learned orthogonal rotations. In addition, a supervised inductive manifold hashing framework is developed by incorporating the label information, which is shown to greatly advance the semantic retrieval performance.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0985",
        "title": "Covariance estimation using conjugate gradient for 3D classification in Cryo-EM",
        "authors": [
            "Joakim And\u00e9n",
            "Eugene Katsevich",
            "Amit Singer"
        ],
        "abstract": "Classifying structural variability in noisy projections of biological macromolecules is a central problem in Cryo-EM. In this work, we build on a previous method for estimating the covariance matrix of the three-dimensional structure present in the molecules being imaged. Our proposed method allows for incorporation of contrast transfer function and non-uniform distribution of viewing angles, making it more suitable for real-world data. We evaluate its performance on a synthetic dataset and an experimental dataset obtained by imaging a 70S ribosome complex.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1123",
        "title": "DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection",
        "authors": [
            "Gedas Bertasius",
            "Jianbo Shi",
            "Lorenzo Torresani"
        ],
        "abstract": "Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection.\n",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1135",
        "title": "Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning",
        "authors": [
            "Judy Hoffman",
            "Deepak Pathak",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.\n    ",
        "submission_date": "2014-12-02T00:00:00",
        "last_modified_date": "2014-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1194",
        "title": "Gradient Boundary Histograms for Action Recognition",
        "authors": [
            "Feng Shi",
            "Robert Laganiere",
            "Emil Petriu"
        ],
        "abstract": "This paper introduces a high efficient local spatiotemporal descriptor, called gradient boundary histograms (GBH). The proposed GBH descriptor is built on simple spatio-temporal gradients, which are fast to compute. We demonstrate that it can better represent local structure and motion than other gradient-based descriptors, and significantly outperforms them on large realistic datasets. A comprehensive evaluation shows that the recognition accuracy is preserved while the spatial resolution is greatly reduced, which yields both high efficiency and low memory usage.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1216",
        "title": "Simple Two-Dimensional Object Tracking based on a Graph Algorithm",
        "authors": [
            "Alexandra Heidsieck"
        ],
        "abstract": "The visual observation and tracking of cells and other micrometer-sized objects has many different biomedical applications. The automation of those tasks based on computer methods helps in the evaluation of such measurements. In this work, we present a general purpose algorithm that excels at evaluating deterministic behavior of micrometer-sized objects. Our concrete application is the tracking of fast moving objects over large distances along deterministic trajectories in a microscopic video. Thereby, we are able to determine characteristic properties of the objects. For this purpose, we use a set of basic algorithms, including blob recognition, feature-based shape recognition and a graph algorithm, and combined them in a novel way. An evaluation of the algorithms performance shows a high accuracy in the recognition of objects as well as of complete trajectories. Moreover, a direct comparison to a similar algorithm shows superior recognition rates.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1265",
        "title": "Deeply learned face representations are sparse, selective, and robust",
        "authors": [
            "Yi Sun",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1283",
        "title": "Convolutional Feature Masking for Joint Object and Stuff Segmentation",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and \"stuff\" (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1441",
        "title": "Scalable, High-Quality Object Detection",
        "authors": [
            "Christian Szegedy",
            "Scott Reed",
            "Dumitru Erhan",
            "Dragomir Anguelov",
            "Sergey Ioffe"
        ],
        "abstract": "Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox~method: AP increases from $0.42$ to $0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1442",
        "title": "Memory Bounded Deep Convolutional Networks",
        "authors": [
            "Maxwell D. Collins",
            "Pushmeet Kohli"
        ],
        "abstract": "In this work, we investigate the use of sparsity-inducing regularizers during training of Convolution Neural Networks (CNNs). These regularizers encourage that fewer connections in the convolution and fully connected layers take non-zero values and in effect result in sparse connectivity between hidden units in the deep network. This in turn reduces the memory and runtime cost involved in deploying the learned CNNs. We show that training with such regularization can still be performed using stochastic gradient descent implying that it can be used easily in existing codebases. Experimental evaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows that our regularizers can result in dramatic reductions in memory requirements. For instance, when applied on AlexNet, our method can reduce the memory consumption by a factor of four with minimal loss in accuracy.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1455",
        "title": "Event Retrieval Using Motion Barcodes",
        "authors": [
            "Gil Ben-Artzi",
            "Michael Werman",
            "Shmuel Peleg"
        ],
        "abstract": "We introduce a simple and effective method for retrieval of videos showing a specific event, even when the videos of that event were captured from significantly different viewpoints. Appearance-based methods fail in such cases, as appearances change with large changes of viewpoints.\n",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1506",
        "title": "Textural Approach for Mass Abnormality Segmentation in Mammographic Images",
        "authors": [
            "Khamsa Djaroudib",
            "Abdelmalik Taleb Ahmed",
            "Abdelmadjid Zidani"
        ],
        "abstract": "Mass abnormality segmentation is a vital step for the medical diagnostic process and is attracting more and more the interest of many research groups. Currently, most of the works achieved in this area have used the Gray Level Co-occurrence Matrix (GLCM) as texture features with a region-based approach. These features come in previous phase for segmentation stage or are using as inputs to classification stage. The work discussed in this paper attempts to experiment the GLCM method under a contour-based approach. Besides, we experiment the proposed approach on various tissues densities to bring more significant results. At this end, we explored some challenging breast images from BIRADS medical Data Base. Our first experimentations showed promising results with regard to the edges mass segmentation methods. This paper discusses first the main works achieved in this area. Sections 2 and 3 present materials and our methodology. The main results are showed and evaluated before concluding our paper.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1526",
        "title": "Parsing Occluded People by Flexible Compositions",
        "authors": [
            "Xianjie Chen",
            "Alan Yuille"
        ],
        "abstract": "This paper presents an approach to parsing humans when there is significant occlusion. We model humans using a graphical model which has a tree structure building on recent work [32, 6] and exploit the connectivity prior that, even in presence of occlusion, the visible nodes form a connected subtree of the graphical model. We call each connected subtree a flexible composition of object parts. This involves a novel method for learning occlusion cues. During inference we need to search over a mixture of different flexible models. By exploiting part sharing, we show that this inference can be done extremely efficiently requiring only twice as many computations as searching for the entire object (i.e., not modeling occlusion). We evaluate our model on the standard benchmarked \"We Are Family\" Stickmen dataset and obtain significant performance improvements over the best alternative algorithms.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1574",
        "title": "Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking",
        "authors": [
            "Liming Zhao",
            "Xi Li",
            "Jun Xiao",
            "Fei Wu",
            "Yueting Zhuang"
        ],
        "abstract": "As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1628",
        "title": "Fisher Kernel for Deep Neural Activations",
        "authors": [
            "Donggeun Yoo",
            "Sunggyun Park",
            "Joon-Young Lee",
            "In So Kweon"
        ],
        "abstract": "Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1710",
        "title": "Convolutional Neural Networks at Constrained Time Cost",
        "authors": [
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than \"AlexNet\" (16.0% top-5 error, 10-view test).\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1842",
        "title": "Reading Text in the Wild with Convolutional Neural Networks",
        "authors": [
            "Max Jaderberg",
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "abstract": "In this work we present an end-to-end system for text spotting -- localising and recognising text in natural scene images -- and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data.\n",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1897",
        "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
        "authors": [
            "Anh Nguyen",
            "Jason Yosinski",
            "Jeff Clune"
        ],
        "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \"fooling images\" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1908",
        "title": "Person Re-identification by Saliency Learning",
        "authors": [
            "Rui Zhao",
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "abstract": "Human eyes can recognize person identities based on small salient regions, i.e. human saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on human saliency, we propose a novel perspective for person re-identification based on learning human saliency and matching saliency distribution. The proposed saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch, through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the state-of-the-art person re-identification methods on both datasets.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1945",
        "title": "Background Modelling using Octree Color Quantization",
        "authors": [
            "Aditya A.V. Sastry"
        ],
        "abstract": "By assuming that the most frequently occuring color in a video or a region of a video I propose a new algorithm for detecting foreground objects in a video. The process of detecting the foreground objects is complicated because of the fact that there may be swaying trees, objects of the background being moved around or lighting changes in the video. To deal with such complexities many have come up with solutions which heavily rely on expensive floating point operations. In this paper I used a data structure called Octree which is implemented only using binary operations. Traditionally octrees were used for color quantization but here in this paper I used it as a data structure to store the most frequently occuring colors in a video as well. For each of the starting few video frames, I constructed a Octree using all the colors of that frame. Next I pruned all the trees by removing nodes below a certain height and gave the leaf nodes a color which is dependant on the topological path from that node to its parent. Hence any two leaf nodes in two different octrees with the same topological path from themselves to the root will represent the same color. Next I merged all these individual trees into a single tree retaining only those nodes whose topological path to itself from the root is most common among all the trees. The colors represented by the leaf nodes in the resultant tree will be the most frequently occuring colors in the starting video frames of the video. Hence any color of an incomming frame that is not close to any of the colors represented by the leaf node of the merged tree can be regarded as belonging to a foreground object.\n",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1957",
        "title": "CoMIC: Good features for detection and matching at object boundaries",
        "authors": [
            "Swarna Kamlam Ravindran",
            "Anurag Mittal"
        ],
        "abstract": "Feature or interest points typically use information aggregation in 2D patches which does not remain stable at object boundaries when there is object motion against a significantly varying background. Level or iso-intensity curves are much more stable under such conditions, especially the longer ones. In this paper, we identify stable portions on long iso-curves and detect corners on them. Further, the iso-curve associated with a corner is used to discard portions from the background and improve matching. Such CoMIC (Corners on Maximally-stable Iso-intensity Curves) points yield superior results at the object boundary regions compared to state-of-the-art detectors while performing comparably at the interior regions as well. This is illustrated in exhaustive matching experiments for both boundary and non-boundary regions in applications such as stereo and point tracking for structure from motion in video sequences.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2066",
        "title": "Learning Multi-target Tracking with Quadratic Object Interactions",
        "authors": [
            "Shaofei Wang",
            "Charless C. Fowlkes"
        ],
        "abstract": "We describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video. In order to model pairwise interactions between different tracks, such as suppression of overlapping tracks and contextual cues about co-occurence of different objects, we augment a standard min-cost flow objective with quadratic terms between detection variables. We learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy. We evaluate two different approaches to finding an optimal set of tracks under model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves equivalent performance to the LP relaxation while being 2-7x faster than a commercial solver. The resulting model with learned parameters outperforms existing methods across several categories on the KITTI tracking benchmark.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2067",
        "title": "An algorithm for improving Non-Local Means operators via low-rank approximation",
        "authors": [
            "Victor May",
            "Yosi Keller",
            "Nir Sharon",
            "Yoel Shkolnisky"
        ],
        "abstract": "We present a method for improving a Non Local Means operator by computing its low-rank approximation. The low-rank operator is constructed by applying a filter to the spectrum of the original Non Local Means operator. This results in an operator which is less sensitive to noise while preserving important properties of the original operator. The method is efficiently implemented based on Chebyshev polynomials and is demonstrated on the application of natural images denoising. For this application, we provide a comprehensive comparison of our method with leading denoising methods.\n    ",
        "submission_date": "2014-11-20T00:00:00",
        "last_modified_date": "2014-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2210",
        "title": "Risk Estimation Without Using Stein's Lemma -- Application to Image Denoising",
        "authors": [
            "Sagar Venkatesh Gubbi",
            "Chandra Sekhar Seelamantula"
        ],
        "abstract": "We address the problem of image denoising in additive white noise without placing restrictive assumptions on its statistical distribution. In the recent literature, specific noise distributions have been considered and correspondingly, optimal denoising techniques have been developed. One of the successful approaches for denoising relies on the notion of unbiased risk estimation, which enables one to obtain a useful substitute for the mean-square error. For the case of additive white Gaussian noise contamination, the risk estimation procedure relies on Stein's lemma. Sophisticated wavelet-based denoising techniques, which are essentially nonlinear, have been developed with the help of the lemma. We show that, for linear, shift-invariant denoisers, it is possible to obtain unbiased risk estimates of the mean-square error without using Stein's lemma. An interesting consequence of this development is that the unbiased risk estimator becomes agnostic to the statistical distribution of the noise. As a proof of principle, we show how the new methodology can be used to optimize the parameters of a simple Gaussian smoother. By locally adapting the parameters of the Gaussian smoother, we obtain a shift-variant smoother, which has a denoising performance (quantified by the improvement in peak signal-to-noise ratio (PSNR)) that is competitive to far more sophisticated methods reported in the literature. The proposed solution exhibits considerable parallelism, which we exploit in a Graphics Processing Unit (GPU) implementation.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2231",
        "title": "Generalized Singular Value Thresholding",
        "authors": [
            "Canyi Lu",
            "Changbo Zhu",
            "Chunyan Xu",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\\text{Prox}}_{g}^{\\sigma}(\\cdot)$, \\begin{equation*}\n",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2018-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2306",
        "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
        "authors": [
            "Andrej Karpathy",
            "Li Fei-Fei"
        ],
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2342",
        "title": "Bayesian Image Restoration for Poisson Corrupted Image using a Latent Variational Method with Gaussian MRF",
        "authors": [
            "Hayaru Shouno"
        ],
        "abstract": "We treat an image restoration problem with a Poisson noise chan- nel using a Bayesian framework. The Poisson randomness might be appeared in observation of low contrast object in the field of imaging. The noise observation is often hard to treat in a theo- retical analysis. In our formulation, we interpret the observation through the Poisson noise channel as a likelihood, and evaluate the bound of it with a Gaussian function using a latent variable method. We then introduce a Gaussian Markov random field (GMRF) as the prior for the Bayesian approach, and derive the posterior as a Gaussian distribution. The latent parameters in the likelihood and the hyperparameter in the GMRF prior could be treated as hid- den parameters, so that, we propose an algorithm to infer them in the expectation maximization (EM) framework using loopy belief propagation(LBP). We confirm the ability of our algorithm in the computer simulation, and compare it with the results of other im- age restoration frameworks.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2014-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2444",
        "title": "An Approach for Reducing Outliers of Non Local Means Image Denoising Filter",
        "authors": [
            "Raka Kundu",
            "Amlan Chakrabarti",
            "Prasanna Lenka"
        ],
        "abstract": "We propose an adaptive approach for non local means (NLM) image filtering termed as non local adaptive clipped means (NLACM), which reduces the effect of outliers and improves the denoising quality as compared to traditional NLM. Common method to neglect outliers from a data population is computation of mean in a range defined by mean and standard deviation. In NLACM we perform the median within the defined range based on statistical estimation of the neighbourhood region of a pixel to be denoised. As parameters of the range are independent of any additional input and is based on local intensity values, hence the approach is adaptive. Experimental results for NLACM show better estimation of true intensity from noisy neighbourhood observation as compared to NLM at high noise levels. We have verified the technique for speckle noise reduction and we have tested it on ultrasound (US) image of lumbar spine. These ultrasound images act as guidance for injection therapy for treatment of lumbar radiculopathy. We believe that the proposed approach for image denoising is first of its kind and its efficiency can be well justified as it shows better performance in image restoration.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2604",
        "title": "Actions and Attributes from Wholes and Parts",
        "authors": [
            "Georgia Gkioxari",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2697",
        "title": "Image quality assessment measure based on natural image statistics in the Tetrolet domain",
        "authors": [
            "Abdelkaher Ait Abdelouahad",
            "Mohammed El Hassouni",
            "Hocine Cherifi",
            "Driss Aboutajdine"
        ],
        "abstract": "This paper deals with a reduced reference (RR) image quality measure based on natural image statistics modeling. For this purpose, Tetrolet transform is used since it provides a convenient way to capture local geometric structures. This transform is applied to both reference and distorted images. Then, Gaussian Scale Mixture (GSM) is proposed to model subbands in order to take account statistical dependencies between tetrolet coefficients. In order to quantify the visual degradation, a measure based on Kullback Leibler Divergence (KLD) is provided. The proposed measure was tested on the Cornell VCL A-57 dataset and compared with other measures according to FR-TV1 VQEG framework.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2813",
        "title": "Joint Segmentation and Deconvolution of Ultrasound Images Using a Hierarchical Bayesian Model based on Generalized Gaussian Priors",
        "authors": [
            "Ningning Zhao",
            "Adrian Basarab",
            "Denis Kouame",
            "Jean-Yves Tourneret"
        ],
        "abstract": "This paper proposes a joint segmentation and deconvolution Bayesian method for medical ultrasound (US) images. Contrary to piecewise homogeneous images, US images exhibit heavy characteristic speckle patterns correlated with the tissue structures. The generalized Gaussian distribution (GGD) has been shown to be one of the most relevant distributions for characterizing the speckle in US images. Thus, we propose a GGD-Potts model defined by a label map coupling US image segmentation and deconvolution. The Bayesian estimators of the unknown model parameters, including the US image, the label map and all the hyperparameters are difficult to be expressed in closed form. Thus, we investigate a Gibbs sampler to generate samples distributed according to the posterior of interest. These generated samples are finally used to compute the Bayesian estimators of the unknown parameters. The performance of the proposed Bayesian model is compared with existing approaches via several experiments conducted on realistic synthetic data and in vivo US images.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2873",
        "title": "Cancer Detection with Multiple Radiologists via Soft Multiple Instance Logistic Regression and $L_1$ Regularization",
        "authors": [
            "Inna Stainvas",
            "Alexandra Manevitch",
            "Isaac Leichter"
        ],
        "abstract": "This paper deals with the multiple annotation problem in medical application of cancer detection in digital images. The main assumption is that though images are labeled by many experts, the number of images read by the same expert is not large. Thus differing with the existing work on modeling each expert and ground truth simultaneously, the multi annotation information is used in a soft manner. The multiple labels from different experts are used to estimate the probability of the findings to be marked as malignant. The learning algorithm minimizes the Kullback Leibler (KL) divergence between the modeled probabilities and desired ones constraining the model to be compact. The probabilities are modeled by logit regression and multiple instance learning concept is used by us.\n",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3009",
        "title": "Brain Tumor Detection Based on Bilateral Symmetry Information",
        "authors": [
            "Narkhede Sachin",
            "Deven Shah",
            "Vaishali Khairnar",
            "Sujata Kadu"
        ],
        "abstract": "Advances in computing technology have allowed researchers across many fields of endeavor to collect and maintain vast amounts of observational statistical data such as clinical data,biological patient data,data regarding access of web sites,financial data,and the ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2014-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3159",
        "title": "Road Detection via On--line Label Transfer",
        "authors": [
            "Jos\u00e9 M. \u00c1lvarez",
            "Ferran Diego",
            "Joan Serrat",
            "Antonio M. L\u00f3pez"
        ],
        "abstract": "Vision-based road detection is an essential functionality for supporting advanced driver assistance systems (ADAS) such as road following and vehicle and pedestrian detection. The major challenges of road detection are dealing with shadows and lighting variations and the presence of other objects in the scene. Current road detection algorithms characterize road areas at pixel level and group pixels accordingly. However, these algorithms fail in presence of strong shadows and lighting variations. Therefore, we propose a road detection algorithm based on video alignment. The key idea of the algorithm is to exploit the similarities occurred when a vehicle follows the same trajectory more than once. In this way, road areas are learned in a first ride and then, this road knowledge is used to infer areas depicting drivable road surfaces in subsequent rides. Two different experiments are conducted to validate the proposal on different video sequences taken at different scenarios and different daytime. The former aims to perform on-line road detection. The latter aims to perform off-line road detection and is applied to automatically generate the ground-truth necessary to validate road detection algorithms. Qualitative and quantitative evaluations prove that the proposed algorithm is a valid road detection approach.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3161",
        "title": "Object-centric Sampling for Fine-grained Image Classification",
        "authors": [
            "Xiaoyu Wang",
            "Tianbao Yang",
            "Guobin Chen",
            "Yuanqing Lin"
        ],
        "abstract": "This paper proposes to go beyond the state-of-the-art deep convolutional neural network (CNN) by incorporating the information from object detection, focusing on dealing with fine-grained image classification. Unfortunately, CNN suffers from over-fiting when it is trained on existing fine-grained image classification benchmarks, which typically only consist of less than a few tens of thousands training images. Therefore, we first construct a large-scale fine-grained car recognition dataset that consists of 333 car classes with more than 150 thousand training images. With this large-scale dataset, we are able to build a strong baseline for CNN with top-1 classification accuracy of 81.6%. One major challenge in fine-grained image classification is that many classes are very similar to each other while having large within-class variation. One contributing factor to the within-class variation is cluttered image background. However, the existing CNN training takes uniform window sampling over the image, acting as blind on the location of the object of interest. In contrast, this paper proposes an \\emph{object-centric sampling} (OCS) scheme that samples image windows based on the object location information. The challenge in using the location information lies in how to design powerful object detector and how to handle the imperfectness of detection results. To that end, we design a saliency-aware object detection approach specific for the setting of fine-grained image classification, and the uncertainty of detection results are naturally handled in our OCS scheme. Our framework is demonstrated to be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the large-scale fine-grained car classification dataset.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3328",
        "title": "Memory vectors for similarity search in high-dimensional spaces",
        "authors": [
            "Ahmet Iscen",
            "Teddy Furon",
            "Vincent Gripon",
            "Michael Rabbat",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "We study an indexing architecture to store and search in a database of high-dimensional vectors from the perspective of statistical signal processing and decision theory. This architecture is composed of several memory units, each of which summarizes a fraction of the database by a single representative vector. The potential similarity of the query to one of the vectors stored in the memory unit is gauged by a simple correlation with the memory unit's representative vector. This representative optimizes the test of the following hypothesis: the query is independent from any vector in the memory unit vs. the query is a simple perturbation of one of the stored vectors.\n",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2017-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3352",
        "title": "Web image annotation by diffusion maps manifold learning algorithm",
        "authors": [
            "Neda Pourali"
        ],
        "abstract": "Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3369",
        "title": "Candidate Constrained CRFs for Loss-Aware Structured Prediction",
        "authors": [
            "Faruk Ahmed",
            "Daniel Tarlow",
            "Dhruv Batra"
        ],
        "abstract": "When evaluating computer vision systems, we are often concerned with performance on a task-specific evaluation measure such as the Intersection-Over-Union score used in the PASCAL VOC image segmentation challenge. Ideally, our systems would be tuned specifically to these evaluation measures. However, despite much work on loss-aware structured prediction, top performing systems do not use these techniques. In this work, we seek to address this problem, incorporating loss-aware prediction in a manner that is amenable to the approaches taken by top performing systems. Our main idea is to simultaneously leverage two systems: a highly tuned pipeline system as is found on top of leaderboards, and a traditional CRF. We show how to combine high quality candidate solutions from the pipeline with the probabilistic approach of the CRF that is amenable to loss-aware prediction. The result is that we can use loss-aware prediction methodology to improve performance of the highly tuned pipeline system.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3421",
        "title": "Multi-Atlas Segmentation of Biomedical Images: A Survey",
        "authors": [
            "Juan Eugenio Iglesias",
            "Mert Rory Sabuncu"
        ],
        "abstract": "Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \"atlases\" (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 - 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3474",
        "title": "Deep Domain Confusion: Maximizing for Domain Invariance",
        "authors": [
            "Eric Tzeng",
            "Judy Hoffman",
            "Ning Zhang",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3506",
        "title": "Road Detection by One-Class Color Classification: Dataset and Experiments",
        "authors": [
            "Jose M. Alvarez",
            "Theo Gevers",
            "Antonio M. Lopez"
        ],
        "abstract": "Detecting traversable road areas ahead a moving vehicle is a key process for modern autonomous driving systems. A common approach to road detection consists of exploiting color features to classify pixels as road or background. These algorithms reduce the effect of lighting variations and weather conditions by exploiting the discriminant/invariant properties of different color representations. Furthermore, the lack of labeled datasets has motivated the development of algorithms performing on single images based on the assumption that the bottom part of the image belongs to the road surface.\n",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3596",
        "title": "EgoSampling: Fast-Forward and Stereo for Egocentric Videos",
        "authors": [
            "Yair Poleg",
            "Tavi Halperin",
            "Chetan Arora",
            "Shmuel Peleg"
        ],
        "abstract": "While egocentric cameras like GoPro are gaining popularity, the videos they capture are long, boring, and difficult to watch from start to end. Fast forwarding (i.e. frame sampling) is a natural choice for faster video browsing. However, this accentuates the shake caused by natural head motion, making the fast forwarded video useless.\n",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3613",
        "title": "A Novel Adaptive Possibilistic Clustering Algorithm",
        "authors": [
            "Spyridoula D. Xenaki",
            "Konstantinos D. Koutroumbas",
            "Athanasios A. Rontogiannis"
        ],
        "abstract": "In this paper a novel possibilistic c-means clustering algorithm, called Adaptive Possibilistic c-means, is presented. Its main feature is that {\\it all} its parameters, after their initialization, are properly adapted during its execution. Provided that the algorithm starts with a reasonable overestimate of the number of physical clusters formed by the data, it is capable, in principle, to unravel them (a long-standing issue in the clustering literature). This is due to the fully adaptive nature of the proposed algorithm that enables the removal of the clusters that gradually become obsolete. In addition, the adaptation of all its parameters increases the flexibility of the algorithm in following the variations in the formation of the clusters that occur from iteration to iteration. Theoretical results that are indicative of the convergence behavior of the algorithm are also provided. Finally, extensive simulation results on both synthetic and real data highlight the effectiveness of the proposed algorithm.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3684",
        "title": "Object Recognition Using Deep Neural Networks: A Survey",
        "authors": [
            "Soren Goyal",
            "Paul Benjamin"
        ],
        "abstract": "Recognition of objects using Deep Neural Networks is an active area of research and many breakthroughs have been made in the last few years. The paper attempts to indicate how far this field has progressed. The paper briefly describes the history of research in Neural Networks and describe several of the recent advances in this field. The performances of recently developed Neural Network Algorithm over benchmark datasets have been tabulated. Finally, some the applications of this field have been provided.\n    ",
        "submission_date": "2014-12-10T00:00:00",
        "last_modified_date": "2014-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3708",
        "title": "Compact Compositional Models",
        "authors": [
            "Marc Goessling",
            "Yali Amit"
        ],
        "abstract": "Learning compact and interpretable representations is a very natural task, which has not been solved satisfactorily even for simple binary datasets. In this paper, we review various ways of composing experts for binary data and argue that competitive forms of interaction are best suited to learn low-dimensional representations. We propose a new composition rule that discourages experts from focusing on similar structures and that penalizes opposing votes strongly so that abstaining from voting becomes more attractive. We also introduce a novel sequential initialization procedure, which is based on a process of oversimplification and correction. Experiments show that with our approach very intuitive models can be learned.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2016-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3709",
        "title": "An active search strategy for efficient object class detection",
        "authors": [
            "Abel Gonzalez-Garcia",
            "Alexander Vezhnevets",
            "Vittorio Ferrari"
        ],
        "abstract": "Object class detectors typically apply a window classifier to all the windows in a large set, either in a sliding window manner or using object proposals. In this paper, we develop an active search strategy that sequentially chooses the next window to evaluate based on all the information gathered before. This results in a substantial reduction in the number of classifier evaluations and in a more elegant approach in general. Our search strategy is guided by two forces. First, we exploit context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. This enables to jump across distant regions in the image (e.g. observing a sky region suggests that cars might be far below) and is done efficiently in a Random Forest framework. Second, we exploit the score of the classifier to attract the search to promising areas surrounding a highly scored window, and to keep away from areas near low scored ones. Our search strategy can be applied on top of any classifier as it treats it as a black-box. In experiments with R-CNN on the challenging SUN2012 dataset, our method matches the detection accuracy of evaluating all windows independently, while evaluating 9x fewer windows.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3717",
        "title": "Unsupervised Neural Architecture for Saliency Detection: Extended Version",
        "authors": [
            "Natalia Efremova",
            "Sergey Tarasenko"
        ],
        "abstract": "We propose a novel neural network architecture for visual saliency detections, which utilizes neurophysiologically plausible mechanisms for extraction of salient regions. The model has been significantly inspired by recent findings from neurophysiology and aimed to simulate the bottom-up processes of human selective attention. Two types of features were analyzed: color and direction of maximum variance. The mechanism we employ for processing those features is PCA, implemented by means of normalized Hebbian learning and the waves of spikes. To evaluate performance of our model we have conducted psychological experiment. Comparison of simulation results with those of experiment indicates good performance of our model.\n    ",
        "submission_date": "2014-11-18T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3914",
        "title": "Edge Preserving Multi-Modal Registration Based On Gradient Intensity Self-Similarity",
        "authors": [
            "Tamar Rott",
            "Dorin Shriki",
            "Tamir Bendory"
        ],
        "abstract": "Image registration is a challenging task in the world of medical imaging. Particularly, accurate edge registration plays a central role in a variety of clinical conditions. The Modality Independent Neighbourhood Descriptor (MIND) demonstrates state of the art alignment, based on the image self-similarity. However, this method appears to be less accurate regarding edge registration. In this work, we propose a new registration method, incorporating gradient intensity and MIND self-similarity metric. Experimental results show the superiority of this method in edge registration tasks, while preserving the original MIND performance for other image features and textures.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3949",
        "title": "CITlab ARGUS for historical handwritten documents",
        "authors": [
            "Tobias Strau\u00df",
            "Tobias Gr\u00fcning",
            "Gundram Leifert",
            "Roger Labahn"
        ],
        "abstract": "We describe CITlab's recognition system for the HTRtS competition attached to the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR 2014. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3958",
        "title": "An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation",
        "authors": [
            "Mohammed M. Abdelsamea"
        ],
        "abstract": "In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4102",
        "title": "Representing Data by a Mixture of Activated Simplices",
        "authors": [
            "Chunyu Wang",
            "John Flynn",
            "Yizhou Wang",
            "Alan L. Yuille"
        ],
        "abstract": "We present a new model which represents data as a mixture of simplices. Simplices are geometric structures that generalize triangles. We give a simple geometric understanding that allows us to learn a simplicial structure efficiently. Our method requires that the data are unit normalized (and thus lie on the unit sphere). We show that under this restriction, building a model with simplices amounts to constructing a convex hull inside the sphere whose boundary facets is close to the data. We call the boundary facets of the convex hull that are close to the data Activated Simplices. While the total number of bases used to build the simplices is a parameter of the model, the dimensions of the individual activated simplices are learned from the data. Simplices can have different dimensions, which facilitates modeling of inhomogeneous data sources. The simplicial structure is bounded --- this is appropriate for modeling data with constraints, such as human elbows can not bend more than 180 degrees. The simplices are easy to interpret and extremes within the data can be discovered among the vertices. The method provides good reconstruction and regularization. It supports good nearest neighbor classification and it allows realistic generative models to be constructed. It achieves state-of-the-art results on benchmark datasets, including 3D poses and digits.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4172",
        "title": "Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices",
        "authors": [
            "Sadeep Jayasumana",
            "Richard Hartley",
            "Mathieu Salzmann",
            "Hongdong Li",
            "Mehrtash Harandi"
        ],
        "abstract": "Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian ker- nel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, ob- ject categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4174",
        "title": "A Framework for Shape Analysis via Hilbert Space Embedding",
        "authors": [
            "Sadeep Jayasumana",
            "Mathieu Salzmann",
            "Hongdong Li",
            "Mehrtash Harandi"
        ],
        "abstract": "We propose a framework for 2D shape analysis using positive definite kernels defined on Kendall's shape manifold. Different representations of 2D shapes are known to generate different nonlinear spaces. Due to the nonlinearity of these spaces, most existing shape classification algorithms resort to nearest neighbor methods and to learning distances on shape spaces. Here, we propose to map shapes on Kendall's shape manifold to a high dimensional Hilbert space where Euclidean geometry applies. To this end, we introduce a kernel on this manifold that permits such a mapping, and prove its positive definiteness. This kernel lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the benefits of our approach over the state-of-the-art methods on shape classification, clustering and retrieval.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4175",
        "title": "Optimizing Over Radial Kernels on Compact Manifolds",
        "authors": [
            "Sadeep Jayasumana",
            "Richard Hartley",
            "Mathieu Salzmann",
            "Hongdong Li",
            "Mehrtash Harandi"
        ],
        "abstract": "We tackle the problem of optimizing over all possible positive definite radial kernels on Riemannian manifolds for classification. Kernel methods on Riemannian manifolds have recently become increasingly popular in computer vision. However, the number of known positive definite kernels on manifolds remain very limited. Furthermore, most kernels typically depend on at least one parameter that needs to be tuned for the problem at hand. A poor choice of kernel, or of parameter value, may yield significant performance drop-off. Here, we show that positive definite radial kernels on the unit n-sphere, the Grassmann manifold and Kendall's shape manifold can be expressed in a simple form whose parameters can be automatically optimized within a support vector machine framework. We demonstrate the benefits of our kernel learning algorithm on object, face, action and shape recognition.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4181",
        "title": "Oriented Edge Forests for Boundary Detection",
        "authors": [
            "Sam Hallman",
            "Charless C. Fowlkes"
        ],
        "abstract": "We present a simple, efficient model for learning boundary detection based on a random forest classifier. Our approach combines (1) efficient clustering of training examples based on simple partitioning of the space of local edge orientations and (2) scale-dependent calibration of individual tree output probabilities prior to multiscale combination. The resulting model outperforms published results on the challenging BSDS500 boundary detection benchmark. Further, on large datasets our model requires substantially less memory for training and speeds up training time by a factor of 10 over the structured forest model.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4183",
        "title": "A survey of modern optical character recognition techniques",
        "authors": [
            "Eugene Borovikov"
        ],
        "abstract": "This report explores the latest advances in the field of digital document recognition. With the focus on printed document imagery, we discuss the major developments in optical character recognition (OCR) and document image enhancement/restoration in application to Latin and non-Latin scripts. In addition, we review and discuss the available technologies for hand-written document recognition. In this report, we also provide some company-accumulated benchmark results on available OCR engines.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4196",
        "title": "Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in the Homography Space",
        "authors": [
            "Yuan-Ting Hu",
            "Yen-Yu Lin",
            "Hsin-Yi Chen",
            "Kuang-Jui Hsu",
            "Bing-Yu Chen"
        ],
        "abstract": "With the aim to improve the performance of feature matching, we present an unsupervised approach to fuse various local descriptors in the space of homographies. Inspired by the observation that the homographies of correct feature correspondences vary smoothly along the spatial domain, our approach stands on the unsupervised nature of feature matching, and can select a good descriptor for matching each feature point. Specifically, the homography space serves as the common domain, in which a correspondence obtained by any descriptor is considered as a point, for integrating various heterogeneous descriptors. Both geometric coherence and spatial continuity among correspondences are considered via computing their geodesic distances in the space. In this way, mutual verification across different descriptors is allowed, and correct correspondences will be highlighted with a high degree of consistency (i.e., short geodesic distances here). It follows that one-class SVM can be applied to identifying these correct correspondences, and boosts the performance of feature matching. The proposed approach is comprehensively compared with the state-of-the-art approaches, and evaluated on four benchmarks of image matching. The promising results manifest its effectiveness.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4205",
        "title": "The application of the Bayes Ying Yang harmony based GMMs in on-line signature verification",
        "authors": [
            "Xiaosha Zhao",
            "Mandan Liu"
        ],
        "abstract": "In this contribution, a Bayes Ying Yang(BYY) harmony based approach for on-line signature verification is presented. In the proposed method, a simple but effective Gaussian Mixture Models(GMMs) is used to represent for each user's signature model based on the prior information collected. Different from the early works, in this paper, we use the Bayes Ying Yang machine combined with the harmony function to achieve Automatic Model Selection(AMS) during the parameter learning for the GMMs, so that a better approximation of the user model is assured. Experiments on a database from the First International Signature Verification Competition(SVC 2004) confirm that this combined algorithm yields quite satisfactory results.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4217",
        "title": "A Study of Sindhi Related and Arabic Script Adapted languages Recognition",
        "authors": [
            "Dil Nawaz Hakro",
            "A. Z. Talib",
            "Zeeshan Bhatti",
            "G. N. Moja"
        ],
        "abstract": "A large number of publications are available for the Optical Character Recognition (OCR). Significant researches, as well as articles are present for the Latin, Chinese and Japanese scripts. Arabic script is also one of mature script from OCR perspective. The adaptive languages which share Arabic script or its extended characters; still lacking the OCRs for their language. In this paper we present the efforts of researchers on Arabic and its related and adapted languages. This survey is organized in different sections, in which introduction is followed by properties of Sindhi Language. OCR process techniques and methods used by various researchers are presented. The last section is dedicated for future work and conclusion is also discussed.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4313",
        "title": "Combining the Best of Graphical Models and ConvNets for Semantic Segmentation",
        "authors": [
            "Michael Cogswell",
            "Xiao Lin",
            "Senthil Purushwalkam",
            "Dhruv Batra"
        ],
        "abstract": "We present a two-module approach to semantic segmentation that incorporates Convolutional Networks (CNNs) and Graphical Models. Graphical models are used to generate a small (5-30) set of diverse segmentations proposals, such that this set has high recall. Since the number of required proposals is so low, we can extract fairly complex features to rank them. Our complex feature of choice is a novel CNN called SegNet, which directly outputs a (coarse) semantic segmentation. Importantly, SegNet is specifically trained to optimize the corpus-level PASCAL IOU loss function. To the best of our knowledge, this is the first CNN specifically designed for semantic segmentation. This two-module approach achieves $52.5\\%$ on the PASCAL 2012 segmentation challenge.\n    ",
        "submission_date": "2014-12-14T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4433",
        "title": "Inexact Alternating Direction Method Based on Newton descent algorithm with Application to Poisson Image Deblurring",
        "authors": [
            "Dai-Qiang Chen"
        ],
        "abstract": "The recovery of images from the observations that are degraded by a linear operator and further corrupted by Poisson noise is an important task in modern imaging applications such as astronomical and biomedical ones. Gradient-based regularizers involve the popular total variation semi-norm have become standard techniques for Poisson image restoration due to its edge-preserving ability. Various efficient algorithms have been developed for solving the corresponding minimization problem with non-smooth regularization terms. In this paper, motivated by the idea of the alternating direction minimization algorithm and the Newton's method with upper convergent rate, we further propose inexact alternating direction methods utilizing the proximal Hessian matrix information of the objective function, in a way reminiscent of Newton descent methods. Besides, we also investigate the global convergence of the proposed algorithms under certain conditions. Finally, we illustrate that the proposed algorithms outperform the current state-of-the-art algorithms through numerical experiments on Poisson image deblurring.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4438",
        "title": "Fixed Point Algorithm Based on Quasi-Newton Method for Convex Minimization Problem with Application to Image Deblurring",
        "authors": [
            "Dai-Qiang Chen"
        ],
        "abstract": "Solving an optimization problem whose objective function is the sum of two convex functions has received considerable interests in the context of image processing recently. In particular, we are interested in the scenario when a non-differentiable convex function such as the total variation (TV) norm is included in the objective function due to many variational models established in image processing have this nature. In this paper, we propose a fast fixed point algorithm based on the quasi-Newton method for solving this class of problem, and apply it in the field of TV-based image deblurring. The novel method is derived from the idea of the quasi-Newton method, and the fixed-point algorithms based on the proximity operator, which were widely investigated very recently. Utilizing the non-expansion property of the proximity operator we further investigate the global convergence of the proposed algorithm. Numerical experiments on image deblurring problem with additive or multiplicative noise are presented to demonstrate that the proposed algorithm is superior to the recently developed fixed-point algorithm in the computational efficiency.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4470",
        "title": "Automatic video scene segmentation based on spatial-temporal clues and rhythm",
        "authors": [
            "Walid Mahdi",
            "Liming Chen",
            "Mohsen Ardebilian"
        ],
        "abstract": "With ever increasing computing power and data storage capacity, the potential for large digital video libraries is growing ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4526",
        "title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
        "authors": [
            "Hongsheng Li",
            "Rui Zhao",
            "Xiaogang Wang"
        ],
        "abstract": "We present highly efficient algorithms for performing forward and backward propagation of Convolutional Neural Network (CNN) for pixelwise classification on images. For pixelwise classification tasks, such as image segmentation and object detection, surrounding image patches are fed into CNN for predicting the classes of centered pixels via forward propagation and for updating CNN parameters via backward propagation. However, forward and backward propagation was originally designed for whole-image classification. Directly applying it to pixelwise classification in a patch-by-patch scanning manner is extremely inefficient, because surrounding patches of pixels have large overlaps, which lead to a lot of redundant computation.\n",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4564",
        "title": "MatConvNet - Convolutional Neural Networks for MATLAB",
        "authors": [
            "Andrea Vedaldi",
            "Karel Lenc"
        ],
        "abstract": "MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4729",
        "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
        "authors": [
            "Subhashini Venugopalan",
            "Huijuan Xu",
            "Jeff Donahue",
            "Marcus Rohrbach",
            "Raymond Mooney",
            "Kate Saenko"
        ],
        "abstract": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4940",
        "title": "Discovering beautiful attributes for aesthetic image analysis",
        "authors": [
            "Luca Marchesotti",
            "Naila Murray",
            "Florent Perronnin"
        ],
        "abstract": "Aesthetic image analysis is the study and assessment of the aesthetic properties of images. Current computational approaches to aesthetic image analysis either provide accurate or interpretable results. To obtain both accuracy and interpretability by humans, we advocate the use of learned and nameable visual attributes as mid-level features. For this purpose, we propose to discover and learn the visual appearance of attributes automatically, using a recently introduced database, called AVA, which contains more than 250,000 images together with their aesthetic scores and textual comments given by photography enthusiasts. We provide a detailed analysis of these annotations as well as the context in which they were given. We then describe how these three key components of AVA - images, scores, and comments - can be effectively leveraged to learn visual attributes. Lastly, we show that these learned attributes can be successfully used in three applications: aesthetic quality prediction, image tagging and retrieval.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4944",
        "title": "Efficient GPU Implementation for Single Block Orthogonal Dictionary Learning",
        "authors": [
            "Paul Irofti"
        ],
        "abstract": "Dictionary training for sparse representations involves dealing with large chunks of data and complex algorithms that determine time consuming implementations. SBO is an iterative dictionary learning algorithm based on constructing unions of orthonormal bases via singular value decomposition, that represents each data item through a single best fit orthobase. In this paper we present a GPGPU approach of implementing SBO in OpenCL. We provide a lock-free solution that ensures full-occupancy of the GPU by following the map-reduce model for the sparse-coding stage and by making use of the Partitioned Global Address Space (PGAS) model for developing parallel dictionary updates. The resulting implementation achieves a favourable trade-off between algorithm complexity and data representation quality compared to PAK-SVD which is the standard overcomplete dictionary learning approach. We present and discuss numerical results showing a significant acceleration of the execution time for the dictionary learning process.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5027",
        "title": "What is a salient object? A dataset and a baseline model for salient object detection",
        "authors": [
            "Ali Borji"
        ],
        "abstract": "Salient object detection or salient region detection models, diverging from fixation prediction models, have traditionally been dealing with locating and segmenting the most salient object or region in a scene. While the notion of most salient object is sensible when multiple objects exist in a scene, current datasets for evaluation of saliency detection approaches often have scenes with only one single object. We introduce three main contributions in this paper: First, we take an indepth look at the problem of salient object detection by studying the relationship between where people look in scenes and what they choose as the most salient object when they are explicitly asked. Based on the agreement between fixations and saliency judgments, we then suggest that the most salient object is the one that attracts the highest fraction of fixations. Second, we provide two new less biased benchmark datasets containing scenes with multiple objects that challenge existing saliency models. Indeed, we observed a severe drop in performance of 8 state-of-the-art models on our datasets (40% to 70%). Third, we propose a very simple yet powerful model based on superpixels to be used as a baseline for model evaluation and comparison. While on par with the best models on MSRA-5K dataset, our model wins over other models on our data highlighting a serious drawback of existing models, which is convoluting the processes of locating the most salient object and its segmentation. We also provide a review and statistical analysis of some labeled scene datasets that can be used for evaluating salient object detection models. We believe that our work can greatly help remedy the over-fitting of models to existing biased datasets and opens new venues for future research in this fast-evolving field.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5083",
        "title": "Random Forests Can Hash",
        "authors": [
            "Qiang Qiu",
            "Guillermo Sapiro",
            "Alex Bronstein"
        ],
        "abstract": "Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5104",
        "title": "Locally Scale-Invariant Convolutional Neural Networks",
        "authors": [
            "Angjoo Kanazawa",
            "Abhishek Sharma",
            "David Jacobs"
        ],
        "abstract": "Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2014-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5126",
        "title": "A Robust Regression Approach for Background/Foreground Segmentation",
        "authors": [
            "Shervin Minaee",
            "Haoping Yu",
            "Yao Wang"
        ],
        "abstract": "Background/foreground segmentation has a lot of applications in image and video processing. In this paper, a segmentation algorithm is proposed which is mainly designed for text and line extraction in screen content. The proposed method makes use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can fit well will be considered as background, while remaining outlier pixels will be considered foreground. This algorithm has been extensively tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other methods, such as the k-means clustering based segmentation algorithm in DjVu. This background/foreground segmentation can be used in different applications such as: text extraction, separate coding of background and foreground for compression of screen content and mixed content documents, principle line extraction from palmprint and crease detection in fingerprint images.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5275",
        "title": "Iranian cashes recognition using mobile",
        "authors": [
            "Ismail Nojavani",
            "Azade Rezaeezade",
            "Amirhassan Monadjemi"
        ],
        "abstract": "In economical societies of today, using cash is an inseparable aspect of human life. People use cashes for marketing, services, entertainments, bank operations and so on. This huge amount of contact with cash and the necessity of knowing the monetary value of it caused one of the most challenging problems for visually impaired people. In this paper we propose a mobile phone based approach to identify monetary value of a picture taken from cashes using some image processing and machine vision techniques. While the developed approach is very fast, it can recognize the value of cash by average accuracy of about 95% and can overcome different challenges like rotation, scaling, collision, illumination changes, perspective, and some others.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5322",
        "title": "An Algebraical Model for Gray Level Images",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper we propose a new algebraical model for the gray level images. It can be used for digital image processing. The model adresses to those images which are generated in improper light conditions (very low or high level). The vector space structure is able to illustrate some features into the image using modified level of contrast and luminosity. Also, the defined structure could be used in image enhancement. The general approach is presented with experimental results to demonstrate image enhancement.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5325",
        "title": "Color Image Enhancement In the Framework of Logarithmic Models",
        "authors": [
            "Vasile Patrascu",
            "Vasile Buzuloiu"
        ],
        "abstract": "In this paper, we propose a mathematical model for color image processing. It is a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of values for the color space. We define two operations: addition <+> and real scalar multiplication <x>. With these operations the space of colors becomes a real vector space. Then, defining the scalar product (.|.) and the norm || . ||, we obtain a (logarithmic) Euclidean space. We show how we can use this model for color image enhancement and we present some experimental results.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5328",
        "title": "A Mathematical Model for Logarithmic Image Processing",
        "authors": [
            "Vasile Patrascu",
            "Vasile Buzuloiu"
        ],
        "abstract": "In this paper, we propose a new mathematical model for image processing. It is a logarithmical one. We consider the bounded interval (-1, 1) as the set of gray levels. Firstly, we define two operations: addition <+> and real scalar multiplication <x>. With these operations, the set of gray levels becomes a real vector space. Then, defining the scalar product (.|.) and the norm || . ||, we obtain an Euclidean space of the gray levels. Secondly, we extend these operations and functions for color images. We finally show the effect of various simple operations on an image.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5334",
        "title": "The Affine Transforms for Image Enhancement in the Context of Logarithmic Models",
        "authors": [
            "Vasile Patrascu",
            "Vasile Buzuloiu"
        ],
        "abstract": "The logarithmic model offers new tools for image processing. An efficient method for image enhancement is to use an affine transformation with the logarithmic operations: addition and scalar multiplication. We define some criteria for automatically determining the parameters of the processing and this is done via mean and variance computed by logarithmic operations.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5488",
        "title": "Full-reference image quality assessment by combining global and local distortion measures",
        "authors": [
            "Ashirbani Saha",
            "Q. M. Jonathan Wu"
        ],
        "abstract": "Full-reference image quality assessment (FR-IQA) techniques compare a reference and a distorted/test image and predict the perceptual quality of the test image in terms of a scalar value representing an objective score. The evaluation of FR-IQA techniques is carried out by comparing the objective scores from the techniques with the subjective scores (obtained from human observers) provided in the image databases used for the IQA. Hence, we reasonably assume that the goal of a human observer is to rate the distortion present in the test image. The goal oriented tasks are processed by the human visual system (HVS) through top-down processing which actively searches for local distortions driven by the goal. Therefore local distortion measures in an image are important for the top-down processing. At the same time, bottom-up processing also takes place signifying spontaneous visual functions in the HVS. To account for this, global perceptual features can be used. Therefore, we hypothesize that the resulting objective score for an image can be derived from the combination of local and global distortion measures calculated from the reference and test images. We calculate the local distortion by measuring the local correlation differences from the gradient and contrast information. For global distortion, dissimilarity of the saliency maps computed from a bottom-up model of saliency is used. The motivation behind the proposed approach has been thoroughly discussed, accompanied by an intuitive analysis. Finally, experiments are conducted in six benchmark databases suggesting the effectiveness of the proposed approach that achieves competitive performance with the state-of-the-art methods providing an improvement in the overall performance.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5490",
        "title": "High Frequency Content based Stimulus for Perceptual Sharpness Assessment in Natural Images",
        "authors": [
            "Ashirbani Saha",
            "Q. M. Jonathan Wu"
        ],
        "abstract": "A blind approach to evaluate the perceptual sharpness present in a natural image is proposed. Though the literature demonstrates a set of variegated visual cues to detect or evaluate the absence or presence of sharpness, we emphasize in the current work that high frequency content and local standard deviation can form strong features to compute perceived sharpness in any natural image, and can be considered an able alternative for the existing cues. Unsharp areas in a natural image happen to exhibit uniform intensity or lack of sharp changes between regions. Sharp region transitions in an image are caused by the presence of spatial high frequency content. Therefore, in the proposed approach, we hypothesize that using the high frequency content as the principal stimulus, the perceived sharpness can be quantified in an image. When an image is convolved with a high pass filter, higher values at any pixel location signify the presence of high frequency content at those locations. Considering these values as the stimulus, the exponent of the stimulus is weighted by local standard deviation to impart the contribution of the local contrast within the formation of the sharpness map. The sharpness map highlights the relatively sharper regions in the image and is used to calculate the perceived sharpness score of the image. The advantages of the proposed method lie in its use of simple visual cues of high frequency content and local contrast to arrive at the perceptual score, and requiring no training with the images. The promise of the proposed method is demonstrated by its ability to compute perceived sharpness for within image and across image sharpness changes and for blind evaluation of perceptual degradation resulting due to presence of blur. Experiments conducted on several databases demonstrate improved performance of the proposed method over that of the state-of-the-art techniques.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5661",
        "title": "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection",
        "authors": [
            "Wanli Ouyang",
            "Xiaogang Wang",
            "Xingyu Zeng",
            "Shi Qiu",
            "Ping Luo",
            "Yonglong Tian",
            "Hongsheng Li",
            "Shuo Yang",
            "Zhe Wang",
            "Chen-Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN \\cite{girshick2014rich}, which was the state-of-the-art, from 31\\% to 50.3\\% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1\\%. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline.\n    ",
        "submission_date": "2014-12-17T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5687",
        "title": "Towards Open World Recognition",
        "authors": [
            "Abhijit Bendale",
            "Terrance Boult"
        ],
        "abstract": "With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimum down time, even to learn. To handle these operational issues, we present the problem of Open World recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance \"open space risk\" and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5758",
        "title": "Decomposition-Based Domain Adaptation for Real-World Font Recognition",
        "authors": [
            "Zhangyang Wang",
            "Jianchao Yang",
            "Hailin Jin",
            "Eli Shechtman",
            "Aseem Agarwala",
            "Jonathan Brandt",
            "Thomas S. Huang"
        ],
        "abstract": "We present a domain adaption framework to address a domain mismatch between synthetic training and real-world testing data. We demonstrate our method on a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous font recognition methods (Chen et al. (2014)). In this paper, we introduce a Convolutional Neural Network decomposition approach, leveraging a large training corpus of synthetic data to obtain effective features for classification. This is done using an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits a large collection of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. The proposed DeepFont method achieves an accuracy of higher than 80% (top-5) on a new large labeled real-world dataset we collected.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5764",
        "title": "Image Dynamic Range Enhancement in the Context of Logarithmic Models",
        "authors": [
            "Vasile Patrascu",
            "Vasile Buzuloiu"
        ],
        "abstract": "Images of a scene observed under a variable illumination or with a variable optical aperture are not identical. Does a privileged representant exist? In which mathematical context? How to obtain it? The authors answer to such questions in the context of logarithmic models for images. After a short presentation of the model, the paper presents two image transforms: one performs an optimal enhancement of the dynamic range, and the other does the same for the mean dynamic range. Experimental results are shown.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5769",
        "title": "Gray level image enhancement using the Bernstein polynomials",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "This paper presents a method for enhancing the gray level images. This presented method takes part from the category of point operations and it is based on piecewise linear functions. The interpolation nodes of these functions are calculated using the Bernstein polynomials.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5787",
        "title": "Gray Level Image Enhancement Using Polygonal Functions",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "This paper presents a method for enhancing the gray level images. This method takes part from the category of point transforms and it is based on interpolation functions. The latter have a graphic represented by polygonal lines. The interpolation nodes of these functions are calculated taking into account the statistics of gray levels belonging to the image.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5796",
        "title": "Image Enhancement Using a Generalization of Homographic Function",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "This paper presents a new method of gray level image enhancement, based on point transforms. In order to define the transform function, it was used a generalization of the homographic function.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5802",
        "title": "Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper we use a new logarithmic model of image representation, developed in [1,2], for edge detection. In fact, in the framework of the new model we obtain the formulas for computing the \"contrast of a pixel\" and the \"contrast\" image is just the \"contour\" or edge image. In our setting the range of values is preserved and the quality of the contour is good for high as well as for low luminosity regions. We present the comparison of our results with the results using classical edge detection operators.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5808",
        "title": "Minimizing the Number of Matching Queries for Object Retrieval",
        "authors": [
            "Johannes Niedermayer",
            "Peer Kr\u00f6ger"
        ],
        "abstract": "To increase the computational efficiency of interest-point based object retrieval, researchers have put remarkable research efforts into improving the efficiency of kNN-based feature matching, pursuing to match thousands of features against a database within fractions of a second. However, due to the high-dimensional nature of image features that reduces the effectivity of index structures (curse of dimensionality), due to the vast amount of features stored in image databases (images are often represented by up to several thousand features), this ultimate goal demanded to trade query runtimes for query precision. In this paper we address an approach complementary to indexing in order to improve the runtimes of retrieval by querying only the most promising keypoint descriptors, as this affects matching runtimes linearly and can therefore lead to increased efficiency. As this reduction of kNN queries reduces the number of tentative correspondences, a loss of query precision is minimized by an additional image-level correspondence generation stage with a computational performance independent of the underlying indexing structure. We evaluate such an adaption of the standard recognition pipeline on a variety of datasets using both SIFT and state-of-the-art binary descriptors. Our results suggest that decreasing the number of queried descriptors does not necessarily imply a reduction in the result quality as long as alternative ways of increasing query recall (by thoroughly selecting k) and MAP (using image-level correspondence generation) are considered.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5903",
        "title": "Deep Structured Output Learning for Unconstrained Text Recognition",
        "authors": [
            "Max Jaderberg",
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "abstract": "We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length.\n",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6012",
        "title": "CITlab ARGUS for historical data tables",
        "authors": [
            "Gundram Leifert",
            "Tobias Gr\u00fcning",
            "Tobias Strau\u00df",
            "Roger Labahn"
        ],
        "abstract": "We describe CITlab's recognition system for the ANWRESH-2014 competition attached to the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR 2014. The task comprises word recognition from segmented historical documents. The core components of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6018",
        "title": "Automatic Training Data Synthesis for Handwriting Recognition Using the Structural Crossing-Over Technique",
        "authors": [
            "Sirisak Visessenee",
            "Sanparith Marukatat",
            "Rachada Kongkachandra"
        ],
        "abstract": "The paper presents a novel technique called \"Structural Crossing-Over\" to synthesize qualified data for training machine learning-based handwriting recognition. The proposed technique can provide a greater variety of patterns of training data than the existing approaches such as elastic distortion and tangent-based affine transformation. A couple of training characters are chosen, then they are analyzed by their similar and different structures, and finally are crossed over to generate the new characters. The experiments are set to compare the performances of tangent-based affine transformation and the proposed approach in terms of the variety of generated characters and percent of recognition errors. The standard MNIST corpus including 60,000 training characters and 10,000 test characters is employed in the experiments. The proposed technique uses 1,000 characters to synthesize 60,000 characters, and then uses these data to train and test the benchmark handwriting recognition system that exploits Histogram of Gradient (HOG) as features and Support Vector Machine (SVM) as recognizer. The experimental result yields 8.06% of errors. It significantly outperforms the tangent-based affine transformation and the original MNIST training data, which are 11.74% and 16.55%, respectively.\n    ",
        "submission_date": "2014-10-09T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6056",
        "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics",
        "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "David Eigen",
            "Yann LeCun"
        ],
        "abstract": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6061",
        "title": "CITlab ARGUS for Arabic Handwriting",
        "authors": [
            "Gundram Leifert",
            "Roger Labahn",
            "Tobias Strau\u00df"
        ],
        "abstract": "In the recent years it turned out that multidimensional recurrent neural networks (MDRNN) perform very well for offline handwriting recognition tasks like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and dictionary lookup, our ARGUS software completed this task with an error rate of 26.27% in its primary setup.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2014-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6071",
        "title": "Fractional Max-Pooling",
        "authors": [
            "Benjamin Graham"
        ],
        "abstract": "Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6092",
        "title": "Image enhancement using the mean dynamic range maximization with logarithmic operations",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper we use a logarithmic model for gray level image enhancement. We begin with a short presentation of the model and then, we propose a new formula for the mean dynamic range. After that we present two image transforms: one performs an optimal enhancement of the mean dynamic range using the logarithmic addition, and the other does the same for positive and negative values using the logarithmic scalar multiplication. We present the comparison of the results obtained by dynamic ranges optimization with the results obtained using classical image enhancement methods like gamma correction and histogram equalization.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6115",
        "title": "Compressing Deep Convolutional Networks using Vector Quantization",
        "authors": [
            "Yunchao Gong",
            "Liu Liu",
            "Ming Yang",
            "Lubomir Bourdev"
        ],
        "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6124",
        "title": "Semantic Part Segmentation using Compositional Model combining Shape and Appearance",
        "authors": [
            "Jianyu Wang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we study the problem of semantic part segmentation for animals. This is more challenging than standard object detection, object segmentation and pose estimation tasks because semantic parts of animals often have similar appearance and highly varying shapes. To tackle these challenges, we build a mixture of compositional models to represent the object boundary and the boundaries of semantic parts. And we incorporate edge, appearance, and semantic part cues into the compositional model. Given part-level segmentation annotation, we develop a novel algorithm to learn a mixture of compositional models under various poses and viewpoints for certain animal classes. Furthermore, a linear complexity algorithm is offered for efficient inference of the compositional model using dynamic programming. We evaluate our method for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has pixelwise part labels. Experimental results demonstrate the effectiveness of our method.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6134",
        "title": "Data Representation using the Weyl Transform",
        "authors": [
            "Qiang Qiu",
            "Andrew Thompson",
            "Robert Calderbank",
            "Guillermo Sapiro"
        ],
        "abstract": "The Weyl transform is introduced as a rich framework for data representation. Transform coefficients are connected to the Walsh-Hadamard transform of multiscale autocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in its Weyl coefficients. The Weyl transform has a high degree of symmetry with respect to a large group of multiscale transformations, which allows compact yet discriminative representations to be obtained by pooling coefficients. The effectiveness of the Weyl transform is demonstrated through the example of textured image classification.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6154",
        "title": "Effective persistent homology of digital images",
        "authors": [
            "Ana Romero",
            "Julio Rubio",
            "Francis Sergeraert"
        ],
        "abstract": "In this paper, three Computational Topology methods (namely effective homology, persistent homology and discrete vector fields) are mixed together to produce algorithms for homological digital image processing. The algorithms have been implemented as extensions of the Kenzo system and have shown a good performance when applied on some actual images extracted from a public dataset.\n    ",
        "submission_date": "2014-10-06T00:00:00",
        "last_modified_date": "2014-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6163",
        "title": "Automated Objective Surgical Skill Assessment in the Operating Room Using Unstructured Tool Motion",
        "authors": [
            "Piyush Poddar",
            "Narges Ahmidi",
            "S. Swaroop Vedula",
            "Lisa Ishii",
            "Gregory D. Hager",
            "Masaru Ishii"
        ],
        "abstract": "Previous work on surgical skill assessment using intraoperative tool motion in the operating room (OR) has focused on highly-structured surgical tasks such as cholecystectomy. Further, these methods only considered generic motion metrics such as time and number of movements, which are of limited instructive value. In this paper, we developed and evaluated an automated approach to the surgical skill assessment of nasal septoplasty in the OR. The obstructed field of view and highly unstructured nature of septoplasty precludes trainees from efficiently learning the procedure. We propose a descriptive structure of septoplasty consisting of two types of activity: (1) brushing activity directed away from the septum plane characterizing the consistency of the surgeon's wrist motion and (2) activity along the septal plane characterizing the surgeon's coverage pattern. We derived features related to these two activity types that classify a surgeon's level of training with an average accuracy of about 72%. The features we developed provide surgeons with personalized, actionable feedback regarding their tool motion.\n    ",
        "submission_date": "2014-12-18T00:00:00",
        "last_modified_date": "2014-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6279",
        "title": "Non-parametric PSF estimation from celestial transit solar images using blind deconvolution",
        "authors": [
            "Adriana Gonzalez",
            "V\u00e9ronique Delouille",
            "Laurent Jacques"
        ],
        "abstract": "Context: Characterization of instrumental effects in astronomical imaging is important in order to extract accurate physical information from the observations. The measured image in a real optical instrument is usually represented by the convolution of an ideal image with a Point Spread Function (PSF). Additionally, the image acquisition process is also contaminated by other sources of noise (read-out, photon-counting). The problem of estimating both the PSF and a denoised image is called blind deconvolution and is ill-posed.\n",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6296",
        "title": "Generative Modeling of Convolutional Neural Networks",
        "authors": [
            "Jifeng Dai",
            "Yang Lu",
            "Ying-Nian Wu"
        ],
        "abstract": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6391",
        "title": "Py3DFreeHandUS: a library for voxel-array reconstruction using Ultrasonography and attitude sensors",
        "authors": [
            "Davide Monari",
            "Francesco Cenni",
            "Erwin Aertbeli\u00ebn",
            "Kaat Desloovere"
        ],
        "abstract": "In medical imaging, there is a growing interest to provide real-time images with good quality for large anatomical structures. To cope with this issue, we developed a library that allows to replace, for some specific clinical applications, more robust systems such as Computer Tomography (CT) and Magnetic Resonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for processing data acquired simultaneously by ultra-sonographic systems (US) and marker-based optoelectronic systems. In particular, US data enables to visualize subcutaneous body structures, whereas the optoelectronic system is able to collect the 3D position in space for reflective objects, that are called markers. By combining these two measurement devices, it is possible to reconstruct the real 3D morphology of body structures such as muscles, for relevant clinical implications. In the present research work, the different steps which allow to obtain a relevant 3D data set as well as the procedures for calibrating the systems and for determining the quality of the reconstruction.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6504",
        "title": "Learning to Segment Moving Objects in Videos",
        "authors": [
            "Katerina Fragkiadaki",
            "Pablo Arbelaez",
            "Panna Felsen",
            "Jitendra Malik"
        ],
        "abstract": "We segment moving objects in videos by ranking spatio-temporal segment proposals according to \"moving objectness\": how likely they are to contain a moving object. In each video frame, we compute segment proposals using multiple figure-ground segmentations on per frame motion boundaries. We rank them with a Moving Objectness Detector trained on image and motion fields to detect moving objects and discard over/under segmentations or background parts of the scene. We extend the top ranked segments into spatio-temporal tubes using random walkers on motion affinities of dense point trajectories. Our final tube ranking consistently outperforms previous segmentation methods in the two largest video segmentation benchmarks currently available, for any number of proposals. Further, our per frame moving object proposals increase the detection rate up to 7\\% over previous state-of-the-art static proposal methods.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6505",
        "title": "Pooled Motion Features for First-Person Videos",
        "authors": [
            "M. S. Ryoo",
            "Brandon Rothrock",
            "Larry Matthies"
        ],
        "abstract": "In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6537",
        "title": "Fracking Deep Convolutional Image Descriptors",
        "authors": [
            "Edgar Simo-Serra",
            "Eduard Trulls",
            "Luis Ferraz",
            "Iasonas Kokkinos",
            "Francesc Moreno-Noguer"
        ],
        "abstract": "In this paper we propose a novel framework for learning local image descriptors in a discriminative manner. For this purpose we explore a siamese architecture of Deep Convolutional Neural Networks (CNN), with a Hinge embedding loss on the L2 distance between descriptors. Since a siamese architecture uses pairs rather than single image patches to train, there exist a large number of positive samples and an exponential number of negative samples. We propose to explore this space with a stochastic sampling of the training set, in combination with an aggressive mining strategy over both the positive and negative samples which we denote as \"fracking\". We perform a thorough evaluation of the architecture hyper-parameters, and demonstrate large performance gains compared to both standard CNN learning strategies, hand-crafted image descriptors like SIFT, and the state-of-the-art on learned descriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms of the area under the curve (AUC) of the Precision-Recall curve.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6553",
        "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition",
        "authors": [
            "Vadim Lebedev",
            "Yaroslav Ganin",
            "Maksim Rakhuba",
            "Ivan Oseledets",
            "Victor Lempitsky"
        ],
        "abstract": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process.\n",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6574",
        "title": "Visual Instance Retrieval with Deep Convolutional Networks",
        "authors": [
            "Ali Sharif Razavian",
            "Josephine Sullivan",
            "Stefan Carlsson",
            "Atsuto Maki"
        ],
        "abstract": "This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efficient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using five standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6596",
        "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping",
        "authors": [
            "Scott Reed",
            "Honglak Lee",
            "Dragomir Anguelov",
            "Christian Szegedy",
            "Dumitru Erhan",
            "Andrew Rabinovich"
        ],
        "abstract": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6597",
        "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
        "authors": [
            "Tom Le Paine",
            "Pooya Khorrami",
            "Wei Han",
            "Thomas S. Huang"
        ],
        "abstract": "Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6598",
        "title": "Automatic Discovery and Optimization of Parts for Image Classification",
        "authors": [
            "Sobhan Naderi Parizi",
            "Andrea Vedaldi",
            "Andrew Zisserman",
            "Pedro Felzenszwalb"
        ],
        "abstract": "Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using L1/L2 regularization. All steps are driven \"directly\" by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of \"negative parts\", intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6607",
        "title": "Visual Scene Representations: Contrast, Scaling and Occlusion",
        "authors": [
            "Stefano Soatto",
            "Jingming Dong",
            "Nikolaos Karianakis"
        ],
        "abstract": "We study the structure of representations, defined as approximations of minimal sufficient statistics that are maximal invariants to nuisance factors, for visual data subject to scaling and occlusion of line-of-sight. We derive analytical expressions for such representations and show that, under certain restrictive assumptions, they are related to features commonly in use in the computer vision community. This link highlights the condition tacitly assumed by these descriptors, and also suggests ways to improve and generalize them. This new interpretation draws connections to the classical theories of sampling, hypothesis testing and group invariance.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6618",
        "title": "Permutohedral Lattice CNNs",
        "authors": [
            "Martin Kiefel",
            "Varun Jampani",
            "Peter V. Gehler"
        ],
        "abstract": "This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6626",
        "title": "The local low-dimensionality of natural images",
        "authors": [
            "Olivier J. H\u00e9naff",
            "Johannes Ball\u00e9",
            "Neil C. Rabinowitz",
            "Eero P. Simoncelli"
        ],
        "abstract": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6631",
        "title": "Visualizing and Comparing Convolutional Neural Networks",
        "authors": [
            "Wei Yu",
            "Kuiyuan Yang",
            "Yalong Bai",
            "Hongxun Yao",
            "Yong Rui"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) have achieved comparable error rates to well-trained human on ILSVRC2014 image classification task. To achieve better performance, the complexity of CNNs is continually increasing with deeper and bigger architectures. Though CNNs achieved promising external classification behavior, understanding of their internal work mechanism is still limited. In this work, we attempt to understand the internal work mechanism of CNNs by probing the internal representations in two comprehensive aspects, i.e., visualizing patches in the representation spaces constructed by different layers, and visualizing visual information kept in each layer. We further compare CNNs with different depths and show the advantages brought by deeper architecture.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6632",
        "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Zhiheng Huang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6749",
        "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction",
        "authors": [
            "Abdulrahman Oladipupo Ibraheem"
        ],
        "abstract": "By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes, but minimized for those belonging to the same class, while simultaneously enforcing sparsity on the ANN outputs. The vectors that result from the projection can then be used as features in any classifier of choice. Mathematically, I formulate the proposed method as the minimisation of an objective function which can be interpreted, in the ANN output space, as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes, added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes, plus sparsity and weight decay terms. To derive an algorithm for minimizing the objective function via gradient descent, I use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ANN weights and biases, and find that each of the required partial derivatives can be expressed as a sum of six terms. As it turns out, four of those six terms can be computed using the standard back propagation algorithm; the fifth can be computed via a slight modification of the standard backpropagation algorithm; while the sixth one can be computed via simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora of digits and letters, the CMU PIE database of faces, the MNIST digits database, and other standard machine learning databases.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6759",
        "title": "Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d Shape Warping in Quadratic Time?",
        "authors": [
            "Abdulrahman Oladipupo Ibraheem"
        ],
        "abstract": "We propose Bidirectional Shape Correspondence (BSC) as a possible improvement on the famous shape contexts (SC) framework. Our proposals derive from the observation that the SC framework enforces a one-to-one correspondence between sample points, and that this leads to two possible drawbacks. First, this denies the framework the opportunity to effect advantageous many-to-many matching between points on the two shapes being compared. Second, this calls for the Hungarian algorithm which unfortunately usurps cubic time. While the dynamic-space-warping dynamic programming algorithm has provided a standard solution to the first problem above, it demands quintic time for general multi-contour shapes, and w times quadratic time for the special case of single-contour shapes, even after an heuristic search window of width w has been chosen. Therefore, in this work, we propose a simple method for computing \"many-to-many\" correspondences for the class of all 2-d shapes in quadratic time. Our approach is to explicitly let each point on the first shape choose a best match on the second shape, and vice versa. Along the way, we also propose the use of data-clustering techniques for dealing with the outliers problem, and, from another viewpoint, it turns out that this clustering can be seen as an autonomous, rather than pre-computed, sampling of shape boundary.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6791",
        "title": "Mixture of Parts Revisited: Expressive Part Interactions for Pose Estimation",
        "authors": [
            "Anoop Katti",
            "Anurag Mittal"
        ],
        "abstract": "Part-based models with restrictive tree-structured interactions for the Human Pose Estimation problem, leaves many part interactions unhandled. Two of the most common and strong manifestations of such unhandled interactions are self-occlusion among the parts and the confusion in the localization of the non-adjacent symmetric parts. By handling the self-occlusion in a data efficient manner, we improve the performance of the basic Mixture of Parts model by a large margin, especially on uncommon poses. Through addressing the confusion in the symmetric limb localization using a combination of two complementing trees, we improve the performance on all the parts by atmost doubling the running time. Finally, we show that the combination of the two solutions improves the results. We report results that are equivalent to the state-of-the-art on two standard datasets. Because of maintaining the tree-structured interactions and only part-level modeling of the base Mixture of Parts model, this is achieved in time that is much less than the best performing part-based model.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6847",
        "title": "A New Way to Factorize Linear Cameras",
        "authors": [
            "Feng Lu",
            "Ziqiang Chen"
        ],
        "abstract": "The implementation details of factorizing the 3x4 projection matrices of linear cameras into their left matrix factors and the 4x4 homogeneous central(also parallel for infinite center cases) projection factors are presented in this work. Any full row rank 3x4 real matrix can be factorized into such basic matrices which will be called LC factors.\n",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6856",
        "title": "Object Detectors Emerge in Deep Scene CNNs",
        "authors": [
            "Bolei Zhou",
            "Aditya Khosla",
            "Agata Lapedriza",
            "Aude Oliva",
            "Antonio Torralba"
        ],
        "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6857",
        "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks",
        "authors": [
            "Jyh-Jing Hwang",
            "Tyng-Luh Liu"
        ],
        "abstract": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6885",
        "title": "Half-CNN: A General Framework for Whole-Image Regression",
        "authors": [
            "Jun Yuan",
            "Bingbing Ni",
            "Ashraf A.Kassim"
        ],
        "abstract": "The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2014-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7006",
        "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks",
        "authors": [
            "Michael Giering",
            "Vivek Venugopalan",
            "Kishore Reddy"
        ],
        "abstract": "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7007",
        "title": "Occlusion Edge Detection in RGB-D Frames using Deep Convolutional Networks",
        "authors": [
            "Soumik Sarkar",
            "Vivek Venugopalan",
            "Kishore Reddy",
            "Michael Giering",
            "Julian Ryde",
            "Navdeep Jaitly"
        ],
        "abstract": "Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although they can be extracted from range data however extracting them from images and videos would be extremely beneficial. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7054",
        "title": "Attention for Fine-Grained Categorization",
        "authors": [
            "Pierre Sermanet",
            "Andrea Frome",
            "Esteban Real"
        ],
        "abstract": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7062",
        "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
        "authors": [
            "Liang-Chieh Chen",
            "George Papandreou",
            "Iasonas Kokkinos",
            "Kevin Murphy",
            "Alan L. Yuille"
        ],
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7122",
        "title": "Learning Deep Object Detectors from 3D Models",
        "authors": [
            "Xingchao Peng",
            "Baochen Sun",
            "Karim Ali",
            "Kate Saenko"
        ],
        "abstract": "Crowdsourced 3D CAD models are becoming easily accessible online, and can potentially generate an infinite number of training images for almost any object ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7144",
        "title": "Fully Convolutional Multi-Class Multiple Instance Learning",
        "authors": [
            "Deepak Pathak",
            "Evan Shelhamer",
            "Jonathan Long",
            "Trevor Darrell"
        ],
        "abstract": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7155",
        "title": "Learning Compact Convolutional Neural Networks with Nested Dropout",
        "authors": [
            "Chelsea Finn",
            "Lisa Anne Hendricks",
            "Trevor Darrell"
        ],
        "abstract": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7190",
        "title": "Convolutional Neural Networks for joint object detection and pose estimation: A comparative study",
        "authors": [
            "Francisco Massa",
            "Mathieu Aubry",
            "Renaud Marlet"
        ],
        "abstract": "In this paper we study the application of convolutional neural networks for jointly detecting objects depicted in still images and estimating their 3D pose. We identify different feature representations of oriented objects, and energies that lead a network to learn this representations. The choice of the representation is crucial since the pose of an object has a natural, continuous structure while its category is a discrete variable. We evaluate the different approaches on the joint object detection and pose estimation task of the Pascal3D+ benchmark using Average Viewpoint Precision. We show that a classification approach on discretized viewpoints achieves state-of-the-art performance for joint object detection and pose estimation, and significantly outperforms existing baselines on this benchmark.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7242",
        "title": "Learning of Proto-object Representations via Fixations on Low Resolution",
        "authors": [
            "Chengyao Shen",
            "Xun Huang",
            "Qi Zhao"
        ],
        "abstract": "While previous researches in eye fixation prediction typically rely on integrating low-level features (e.g. color, edge) to form a saliency map, recently it has been found that the structural organization of these features into a proto-object representation can play a more significant role. In this work, we present a computational framework based on deep network to demonstrate that proto-object representations can be learned from low-resolution image patches from fixation regions. We advocate the use of low-resolution inputs in this work due to the following reasons: (1) Proto-objects are computed in parallel over an entire visual field (2) People can perceive or recognize objects well even it is in low resolution. (3) Fixations from lower resolution images can predict fixations on higher resolution images. In the proposed computational model, we extract multi-scale image patches on fixation regions from eye fixation datasets, resize them to low resolution and feed them into a hierarchical. With layer-wise unsupervised feature learning, we find that many proto-objects like features responsive to different shapes of object blobs are learned out. Visualizations also show that these features are selective to potential objects in the scene and the responses of these features work well in predicting eye fixations on the images when combined with learned weights.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2014-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7259",
        "title": "Unsupervised Feature Learning with C-SVDDNet",
        "authors": [
            "Dong Wang",
            "Xiaoyang Tan"
        ],
        "abstract": "In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers' great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from K-means with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of C-SVDD can be solved very efficiently as a linear programming problem. Additionally, traditional unsupervised feature learning methods usually take an average or sum of local representations to obtain global representation which ignore spatial relationship among them. To use spatial information we propose a global representation with a variant of SIFT descriptor. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7277",
        "title": "Fusing Color and Texture Cues to Categorize the Fruit Diseases from Images",
        "authors": [
            "Shiv Ram Dubey",
            "Anand Singh Jalal"
        ],
        "abstract": "The economic and production losses in agricultural industry worldwide are due to the presence of diseases in the several kinds of fruits. In this paper, a method for the classification of fruit diseases is proposed and experimentally validated. The image processing based proposed approach is composed of the following main steps; in the first step K-Means clustering technique is used for the defect segmentation, in the second step color and textural cues are extracted and fused from the segmented image, and finally images are classified into one of the classes by using a Multi-class Support Vector Machine. We have considered diseases of apple as a test case and evaluated our approach for three types of apple diseases namely apple scab, apple blotch and apple rot and normal apples without diseases. Our experimentation points out that the proposed fusion scheme can significantly support accurate detection and automatic classification of fruit diseases.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2014-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7504",
        "title": "Higher-order Spatial Accuracy in Diffeomorphic Image Registration",
        "authors": [
            "Henry O. Jacobs",
            "Stefan Sommer"
        ],
        "abstract": "We discretize a cost functional for image registration problems by deriving Taylor expansions for the matching term. Minima of the discretized cost functionals can be computed with no spatial discretization error, and the optimal solutions are equivalent to minimal energy curves in the space of $k$-jets. We show that the solutions convergence to optimal solutions of the original cost functional as the number of particles increases with a convergence rate of $O(h^{d+k})$ where $h$ is a resolution parameter. The effect of this approach over traditional particle methods is illustrated on synthetic examples and real images.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7513",
        "title": "Symmetry in Image Registration and Deformation Modeling",
        "authors": [
            "Stefan Sommer",
            "Henry O. Jacobs"
        ],
        "abstract": "We survey the role of symmetry in diffeomorphic registration of landmarks, curves, surfaces, images and higher-order data. The infinite dimensional problem of finding correspondences between objects can for a range of concrete data types be reduced resulting in compact representations of shape and spatial structure. This reduction is possible because the available data is incomplete in encoding the full deformation model. Using reduction by symmetry, we describe the reduced models in a common theoretical framework that draws on links between the registration problem and geometric mechanics. Symmetry also arises in reduction to the Lie algebra using particle relabeling symmetry allowing the equations of motion to be written purely in terms of Eulerian velocity field. Reduction by symmetry has recently been applied for jet-matching and higher-order discrete approximations of the image matching problem. We outline these constructions and further cases where reduction by symmetry promises new approaches to registration of complex data types.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7626",
        "title": "AltecOnDB: A Large-Vocabulary Arabic Online Handwriting Recognition Database",
        "authors": [
            "Ibrahim Abdelaziz",
            "Sherif Abdou"
        ],
        "abstract": "Arabic is a semitic language characterized by a complex and rich morphology. The exceptional degree of ambiguity in the writing system, the rich morphology, and the highly complex word formation process of roots and patterns all contribute to making computational approaches to Arabic very challenging. As a result, a practical handwriting recognition system should support large vocabulary to provide a high coverage and use the context information for disambiguation. Several research efforts have been devoted for building online Arabic handwriting recognition systems. Most of these methods are either using their small private test data sets or a standard database with limited lexicon and coverage. A large scale handwriting database is an essential resource that can advance the research of online handwriting recognition. Currently, there is no online Arabic handwriting database with large lexicon, high coverage, large number of writers and training/testing data.\n",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7680",
        "title": "A Fuzzy Based Model to Identify Printed Sinhala Characters (ICIAfS14)",
        "authors": [
            "G. I. Gunarathna",
            "M. A. P. Chamikara",
            "R. G. Ragel"
        ],
        "abstract": "Character recognition techniques for printed documents are widely used for English language. However, the systems that are implemented to recognize Asian languages struggle to increase the accuracy of recognition. Among other Asian languages (such as Arabic, Tamil, Chinese), Sinhala characters are unique, mainly because they are round in shape. This unique feature makes it a challenge to extend the prevailing techniques to improve recognition of Sinhala characters. Therefore, a little attention has been given to improve the accuracy of Sinhala character recognition. A novel method, which makes use of this unique feature, could be advantageous over other methods. This paper describes the use of a fuzzy inference system to recognize Sinhala characters. Feature extraction is mainly focused on distance and intersection measurements in different directions from the center of the letter making use of the round shape of characters. The results showed an overall accuracy of 90.7% for 140 instances of letters tested, much better than similar systems.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7689",
        "title": "Locating Tables in Scanned Documents for Reconstructing and Republishing (ICIAfS14)",
        "authors": [
            "Akmal Jahan Mac",
            "Roshan G Ragel"
        ],
        "abstract": "Pool of knowledge available to the mankind depends on the source of learning resources, which can vary from ancient printed documents to present electronic material. The rapid conversion of material available in traditional libraries to digital form needs a significant amount of work if we are to maintain the format and the look of the electronic documents as same as their printed counterparts. Most of the printed documents contain not only characters and its formatting but also some associated non text objects such as tables, charts and graphical objects. It is challenging to detect them and to concentrate on the format preservation of the contents while reproducing them. To address this issue, we propose an algorithm using local thresholds for word space and line height to locate and extract all categories of tables from scanned document images. From the experiments performed on 298 documents, we conclude that our algorithm has an overall accuracy of about 75% in detecting tables from the scanned document images. Since the algorithm does not completely depend on rule lines, it can detect all categories of tables in a range of scanned documents with different font types, styles and sizes to extract their formatting features. Moreover, the algorithm can be applied to locate tables in multi column layouts with small modification in layout analysis. Treating tables with their existing formatting features will tremendously help the reproducing of printed documents for reprinting and updating purposes.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2014-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7725",
        "title": "Automatic Photo Adjustment Using Deep Neural Networks",
        "authors": [
            "Zhicheng Yan",
            "Hao Zhang",
            "Baoyuan Wang",
            "Sylvain Paris",
            "Yizhou Yu"
        ],
        "abstract": "Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7844",
        "title": "Texture analysis using volume-radius fractal dimension",
        "authors": [
            "Andr\u00e9 R. Backes",
            "Odemir M. Bruno"
        ],
        "abstract": "Texture plays an important role in computer vision. It is one of the most important visual attributes used in image analysis, once it provides information about pixel organization at different regions of the image. This paper presents a novel approach for texture characterization, based on complexity analysis. The proposed approach expands the idea of the Mass-radius fractal dimension, a method originally developed for shape analysis, to a set of coordinates in 3D-space that represents the texture under analysis in a signature able to characterize efficiently different texture classes in terms of complexity. An experiment using images from the Brodatz album illustrates the method performance.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7849",
        "title": "Brachiaria species identification using imaging techniques based on fractal descriptors",
        "authors": [
            "Jo\u00e3o Batista Florindo",
            "N\u00fabia Rosa da Silva",
            "Liliane Maria Romualdo",
            "Fernanda de F\u00e1tima da Silva",
            "Pedro Henrique de Cerqueira Luz",
            "Valdo Rodrigues Herling",
            "Odemir Martinez Bruno"
        ],
        "abstract": "The use of a rapid and accurate method in diagnosis and classification of species and/or cultivars of forage has practical relevance, scientific and trade in various areas of study. Thus, leaf samples of fodder plant species \\textit{Brachiaria} were previously identified, collected and scanned to be treated by means of artificial vision to make the database and be used in subsequent classifications. Forage crops used were: \\textit{Brachiaria decumbens} cv. IPEAN; \\textit{Brachiaria ruziziensis} Germain \\& Evrard; \\textit{Brachiaria Brizantha} (Hochst. ex. A. Rich.) Stapf; \\textit{Brachiaria arrecta} (Hack.) Stent. and \\textit{Brachiaria spp}. The images were analyzed by the fractal descriptors method, where a set of measures are obtained from the values of the fractal dimension at different scales. Therefore such values are used as inputs for a state-of-the-art classifier, the Support Vector Machine, which finally discriminates the images according to the respective species.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7851",
        "title": "Fractal descriptors based on the probability dimension: a texture analysis and classification approach",
        "authors": [
            "Jo\u00e3o Batista Florindo",
            "Odemir Martinez Bruno"
        ],
        "abstract": "In this work, we propose a novel technique for obtaining descriptors of gray-level texture images. The descriptors are provided by applying a multiscale transform to the fractal dimension of the image estimated through the probability (Voss) method. The effectiveness of the descriptors is verified in a classification task using benchmark over texture datasets. The results obtained demonstrate the efficiency of the proposed method as a tool for the description and discrimination of texture images.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7854",
        "title": "Joint Deep Learning for Car Detection",
        "authors": [
            "Seyedshams Feyzabadi"
        ],
        "abstract": "Traditional object recognition approaches apply feature extraction, part deformation handling, occlusion handling and classification sequentially while they are independent from each other. Ouyang and Wang proposed a model for jointly learning of all of the mentioned processes using one deep neural network. We utilized, and manipulated their toolbox in order to apply it in car detection scenarios where it had not been tested. Creating a single deep architecture from these components, improves the interaction between them and can enhance the performance of the whole system. We believe that the approach can be used as a general purpose object detection toolbox. We tested the algorithm on UIUC car dataset, and achieved an outstanding result. The accuracy of our method was 97 % while the previously reported results showed an accuracy of up to 91 %. We strongly believe that having an experiment on a larger dataset can show the advantage of using deep models over shallow ones.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7856",
        "title": "Gabor wavelets combined with volumetric fractal dimension applied to texture analysis",
        "authors": [
            "\u00c1lvaro Gomez Z.",
            "Jo\u00e3o B. Florindo",
            "Odemir M. Bruno"
        ],
        "abstract": "Texture analysis and classification remain as one of the biggest challenges for the field of computer vision and pattern recognition. On this matter, Gabor wavelets has proven to be a useful technique to characterize distinctive texture patterns. However, most of the approaches used to extract descriptors of the Gabor magnitude space usually fail in representing adequately the richness of detail present into a unique feature vector. In this paper, we propose a new method to enhance the Gabor wavelets process extracting a fractal signature of the magnitude spaces. Each signature is reduced using a canonical analysis function and concatenated to form the final feature vector. Experiments were conducted on several texture image databases to prove the power and effectiveness of the proposed method. Results obtained shown that this method outperforms other early proposed method, creating a more reliable technique for texture feature extraction.\n    ",
        "submission_date": "2014-12-25T00:00:00",
        "last_modified_date": "2014-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7884",
        "title": "Sparkle Vision: Seeing the World through Random Specular Microfacets",
        "authors": [
            "Zhengdong Zhang",
            "Phillip Isola",
            "Edward H. Adelson"
        ],
        "abstract": "In this paper, we study the problem of reproducing the world lighting from a single image of an object covered with random specular microfacets on the surface. We show that such reflectors can be interpreted as a randomized mapping from the lighting to the image. Such specular objects have very different optical properties from both diffuse surfaces and smooth specular objects like metals, so we design special imaging system to robustly and effectively photograph them. We present simple yet reliable algorithms to calibrate the proposed system and do the inference. We conduct experiments to verify the correctness of our model assumptions and prove the effectiveness of our pipeline.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7889",
        "title": "Improved texture image classification through the use of a corrosion-inspired cellular automaton",
        "authors": [
            "N\u00fabia Rosa da Silva",
            "Pieter Van der Wee\u00ebn",
            "Bernard De Baets",
            "Odemir Martinez Bruno"
        ],
        "abstract": "In this paper, the problem of classifying synthetic and natural texture images is addressed. To tackle this problem, an innovative method is proposed that combines concepts from corrosion modeling and cellular automata to generate a texture descriptor. The core processes of metal (pitting) corrosion are identified and applied to texture images by incorporating the basic mechanisms of corrosion in the transition function of the cellular automaton. The surface morphology of the image is analyzed before and during the application of the transition function of the cellular automaton. In each iteration the cumulative mass of corroded product is obtained to construct each of the attributes of the texture descriptor. In a final step, this texture descriptor is used for image classification by applying Linear Discriminant Analysis. The method was tested on the well-known Brodatz and Vistex databases. In addition, in order to verify the robustness of the method, its invariance to noise and rotation were tested. To that end, different variants of the original two databases were obtained through addition of noise to and rotation of the images. The results showed that the method is effective for texture classification according to the high success rates obtained in all cases. This indicates the potential of employing methods inspired on natural phenomena in other fields.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7957",
        "title": "Detect2Rank : Combining Object Detectors Using Learning to Rank",
        "authors": [
            "Sezer Karaoglu",
            "Yang Liu",
            "Theo Gevers"
        ],
        "abstract": "Object detection is an important research area in the field of computer vision. Many detection algorithms have been proposed. However, each object detector relies on specific assumptions of the object appearance and imaging conditions. As a consequence, no algorithm can be considered as universal. With the large variety of object detectors, the subsequent question is how to select and combine them.\n",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7963",
        "title": "Texture analysis by multi-resolution fractal descriptors",
        "authors": [
            "Jo\u00e3o B. Florindo",
            "Odemir M. Bruno"
        ],
        "abstract": "This work proposes a texture descriptor based on fractal theory. The method is based on the Bouligand-Minkowski descriptors. We decompose the original image recursively into 4 equal parts. In each recursion step, we estimate the average and the deviation of the Bouligand-Minkowski descriptors computed over each part. Thus, we extract entropy features from both average and deviation. The proposed descriptors are provided by the concatenation of such measures. The method is tested in a classification experiment under well known datasets, that is, Brodatz and Vistex. The results demonstrate that the proposed technique achieves better results than classical and state-of-the-art texture descriptors, such as Gabor-wavelets and co-occurrence matrix.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8070",
        "title": "Functional correspondence by matrix completion",
        "authors": [
            "Artiom Kovnatsky",
            "Michael M. Bronstein",
            "Xavier Bresson",
            "Pierre Vandergheynst"
        ],
        "abstract": "In this paper, we consider the problem of finding dense intrinsic correspondence between manifolds using the recently introduced functional framework. We pose the functional correspondence problem as matrix completion with manifold geometric structure and inducing functional localization with the $L_1$ norm. We discuss efficient numerical procedures for the solution of our problem. Our method compares favorably to the accuracy of state-of-the-art correspondence algorithms on non-rigid shape matching benchmarks, and is especially advantageous in settings when only scarce data is available.\n    ",
        "submission_date": "2014-12-27T00:00:00",
        "last_modified_date": "2014-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8197",
        "title": "Metacarpal Bones Localization in X-ray Imagery Using Particle Filter Segmentation",
        "authors": [
            "Z. Bardosi",
            "D. Granata",
            "G. Lugos",
            "A. P. Tafti",
            "S. Saxena"
        ],
        "abstract": "Statistical methods such as sequential Monte Carlo Methods were proposed for detection, segmentation and tracking of objects in digital images. A similar approach, called Shape Particle Filters was introduced for the segmentation of vertebra, lungs and hearts [1]. In this contribution, a global shape and a local appearance model are derived from specific object annotated X-ray images of the metacarpal bones. In the test data a unique labeling of the bone boundary and the background points and a manual annotation is given. Using a set of local features (Haar-like) in the neighborhood of each pixel a probabilistic pixel classifier is built using the random forest algorithm. To fit the shape model to a new image, a label probability map is extracted and then the optimal shape is obtained by maximizing the probability of each landmark with the Differential Evolution algorithm.\n    ",
        "submission_date": "2014-12-28T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8287",
        "title": "Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in Images",
        "authors": [
            "Junyan Wang",
            "Kap-Luk Chan"
        ],
        "abstract": "The same type of objects in different images may vary in their shapes because of rigid and non-rigid shape deformations, occluding foreground as well as cluttered background. The problem concerned in this work is the shape extraction in such challenging situations. We approach the shape extraction through shape alignment and recovery. This paper presents a novel and general method for shape alignment and recovery by using one example shapes based on deterministic energy minimization. Our idea is to use general model of shape deformation in minimizing active contour energies. Given \\emph{a priori} form of the shape deformation, we show how the curve evolution equation corresponding to the shape deformation can be derived. The curve evolution is called the prior variation shape evolution (PVSE). We also derive the energy-minimizing PVSE for minimizing active contour energies. For shape recovery, we propose to use the PVSE that deforms the shape while preserving its shape characteristics. For choosing such shape-preserving PVSE, a theory of shape preservability of the PVSE is established. Experimental results validate the theory and the formulations, and they demonstrate the effectiveness of our method.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2014-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8341",
        "title": "Spectral classification using convolutional neural networks",
        "authors": [
            "Pavel H\u00e1la"
        ],
        "abstract": "There is a great need for accurate and autonomous spectral classification methods in astrophysics. This thesis is about training a convolutional neural network (ConvNet) to recognize an object class (quasar, star or galaxy) from one-dimension spectra only. Author developed several scripts and C programs for datasets preparation, preprocessing and postprocessing of the data. EBLearn library (developed by Pierre Sermanet and Yann LeCun) was used to create ConvNets. Application on dataset of more than 60000 spectra yielded success rate of nearly 95%. This thesis conclusively proved great potential of convolutional neural networks and deep learning methods in astrophysics.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2014-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8496",
        "title": "Accurate Localization in Dense Urban Area Using Google Street View Image",
        "authors": [
            "Mahdi Salarian"
        ],
        "abstract": "Accurate information about the location and orientation of a camera in mobile devices is central to the utilization of location-based services (LBS). Most of such mobile devices rely on GPS data but this data is subject to inaccuracy due to imperfections in the quality of the signal provided by satellites. This shortcoming has spurred the research into improving the accuracy of localization. Since mobile devices have camera, a major thrust of this research has been seeks to acquire the local scene and apply image retrieval techniques by querying a GPS-tagged image database to find the best match for the acquired scene.. The techniques are however computationally demanding and unsuitable for real-time applications such as assistive technology for navigation by the blind and visually impaired which motivated out work. To overcome the high complexity of those techniques, we investigated the use of inertial sensors as an aid in image-retrieval-based approach. Armed with information of media other than images, such as data from the GPS module along with orientation sensors such as accelerometer and gyro, we sought to limit the size of the image set to c search for the best match. Specifically, data from the orientation sensors along with Dilution of precision (DOP) from GPS are used to find the angle of view and estimation of position. We present analysis of the reduction in the image set size for the search as well as simulations to demonstrate the effectiveness in a fast implementation with 98% Estimated Position Error.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2014-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8556",
        "title": "Domain-Size Pooling in Local Descriptors: DSP-SIFT",
        "authors": [
            "Jingming Dong",
            "Stefano Soatto"
        ],
        "abstract": "We introduce a simple modification of local image descriptors, such as SIFT, based on pooling gradient orientations across different domain sizes, in addition to spatial locations. The resulting descriptor, which we call DSP-SIFT, outperforms other methods in wide-baseline matching benchmarks, including those based on convolutional neural networks, despite having the same dimension of SIFT and requiring no training.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8656",
        "title": "A multistep segmentation algorithm for vessel extraction in medical imaging",
        "authors": [
            "Nasser Aghazadeh",
            "Ladan Sharafyan Cigaroudy"
        ],
        "abstract": "The main contribution of this paper is to propose an iterative procedure for tubular structure segmentation of 2D images, which combines tight frame of Curvelet transforms with a SURE technique thresholding which is based on principle obtained by minimizing Stein Unbiased Risk Estimate for denoising. This proposed algorithm is mainly based on the TFA proposal presented in [1, 9], which we use eigenvectors of Hessian matrix of image for improving this iterative part in segmenting unclear and narrow vessels and filling the gap between separate pieces of detected vessels. The experimental results are presented to demonstrate the effectiveness of the proposed model.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2025-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8659",
        "title": "Deep Roto-Translation Scattering for Object Classification",
        "authors": [
            "Edouard Oyallon",
            "St\u00e9phane Mallat"
        ],
        "abstract": "Dictionary learning algorithms or supervised deep convolution networks have considerably improved the efficiency of predefined feature representations such as SIFT. We introduce a deep scattering convolution network, with predefined wavelet filters over spatial and angular variables. This representation brings an important improvement to results previously obtained with predefined features over object image databases such as Caltech and CIFAR. The resulting accuracy is comparable to results obtained with unsupervised deep learning and dictionary based representations. This shows that refining image representations by using geometric priors is a promising direction to improve image classification and its understanding.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0131",
        "title": "System Analysis And Design For Multimedia Retrieval Systems",
        "authors": [
            "Avinash N Bhute",
            "B B Meshram"
        ],
        "abstract": "Due to the extensive use of information technology and the recent developments in multimedia systems, the amount of multimedia data available to users has increased exponentially. Video is an example of multimedia data as it contains several kinds of data such as text, image, meta-data, visual and audio. Content based video retrieval is an approach for facilitating the searching and browsing of large multimedia collections over WWW. In order to create an effective video retrieval system, visual perception must be taken into account. We conjectured that a technique which employs multiple features for indexing and retrieval would be more effective in the discrimination and search tasks of videos. In order to validate this, content based indexing and retrieval systems were implemented using color histogram, Texture feature (GLCM), edge density and motion..\n    ",
        "submission_date": "2013-12-31T00:00:00",
        "last_modified_date": "2013-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.0767",
        "title": "From Kernel Machines to Ensemble Learning",
        "authors": [
            "Chunhua Shen",
            "Fayao Liu"
        ],
        "abstract": "Ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner. Here we propose a principled framework for directly constructing ensemble learning methods from kernel methods. Unlike previous studies showing the equivalence between boosting and support vector machines (SVMs), which needs a translation procedure, we show that it is possible to design boosting-like procedure to solve the SVM optimization problems.\n",
        "submission_date": "2014-01-04T00:00:00",
        "last_modified_date": "2014-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1489",
        "title": "Key point selection and clustering of swimmer coordination through Sparse Fisher-EM",
        "authors": [
            "John Komar",
            "Romain H\u00e9rault",
            "Ludovic Seifert"
        ],
        "abstract": "To answer the existence of optimal swimmer learning/teaching strategies, this work introduces a two-level clustering in order to analyze temporal dynamics of motor learning in breaststroke swimming. Each level have been performed through Sparse Fisher-EM, a unsupervised framework which can be applied efficiently on large and correlated datasets. The induced sparsity selects key points of the coordination phase without any prior knowledge.\n    ",
        "submission_date": "2014-01-07T00:00:00",
        "last_modified_date": "2014-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1558",
        "title": "The Continuity of Images by Transmission Imaging Revisited",
        "authors": [
            "Zhitao Fan",
            "Feng Guan",
            "Chunlin Wu",
            "Ming Yan"
        ],
        "abstract": "Transmission imaging, as an important imaging technique widely used in astronomy, medical diagnosis, and biology science, has been shown in [49] quite different from reflection imaging used in our everyday life. Understanding the structures of images (the prior information) is important for designing, testing, and choosing image processing methods, and good image processing methods are helpful for further uses of the image data, e.g., increasing the accuracy of the object reconstruction methods in transmission imaging applications. In reflection imaging, the images are usually modeled as discontinuous functions and even piecewise constant functions. In transmission imaging, it was shown very recently in [49] that almost all images are continuous functions. However, the author in [49] considered only the case of parallel beam geometry and used some too strong assumptions in the proof, which exclude some common cases such as cylindrical objects. In this paper, we consider more general beam geometries and simplify the assumptions by using totally different techniques. In particular, we will prove that almost all images in transmission imaging with both parallel and divergent beam geometries (two most typical beam geometries) are continuous functions, under much weaker assumptions than those in [49], which admit almost all practical cases. Besides, taking into accounts our analysis, we compare two image processing methods for Poisson noise (which is the most significant noise in transmission imaging) removal. Numerical experiments will be provided to demonstrate our analysis.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1605",
        "title": "Fast nonparametric clustering of structured time-series",
        "authors": [
            "James Hensman",
            "Magnus Rattray",
            "Neil D. Lawrence"
        ],
        "abstract": "In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.\n    ",
        "submission_date": "2014-01-08T00:00:00",
        "last_modified_date": "2014-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1882",
        "title": "Image reconstruction from few views by L0-norm optimization",
        "authors": [
            "Yuli Sun",
            "Jinxu Tao"
        ],
        "abstract": "The L1-norm of the gradient-magnitude images (GMI), which is the well-known total variation (TV) model, is widely used as regularization in the few views CT reconstruction. As the L1-norm TV regularization is tending to uniformly penalize the image gradient and the low-contrast structures are sometimes over smoothed, we proposed a new algorithm based on the L0-norm of the GMI to deal with the few views problem. To rise to the challenges introduced by the L0-norm DGT, the algorithm uses a pseudo-inverse transform of DGT and adapts an iterative hard thresholding (IHT) algorithm, whose convergence and effective efficiency have been theoretically proven. The simulation indicates that the algorithm proposed in this paper can obviously improve the reconstruction quality.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.1946",
        "title": "Hand-guided 3D surface acquisition by combining simple light sectioning with real-time algorithms",
        "authors": [
            "Oliver Arold",
            "Svenja Ettl",
            "Florian Willomitzer",
            "Gerd H\u00e4usler"
        ],
        "abstract": "Precise 3D measurements of rigid surfaces are desired in many fields of application like quality control or surgery. Often, views from all around the object have to be acquired for a full 3D description of the object surface. We present a sensor principle called \"Flying Triangulation\" which avoids an elaborate \"stop-and-go\" procedure. It combines a low-cost classical light-section sensor with an algorithmic pipeline. A hand-guided sensor captures a continuous movie of 3D views while being moved around the object. The views are automatically aligned and the acquired 3D model is displayed in real time. In contrast to most existing sensors no bandwidth is wasted for spatial or temporal encoding of the projected lines. Nor is an expensive color camera necessary for 3D acquisition. The achievable measurement uncertainty and lateral resolution of the generated 3D data is merely limited by physics. An alternating projection of vertical and horizontal lines guarantees the existence of corresponding points in successive 3D views. This enables a precise registration without surface interpolation. For registration, a variant of the iterative closest point algorithm - adapted to the specific nature of our 3D views - is introduced. Furthermore, data reduction and smoothing without losing lateral resolution as well as the acquisition and mapping of a color texture is presented. The precision and applicability of the sensor is demonstrated by simulation and measurement results.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.2051",
        "title": "Enhancement performance of road recognition system of autonomous robots in shadow scenario",
        "authors": [
            "Olusanya Y. Agunbiade",
            "Tranos Zuva",
            "Awosejo O. Johnson",
            "Keneilwe Zuva"
        ],
        "abstract": "Road region recognition is a main feature that is gaining increasing attention from intellectuals because it helps autonomous vehicle to achieve a successful navigation without accident. However, different techniques based on camera sensor have been used by various researchers and outstanding results have been achieved. Despite their success, environmental noise like shadow leads to inaccurate recognition of road region which eventually leads to accident for autonomous vehicle. In this research, we conducted an investigation on shadow and its effects, optimized the road region recognition system of autonomous vehicle by introducing an algorithm capable of detecting and eliminating the effects of shadow. The experimental performance of our system was tested and compared using the following schemes: Total Positive Rate (TPR), False Negative Rate (FNR), Total Negative Rate (TNR), Error Rate (ERR) and False Positive Rate (FPR). The performance result of the system improved on road recognition in shadow scenario and this advancement has added tremendously to successful navigation approaches for autonomous vehicle.\n    ",
        "submission_date": "2014-01-09T00:00:00",
        "last_modified_date": "2014-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3385",
        "title": "A programme to determine the exact interior of any connected digital picture",
        "authors": [
            "Antonio Elias Fabris",
            "Val\u00e9rio Ramos Batista"
        ],
        "abstract": "Region filling is one of the most important and fundamental operations in computer graphics and image processing. Many filling algorithms and their implementations are based on the Euclidean geometry, which are then translated into computational models moving carelessly from the continuous to the finite discrete space of the computer. The consequences of this approach is that most implementations fail when tested for challenging degenerate and nearly degenerate regions. We present a correct integer-only procedure that works for all connected digital pictures. It finds all possible interior points, which are then displayed and stored in a locating matrix. Namely, we present a filling and locating procedure that can be used in computer graphics and image processing applications.\n    ",
        "submission_date": "2014-01-14T00:00:00",
        "last_modified_date": "2014-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3615",
        "title": "Performance Engineering for a Medical Imaging Application on the Intel Xeon Phi Accelerator",
        "authors": [
            "Johannes Hofmann",
            "Jan Treibig",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "abstract": "We examine the Xeon Phi, which is based on Intel's Many Integrated Cores architecture, for its suitability to run the FDK algorithm--the most commonly used algorithm to perform the 3D image reconstruction in cone-beam computed tomography. We study the challenges of efficiently parallelizing the application and means to enable sensible data sharing between threads despite the lack of a shared last level cache. Apart from parallelization, SIMD vectorization is critical for good performance on the Xeon Phi; we perform various micro-benchmarks to investigate the platform's new set of vector instructions and put a special emphasis on the newly introduced vector gather capability. We refine a previous performance model for the application and adapt it for the Xeon Phi to validate the performance of our optimized hand-written assembly implementation, as well as the performance of several different auto-vectorization approaches.\n    ",
        "submission_date": "2013-12-17T00:00:00",
        "last_modified_date": "2013-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.3973",
        "title": "An Empirical Evaluation of Similarity Measures for Time Series Classification",
        "authors": [
            "Joan Serr\u00e0",
            "Josep Lluis Arcos"
        ],
        "abstract": "Time series are ubiquitous, and a measure to assess their similarity is a core part of many computational systems. In particular, the similarity measure is the most essential ingredient of time series clustering and classification systems. Because of this importance, countless approaches to estimate time series similarity have been proposed. However, there is a lack of comparative studies using empirical, rigorous, quantitative, and large-scale assessment strategies. In this article, we provide an extensive evaluation of similarity measures for time series classification following the aforementioned principles. We consider 7 different measures coming from alternative measure `families', and 45 publicly-available time series data sets coming from a wide variety of scientific domains. We focus on out-of-sample classification accuracy, but in-sample accuracies and parameter choices are also discussed. Our work is based on rigorous evaluation methodologies and includes the use of powerful statistical significance tests to derive meaningful conclusions. The obtained results show the equivalence, in terms of accuracy, of a number of measures, but with one single candidate outperforming the rest. Such findings, together with the followed methodology, invite researchers on the field to adopt a more consistent evaluation criteria and a more informed decision regarding the baseline measures to which new developments should be compared.\n    ",
        "submission_date": "2014-01-16T00:00:00",
        "last_modified_date": "2014-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.4612",
        "title": "Modelling Observation Correlations for Active Exploration and Robust Object Detection",
        "authors": [
            "Javier Velez",
            "Garrett Hemann",
            "Albert S. Huang",
            "Ingmar Posner",
            "Nicholas Roy"
        ],
        "abstract": "Today, mobile robots are expected to carry out increasingly complex tasks in multifarious, real-world environments. Often, the tasks require a certain semantic understanding of the workspace. Consider, for example, spoken instructions from a human collaborator referring to objects of interest; the robot must be able to accurately detect these objects to correctly understand the instructions. However, existing object detection, while competent, is not perfect. In particular, the performance of detection algorithms is commonly sensitive to the position of the sensor relative to the objects in the scene.  This paper presents an online planning algorithm which learns an explicit model of the spatial dependence of object detection and generates plans which maximize the expected performance of the detection, and by extension the overall plan performance. Crucially, the learned sensor model incorporates spatial correlations between measurements, capturing the fact that successive measurements taken at the same or nearby locations are not independent. We show how this sensor model can be incorporated into an efficient forward search algorithm in the information space of detected objects, allowing the robot to generate motion plans efficiently.  We investigate the performance of our approach by addressing the tasks of door and text detection in indoor environments and demonstrate significant improvement in detection performance during task execution over alternative methods in simulated and real robot experiments.\n    ",
        "submission_date": "2014-01-18T00:00:00",
        "last_modified_date": "2014-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5589",
        "title": "The Gabor-Einstein Wavelet: A Model for the Receptive Fields of V1 to MT Neurons",
        "authors": [
            "Stephen G. Odaibo"
        ],
        "abstract": "Our visual system is astonishingly efficient at detecting moving objects. This process is mediated by the neurons which connect the primary visual cortex (V1) to the middle temporal (MT) area. Interestingly, since Kuffler's pioneering experiments on retinal ganglion cells, mathematical models have been vital for advancing our understanding of the receptive fields of visual neurons. However, existing models were not designed to describe the most salient attributes of the highly specialized neurons in the V1 to MT motion processing stream; and they have not been able to do so. Here, we introduce the Gabor-Einstein wavelet, a new family of functions for representing the receptive fields of V1 to MT neurons. We show that the way space and time are mixed in the visual cortex is analogous to the way they are mixed in the special theory of relativity (STR). Hence we constrained the Gabor-Einstein model by requiring: (i) relativistic-invariance of the wave carrier, and (ii) the minimum possible number of parameters. From these two constraints, the sinc function emerged as a natural descriptor of the wave carrier. The particular distribution of lowpass to bandpass temporal frequency filtering properties of V1 to MT neurons (Foster et al 1985; DeAngelis et al 1993b; Hawken et al 1996) is clearly explained by the Gabor-Einstein basis. Furthermore, it does so in a manner innately representative of the motion-processing stream's neuronal hierarchy. Our analysis and computer simulations show that the distribution of temporal frequency filtering properties along the motion processing stream is a direct effect of the way the brain jointly encodes space and time. We uncovered this fundamental link by demonstrating that analogous mathematical structures underlie STR and joint cortical spacetime encoding. This link will provide new physiological insights into how the brain represents visual information.\n    ",
        "submission_date": "2014-01-22T00:00:00",
        "last_modified_date": "2014-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.5966",
        "title": "Image Block Loss Restoration Using Sparsity Pattern as Side Information",
        "authors": [
            "Hossein Hosseini",
            "Ali Goli",
            "Neda Barzegar Marvasti",
            "Masoume Azghani",
            "Farokh Marvasti"
        ],
        "abstract": "In this paper, we propose a method for image block loss restoration based on the notion of sparse representation. We use the sparsity pattern as side information to efficiently restore block losses by iteratively imposing the constraints of spatial and transform domains on the corrupted image. Two novel features, including a pre-interpolation and a criterion for stopping the iterations, are proposed to improve the performance. Also, to deal with practical applications, we develop a technique to transmit the side information along with the image. In this technique, we first compress the side information and then embed its LDPC coded version in the least significant bits of the image pixels. This technique ensures the error-free transmission of the side information, while causing only a small perturbation on the transmitted image. Mathematical analysis and extensive simulations are performed to validate the method and investigate the efficiency of the proposed techniques. The results verify that the proposed method outperforms its counterparts for image block loss restoration.\n    ",
        "submission_date": "2014-01-23T00:00:00",
        "last_modified_date": "2016-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6130",
        "title": "smart application for AMS using Face Recognition",
        "authors": [
            "MuthuKalyani.K",
            "VeeraMuthu.A"
        ],
        "abstract": "Attendance Management System (AMS) can be made into smarter way by using face recognition technique, where we use a CCTV camera to be fixed at the entry point of a classroom, which automatically captures the image of the person and checks the observed image with the face database using android enhanced smart phone. It is typically used for two purposes. Firstly, marking attendance for student by comparing the face images produced recently and secondly, recognition of human who are strange to the environment i.e. an unauthorized person For verification of image, a newly emerging trend 3D Face Recognition is used which claims to provide more accuracy in matching the image databases and has an ability to recognize a subject at different view angles.\n    ",
        "submission_date": "2013-11-13T00:00:00",
        "last_modified_date": "2013-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6497",
        "title": "Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination",
        "authors": [
            "Qibin Zhao",
            "Liqing Zhang",
            "Andrzej Cichocki"
        ],
        "abstract": "CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance.\n    ",
        "submission_date": "2014-01-25T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.6929",
        "title": "Computing support for advanced medical data analysis and imaging",
        "authors": [
            "W. Wi\u015blicki",
            "T. Bednarski",
            "P. Bia\u0142as",
            "E. Czerwi\u0144ski",
            "\u0141. Kap\u0142on",
            "A. Kochanowski",
            "G. Korcyl",
            "J. Kowal",
            "P. Kowalski",
            "T. Kozik",
            "W. Krzemie\u0144",
            "M. Molenda",
            "P. Moskal",
            "S. Nied\u017awiecki",
            "M. Pa\u0142ka",
            "M. Pawlik",
            "L. Raczy\u0144ski",
            "Z. Rudy",
            "P. Salabura",
            "N.G. Sharma",
            "M. Silarski",
            "A. S\u0142omski",
            "J. Smyrski",
            "A. Strzelecki",
            "A. Wieczorek",
            "M. Zieli\u0144ski",
            "N. Zo\u0144"
        ],
        "abstract": "We discuss computing issues for data analysis and image reconstruction of PET-TOF medical scanner or other medical scanning devices producing large volumes of data. Service architecture based on the grid and cloud concepts for distributed processing is proposed and critically discussed.\n    ",
        "submission_date": "2014-01-27T00:00:00",
        "last_modified_date": "2014-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7413",
        "title": "Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted Least Squares Minimization",
        "authors": [
            "Canyi Lu",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "abstract": "This work presents a general framework for solving the low rank and/or sparse matrix minimization problems, which may involve multiple non-smooth terms. The Iteratively Reweighted Least Squares (IRLS) method is a fast solver, which smooths the objective function and minimizes it by alternately updating the variables and their weights. However, the traditional IRLS can only solve a sparse only or low rank only minimization problem with squared loss or an affine constraint. This work generalizes IRLS to solve joint/mixed low rank and sparse minimization problems, which are essential formulations for many tasks. As a concrete example, we solve the Schatten-$p$ norm and $\\ell_{2,q}$-norm regularized Low-Rank Representation (LRR) problem by IRLS, and theoretically prove that the derived solution is a stationary point (globally optimal if $p,q\\geq1$). Our convergence proof of IRLS is more general than previous one which depends on the special properties of the Schatten-$p$ norm and $\\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets demonstrate that our IRLS is much more efficient.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.7623",
        "title": "Graph matching: relax or not?",
        "authors": [
            "Yonathan Aflalo",
            "Alex Bronstein",
            "Ron Kimmel"
        ],
        "abstract": "We consider the problem of exact and inexact matching of weighted undirected graphs, in which a bijective correspondence is sought to minimize a quadratic weight disagreement. This computationally challenging problem is often relaxed as a convex quadratic program, in which the space of permutations is replaced by the space of doubly-stochastic matrices. However, the applicability of such a relaxation is poorly understood. We define a broad class of friendly graphs characterized by an easily verifiable spectral property. We prove that for friendly graphs, the convex relaxation is guaranteed to find the exact isomorphism or certify its inexistence. This result is further extended to approximately isomorphic graphs, for which we develop an explicit bound on the amount of weight disagreement under which the relaxation is guaranteed to find the globally optimal approximate isomorphism. We also show that in many cases, the graph matching problem can be further harmlessly relaxed to a convex quadratic program with only n separable linear equality constraints, which is substantially more efficient than the standard relaxation involving 2n equality and n^2 inequality constraints. Finally, we show that our results are still valid for unfriendly graphs if additional information in the form of seeds or attributes is allowed, with the latter satisfying an easy to verify spectral characteristic.\n    ",
        "submission_date": "2014-01-29T00:00:00",
        "last_modified_date": "2014-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1401.8126",
        "title": "Extrinsic Methods for Coding and Dictionary Learning on Grassmann Manifolds",
        "authors": [
            "Mehrtash Harandi",
            "Richard Hartley",
            "Chunhua Shen",
            "Brian Lovell",
            "Conrad Sanderson"
        ],
        "abstract": "Sparsity-based representations have recently led to notable results in various visual recognition tasks. In a separate line of research, Riemannian manifolds have been shown useful for dealing with features and models that do not lie in Euclidean spaces. With the aim of building a bridge between the two realms, we address the problem of sparse coding and dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping. This in turn enables us to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we propose closed-form solutions for learning a Grassmann dictionary, atom by atom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann sparse coding and dictionary learning algorithms through embedding into Hilbert spaces.\n",
        "submission_date": "2014-01-31T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.0240",
        "title": "Graph Cuts with Interacting Edge Costs - Examples, Approximations, and Algorithms",
        "authors": [
            "Stefanie Jegelka",
            "Jeff Bilmes"
        ],
        "abstract": "We study an extension of the classical graph cut problem, wherein we replace the modular (sum of edge weights) cost function by a submodular set function defined over graph edges. Special cases of this problem have appeared in different applications in signal processing, machine learning, and computer vision. In this paper, we connect these applications via the generic formulation of \"cooperative graph cuts\", for which we study complexity, algorithms, and connections to polymatroidal network flows. Finally, we compare the proposed algorithms empirically.\n    ",
        "submission_date": "2014-02-02T00:00:00",
        "last_modified_date": "2016-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1473",
        "title": "Near-Optimal Joint Object Matching via Convex Relaxation",
        "authors": [
            "Yuxin Chen",
            "Leonidas J. Guibas",
            "Qi-Xing Huang"
        ],
        "abstract": "Joint matching over a collection of objects aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve maps between pairs of them. Given multiple matches computed between a few object pairs in isolation, the goal is to recover an entire collection of maps that are (1) globally consistent, and (2) close to the provided maps --- and under certain conditions provably the ground-truth maps. Despite recent advances on this problem, the best-known recovery guarantees are limited to a small constant barrier --- none of the existing methods find theoretical support when more than $50\\%$ of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding to match instances that are only partially similar to each other.\n",
        "submission_date": "2014-02-06T00:00:00",
        "last_modified_date": "2014-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1783",
        "title": "Active Clustering with Model-Based Uncertainty Reduction",
        "authors": [
            "Caiming Xiong",
            "David Johnson",
            "Jason J. Corso"
        ],
        "abstract": "Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are \\emph{passive} in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an \\emph{active} clustering method---i.e. an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.\n    ",
        "submission_date": "2014-02-07T00:00:00",
        "last_modified_date": "2014-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1801",
        "title": "Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP Modeling",
        "authors": [
            "SayedMasoud Hashemi",
            "Soosan Beheshti",
            "Patrick R. Gill",
            "Narinder S. Paul",
            "Richard S.C. Cobbold"
        ],
        "abstract": "Ultra low radiation dose in X-ray Computed Tomography (CT) is an important clinical objective in order to minimize the risk of carcinogenesis. Compressed Sensing (CS) enables significant reductions in radiation dose to be achieved by producing diagnostic images from a limited number of CT projections. However, the excessive computation time that conventional CS-based CT reconstruction typically requires has limited clinical implementation. In this paper, we first demonstrate that a thorough analysis of CT reconstruction through a Maximum a Posteriori objective function results in a weighted compressive sensing problem. This analysis enables us to formulate a low dose fan beam and helical cone beam CT reconstruction. Subsequently, we provide an efficient solution to the formulated CS problem based on a Fast Composite Splitting Algorithm-Latent Expected Maximization (FCSA-LEM) algorithm. In the proposed method we use pseudo polar Fourier transform as the measurement matrix in order to decrease the computational complexity; and rebinning of the projections to parallel rays in order to extend its application to fan beam and helical cone beam scans. The weight involved in the proposed weighted CS model, denoted by Error Adaptation Weight (EAW), is calculated based on the statistical characteristics of CT reconstruction and is a function of Poisson measurement noise and rebinning interpolation error. Simulation results show that low computational complexity of the proposed method made the fast recovery of the CT images possible and using EAW reduces the reconstruction error by one order of magnitude. Recovery of a high quality 512$\\times$ 512 image was achieved in less than 20 sec on a desktop computer without numerical optimizations.\n    ",
        "submission_date": "2014-02-08T00:00:00",
        "last_modified_date": "2014-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1921",
        "title": "A Hybrid Loss for Multiclass and Structured Prediction",
        "authors": [
            "Qinfeng Shi",
            "Mark Reid",
            "Tiberio Caetano",
            "Anton van den Hengel",
            "Zhenhua Wang"
        ],
        "abstract": "We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels--specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as--and often better than--both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.1947",
        "title": "Classification Tree Diagrams in Health Informatics Applications",
        "authors": [
            "Farrukh Arslan"
        ],
        "abstract": "Health informatics deal with the methods used to optimize the acquisition, storage and retrieval of medical data, and classify information in healthcare applications. Healthcare analysts are particularly interested in various computer informatics areas such as; knowledge representation from data, anomaly detection, outbreak detection methods and syndromic surveillance applications. Although various parametric and non-parametric approaches are being proposed to classify information from data, classification tree diagrams provide an interactive visualization to analysts as compared to other methods. In this work we discuss application of classification tree diagrams to classify information from medical data in healthcare applications.\n    ",
        "submission_date": "2014-02-09T00:00:00",
        "last_modified_date": "2014-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2088",
        "title": "Signal Reconstruction Framework Based On Projections Onto Epigraph Set Of A Convex Cost Function (PESC)",
        "authors": [
            "Mohammad Tofighi",
            "Kivanc Kose",
            "A. Enis Cetin"
        ],
        "abstract": "A new signal processing framework based on making orthogonal Projections onto the Epigraph Set of a Convex cost function (PESC) is developed. In this way it is possible to solve convex optimization problems using the well-known Projections onto Convex Set (POCS) approach. In this algorithm, the dimension of the minimization problem is lifted by one and a convex set corresponding to the epigraph of the cost function is defined. If the cost function is a convex function in $R^N$, the corresponding epigraph set is also a convex set in R^{N+1}. The PESC method provides globally optimal solutions for total-variation (TV), filtered variation (FV), L_1, L_2, and entropic cost function based convex optimization problems. In this article, the PESC based denoising and compressive sensing algorithms are developed. Simulation examples are presented.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2232",
        "title": "Image Search Reranking",
        "authors": [
            "V Rajakumar",
            "Vipeen V Bopche"
        ],
        "abstract": "The existing methods for image search reranking suffer from the unfaithfulness of the assumptions under which the text-based images search result. The resulting images contain more irrelevant images. Hence the re ranking concept arises to re rank the retrieved images based on the text around the image and data of data of image and visual feature of image. A number of methods are differentiated for this re-ranking. The high ranked images are used as noisy data and a k means algorithm for classification is learned to rectify the ranking further. We are study the affect ability of the cross validation method to this training data. The pre eminent originality of the overall method is in collecting text/metadata of image and visual features in order to achieve an automatic ranking of the images. Supervision is initiated to learn the model weights offline, previous to reranking process. While model learning needs manual labeling of the results for a some limited queries, the resulting model is query autonomous and therefore applicable to any other query .Examples are given for a selection of other classes like vehicles, animals and other classes.\n    ",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2333",
        "title": "Modeling sequential data using higher-order relational features and predictive training",
        "authors": [
            "Vincent Michalski",
            "Roland Memisevic",
            "Kishore Konda"
        ],
        "abstract": "Bi-linear feature learning models, like the gated autoencoder, were proposed as a way to model relationships between frames in a video. By minimizing reconstruction error of one frame, given the previous frame, these models learn \"mapping units\" that encode the transformations inherent in a sequence, and thereby learn to encode motion. In this work we extend bi-linear models by introducing \"higher-order mapping units\" that allow us to encode transformations between frames and transformations between transformations.\n",
        "submission_date": "2014-02-10T00:00:00",
        "last_modified_date": "2014-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2335",
        "title": "Sparsity averaging for radio-interferometric imaging",
        "authors": [
            "Rafael E. Carrillo",
            "Jason D. McEwen",
            "Yves Wiaux"
        ],
        "abstract": "We propose a novel regularization method for compressive imaging in the context of the compressed sensing (CS) theory with coherent and redundant dictionaries. Natural images are often complicated and several types of structures can be present at once. It is well known that piecewise smooth images exhibit gradient sparsity, and that images with extended structures are better encapsulated in wavelet frames. Therefore, we here conjecture that promoting average sparsity or compressibility over multiple frames rather than single frames is an extremely powerful regularization prior.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.2363",
        "title": "Animation of 3D Human Model Using Markerless Motion Capture Applied To Sports",
        "authors": [
            "Ashish Shingade",
            "Archana Ghotkar"
        ],
        "abstract": "Markerless motion capture is an active research in 3D virtualization. In proposed work we presented a system for markerless motion capture for 3D human character animation, paper presents a survey on motion and skeleton tracking techniques which are developed or are under development. The paper proposed a method to transform the motion of a performer to a 3D human character (model), the 3D human character performs similar movements as that of a performer in real time. In the proposed work, human model data will be captured by Kinect camera, processed data will be applied on 3D human model for animation. 3D human model is created using open source software (MakeHuman). Anticipated dataset for sport activity is considered as input which can be applied to any HCI application.\n    ",
        "submission_date": "2014-02-11T00:00:00",
        "last_modified_date": "2014-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.3337",
        "title": "Zero-bias autoencoders and the benefits of co-adapting features",
        "authors": [
            "Kishore Konda",
            "Roland Memisevic",
            "David Krueger"
        ],
        "abstract": "Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.\n    ",
        "submission_date": "2014-02-13T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4053",
        "title": "The Algebraic Approach to Phase Retrieval and Explicit Inversion at the Identifiability Threshold",
        "authors": [
            "Franz J Kir\u00e1ly",
            "Martin Ehler"
        ],
        "abstract": "We study phase retrieval from magnitude measurements of an unknown signal as an algebraic estimation problem. Indeed, phase retrieval from rank-one and more general linear measurements can be treated in an algebraic way. It is verified that a certain number of generic rank-one or generic linear measurements are sufficient to enable signal reconstruction for generic signals, and slightly more generic measurements yield reconstructability for all signals. Our results solve a few open problems stated in the recent literature. Furthermore, we show how the algebraic estimation problem can be solved by a closed-form algebraic estimation technique, termed ideal regression, providing non-asymptotic success guarantees.\n    ",
        "submission_date": "2014-02-17T00:00:00",
        "last_modified_date": "2014-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.4888",
        "title": "Survey on Sparse Coded Features for Content Based Face Image Retrieval",
        "authors": [
            "D. Johnvictor",
            "G. Selvavinayagam"
        ],
        "abstract": "Content based image retrieval, a technique which uses visual contents of image to search images from large scale image databases according to users' interests. This paper provides a comprehensive survey on recent technology used in the area of content based face image retrieval. Nowadays digital devices and photo sharing sites are getting more popularity, large human face photos are available in database. Multiple types of facial features are used to represent discriminality on large scale human facial image database. Searching and mining of facial images are challenging problems and important research issues. Sparse representation on features provides significant improvement in indexing related images to query image.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5047",
        "title": "Real-time Automatic Emotion Recognition from Body Gestures",
        "authors": [
            "Stefano Piana",
            "Alessandra Staglian\u00f2",
            "Francesca Odone",
            "Alessandro Verri",
            "Antonio Camurri"
        ],
        "abstract": "Although psychological research indicates that bodily expressions convey important affective information, to date research in emotion recognition focused mainly on facial expression or voice analysis. In this paper we propose an approach to realtime automatic emotion recognition from body movements. A set of postural, kinematic, and geometrical features are extracted from sequences 3D skeletons and fed to a multi-class SVM classifier. The proposed method has been assessed on data acquired through two different systems: a professionalgrade optical motion capture system, and Microsoft Kinect. The system has been assessed on a \"six emotions\" recognition problem, and using a leave-one-subject-out cross validation strategy, reached an overall recognition rate of 61.3% which is very close to the recognition rate of 61.9% obtained by human observers. To provide further testing of the system, two games were developed, where one or two users have to interact to understand and express emotions with their body.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5077",
        "title": "Group-sparse Matrix Recovery",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "We apply the OSCAR (octagonal selection and clustering algorithms for regression) in recovering group-sparse matrices (two-dimensional---2D---arrays) from compressive measurements. We propose a 2D version of OSCAR (2OSCAR) consisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which is convex but non-differentiable. We show that the proximity operator of 2OSCAR can be computed based on that of OSCAR. The 2OSCAR problem can thus be efficiently solved by state-of-the-art proximal splitting algorithms. Experiments on group-sparse 2D array recovery show that 2OSCAR regularization solved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm (with debiasing) yields the most accurate results.\n    ",
        "submission_date": "2014-02-20T00:00:00",
        "last_modified_date": "2014-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5497",
        "title": "Efficient Semidefinite Spectral Clustering via Lagrange Duality",
        "authors": [
            "Yan Yan",
            "Chunhua Shen",
            "Hanzi Wang"
        ],
        "abstract": "We propose an efficient approach to semidefinite spectral clustering (SSC), which addresses the Frobenius normalization with the positive semidefinite (p.s.d.) constraint for spectral clustering. Compared with the original Frobenius norm approximation based algorithm, the proposed algorithm can more accurately find the closest doubly stochastic approximation to the affinity matrix by considering the p.s.d. constraint. In this paper, SSC is formulated as a semidefinite programming (SDP) problem. In order to solve the high computational complexity of SDP, we present a dual algorithm based on the Lagrange dual formalization. Two versions of the proposed algorithm are proffered: one with less memory usage and the other with faster convergence rate. The proposed algorithm has much lower time complexity than that of the standard interior-point based SDP solvers. Experimental results on both UCI data sets and real-world image data sets demonstrate that 1) compared with the state-of-the-art spectral clustering methods, the proposed algorithm achieves better clustering performance; and 2) our algorithm is much more efficient and can solve larger-scale SSC problems than those standard interior-point SDP solvers.\n    ",
        "submission_date": "2014-02-22T00:00:00",
        "last_modified_date": "2014-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5684",
        "title": "Discriminative Functional Connectivity Measures for Brain Decoding",
        "authors": [
            "Orhan Firat",
            "Mete Ozay",
            "Ilke Oztekin",
            "Fatos T. Yarman Vural"
        ],
        "abstract": "We propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning method, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using a functional neighbourhood concept. In order to define the functional neighbourhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighbouring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Functional Connectivity aware Local Relational Features (FC-LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn) and Support Vector Machine (SVM), are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The classification performance of the Functional Mesh Learning model, which range in 62%-71% is superior to the classical multi-voxel pattern analysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n    ",
        "submission_date": "2014-02-23T00:00:00",
        "last_modified_date": "2014-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5766",
        "title": "No more meta-parameter tuning in unsupervised sparse feature learning",
        "authors": [
            "Adriana Romero",
            "Petia Radeva",
            "Carlo Gatta"
        ],
        "abstract": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.\n    ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2014-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.5979",
        "title": "A Multiplierless Pruned DCT-like Transformation for Image and Video Compression that Requires 10 Additions Only",
        "authors": [
            "V. A. Coutinho",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Madanayake"
        ],
        "abstract": "A multiplierless pruned approximate 8-point discrete cosine transform (DCT) requiring only 10 additions is introduced. The proposed algorithm was assessed in image and video compression, showing competitive performance with state-of-the-art methods. Digital implementation in 45 nm CMOS technology up to place-and-route level indicates clock speed of 288 MHz at a 1.1 V supply. The 8x8 block rate is 36 ",
        "submission_date": "2014-02-24T00:00:00",
        "last_modified_date": "2016-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1402.6034",
        "title": "A DCT Approximation for Image Compression",
        "authors": [
            "R. J. Cintra",
            "F. M. Bayer"
        ],
        "abstract": "An orthogonal approximation for the 8-point discrete cosine transform (DCT) is introduced. The proposed transformation matrix contains only zeros and ones; multiplications and bit-shift operations are absent. Close spectral behavior relative to the DCT was adopted as design criterion. The proposed algorithm is superior to the signed discrete cosine transform. It could also outperform state-of-the-art algorithms in low and high image compression scenarios, exhibiting at the same time a comparable computational complexity.\n    ",
        "submission_date": "2014-02-25T00:00:00",
        "last_modified_date": "2014-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1343",
        "title": "Ubic: Bridging the gap between digital cryptography and the physical world",
        "authors": [
            "Mark Simkin",
            "Dominique Schroeder",
            "Andreas Bulling",
            "Mario Fritz"
        ],
        "abstract": "Advances in computing technology increasingly blur the boundary between the digital domain and the physical world. Although the research community has developed a large number of cryptographic primitives and has demonstrated their usability in all-digital communication, many of them have not yet made their way into the real world due to usability aspects. We aim to make another step towards a tighter integration of digital cryptography into real world interactions. We describe Ubic, a framework that allows users to bridge the gap between digital cryptography and the physical world. Ubic relies on head-mounted displays, like Google Glass, resource-friendly computer vision techniques as well as mathematically sound cryptographic primitives to provide users with better security and privacy guarantees. The framework covers key cryptographic primitives, such as secure identification, document verification using a novel secure physical document format, as well as content hiding. To make a contribution of practical value, we focused on making Ubic as simple, easily deployable, and user friendly as possible.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1430",
        "title": "Sparse Principal Component Analysis via Rotation and Truncation",
        "authors": [
            "Zhenfang Hu",
            "Gang Pan",
            "Yueming Wang",
            "Zhaohui Wu"
        ],
        "abstract": "Sparse principal component analysis (sparse PCA) aims at finding a sparse basis to improve the interpretability over the dense basis of PCA, meanwhile the sparse basis should cover the data subspace as much as possible. In contrast to most of existing work which deal with the problem by adding some sparsity penalties on various objectives of PCA, in this paper, we propose a new method SPCArt, whose motivation is to find a rotation matrix and a sparse basis such that the sparse basis approximates the basis of PCA after the rotation. The algorithm of SPCArt consists of three alternating steps: rotate PCA basis, truncate small entries, and update the rotation matrix. Its performance bounds are also given. SPCArt is efficient, with each iteration scaling linearly with the data dimension. It is easy to choose parameters in SPCArt, due to its explicit physical explanations. Besides, we give a unified view to several existing sparse PCA methods and discuss the connection with SPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA algorithm, to overcome its drawback. Experimental results demonstrate that SPCArt achieves the state-of-the-art performance. It also achieves a good tradeoff among various criteria, including sparsity, explained variance, orthogonality, balance of sparsity among loadings, and computational speed.\n    ",
        "submission_date": "2014-03-06T00:00:00",
        "last_modified_date": "2014-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1697",
        "title": "Compressive Hyperspectral Imaging Using Progressive Total Variation",
        "authors": [
            "Simeon Kamdem Kuiteing",
            "Giulio Coluccia",
            "Alessandro Barducci",
            "Mauro Barni",
            "Enrico Magli"
        ],
        "abstract": "Compressed Sensing (CS) is suitable for remote acquisition of hyperspectral images for earth observation, since it could exploit the strong spatial and spectral correlations, llowing to simplify the architecture of the onboard sensors. Solutions proposed so far tend to decouple spatial and spectral dimensions to reduce the complexity of the reconstruction, not taking into account that onboard sensors progressively acquire spectral rows rather than acquiring spectral channels. For this reason, we propose a novel progressive CS architecture based on separate sensing of spectral rows and joint reconstruction employing Total Variation. Experimental results run on raw AVIRIS and AIRS images confirm the validity of the proposed system.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1735",
        "title": "Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation",
        "authors": [
            "Ahmed.H.Asad",
            "Ahmad Taher Azar",
            "Nashwa El-Bendary",
            "Aboul Ella Hassaanien"
        ],
        "abstract": "Features selection is an essential step for successful data classification, since it reduces the data dimensionality by removing redundant features. Consequently, that minimizes the classification complexity and time in addition to maximizing its accuracy. In this article, a comparative study considering six features selection heuristics is conducted in order to select the best relevant features subset. The tested features vector consists of fourteen features that are computed for each pixel in the field of view of retinal images in the DRIVE database. The comparison is assessed in terms of sensitivity, specificity, and accuracy measurements of the recommended features subset resulted by each heuristic when applied with the ant colony system. Experimental results indicated that the features subset recommended by the relief heuristic outperformed the subsets recommended by the other experienced heuristics.\n    ",
        "submission_date": "2014-03-07T00:00:00",
        "last_modified_date": "2014-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1937",
        "title": "A fast eikonal equation solver using the Schrodinger wave equation",
        "authors": [
            "Karthik S. Gurumoorthy",
            "Adrian M. Peter",
            "Birmingham Hang Guan",
            "Anand Rangarajan"
        ],
        "abstract": "We use a Schr\u00f6dinger wave equation formalism to solve the eikonal equation. In our framework, a solution to the eikonal equation is obtained in the limit as Planck's constant $\\hbar$ (treated as a free parameter) tends to zero of the solution to the corresponding linear Schr\u00f6dinger equation. The Schr\u00f6dinger equation corresponding to the eikonal turns out to be a \\emph{generalized, screened Poisson equation}. Despite being linear, it does not have a closed-form solution for arbitrary forcing functions. We present two different techniques to solve the screened Poisson equation. In the first approach we use a standard perturbation analysis approach to derive a new algorithm which is guaranteed to converge provided the forcing function is bounded and positive. The perturbation technique requires a sequence of discrete convolutions which can be performed in $O(N\\log N)$ using the Fast Fourier Transform (FFT) where $N$ is the number of grid points. In the second method we discretize the linear Laplacian operator by the finite difference method leading to a sparse linear system of equations which can be solved using the plethora of sparse solvers. The eikonal solution is recovered from the exponent of the resultant scalar field. Our approach eliminates the need to explicitly construct viscosity solutions as customary with direct solutions to the eikonal. Since the linear equation is computed for a small but non-zero $\\hbar$, the obtained solution is an approximation. Though our solution framework is applicable to the general class of eikonal problems, we detail specifics for the popular vision applications of shape-from-shading, vessel segmentation, and path planning.\n    ",
        "submission_date": "2014-03-08T00:00:00",
        "last_modified_date": "2015-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.1944",
        "title": "Multi-label ensemble based on variable pairwise constraint projection",
        "authors": [
            "Ping Li",
            "Hong Li",
            "Min Wu"
        ],
        "abstract": "Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches.\n    ",
        "submission_date": "2014-03-08T00:00:00",
        "last_modified_date": "2014-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2295",
        "title": "Sublinear Models for Graphs",
        "authors": [
            "Brijnesh J. Jain"
        ],
        "abstract": "This contribution extends linear models for feature vectors to sublinear models for graphs and analyzes their properties. The results are (i) a geometric interpretation of sublinear classifiers, (ii) a generic learning rule based on the principle of empirical risk minimization, (iii) a convergence theorem for the margin perceptron in the sublinearly separable case, and (iv) the VC-dimension of sublinear functions. Empirical results on graph data show that sublinear models on graphs have similar properties as linear models for feature vectors.\n    ",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.2395",
        "title": "A-infinity Persistence",
        "authors": [
            "Francisco Belch\u00ed Guillam\u00f3n",
            "Aniceto Murillo Mas"
        ],
        "abstract": "We introduce and study A-infinity persistence of a given homology filtration of topological spaces. This is a family, one for each n > 0, of homological invariants which provide information not readily available by the (persistent) Betti numbers of the given filtration. This may help to detect noise, not just in the simplicial structure of the filtration but in further geometrical properties in which the higher codiagonals of the A-infinity structure are translated. Based in the classification of zigzag modules, a characterization of the A-infinity persistence in terms of its associated barcode is given.\n    ",
        "submission_date": "2014-03-10T00:00:00",
        "last_modified_date": "2014-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3057",
        "title": "Evaluation of Image Segmentation and Filtering With ANN in the Papaya Leaf",
        "authors": [
            "Maicon A. Sartin",
            "Alexandre C. R. da Silva"
        ],
        "abstract": "Precision agriculture is area with lack of cheap technology. The refinement of the production system brings large advantages to the producer and the use of images makes the monitoring a more cheap methodology. Macronutrients monitoring can to determine the health and vulnerability of the plant in specific stages. In this paper is analyzed the method based on computational intelligence to work with image segmentation in the identification of symptoms of plant nutrient deficiency. Artificial neural networks are evaluated for image segmentation and filtering, several variations of parameters and insertion impulsive noise were evaluated too. Satisfactory results are achieved with artificial neural for segmentation same with high noise levels.\n    ",
        "submission_date": "2014-03-12T00:00:00",
        "last_modified_date": "2014-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3320",
        "title": "Numerical Approaches for Linear Left-invariant Diffusions on SE(2), their Comparison to Exact Solutions, and their Applications in Retinal Imaging",
        "authors": [
            "Jiong Zhang",
            "Remco Duits",
            "Gonzalo Sanguinetti",
            "Bart M. ter Haar Romeny"
        ],
        "abstract": "Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (and their resolvent equations) have been widely studied in the fields of cortical modeling and image analysis. They include hypo-elliptic diffusion (for contour enhancement) proposed by Citti & Sarti, and Petitot, and they include the direction process (for contour completion) proposed by Mumford. This paper presents a thorough study and comparison of the many numerical approaches, which, remarkably, is missing in the literature. Existing numerical approaches can be classified into 3 categories: Finite difference methods, Fourier based methods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (Monte Carlo simulations). There are also 3 types of exact solutions to the PDE-evolutions that were derived explicitly (in the spatial Fourier domain) in previous works by Duits and van Almsick in 2005. Here we provide an overview of these 3 types of exact solutions and explain how they relate to each of the 3 numerical approaches. We compute relative errors of all numerical approaches to the exact solutions, and the Fourier based methods show us the best performance with smallest relative errors. We also provide an improvement of Mathematica algorithms for evaluating Mathieu-functions, crucial in implementations of the exact solutions. Furthermore, we include an asymptotical analysis of the singularities within the kernels and we propose a probabilistic extension of underlying stochastic processes that overcomes the singular behavior in the origin of time-integrated kernels. Finally, we show retinal imaging applications of combining left-invariant PDE-evolutions with invertible orientation scores.\n    ",
        "submission_date": "2014-03-13T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.3780",
        "title": "Automatic Classification of Human Epithelial Type 2 Cell Indirect Immunofluorescence Images using Cell Pyramid Matching",
        "authors": [
            "Arnold Wiliem",
            "Conrad Sanderson",
            "Yongkang Wong",
            "Peter Hobson",
            "Rodney F. Minchin",
            "Brian C. Lovell"
        ],
        "abstract": "This paper describes a novel system for automatic classification of images obtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial type 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The IIF protocol on HEp-2 cells has been the hallmark method to identify the presence of ANAs, due to its high sensitivity and the large range of antigens that can be detected. However, it suffers from numerous shortcomings, such as being subjective as well as time and labour intensive. Computer Aided Diagnostic (CAD) systems have been developed to address these problems, which automatically classify a HEp-2 cell image into one of its known patterns (eg. speckled, homogeneous). Most of the existing CAD systems use handpicked features to represent a HEp-2 cell image, which may only work in limited scenarios. We propose a novel automatic cell image classification method termed Cell Pyramid Matching (CPM), which is comprised of regional histograms of visual words coupled with the Multiple Kernel Learning framework. We present a study of several variations of generating histograms and show the efficacy of the system on two publicly available datasets: the ICPR HEp-2 cell classification contest dataset and the SNPHEp-2 dataset.\n    ",
        "submission_date": "2014-03-15T00:00:00",
        "last_modified_date": "2014-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.4238",
        "title": "Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing",
        "authors": [
            "Guohui Wang",
            "Yingen Xiong",
            "Jay Yun",
            "Joseph R. Cavallaro"
        ],
        "abstract": "In this paper, we present an OpenCL-based heterogeneous implementation of a computer vision algorithm -- image inpainting-based object removal algorithm -- on mobile devices. To take advantage of the computation power of the mobile processor, the algorithm workflow is partitioned between the CPU and the GPU based on the profiling results on mobile devices, so that the computationally-intensive kernels are accelerated by the mobile GPGPU (general-purpose computing using graphics processing units). By exploring the implementation trade-offs and utilizing the proposed optimization strategies at different levels including algorithm optimization, parallelism optimization, and memory access optimization, we significantly speed up the algorithm with the CPU-GPU heterogeneous implementation, while preserving the quality of the output images. Experimental results show that heterogeneous computing based on GPGPU co-processing can significantly speed up the computer vision algorithms and makes them practical on real-world mobile devices.\n    ",
        "submission_date": "2014-03-17T00:00:00",
        "last_modified_date": "2014-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5370",
        "title": "Using n-grams models for visual semantic place recognition",
        "authors": [
            "Mathieu Dubois",
            "Frenoux Emmanuelle",
            "Philippe Tarroux"
        ],
        "abstract": "The aim of this paper is to present a new method for visual place recognition. Our system combines global image characterization and visual words, which allows to use efficient Bayesian filtering methods to integrate several images. More precisely, we extend the classical HMM model with techniques inspired by the field of Natural Language Processing. This paper presents our system and the Bayesian filtering algorithm. The performance of our system and the influence of the main parameters are evaluated on a standard database. The discussion highlights the interest of using such models and proposes improvements.\n    ",
        "submission_date": "2014-03-21T00:00:00",
        "last_modified_date": "2014-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.5912",
        "title": "The state of play of ASC-Inclusion: An Integrated Internet-Based Environment for Social Inclusion of Children with Autism Spectrum Conditions",
        "authors": [
            "Bj\u00f6rn Schuller",
            "Erik Marchi",
            "Simon Baron-Cohen",
            "Helen O'Reilly",
            "Delia Pigat",
            "Peter Robinson",
            "Ian Daves"
        ],
        "abstract": "Individuals with Autism Spectrum Conditions (ASC) have marked difficulties using verbal and non-verbal communication for social interaction. The running ASC-Inclusion project aims to help children with ASC by allowing them to learn how emotions can be expressed and recognised via playing games in a virtual world. The platform includes analysis of users' gestures, facial, and vocal expressions using standard microphone and web-cam or a depth sensor, training through games, text communication with peers, animation, video and audio clips. We present the state of play in realising such a serious game platform and provide results for the different modalities.\n    ",
        "submission_date": "2014-03-24T00:00:00",
        "last_modified_date": "2014-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6566",
        "title": "Image Retargeting by Content-Aware Synthesis",
        "authors": [
            "Weiming Dong",
            "Fuzhang Wu",
            "Yan Kong",
            "Xing Mei",
            "Tong-Yee Lee",
            "Xiaopeng Zhang"
        ],
        "abstract": "Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on content-aware synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural & non-textural regions with different strategy since they have different natures. We propose to retarget the textural regions by content-aware synthesis and non-textural regions by fast multi-operators. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image targeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6614",
        "title": "QCMC: Quasi-conformal Parameterizations for Multiply-connected domains",
        "authors": [
            "Kin Tat Ho",
            "Lok Ming Lui"
        ],
        "abstract": "This paper presents a method to compute the {\\it quasi-conformal parameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMC computes a quasi-conformal map from a multiply-connected domain $S$ onto a punctured disk $D_S$ associated with a given Beltrami differential. The Beltrami differential, which measures the conformality distortion, is a complex-valued function $\\mu:S\\to\\mathbb{C}$ with supremum norm strictly less than 1. Every Beltrami differential gives a conformal structure of $S$. Hence, the conformal module of $D_S$, which are the radii and centers of the inner circles, can be fully determined by $\\mu$, up to a M\u00f6bius transformation. In this paper, we propose an iterative algorithm to simultaneously search for the conformal module and the optimal quasi-conformal parameterization. The key idea is to minimize the Beltrami energy subject to the boundary constraints. The optimal solution is our desired quasi-conformal parameterization onto a punctured disk. The parameterization of the multiply-connected domain simplifies numerical computations and has important applications in various fields, such as in computer graphics and vision. Experiments have been carried out on synthetic data together with real multiply-connected Riemann surfaces. Results show that our proposed method can efficiently compute quasi-conformal parameterizations of multiply-connected domains and outperforms other state-of-the-art algorithms. Applications of the proposed parameterization technique have also been explored.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6706",
        "title": "Beyond L2-Loss Functions for Learning Sparse Models",
        "authors": [
            "Karthikeyan Natesan Ramamurthy",
            "Aleksandr Y. Aravkin",
            "Jayaraman J. Thiagarajan"
        ],
        "abstract": "Incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. Sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. Assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. In most scenarios, the reconstruction quality is measured using the squared Euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. However, new application domains motivate looking beyond conventional loss functions. For example, robust loss functions such as $\\ell_1$ and Huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. These new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. In particular, we consider the class of piecewise linear quadratic (PLQ) cost functions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge loss, and smoothed variants of these penalties. We propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth PLQ cost function. We provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. Furthermore, we present three case studies that require the use of PLQ cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.6794",
        "title": "KPCA Spatio-temporal trajectory point cloud classifier for recognizing human actions in a CBVR system",
        "authors": [
            "Iv\u00e1n G\u00f3mez-Conde",
            "David N. Olivieri"
        ],
        "abstract": "We describe a content based video retrieval (CBVR) software system for identifying specific locations of a human action within a full length film, and retrieving similar video shots from a query. For this, we introduce the concept of a trajectory point cloud for classifying unique actions, encoded in a spatio-temporal covariant eigenspace, where each point is characterized by its spatial location, local Frenet-Serret vector basis, time averaged curvature and torsion and the mean osculating hyperplane. Since each action can be distinguished by their unique trajectories within this space, the trajectory point cloud is used to define an adaptive distance metric for classifying queries against stored actions. Depending upon the distance to other trajectories, the distance metric uses either large scale structure of the trajectory point cloud, such as the mean distance between cloud centroids or the difference in hyperplane orientation, or small structure such as the time averaged curvature and torsion, to classify individual points in a fuzzy-KNN. Our system can function in real-time and has an accuracy greater than 93% for multiple action recognition within video repositories. We demonstrate the use of our CBVR system in two situations: by locating specific frame positions of trained actions in two full featured films, and video shot retrieval from a database with a web search application.\n    ",
        "submission_date": "2014-03-26T00:00:00",
        "last_modified_date": "2014-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7057",
        "title": "Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation",
        "authors": [
            "Alexander Kolesnikov",
            "Matthieu Guillaumin",
            "Vittorio Ferrari",
            "Christoph H. Lampert"
        ],
        "abstract": "We present LS-CRF, a new method for very efficient large-scale training of Conditional Random Fields (CRFs). It is inspired by existing closed-form expressions for the maximum likelihood parameters of a generative graphical model with tree topology. LS-CRF training requires only solving a set of independent regression problems, for which closed-form expression as well as efficient iterative solvers are available. This makes it orders of magnitude faster than conventional maximum likelihood learning for CRFs that require repeated runs of probabilistic inference. At the same time, the models learned by our method still allow for joint inference at test time. We apply LS-CRF to the task of semantic image segmentation, showing that it is highly efficient, even for loopy models where probabilistic inference is problematic. It allows the training of image segmentation models from significantly larger training sets than had been used previously. We demonstrate this on two new datasets that form a second contribution of this paper. They consist of over 180,000 images with figure-ground segmentation annotations. Our large-scale experiments show that the possibilities of CRF-based image segmentation are far from exhausted, indicating, for example, that semi-supervised learning and the use of non-linear predictors are promising directions for achieving higher segmentation accuracy in the future.\n    ",
        "submission_date": "2014-03-27T00:00:00",
        "last_modified_date": "2014-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7543",
        "title": "A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing",
        "authors": [
            "Dirk A. Lorenz",
            "Stephan Wenger",
            "Frank Sch\u00f6pfer",
            "Marcus Magnor"
        ],
        "abstract": "An algorithmic framework to compute sparse or minimal-TV solutions of linear systems is proposed. The framework includes both the Kaczmarz method and the linearized Bregman method as special cases and also several new methods such as a sparse Kaczmarz solver. The algorithmic framework has a variety of applications and is especially useful for problems in which the linear measurements are slow and expensive to obtain. We present examples for online compressed sensing, TV tomographic reconstruction and radio interferometry.\n    ",
        "submission_date": "2014-03-28T00:00:00",
        "last_modified_date": "2014-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7588",
        "title": "Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods",
        "authors": [
            "Cun Mu",
            "Yuqian Zhang",
            "John Wright",
            "Donald Goldfarb"
        ],
        "abstract": "Recovering matrices from compressive and grossly corrupted observations is a fundamental problem in robust statistics, with rich applications in computer vision and machine learning. In theory, under certain conditions, this problem can be solved in polynomial time via a natural convex relaxation, known as Compressive Principal Component Pursuit (CPCP). However, all existing provable algorithms for CPCP suffer from superlinear per-iteration cost, which severely limits their applicability to large scale problems. In this paper, we propose provable, scalable and efficient methods to solve CPCP with (essentially) linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to update the low-rank component with rank-one SVD and exploit the proximal step for the sparse term. Convergence results and implementation details are also discussed. We demonstrate the scalability of the proposed approach with promising numerical experiments on visual data.\n    ",
        "submission_date": "2014-03-29T00:00:00",
        "last_modified_date": "2017-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1403.7591",
        "title": "Building A Large Concept Bank for Representing Events in Video",
        "authors": [
            "Yin Cui",
            "Dong Liu",
            "Jiawei Chen",
            "Shih-Fu Chang"
        ],
        "abstract": "Concept-based video representation has proven to be effective in complex event detection. However, existing methods either manually design concepts or directly adopt concept libraries not specifically designed for events. In this paper, we propose to build Concept Bank, the largest concept library consisting of 4,876 concepts specifically designed to cover 631 real-world events. To construct the Concept Bank, we first gather a comprehensive event collection from WikiHow, a collaborative writing project that aims to build the world's largest manual for any possible How-To event. For each event, we then search Flickr and discover relevant concepts from the tags of the returned images. We train a Multiple Kernel Linear SVM for each discovered concept as a concept detector in Concept Bank. We organize the concepts into a five-layer tree structure, in which the higher-level nodes correspond to the event categories while the leaf nodes are the event-specific concepts discovered for each event. Based on such tree ontology, we develop a semantic matching method to select relevant concepts for each textual event query, and then apply the corresponding concept detectors to generate concept-based video representations. We use TRECVID Multimedia Event Detection 2013 and Columbia Consumer Video open source event definitions and videos as our test sets and show very promising results on two video event detection tasks: event modeling over concept space and zero-shot event retrieval. To the best of our knowledge, this is the largest concept library covering the largest number of real-world events.\n    ",
        "submission_date": "2014-03-29T00:00:00",
        "last_modified_date": "2014-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.0774",
        "title": "GPU Accelerated Fractal Image Compression for Medical Imaging in Parallel Computing Platform",
        "authors": [
            "Md. Enamul Haque",
            "Abdullah Al Kaisan",
            "Mahmudur R Saniat",
            "Aminur Rahman"
        ],
        "abstract": "In this paper, we implemented both sequential and parallel version of fractal image compression algorithms using CUDA (Compute Unified Device Architecture) programming model for parallelizing the program in Graphics Processing Unit for medical images, as they are highly similar within the image itself. There are several improvement in the implementation of the algorithm as well. Fractal image compression is based on the self similarity of an image, meaning an image having similarity in majority of the regions. We take this opportunity to implement the compression algorithm and monitor the effect of it using both parallel and sequential implementation. Fractal compression has the property of high compression rate and the dimensionless scheme. Compression scheme for fractal image is of two kind, one is encoding and another is decoding. Encoding is very much computational expensive. On the other hand decoding is less computational. The application of fractal compression to medical images would allow obtaining much higher compression ratios. While the fractal magnification an inseparable feature of the fractal compression would be very useful in presenting the reconstructed image in a highly readable form. However, like all irreversible methods, the fractal compression is connected with the problem of information loss, which is especially troublesome in the medical imaging. A very time consuming encoding pro- cess, which can last even several hours, is another bothersome drawback of the fractal compression.\n    ",
        "submission_date": "2014-04-03T00:00:00",
        "last_modified_date": "2014-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1514",
        "title": "Text Based Approach For Indexing And Retrieval Of Image And Video: A Review",
        "authors": [
            "Avinash N Bhute",
            "B.B. Meshram"
        ],
        "abstract": "Text data present in multimedia contain useful information for automatic annotation, indexing. Extracted information used for recognition of the overlay or scene text from a given video or image. The Extracted text can be used for retrieving the videos and images. In this paper, firstly, we are discussed the different techniques for text extraction from images and videos. Secondly, we are reviewed the techniques for indexing and retrieval of image and videos by using extracted text.\n    ",
        "submission_date": "2014-04-05T00:00:00",
        "last_modified_date": "2014-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.1664",
        "title": "Icon Based Information Retrieval and Disease Identification in Agriculture",
        "authors": [
            "Namita Mittal",
            "Basant Agarwal",
            "Ajay Gupta",
            "Hemant Madhur"
        ],
        "abstract": "Recent developments in the ICT industry in past few decades has enabled the quick and easy access to the information available on the internet. But, digital literacy is the pre-requisite for its use. The main purpose of this paper is to provide an interface for digitally illiterate users, especially farmers to efficiently and effectively retrieve information through Internet. In addition, to enable the farmers to identify the disease in their crop, its cause and symptoms using digital image processing and pattern recognition instantly without waiting for an expert to visit the farms and identify the disease.\n    ",
        "submission_date": "2014-04-07T00:00:00",
        "last_modified_date": "2014-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.2728",
        "title": "Real-time Decolorization using Dominant Colors",
        "authors": [
            "Wei Hu",
            "Wei Li",
            "Fan Zhang",
            "Qian Du"
        ],
        "abstract": "Decolorization is the process to convert a color image or video to its grayscale version, and it has received great attention in recent years. An ideal decolorization algorithm should preserve the original color contrast as much as possible. Meanwhile, it should provide the final decolorized result as fast as possible. However, most of the current methods are suffering from either unsatisfied color information preservation or high computational cost, limiting their application value. In this paper, a simple but effective technique is proposed for real-time decolorization. Based on the typical rgb2gray() color conversion model, which produces a grayscale image by linearly combining R, G, and B channels, we propose a dominant color hypothesis and a corresponding distance measurement metric to evaluate the quality of grayscale conversion. The local optimum scheme provides several \"good\" candidates in a confidence interval, from which the \"best\" result can be extracted. Experimental results demonstrate that remarkable simplicity of the proposed method facilitates the process of high resolution images and videos in real-time using a common CPU.\n    ",
        "submission_date": "2014-04-10T00:00:00",
        "last_modified_date": "2014-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.3290",
        "title": "Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and Analysis",
        "authors": [
            "Yehuda Dar",
            "Alfred M. Bruckstein"
        ],
        "abstract": "Block-based motion estimation (ME) and compensation (MC) techniques are widely used in modern video processing algorithms and compression systems. The great variety of video applications and devices results in numerous compression specifications. Specifically, there is a diversity of frame-rates and bit-rates. In this paper, we study the effect of frame-rate and compression bit-rate on block-based ME and MC as commonly utilized in inter-frame coding and frame-rate up conversion (FRUC). This joint examination yields a comprehensive foundation for comparing MC procedures in coding and FRUC. First, the video signal is modeled as a noisy translational motion of an image. Then, we theoretically model the motion-compensated prediction of an available and absent frames as in coding and FRUC applications, respectively. The theoretic MC-prediction error is further analyzed and its autocorrelation function is calculated for coding and FRUC applications. We show a linear relation between the variance of the MC-prediction error and temporal-distance. While the affecting distance in MC-coding is between the predicted and reference frames, MC-FRUC is affected by the distance between the available frames used for the interpolation. Moreover, the dependency in temporal-distance implies an inverse effect of the frame-rate. FRUC performance analysis considers the prediction error variance, since it equals to the mean-squared-error of the interpolation. However, MC-coding analysis requires the entire autocorrelation function of the error; hence, analytic simplicity is beneficial. Therefore, we propose two constructions of a separable autocorrelation function for prediction error in MC-coding. We conclude by comparing our estimations with experimental results.\n    ",
        "submission_date": "2014-04-12T00:00:00",
        "last_modified_date": "2014-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4104",
        "title": "Sparse Bilinear Logistic Regression",
        "authors": [
            "Jianing V. Shi",
            "Yangyang Xu",
            "Richard G. Baraniuk"
        ],
        "abstract": "In this paper, we introduce the concept of sparse bilinear logistic regression for decision problems involving explanatory variables that are two-dimensional matrices. Such problems are common in computer vision, brain-computer interfaces, style/content factorization, and parallel factor analysis. The underlying optimization problem is bi-convex; we study its solution and develop an efficient algorithm based on block coordinate descent. We provide a theoretical guarantee for global convergence and estimate the asymptotical convergence rate using the Kurdyka-\u0141ojasiewicz inequality. A range of experiments with simulated and real data demonstrate that sparse bilinear logistic regression outperforms current techniques in several important applications.\n    ",
        "submission_date": "2014-04-15T00:00:00",
        "last_modified_date": "2014-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.4412",
        "title": "Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness",
        "authors": [
            "Guoxu Zhou",
            "Andrzej Cichocki",
            "Qibin Zhao",
            "Shengli Xie"
        ],
        "abstract": "Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction of nonnegative parts-based and physically meaningful latent components from high-dimensional tensor data while preserving the natural multilinear structure of data. However, as the data tensor often has multiple modes and is large-scale, existing NTD algorithms suffer from a very high computational complexity in terms of both storage and computation time, which has been one major obstacle for practical applications of NTD. To overcome these disadvantages, we show how low (multilinear) rank approximation (LRA) of tensors is able to significantly simplify the computation of the gradients of the cost function, upon which a family of efficient first-order NTD algorithms are developed. Besides dramatically reducing the storage complexity and running time, the new algorithms are quite flexible and robust to noise because any well-established LRA approaches can be applied. We also show how nonnegativity incorporating sparsity substantially improves the uniqueness property and partially alleviates the curse of dimensionality of the Tucker decompositions. Simulation results on synthetic and real-world data justify the validity and high efficiency of the proposed NTD algorithms.\n    ",
        "submission_date": "2014-04-17T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6039",
        "title": "The fshape framework for the variability analysis of functional shapes",
        "authors": [
            "Benjamin Charlier",
            "Nicolas Charon",
            "Alain Trouv\u00e9"
        ],
        "abstract": "This article introduces a full mathematical and numerical framework for treating functional shapes (or fshapes) following the landmarks of shape spaces and shape analysis. Functional shapes can be described as signal functions supported on varying geometrical supports. Analysing variability of fshapes' ensembles require the modelling and quantification of joint variations in geometry and signal, which have been treated separately in previous approaches. Instead, building on the ideas of shape spaces for purely geometrical objects, we propose the extended concept of fshape bundles and define Riemannian metrics for fshape metamorphoses to model geometrico-functional transformations within these bundles. We also generalize previous works on data attachment terms based on the notion of varifolds and demonstrate the utility of these distances. Based on these, we propose variational formulations of the atlas estimation problem on populations of fshapes and prove existence of solutions for the different models. The second part of the article examines the numerical implementation of the models by detailing discrete expressions for the metrics and gradients and proposing an optimization scheme for the atlas estimation problem. We present a few results of the methodology on a synthetic dataset as well as on a population of retinal membranes with thickness maps.\n    ",
        "submission_date": "2014-04-24T00:00:00",
        "last_modified_date": "2014-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6535",
        "title": "Quadratization of Symmetric Pseudo-Boolean Functions",
        "authors": [
            "Martin Anthony",
            "Endre Boros",
            "Yves Crama",
            "Aritanan Gruber"
        ],
        "abstract": "A pseudo-Boolean function is a real-valued function $f(x)=f(x_1,x_2,\\ldots,x_n)$ of $n$ binary variables; that is, a mapping from $\\{0,1\\}^n$ to $\\mathbb{R}$. For a pseudo-Boolean function $f(x)$ on $\\{0,1\\}^n$, we say that $g(x,y)$ is a quadratization of $f$ if $g(x,y)$ is a quadratic polynomial depending on $x$ and on $m$ auxiliary binary variables $y_1,y_2,\\ldots,y_m$ such that $f(x)= \\min \\{g(x,y) : y \\in \\{0,1\\}^m \\}$ for all $x \\in \\{0,1\\}^n$. By means of quadratizations, minimization of $f$ is reduced to minimization (over its extended set of variables) of the quadratic function $g(x,y)$. This is of some practical interest because minimization of quadratic functions has been thoroughly studied for the last few decades, and much progress has been made in solving such problems exactly or heuristically. A related paper \\cite{ABCG} initiated a systematic study of the minimum number of auxiliary $y$-variables required in a quadratization of an arbitrary function $f$ (a natural question, since the complexity of minimizing the quadratic function $g(x,y)$ depends, among other factors, on the number of binary variables). In this paper, we determine more precisely the number of auxiliary variables required by quadratizations of symmetric pseudo-Boolean functions $f(x)$, those functions whose value depends only on the Hamming weight of the input $x$ (the number of variables equal to $1$).\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6538",
        "title": "On Quadratization of Pseudo-Boolean Functions",
        "authors": [
            "Endre Boros",
            "Aritanan Gruber"
        ],
        "abstract": "We survey current term-wise techniques for quadratizing high-degree pseudo-Boolean functions and introduce a new one, which allows multiple splits of terms. We also introduce the first aggregative approach, which splits a collection of terms based on their common parts.\n    ",
        "submission_date": "2014-04-25T00:00:00",
        "last_modified_date": "2014-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6691",
        "title": "Sinogram constrained TV-minimization for metal artifact reduction in CT",
        "authors": [
            "Clemens Schiffer",
            "Kristian Bredies"
        ],
        "abstract": "A new method for reducing metal artifacts in X-ray computed tomography (CT) images is presented. It bases on the solution of a convex optimization problem with inequality constraints on the sinogram, and total variation regularization for the reconstructed image. The Chambolle-Pock algorithm is used to numerically solve the discretized version of the optimization problem. As proof of concept we present and discuss numerical results for synthetic data.\n    ",
        "submission_date": "2014-04-26T00:00:00",
        "last_modified_date": "2014-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.6871",
        "title": "Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization",
        "authors": [
            "Canyi Lu",
            "Yunchao Wei",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "abstract": "This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for solving a general problem, which involves a large body of nonconvex sparse and structured sparse related problems. Comparing with previous iterative solvers for nonconvex sparse problem, PIRE is much more general and efficient. The computational cost of PIRE in each iteration is usually as low as the state-of-the-art convex solvers. We further propose the PIRE algorithm with Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating (PIRE-AU) to handle the multi-variable problems. In theory, we prove that our proposed methods converge and any limit solution is a stationary point. Extensive experiments on both synthesis and real data sets demonstrate that our methods achieve comparative learning performance, but are much more efficient, by comparing with previous nonconvex solvers.\n    ",
        "submission_date": "2014-04-28T00:00:00",
        "last_modified_date": "2014-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1404.7298",
        "title": "Code Minimization for Fringe Projection Based 3D Stereo Sensors by Calibration Improvement",
        "authors": [
            "Christian Br\u00e4uer-Burchardt",
            "Peter K\u00fchmstedt",
            "Gunther Notni"
        ],
        "abstract": "Code minimization provides a speed-up of the processing time of fringe projection based stereo sensors and possibly makes them real-time applicable. This paper reports a methodology which enables such sensors to completely omit Gray code or other additional code. Only a sequence of sinusoidal images is necessary. The code reduction is achieved by involvement of the projection unit into the measurement, double triangulation, and a precise projector calibration or significant projector calibration improvement, respectively.\n    ",
        "submission_date": "2014-04-29T00:00:00",
        "last_modified_date": "2014-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2102",
        "title": "Improving Image Clustering using Sparse Text and the Wisdom of the Crowds",
        "authors": [
            "Anna Ma",
            "Arjuna Flenner",
            "Deanna Needell",
            "Allon G. Percus"
        ],
        "abstract": "We propose a method to improve image clustering using sparse text and the wisdom of the crowds. In particular, we present a method to fuse two different kinds of document features, image and text features, and use a common dictionary or \"wisdom of the crowds\" as the connection between the two different kinds of documents. With the proposed fusion matrix, we use topic modeling via non-negative matrix factorization to cluster documents.\n    ",
        "submission_date": "2014-05-08T00:00:00",
        "last_modified_date": "2014-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.2220",
        "title": "Gaussian-Chain Filters for Heavy-Tailed Noise with Application to Detecting Big Buyers and Big Sellers in Stock Market",
        "authors": [
            "Li-Xin Wang"
        ],
        "abstract": "We propose a new heavy-tailed distribution --- Gaussian-Chain (GC) distribution, which is inspirited by the hierarchical structures prevailing in social organizations. We determine the mean, variance and kurtosis of the Gaussian-Chain distribution to show its heavy-tailed property, and compute the tail distribution table to give specific numbers showing how heavy is the heavy-tails. To filter out the heavy-tailed noise, we construct two filters --- 2nd and 3rd-order GC filters --- based on the maximum likelihood principle. Simulation results show that the GC filters perform much better than the benchmark least-squares algorithm when the noise is heavy-tail distributed. Using the GC filters, we propose a trading strategy, named Ride-the-Mood, to follow the mood of the market by detecting the actions of the big buyers and the big sellers in the market based on the noisy, heavy-tailed price data. Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocks over the recent two-year period from April 2, 2012 to March 31, 2014 shows that their returns are higher than the returns of the benchmark Buy-and-Hold strategy and the Hang Seng Index Fund.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.3173",
        "title": "Image Restoration Using Joint Statistical Modeling in Space-Transform Domain",
        "authors": [
            "Jian Zhang",
            "Debin Zhao",
            "Ruiqin Xiong",
            "Siwei Ma",
            "Wen Gao"
        ],
        "abstract": "This paper presents a novel strategy for high-fidelity image restoration by characterizing both local smoothness and nonlocal self-similarity of natural images in a unified statistical manner. The main contributions are three-folds. First, from the perspective of image statistics, a joint statistical modeling (JSM) in an adaptive hybrid space-transform domain is established, which offers a powerful mechanism of combining local smoothness and nonlocal self-similarity simultaneously to ensure a more reliable and robust estimation. Second, a new form of minimization functional for solving image inverse problem is formulated using JSM under regularization-based framework. Finally, in order to make JSM tractable and robust, a new Split-Bregman based algorithm is developed to efficiently solve the above severely underdetermined inverse problem associated with theoretical proof of convergence. Extensive experiments on image inpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise removal applications verify the effectiveness of the proposed algorithm.\n    ",
        "submission_date": "2014-05-11T00:00:00",
        "last_modified_date": "2014-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.4807",
        "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation",
        "authors": [
            "Qixing Huang",
            "Yuxin Chen",
            "Leonidas Guibas"
        ],
        "abstract": "Maximum a posteriori (MAP) inference over discrete Markov random fields is a fundamental task spanning a wide spectrum of real-world applications, which is known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of the new relaxation. Encouragingly, the proposed procedure allows solving SDR for large-scale problems, e.g., problems on a grid graph comprising hundreds of thousands of variables with multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability, in contrast to the commonly held belief that semidefinite relaxation can only been applied on small-scale MRF problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC in terms of both the quality of the solutions and computation time. Experimental results demonstrate that for a broad class of problems, SDPAD-LR outperforms state-of-the-art algorithms in producing better MAP assignment in an efficient manner.\n    ",
        "submission_date": "2014-05-19T00:00:00",
        "last_modified_date": "2014-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6662",
        "title": "Cognitive-mapping and contextual pyramid based Digital Elevation Model Registration and its effective storage using fractal based compression",
        "authors": [
            "Suma Dawn",
            "Vikas Saxena",
            "Bhudev Sharma"
        ],
        "abstract": "Digital Elevation models (DEM) are images having terrain information embedded into them. Using cognitive mapping concepts for DEM registration, has evolved from this basic idea of using the mapping between the space to objects and defining their relationships to form the basic landmarks that need to be marked, stored and manipulated in and about the environment or other candidate environments, namely, in our case, the DEMs. The progressive two-level encapsulation of methods of geo-spatial cognition includes landmark knowledge and layout knowledge and can be useful for DEM registration. Space-based approach, that emphasizes on explicit extent of the environment under consideration, and object-based approach, that emphasizes on the relationships between objects in the local environment being the two paradigms of cognitive mapping can be methodically integrated in this three-architecture for DEM registration. Initially, P-model based segmentation is performed followed by landmark formation for contextual mapping that uses contextual pyramid formation. Apart from landmarks being used for registration key-point finding, Euclidean distance based deformation calculation has been used for transformation and change detection. Landmarks have been categorized to belong to either being flat-plain areas without much variation in the land heights; peaks that can be found when there is gradual increase in height as compared to the flat areas; valleys, marked with gradual decrease in the height seen in DEM; and finally, ripple areas with very shallow crests and nadirs. Fractal based compression was used for storage of co-registered DEMs. This method may further be extended for DEM-topographic map and DEM-to-remote sensed image registration. Experimental results further cement the fact that DEM registration may be effectively done using the proposed method.\n    ",
        "submission_date": "2014-05-09T00:00:00",
        "last_modified_date": "2014-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.6922",
        "title": "Large Scale, Large Margin Classification using Indefinite Similarity Measures",
        "authors": [
            "Omid Aghazadeh",
            "Stefan Carlsson"
        ],
        "abstract": "Despite the success of the popular kernelized support vector machines, they have two major limitations: they are restricted to Positive Semi-Definite (PSD) kernels, and their training complexity scales at least quadratically with the size of the data. Many natural measures of similarity between pairs of samples are not PSD e.g. invariant kernels, and those that are implicitly or explicitly defined by latent variable models. In this paper, we investigate scalable approaches for using indefinite similarity measures in large margin frameworks. In particular we show that a normalization of similarity to a subset of the data points constitutes a representation suitable for linear classifiers. The result is a classifier which is competitive to kernelized SVM in terms of accuracy, despite having better training and test time complexities. Experimental results demonstrate that on CIFAR-10 dataset, the model equipped with similarity measures invariant to rigid and non-rigid deformations, can be made more than 5 times sparser while being more accurate than kernelized SVM using RBF kernels.\n    ",
        "submission_date": "2014-05-27T00:00:00",
        "last_modified_date": "2014-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1405.7102",
        "title": "Detection Bank: An Object Detection Based Video Representation for Multimedia Event Recognition",
        "authors": [
            "Tim Althoff",
            "Hyun Oh Song",
            "Trevor Darrell"
        ],
        "abstract": "While low-level image features have proven to be effective representations for visual recognition tasks such as object recognition and scene classification, they are inadequate to capture complex semantic meaning required to solve high-level visual tasks such as multimedia event detection and recognition. Recognition or retrieval of events and activities can be improved if specific discriminative objects are detected in a video sequence. In this paper, we propose an image representation, called Detection Bank, based on the detection images from a large number of windowed object detectors where an image is represented by different statistics derived from these detections. This representation is extended to video by aggregating the key frame level image representations through mean and max pooling. We empirically show that it captures complementary information to state-of-the-art representations such as Spatial Pyramid Matching and Object Bank. These descriptors combined with our Detection Bank representation significantly outperforms any of the representations alone on TRECVID MED 2011 data.\n    ",
        "submission_date": "2014-05-28T00:00:00",
        "last_modified_date": "2014-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.0281",
        "title": "On Classification with Bags, Groups and Sets",
        "authors": [
            "Veronika Cheplygina",
            "David M. J. Tax",
            "Marco Loog"
        ],
        "abstract": "Many classification problems can be difficult to formulate directly in terms of the traditional supervised setting, where both training and test samples are individual feature vectors. There are cases in which samples are better described by sets of feature vectors, that labels are only available for sets rather than individual samples, or, if individual labels are available, that these are not independent. To better deal with such problems, several extensions of supervised learning have been proposed, where either training and/or test objects are sets of feature vectors. However, having been proposed rather independently of each other, their mutual similarities and differences have hitherto not been mapped out. In this work, we provide an overview of such learning scenarios, propose a taxonomy to illustrate the relationships between them, and discuss directions for further research in these areas.\n    ",
        "submission_date": "2014-06-02T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1167",
        "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble",
        "authors": [
            "Xu-Cheng Yin",
            "Chun Yang",
            "Hong-Wei Hao"
        ],
        "abstract": "Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers' outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.\n    ",
        "submission_date": "2014-06-04T00:00:00",
        "last_modified_date": "2014-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.1265",
        "title": "Illusory Shapes via Phase Transition",
        "authors": [
            "Yoon Mo Jung",
            "Jianhong Jackie Shen"
        ],
        "abstract": "We propose a new variational illusory shape (VIS) model via phase fields and phase transitions. It is inspired by the first-order variational illusory contour (VIC) model proposed by Jung and Shen [{\\em J. Visual Comm. Image Repres.}, {\\bf 19}:42-55, 2008]. Under the new VIS model, illusory shapes are represented by phase values close to 1 while the rest by values close to 0. The 0-1 transition is achieved by an elliptic energy with a double-well potential, as in the theory of $\\Gamma$-convergence. The VIS model is non-convex, with the zero field as its trivial global optimum. To seek visually meaningful local optima that can induce illusory shapes, an iterative algorithm is designed and its convergence behavior is closely studied. Several generic numerical examples confirm the versatility of the model and the algorithm.\n    ",
        "submission_date": "2014-06-05T00:00:00",
        "last_modified_date": "2014-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2390",
        "title": "Unsupervised Deep Haar Scattering on Graphs",
        "authors": [
            "Xu Chen",
            "Xiuyuan Cheng",
            "St\u00e9phane Mallat"
        ],
        "abstract": "The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.\n    ",
        "submission_date": "2014-06-09T00:00:00",
        "last_modified_date": "2014-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2528",
        "title": "Denosing Using Wavelets and Projections onto the L1-Ball",
        "authors": [
            "A. Enis Cetin",
            "Mohammad Tofighi"
        ],
        "abstract": "Both wavelet denoising and denosing methods using the concept of sparsity are based on soft-thresholding. In sparsity based denoising methods, it is assumed that the original signal is sparse in some transform domains such as the wavelet domain and the wavelet subsignals of the noisy signal are projected onto L1-balls to reduce noise. In this lecture note, it is shown that the size of the L1-ball or equivalently the soft threshold value can be determined using linear algebra. The key step is an orthogonal projection onto the epigraph set of the L1-norm cost function.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2602",
        "title": "Graph Approximation and Clustering on a Budget",
        "authors": [
            "Ethan Fetaya",
            "Ohad Shamir",
            "Shimon Ullman"
        ],
        "abstract": "We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters). We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper.\n    ",
        "submission_date": "2014-06-10T00:00:00",
        "last_modified_date": "2014-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.2895",
        "title": "Acoustic Gait-based Person Identification using Hidden Markov Models",
        "authors": [
            "J\u00fcrgen T. Geiger",
            "Maximilian Knei\u00dfl",
            "Bj\u00f6rn Schuller",
            "Gerhard Rigoll"
        ],
        "abstract": "We present a system for identifying humans by their walking sounds. This problem is also known as acoustic gait recognition. The goal of the system is to analyse sounds emitted by walking persons (mostly the step sounds) and identify those persons. These sounds are characterised by the gait pattern and are influenced by the movements of the arms and legs, but also depend on the type of shoe. We extract cepstral features from the recorded audio signals and use hidden Markov models for dynamic classification. A cyclic model topology is employed to represent individual gait cycles. This topology allows to model and detect individual steps, leading to very promising identification rates. For experimental validation, we use the publicly available TUM GAID database, which is a large gait recognition database containing 3050 recordings of 305 subjects in three variations. In the best setup, an identification rate of 65.5 % is achieved out of 155 subjects. This is a relative improvement of almost 30 % compared to our previous work, which used various audio features and support vector machines.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3010",
        "title": "\"Mental Rotation\" by Optimizing Transforming Distance",
        "authors": [
            "Weiguang Ding",
            "Graham W. Taylor"
        ],
        "abstract": "The human visual system is able to recognize objects despite transformations that can drastically alter their appearance. To this end, much effort has been devoted to the invariance properties of recognition systems. Invariance can be engineered (e.g. convolutional nets), or learned from data explicitly (e.g. temporal coherence) or implicitly (e.g. by data augmentation). One idea that has not, to date, been explored is the integration of latent variables which permit a search over a learned space of transformations. Motivated by evidence that people mentally simulate transformations in space while comparing examples, so-called \"mental rotation\", we propose a transforming distance. Here, a trained relational model actively transforms pairs of examples so that they are maximally similar in some feature space yet respect the learned transformational constraints. We apply our method to nearest-neighbour problems on the Toronto Face Database and NORB.\n    ",
        "submission_date": "2014-06-11T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.3793",
        "title": "Neural tuning size is a key factor underlying holistic face processing",
        "authors": [
            "Cheston Tan",
            "Tomaso Poggio"
        ],
        "abstract": "Faces are a class of visual stimuli with unique significance, for a variety of reasons. They are ubiquitous throughout the course of a person's life, and face recognition is crucial for daily social interaction. Faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics. Furthermore, faces have been empirically found to elicit certain characteristic behavioral phenomena, which are widely held to be evidence of \"holistic\" processing of faces. However, little is known about the neural mechanisms underlying such holistic face processing. In other words, for the processing of faces by the primate visual system, the input and output characteristics are relatively well known, but the internal neural computations are not. The main aim of this work is to further the fundamental understanding of what causes the visual processing of faces to be different from that of objects. In this computational modeling work, we show that a single factor - \"neural tuning size\" - is able to account for three key phenomena that are characteristic of face processing, namely the Composite Face Effect (CFE), Face Inversion Effect (FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle provides specific neural tuning properties that correspond to the poorly-understood notion of holistic face processing, and connects these neural properties to psychophysical behavior. Overall, our work provides a unified and parsimonious theoretical account for the disparate empirical data on face-specific processing, deepening the fundamental understanding of face processing.\n    ",
        "submission_date": "2014-06-15T00:00:00",
        "last_modified_date": "2014-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4205",
        "title": "Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels",
        "authors": [
            "Peter F. Schultz",
            "Dylan M. Paiton",
            "Wei Lu",
            "Garrett T. Kenyon"
        ],
        "abstract": "In sparse coding it is common to tile an image into nonoverlapping patches, and then use a dictionary to create a sparse representation of each tile independently. In this situation, the overcompleteness of the dictionary is the number of dictionary elements divided by the patch size. In deconvolutional neural networks (DCNs), dictionaries learned on nonoverlapping tiles are replaced by a family of convolution kernels. Hence adjacent points in the feature maps (V1 layers) have receptive fields in the image that are translations of each other. The translational distance is determined by the dimensions of V1 in comparison to the dimensions of the image space. We refer to this translational distance as the stride.\n",
        "submission_date": "2014-06-17T00:00:00",
        "last_modified_date": "2014-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.4465",
        "title": "Multi-stage Multi-task feature learning via adaptive threshold",
        "authors": [
            "Yaru Fan",
            "Yilun Wang"
        ],
        "abstract": "Multi-task feature learning aims to identity the shared features among tasks to improve generalization. It has been shown that by minimizing non-convex learning models, a better solution than the convex alternatives can be obtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$ regularization was proposed in \\cite{Gong2013}, and a corresponding efficient multi-stage multi-task feature learning algorithm (MSMTFL) was presented. However, this algorithm harnesses a prescribed fixed threshold in the definition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of adaptivity might result in suboptimal performance. In this paper we propose to employ an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized formulation, where the corresponding variant of MSMTFL will incorporate an additional component to adaptively determine the threshold value. This variant is expected to achieve a better feature selection performance over the original MSMTFL algorithm. In particular, the embedded adaptive threshold component comes from our previously proposed iterative support detection (ISD) method \\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of this new variant over the original MSMTFL.\n    ",
        "submission_date": "2014-06-16T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5429",
        "title": "Playing with Duality: An Overview of Recent Primal-Dual Approaches for Solving Large-Scale Optimization Problems",
        "authors": [
            "Nikos Komodakis",
            "Jean-Christophe Pesquet"
        ],
        "abstract": "Optimization methods are at the core of many problems in signal/image processing, computer vision, and machine learning. For a long time, it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution. Deriving efficient strategies which jointly brings into play the primal and the dual problems is however a more recent idea which has generated many important new contributions in the last years. These novel developments are grounded on recent advances in convex analysis, discrete optimization, parallel processing, and non-smooth optimization with emphasis on sparsity issues. In this paper, we aim at presenting the principles of primal-dual approaches, while giving an overview of numerical methods which have been proposed in different contexts. We show the benefits which can be drawn from primal-dual algorithms both for solving large-scale convex optimization problems and discrete ones, and we provide various application examples to illustrate their usefulness.\n    ",
        "submission_date": "2014-06-20T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.5565",
        "title": "An Open Source Pattern Recognition Toolbox for MATLAB",
        "authors": [
            "Kenneth D. Morton Jr.",
            "Peter Torrione",
            "Leslie Collins",
            "Sam Keene"
        ],
        "abstract": "Pattern recognition and machine learning are becoming integral parts of algorithms in a wide range of applications. Different algorithms and approaches for machine learning include different tradeoffs between performance and computation, so during algorithm development it is often necessary to explore a variety of different approaches to a given task. A toolbox with a unified framework across multiple pattern recognition techniques enables algorithm developers the ability to rapidly evaluate different choices prior to deployment. MATLAB is a widely used environment for algorithm development and prototyping, and although several MATLAB toolboxes for pattern recognition are currently available these are either incomplete, expensive, or restrictively licensed. In this work we describe a MATLAB toolbox for pattern recognition and machine learning known as the PRT (Pattern Recognition Toolbox), licensed under the permissive MIT license. The PRT includes many popular techniques for data preprocessing, supervised learning, clustering, regression and feature selection, as well as a methodology for combining these components using a simple, uniform syntax. The resulting algorithms can be evaluated using cross-validation and a variety of scoring metrics to ensure robust performance when the algorithm is deployed. This paper presents an overview of the PRT as well as an example of usage on Fisher's Iris dataset.\n    ",
        "submission_date": "2014-06-21T00:00:00",
        "last_modified_date": "2014-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6145",
        "title": "Fast, Robust and Non-convex Subspace Recovery",
        "authors": [
            "Gilad Lerman",
            "Tyler Maunu"
        ],
        "abstract": "This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2016-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6247",
        "title": "Recurrent Models of Visual Attention",
        "authors": [
            "Volodymyr Mnih",
            "Nicolas Heess",
            "Alex Graves",
            "Koray Kavukcuoglu"
        ],
        "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.\n    ",
        "submission_date": "2014-06-24T00:00:00",
        "last_modified_date": "2014-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6314",
        "title": "Further heuristics for $k$-means: The merge-and-split heuristic and the $(k,l)$-means",
        "authors": [
            "Frank Nielsen",
            "Richard Nock"
        ],
        "abstract": "Finding the optimal $k$-means clustering is NP-hard in general and many heuristics have been designed for minimizing monotonically the $k$-means objective. We first show how to extend Lloyd's batched relocation heuristic and Hartigan's single-point relocation heuristic to take into account empty-cluster and single-point cluster events, respectively. Those events tend to increasingly occur when $k$ or $d$ increases, or when performing several restarts. First, we show that those special events are a blessing because they allow to partially re-seed some cluster centers while further minimizing the $k$-means objective function. Second, we describe a novel heuristic, merge-and-split $k$-means, that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the $k$-means objective. This novel heuristic can improve Hartigan's $k$-means when it has converged to a local minimum. We show empirically that this merge-and-split $k$-means improves over the Hartigan's heuristic which is the {\\em de facto} method of choice. Finally, we propose the $(k,l)$-means objective that generalizes the $k$-means objective by associating the data points to their $l$ closest cluster centers, and show how to either directly convert or iteratively relax the $(k,l)$-means into a $k$-means in order to reach better local minima.\n    ",
        "submission_date": "2014-06-23T00:00:00",
        "last_modified_date": "2014-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6854",
        "title": "A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning Segmentation Module",
        "authors": [
            "Jinwei Xu",
            "Jiankun Hu",
            "Xiuping Jia"
        ],
        "abstract": "Latent fingerprint has the practical value to identify the suspects who have unintentionally left a trace of fingerprint in the crime scenes. However, designing a fully automated latent fingerprint matcher is a very challenging task as it needs to address many challenging issues including the separation of overlapping structured patterns over the partial and poor quality latent fingerprint image, and finding a match against a large background database that would have different resolutions. Currently there is no fully automated latent fingerprint matcher available to the public and most literature reports have utilized a specialized latent fingerprint matcher COTS3 which is not accessible to the public. This will make it infeasible to assess and compare the relevant research work which is vital for this research community. In this study, we target to develop a fully automated latent matcher for adaptive detection of the region of interest and robust matching of latent prints. Unlike the manually conducted matching procedure, the proposed latent matcher can run like a sealed black box without any manual intervention. This matcher consists of the following two modules: (i) the dictionary learning-based region of interest (ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae set matching unit. Experimental results on NIST SD27 latent fingerprint database demonstrates that the proposed matcher outperforms the currently public state-of-art latent fingerprint matcher.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2014-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.6909",
        "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks",
        "authors": [
            "Alexey Dosovitskiy",
            "Philipp Fischer",
            "Jost Tobias Springenberg",
            "Martin Riedmiller",
            "Thomas Brox"
        ],
        "abstract": "Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.\n    ",
        "submission_date": "2014-06-26T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1406.7799",
        "title": "Subjective and Objective Quality Assessment of Image: A Survey",
        "authors": [
            "Pedram Mohammadi",
            "Abbas Ebrahimi-Moghadam",
            "Shahram Shirani"
        ],
        "abstract": "With the increasing demand for image-based applications, the efficient and reliable evaluation of image quality has increased in importance. Measuring the image quality is of fundamental importance for numerous image processing applications, where the goal of image quality assessment (IQA) methods is to automatically evaluate the quality of images in agreement with human quality judgments. Numerous IQA methods have been proposed over the past years to fulfill this goal. In this paper, a survey of the quality assessment methods for conventional image signals, as well as the newly emerged ones, which includes the high dynamic range (HDR) and 3-D images, is presented. A comprehensive explanation of the subjective and objective IQA and their classification is provided. Six widely used subjective quality datasets, and performance measures are reviewed. Emphasis is given to the full-reference image quality assessment (FR-IQA) methods, and 9 often-used quality measures (including mean squared error (MSE), structural similarity index (SSIM), multi-scale structural similarity index (MS-SSIM), visual information fidelity (VIF), most apparent distortion (MAD), feature similarity measure (FSIM), feature similarity measure for color images (FSIMC), dynamic range independent measure (DRIM), and tone-mapped images quality index (TMQI)) are carefully described, and their performance and computation time on four subjective quality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is provided and the issues related to this area of research are reviewed.\n    ",
        "submission_date": "2014-06-30T00:00:00",
        "last_modified_date": "2014-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0439",
        "title": "Geometric Tight Frame based Stylometry for Art Authentication of van Gogh Paintings",
        "authors": [
            "Haixia Liu",
            "Raymond H. Chan",
            "Yuan Yao"
        ],
        "abstract": "This paper is about authenticating genuine van Gogh paintings from forgeries. The authentication process depends on two key steps: feature extraction and outlier detection. In this paper, a geometric tight frame and some simple statistics of the tight frame coefficients are used to extract features from the paintings. Then a forward stage-wise rank boosting is used to select a small set of features for more accurate classification so that van Gogh paintings are highly concentrated towards some center point while forgeries are spread out as outliers. Numerical results show that our method can achieve 86.08% classification accuracy under the leave-one-out cross-validation procedure. Our method also identifies five features that are much more predominant than other features. Using just these five features for classification, our method can give 88.61% classification accuracy which is the highest so far reported in literature. Evaluation of the five features is also performed on two hundred datasets generated by bootstrap sampling with replacement. The median and the mean are 88.61% and 87.77% respectively. Our results show that a small set of statistics of the tight frame coefficients along certain orientations can serve as discriminative features for van Gogh paintings. It is more important to look at the tail distributions of such directional coefficients than mean values and standard deviations. It reflects a highly consistent style in van Gogh's brushstroke movements, where many forgeries demonstrate a more diverse spread in these features.\n    ",
        "submission_date": "2014-07-02T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.0921",
        "title": "Solving QVIs for Image Restoration with Adaptive Constraint Sets",
        "authors": [
            "Frank Lenzen",
            "Jan Lellmann",
            "Florian Becker",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "We consider a class of quasi-variational inequalities (QVIs) for adaptive image restoration, where the adaptivity is described via solution-dependent constraint sets. In previous work we studied both theoretical and numerical issues. While we were able to show the existence of solutions for a relatively broad class of problems, we encountered problems concerning uniqueness of the solution as well as convergence of existing algorithms for solving QVIs. In particular, it seemed that with increasing image size the growing condition number of the involved differential operator poses severe problems. In the present paper we prove uniqueness for a larger class of problems and in particular independent of the image size. Moreover, we provide a numerical algorithm with proved convergence. Experimental results support our theoretical findings.\n    ",
        "submission_date": "2014-07-03T00:00:00",
        "last_modified_date": "2014-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1151",
        "title": "Optimizing Ranking Measures for Compact Binary Code Learning",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Jianxin Wu"
        ],
        "abstract": "Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest---multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.\n    ",
        "submission_date": "2014-07-04T00:00:00",
        "last_modified_date": "2014-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1531",
        "title": "The jump set under geometric regularisation. Part 1: Basic technique and first-order denoising",
        "authors": [
            "Tuomo Valkonen"
        ],
        "abstract": "Let $u \\in \\mbox{BV}(\\Omega)$ solve the total variation denoising problem with $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model. Simul. 6 (2008), 879--894] have shown the containment $\\mathcal{H}^{m-1}(J_u \\setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proof unfortunately depends heavily on the co-area formula, as do many results in this area, and as such is not directly extensible to higher-order, curvature-based, and other advanced geometric regularisers, such as total generalised variation (TGV) and Euler's elastica. These have received increased attention in recent times due to their better practical regularisation properties compared to conventional total variation or wavelets. We prove analogous jump set containment properties for a general class of regularisers. We do this with novel Lipschitz transformation techniques, and do not require the co-area formula. In the present Part 1 we demonstrate the general technique on first-order regularisers, while in Part 2 we will extend it to higher-order regularisers. In particular, we concentrate in this part on TV and, as a novelty, Huber-regularised TV. We also demonstrate that the technique would apply to non-convex TV models as well as the Perona-Malik anisotropic diffusion, if these approaches were well-posed to begin with.\n    ",
        "submission_date": "2014-07-06T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1723",
        "title": "The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings",
        "authors": [
            "Thomas M\u00f6llenhoff",
            "Evgeny Strekalovskiy",
            "Michael Moeller",
            "Daniel Cremers"
        ],
        "abstract": "This paper deals with the analysis of a recent reformulation of the primal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischof and Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], which allows to apply it to nonconvex regularizers as first proposed for truncated quadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, it investigates variational problems for which the energy to be minimized can be written as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is a linear operator. We study the method and prove convergence in the case where the nonconvexity of $F$ is compensated by the strong convexity of the $G$. The convergence proof yields an interesting requirement for the choice of algorithm parameters, which we show to not only be sufficient, but necessary. Additionally, we show boundedness of the iterates under much weaker conditions. Finally, we demonstrate effectiveness and convergence of the algorithm beyond the theoretical guarantees in several numerical experiments.\n    ",
        "submission_date": "2014-07-07T00:00:00",
        "last_modified_date": "2014-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.1885",
        "title": "PAINTER: a spatio-spectral image reconstruction algorithm for optical interferometry",
        "authors": [
            "Antony Schutz",
            "Andr\u00e9 Ferrari",
            "David Mary",
            "F\u00e9rr\u00e9ol Soulez",
            "\u00c9ric Thi\u00e9baut",
            "Martin Vannier"
        ],
        "abstract": "Astronomical optical interferometers sample the Fourier transform of the intensity distribution of a source at the observation wavelength. Because of rapid perturbations caused by atmospheric turbulence, the phases of the complex Fourier samples (visibilities) cannot be directly exploited. Consequently, specific image reconstruction methods have been devised in the last few decades. Modern polychromatic optical interferometric instruments are now paving the way to multiwavelength imaging. This paper is devoted to the derivation of a spatio-spectral (3D) image reconstruction algorithm, coined PAINTER (Polychromatic opticAl INTErferometric Reconstruction software). The algorithm relies on an iterative process, which alternates estimation of polychromatic images and of complex visibilities. The complex visibilities are not only estimated from squared moduli and closure phases, but also differential phases, which helps to better constrain the polychromatic reconstruction. Simulations on synthetic data illustrate the efficiency of the algorithm and in particular the relevance of injecting a differential phases model in the reconstruction.\n    ",
        "submission_date": "2014-06-29T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2334",
        "title": "The jump set under geometric regularisation. Part 2: Higher-order approaches",
        "authors": [
            "Tuomo Valkonen"
        ],
        "abstract": "In Part 1, we developed a new technique based on Lipschitz pushforwards for proving the jump set containment property $\\mathcal{H}^{m-1}(J_u \\setminus J_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that the technique also applies to Huber-regularised TV. Now, in this Part 2, we extend the technique to higher-order regularisers. We are not quite able to prove the property for total generalised variation (TGV) based on the symmetrised gradient for the second-order term. We show that the property holds under three conditions: First, the solution $u$ is locally bounded. Second, the second-order variable is of locally bounded variation, $w \\in \\mbox{BV}_\\mbox{loc}(\\Omega; \\mathbb{R}^m)$, instead of just bounded deformation, $w \\in \\mbox{BD}(\\Omega)$. Third, $w$ does not jump on $J_u$ parallel to it. The second condition can be achieved for non-symmetric TGV. Both the second and third condition can be achieved if we change the Radon (or $L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p>1$, in which case Korn's inequality holds. We also consider the application of the technique to infimal convolution TV, and study the limiting behaviour of the singular part of $D u$, as the second parameter of $\\mbox{TGV}^2$ goes to zero. Unsurprisingly, it vanishes, but in numerical discretisations the situation looks quite different. Finally, our work additionally includes a result on TGV-strict approximation in $\\mbox{BV}(\\Omega)$.\n    ",
        "submission_date": "2014-07-09T00:00:00",
        "last_modified_date": "2014-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2602",
        "title": "Compressed sensing for longitudinal MRI: An adaptive-weighted approach",
        "authors": [
            "Lior Weizman",
            "Yonina C. Eldar",
            "Dafna Ben Bashat"
        ],
        "abstract": "Purpose: Repeated brain MRI scans are performed in many clinical scenarios, such as follow up of patients with tumors and therapy response assessment. In this paper, the authors show an approach to utilize former scans of the patient for the acceleration of repeated MRI scans.\n",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.2904",
        "title": "An eigenanalysis of data centering in machine learning",
        "authors": [
            "Paul Honeine"
        ],
        "abstract": "Many pattern recognition methods rely on statistical information from centered data, with the eigenanalysis of an empirical central moment, such as the covariance matrix in principal component analysis (PCA), as well as partial least squares regression, canonical-correlation analysis and Fisher discriminant analysis. Recently, many researchers advocate working on non-centered data. This is the case for instance with the singular value decomposition approach, with the (kernel) entropy component analysis, with the information-theoretic learning framework, and even with nonnegative matrix factorization. Moreover, one can also consider a non-centered PCA by using the second-order non-central moment.\n",
        "submission_date": "2014-07-10T00:00:00",
        "last_modified_date": "2014-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3026",
        "title": "An SVM Based Approach for Cardiac View Planning",
        "authors": [
            "Ramasubramanian Sundararajan",
            "Hima Patel",
            "Dattesh Shanbhag",
            "Vivek Vaidya"
        ],
        "abstract": "We consider the problem of automatically prescribing oblique planes (short axis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging (MRI). A concern with technologist-driven acquisitions of these planes is the quality and time taken for the total examination. We propose an automated solution incorporating anatomical features external to the cardiac region. The solution uses support vector machine regression models wherein complexity and feature selection are optimized using multi-objective genetic algorithms. Additionally, we examine the robustness of our approach by training our models on images with additive Rician-Gaussian mixtures at varying Signal to Noise (SNR) levels. Our approach has shown promising results, with an angular deviation of less than 15 degrees on 90% cases across oblique planes, measured in terms of average 6-fold cross validation performance -- this is generally within acceptable bounds of variation as specified by clinicians.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.3234",
        "title": "Image Inpainting Using Directional Tensor Product Complex Tight Framelets",
        "authors": [
            "Yi Shen",
            "Bin Han",
            "Elena Braverman"
        ],
        "abstract": "In this paper we are particularly interested in the image inpainting problem using directional complex tight wavelet frames. Under the assumption that frame coefficients of images are sparse, several iterative thresholding algorithms for the image inpainting problem have been proposed in the literature. The outputs of such iterative algorithms are closely linked to solutions of several convex minimization models using the balanced approach which simultaneously combines the $l_1$-regularization for sparsity of frame coefficients and the $l_2$-regularization for smoothness of the solution. Due to the redundancy of a tight frame, elements of a tight frame could be highly correlated and therefore, their corresponding frame coefficients of an image are expected to close to each other. This is called the grouping effect in statistics. In this paper, we establish the grouping effect property for frame-based convex minimization models using the balanced approach. This result on grouping effect partially explains the effectiveness of models using the balanced approach for several image restoration problems. Inspired by recent development on directional tensor product complex tight framelets (TP-CTFs) and their impressive performance for the image denoising problem, in this paper we propose an iterative thresholding algorithm using a single tight frame derived from TP-CTFs for the image inpainting problem. Experimental results show that our proposed algorithm can handle well both cartoons and textures simultaneously and performs comparably and often better than several well-known frame-based iterative thresholding algorithms for the image inpainting problem without noise. For the image inpainting problem with additive zero-mean i.i.d. Gaussian noise, our proposed algorithm using TP-CTFs performs superior than other known state-of-the-art frame-based image inpainting algorithms.\n    ",
        "submission_date": "2014-07-11T00:00:00",
        "last_modified_date": "2014-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.4118",
        "title": "Machine Learning Classification of SDSS Transient Survey Images",
        "authors": [
            "L. du Buisson",
            "N. Sivanandam",
            "B.A. Bassett",
            "M. Smith"
        ],
        "abstract": "We show that multiple machine learning algorithms can match human performance in classifying transient imaging data from the Sloan Digital Sky Survey (SDSS) supernova survey into real objects and artefacts. This is a first step in any transient science pipeline and is currently still done by humans, but future surveys such as the Large Synoptic Survey Telescope (LSST) will necessitate fully machine-enabled solutions. Using features trained from eigenimage analysis (principal component analysis, PCA) of single-epoch g, r and i-difference images, we can reach a completeness (recall) of 96 per cent, while only incorrectly classifying at most 18 per cent of artefacts as real objects, corresponding to a precision (purity) of 84 per cent. In general, random forests performed best, followed by the k-nearest neighbour and the SkyNet artificial neural net algorithms, compared to other methods such as na\u00efve Bayes and kernel support vector machine. Our results show that PCA-based machine learning can match human success levels and can naturally be extended by including multiple epochs of data, transient colours and host galaxy information which should allow for significant further improvements, especially at low signal-to-noise.\n    ",
        "submission_date": "2014-07-15T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5104",
        "title": "Pixels to Voxels: Modeling Visual Representation in the Human Brain",
        "authors": [
            "Pulkit Agrawal",
            "Dustin Stansbury",
            "Jitendra Malik",
            "Jack L. Gallant"
        ],
        "abstract": "The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function.\n    ",
        "submission_date": "2014-07-18T00:00:00",
        "last_modified_date": "2014-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5536",
        "title": "Multichannel Compressive Sensing MRI Using Noiselet Encoding",
        "authors": [
            "Kamlesh Pawar",
            "Gary F. Egan",
            "Jingxin Zhang"
        ],
        "abstract": "The incoherence between measurement and sparsifying transform matrices and the restricted isometry property (RIP) of measurement matrix are two of the key factors in determining the performance of compressive sensing (CS). In CS-MRI, the randomly under-sampled Fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix. However, the incoherence between the randomly under-sampled Fourier matrix and the wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI. Using the mathematical result that noiselets are maximally incoherent with wavelets, this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and RIP in CS-MRI, and presents a method to design the pulse sequence for the noiselet encoding. This novel encoding scheme is combined with the multichannel compressive sensing (MCS) framework to take the advantage of multichannel data acquisition used in MRI scanners. An empirical RIP analysis is presented to compare the multichannel noiselet and multichannel Fourier measurement matrices in MCS. Simulations are presented in the MCS framework to compare the performance of noiselet encoding reconstructions and Fourier encoding reconstructions at different acceleration factors. The comparisons indicate that multichannel noiselet measurement matrix has better RIP than that of its Fourier counterpart, and that noiselet encoded MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and can achieve higher acceleration factors. To demonstrate the feasibility of the proposed noiselet encoding scheme, two pulse sequences with tailored spatially selective RF excitation pulses was designed and implemented on a 3T scanner to acquire the data in the noiselet domain from a phantom and a human brain.\n    ",
        "submission_date": "2014-07-21T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.5754",
        "title": "Tree-based iterated local search for Markov random fields with applications in image analysis",
        "authors": [
            "Truyen Tran",
            "Dinh Phung",
            "Svetha Venkatesh"
        ],
        "abstract": "The \\emph{maximum a posteriori} (MAP) assignment for general structure Markov random fields (MRFs) is computationally intractable. In this paper, we exploit tree-based methods to efficiently address this problem. Our novel method, named Tree-based Iterated Local Search (T-ILS) takes advantage of the tractability of tree-structures embedded within MRFs to derive strong local search in an ILS framework. The method efficiently explores exponentially large neighborhood and does so with limited memory without any requirement on the cost functions. We evaluate the T-ILS in a simulation of Ising model and two real-world problems in computer vision: stereo matching, image denoising. Experimental results demonstrate that our methods are competitive against state-of-the-art rivals with a significant computational gain.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6067",
        "title": "The U-curve optimization problem: improvements on the original algorithm and time complexity analysis",
        "authors": [
            "Marcelo S. Reis",
            "Carlos E. Ferreira",
            "Junior Barrera"
        ],
        "abstract": "The U-curve optimization problem is characterized by a decomposable in U-shaped curves cost function over the chains of a Boolean lattice. This problem can be applied to model the classical feature selection problem in Machine Learning. Recently, the U-Curve algorithm was proposed to give optimal solutions to the U-curve problem. In this article, we point out that the U-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS) algorithm, which is actually optimal. We also present the results of optimal and suboptimal experiments, in which UCS is compared with the UBB optimal branch-and-bound algorithm and the SFFS heuristic, respectively. We show that, in both experiments, $\\proc{UCS}$ had a better performance than its competitor. Finally, we analyze the obtained results and point out improvements on UCS that might enhance the performance of this algorithm.\n    ",
        "submission_date": "2014-07-22T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6245",
        "title": "scikit-image: Image processing in Python",
        "authors": [
            "Stefan van der Walt",
            "Johannes L. Sch\u00f6nberger",
            "Juan Nunez-Iglesias",
            "Fran\u00e7ois Boulogne",
            "Joshua D. Warner",
            "Neil Yager",
            "Emmanuelle Gouillart",
            "Tony Yu",
            "scikit-image contributors"
        ],
        "abstract": "scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal \"Modified BSD\" open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image.\n    ",
        "submission_date": "2014-07-23T00:00:00",
        "last_modified_date": "2014-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.6432",
        "title": "Learning Structured Outputs from Partial Labels using Forest Ensemble",
        "authors": [
            "Truyen Tran",
            "Dinh Phung",
            "Svetha Venkatesh"
        ],
        "abstract": "Learning structured outputs with general structures is computationally challenging, except for tree-structured models. Thus we propose an efficient boosting-based algorithm ",
        "submission_date": "2014-07-24T00:00:00",
        "last_modified_date": "2014-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.7091",
        "title": "Pushbroom Stereo for High-Speed Navigation in Cluttered Environments",
        "authors": [
            "Andrew J. Barry",
            "Russ Tedrake"
        ],
        "abstract": "We present a novel stereo vision algorithm that is capable of obstacle detection on a mobile-CPU processor at 120 frames per second. Our system performs a subset of standard block-matching stereo processing, searching only for obstacles at a single depth. By using an onboard IMU and state-estimator, we can recover the position of obstacles at all other depths, building and updating a full depth-map at framerate.\n",
        "submission_date": "2014-07-26T00:00:00",
        "last_modified_date": "2014-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1407.8337",
        "title": "A New Model of Array Grammar for generating Connected Patterns on an Image Neighborhood",
        "authors": [
            "G. Vishnu Murthy",
            "Pavan Kumar C.",
            "Vakulabharanam Vijaya Kumar"
        ],
        "abstract": "Study of patterns on images is recognized as an important step in characterization and classification of image. The ability to efficiently analyze and describe image patterns is thus of fundamental importance. The study of syntactic methods of describing pictures has been of interest for researchers. Array Grammars can be used to represent and recognize connected patterns. In any image the patterns are recognized using connected patterns. It is very difficult to represent all connected patterns (CP) even on a small 3 x 3 neighborhood in a pictorial way. The present paper proposes the model of array grammar capable of generating any kind of simple or complex pattern and derivation of connected pattern in an image neighborhood using the proposed grammar is discussed.\n    ",
        "submission_date": "2014-07-31T00:00:00",
        "last_modified_date": "2014-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0204",
        "title": "Functional Principal Component Analysis and Randomized Sparse Clustering Algorithm for Medical Image Analysis",
        "authors": [
            "Nan Lin",
            "Junhai Jiang",
            "Shicheng Guo",
            "Momiao Xiong"
        ],
        "abstract": "Due to advances in sensors, growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs. As a consequence, the medical images have the potential to enhance diagnosis of disease, prediction of clinical outcomes, characterization of disease progression, management of health care and development of treatments, but also pose great methodological and computational challenges for representation and selection of features in image cluster analysis. To address these challenges, we first extend one dimensional functional principal component analysis to the two dimensional functional principle component analyses (2DFPCA) to fully capture space variation of image signals. Image signals contain a large number of redundant and irrelevant features which provide no additional or no useful information for cluster analysis. Widely used methods for removing redundant and irrelevant features are sparse clustering algorithms using a lasso-type penalty to select the features. However, the accuracy of clustering using a lasso-type penalty depends on how to select penalty parameters and a threshold for selecting features. In practice, they are difficult to determine. Recently, randomized algorithms have received a great deal of attention in big data analysis. This paper presents a randomized algorithm for accurate feature selection in image cluster analysis. The proposed method is applied to ovarian and kidney cancer histology image data from the TCGA database. The results demonstrate that the randomized feature selection method coupled with functional principal component analysis substantially outperforms the current sparse clustering algorithms in image cluster analysis.\n    ",
        "submission_date": "2014-08-01T00:00:00",
        "last_modified_date": "2014-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0765",
        "title": "Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet Bayesian Network",
        "authors": [
            "Yu Liu",
            "Osvaldo Simeone",
            "Alexander M. Haimovich",
            "Wei Su"
        ],
        "abstract": "A novel Bayesian modulation classification scheme is proposed for a single-antenna system over frequency-selective fading channels. The method is based on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN). The use of the proposed latent Dirichlet BN provides a systematic solution to the convergence problem encountered by the conventional Gibbs sampling approach for modulation classification. The method generalizes, and is shown to improve upon, the state of the art.\n    ",
        "submission_date": "2014-08-04T00:00:00",
        "last_modified_date": "2014-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0889",
        "title": "Computing With Contextual Numbers",
        "authors": [
            "Vahid Moosavi"
        ],
        "abstract": "Self Organizing Map (SOM) has been applied into several classical modeling tasks including clustering, classification, function approximation and visualization of high dimensional spaces. The final products of a trained SOM are a set of ordered (low dimensional) indices and their associated high dimensional weight vectors. While in the above-mentioned applications, the final high dimensional weight vectors play the primary role in the computational steps, from a certain perspective, one can interpret SOM as a nonparametric encoder, in which the final low dimensional indices of the trained SOM are pointer to the high dimensional space. We showed how using a one-dimensional SOM, which is not common in usual applications of SOM, one can develop a nonparametric mapping from a high dimensional space to a continuous one-dimensional numerical field. These numerical values, called contextual numbers, are ordered in a way that in a given context, similar numbers refer to similar high dimensional states. Further, as these numbers can be treated similarly to usual continuous numbers, they can be replaced with their corresponding high dimensional states within any data driven modeling problem. As a potential application, we showed how using contextual numbers could be used for the problem of high dimensional spatiotemporal dynamics.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0967",
        "title": "Determining the Number of Clusters via Iterative Consensus Clustering",
        "authors": [
            "Shaina Race",
            "Carl Meyer",
            "Kevin Valakuzhy"
        ],
        "abstract": "We use a cluster ensemble to determine the number of clusters, k, in a group of data. A consensus similarity matrix is formed from the ensemble using multiple algorithms and several values for k. A random walk is induced on the graph defined by the consensus matrix and the eigenvalues of the associated transition probability matrix are used to determine the number of clusters. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages a block-diagonal form. It is shown that the resulting consensus matrix is generally superior to existing similarity matrices for this type of spectral analysis.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.0972",
        "title": "A Flexible Iterative Framework for Consensus Clustering",
        "authors": [
            "Shaina Race",
            "Carl Meyer"
        ],
        "abstract": "A novel framework for consensus clustering is presented which has the ability to determine both the number of clusters and a final solution using multiple algorithms. A consensus similarity matrix is formed from an ensemble using multiple algorithms and several values for k. A variety of dimension reduction techniques and clustering algorithms are considered for analysis. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages algorithms to agree upon a common solution. We utilize the theory of nearly uncoupled Markov chains to determine the number, k , of clusters in a dataset by considering a random walk on the graph defined by the consensus matrix. The eigenvalues of the associated transition probability matrix are used to determine the number of clusters. This method succeeds at determining the number of clusters in many datasets where previous methods fail. On every considered dataset, our consensus method provides a final result with accuracy well above the average of the individual algorithms.\n    ",
        "submission_date": "2014-08-05T00:00:00",
        "last_modified_date": "2014-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.1167",
        "title": "Boosted Markov Networks for Activity Recognition",
        "authors": [
            "Truyen Tran",
            "Hung Bui",
            "Svetha Venkatesh"
        ],
        "abstract": "We explore a framework called boosted Markov networks to combine the learning capacity of boosting and the rich modeling semantics of Markov networks and applying the framework for video-based activity recognition. Importantly, we extend the framework to incorporate hidden variables. We show how the framework can be applied for both model learning and feature selection. We demonstrate that boosted Markov networks with hidden variables perform comparably with the standard maximum likelihood estimation. However, our framework is able to learn sparse models, and therefore can provide computational savings when the learned models are used for classification.\n    ",
        "submission_date": "2014-08-06T00:00:00",
        "last_modified_date": "2014-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2584",
        "title": "Homotopy equivalence of finite digital images",
        "authors": [
            "Jason Haarmann",
            "Meg P. Murphy",
            "Casey S. Peters",
            "P. Christopher Staecker"
        ],
        "abstract": "For digital images, there is an established homotopy equivalence relation which parallels that of classical topology. Many classical homotopy equivalence invariants, such as the Euler characteristic and the homology groups, do not remain invariants in the digital setting. This paper develops a numerical digital homotopy invariant and begins to catalog all possible connected digital images on a small number of points, up to homotopy equivalence.\n    ",
        "submission_date": "2014-08-11T00:00:00",
        "last_modified_date": "2014-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.2927",
        "title": "Hashing for Similarity Search: A Survey",
        "authors": [
            "Jingdong Wang",
            "Heng Tao Shen",
            "Jingkuan Song",
            "Jianqiu Ji"
        ],
        "abstract": "Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.\n    ",
        "submission_date": "2014-08-13T00:00:00",
        "last_modified_date": "2014-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3081",
        "title": "Human Activity Learning and Segmentation using Partially Hidden Discriminative Models",
        "authors": [
            "Truyen Tran",
            "Hung Bui",
            "Svetha Venkatesh"
        ],
        "abstract": "Learning and understanding the typical patterns in the daily activities and routines of people from low-level sensory data is an important problem in many application domains such as building smart environments, or providing intelligent assistance. Traditional approaches to this problem typically rely on supervised learning and generative models such as the hidden Markov models and its extensions. While activity data can be readily acquired from pervasive sensors, e.g. in smart environments, providing manual labels to support supervised training is often extremely expensive. In this paper, we propose a new approach based on semi-supervised training of partially hidden discriminative models such as the conditional random field (CRF) and the maximum entropy Markov model (MEMM). We show that these models allow us to incorporate both labeled and unlabeled data for learning, and at the same time, provide us with the flexibility and accuracy of the discriminative framework. Our experimental results in the video surveillance domain illustrate that these models can perform better than their generative counterpart, the partially hidden Markov model, even when a substantial amount of labels are unavailable.\n    ",
        "submission_date": "2014-08-06T00:00:00",
        "last_modified_date": "2014-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.3818",
        "title": "Unsupervised learning segmentation for dynamic speckle activity images",
        "authors": [
            "Lucia I. Passoni",
            "Ana I. Dai Pra",
            "Gustavo J. Meschino",
            "MArcelo Guzman",
            "Chistian Weber",
            "H\u00e9ctor Rabal",
            "Marcelo Trivi"
        ],
        "abstract": "This paper proposes the design of decision models based on Computational Intelligence techniques applied to image sequences of dynamic laser speckle. These models aim to identify image regions of biological specimens illuminated by a coherent beam coming from a laser. The field image is pseudo colored using a Self Organizing Map projection. This process is carried out using a set of descriptors applied to the intensity variations along time in every pixel of an image sequence. The models use descriptors selected to improve effectiveness, depending on the specific application. We present two examples of the application of the proposed techniques to assess biological tissues. The results obtained are encouraging and significantly improve those obtained using a single descriptor.\n    ",
        "submission_date": "2014-08-17T00:00:00",
        "last_modified_date": "2014-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.4576",
        "title": "Introduction to Clustering Algorithms and Applications",
        "authors": [
            "Sibei Yang",
            "Liangde Tao",
            "Bingchen Gong"
        ],
        "abstract": "Data clustering is the process of identifying natural groupings or clusters within multidimensional data based on some similarity measure. Clustering is a fundamental process in many different disciplines. Hence, researchers from different fields are actively working on the clustering problem. This paper provides an overview of the different representative clustering methods. In addition, application of clustering in different field is briefly introduced.\n    ",
        "submission_date": "2014-08-20T00:00:00",
        "last_modified_date": "2014-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.5574",
        "title": "Supervised Hashing Using Graph Cuts and Boosted Decision Trees",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Embedding image features into a binary Hamming space can improve both the speed and accuracy of large-scale query-by-example image retrieval systems. Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps: binary code (hash bits) learning, and hash function learning. The first step can typically be formulated as a binary quadratic problem, and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference, we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data.\n    ",
        "submission_date": "2014-08-24T00:00:00",
        "last_modified_date": "2015-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6299",
        "title": "An inexact Newton-Krylov algorithm for constrained diffeomorphic image registration",
        "authors": [
            "Andreas Mang",
            "George Biros"
        ],
        "abstract": "We propose numerical algorithms for solving large deformation diffeomorphic image registration problems. We formulate the nonrigid image registration problem as a problem of optimal control. This leads to an infinite-dimensional partial differential equation (PDE) constrained optimization problem.\n",
        "submission_date": "2014-08-27T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1408.6974",
        "title": "Fast Disk Conformal Parameterization of Simply-connected Open Surfaces",
        "authors": [
            "Pui Tung Choi",
            "Lok Ming Lui"
        ],
        "abstract": "Surface parameterizations have been widely used in computer graphics and geometry processing. In particular, as simply-connected open surfaces are conformally equivalent to the unit disk, it is desirable to compute the disk conformal parameterizations of the surfaces. In this paper, we propose a novel algorithm for the conformal parameterization of a simply-connected open surface onto the unit disk, which significantly speeds up the computation, enhances the conformality and stability, and guarantees the bijectivity. The conformality distortions at the inner region and on the boundary are corrected by two steps, with the aid of an iterative scheme using quasi-conformal theories. Experimental results demonstrate the effectiveness of our proposed method.\n    ",
        "submission_date": "2014-08-29T00:00:00",
        "last_modified_date": "2014-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0177",
        "title": "Persistent Homology in Sparse Regression and its Application to Brain Morphometry",
        "authors": [
            "Moo K. Chung",
            "Jamie L. Hanson",
            "Jieping Ye",
            "Richard J. Davidson",
            "Seth D. Pollak"
        ],
        "abstract": "Sparse systems are usually parameterized by a tuning parameter that determines the sparsity of the system. How to choose the right tuning parameter is a fundamental and difficult problem in learning the sparse system. In this paper, by treating the the tuning parameter as an additional dimension, persistent homological structures over the parameter space is introduced and explored. The structures are then further exploited in speeding up the computation using the proposed soft-thresholding technique. The topological structures are further used as multivariate features in the tensor-based morphometry (TBM) in characterizing white matter alterations in children who have experienced severe early life stress and maltreatment. These analyses reveal that stress-exposed children exhibit more diffuse anatomical organization across the whole white matter region.\n    ",
        "submission_date": "2014-08-31T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.0749",
        "title": "Image Retrieval And Classification Using Local Feature Vectors",
        "authors": [
            "Vikas Verma"
        ],
        "abstract": "Content Based Image Retrieval(CBIR) is one of the important subfield in the field of Information Retrieval. The goal of a CBIR algorithm is to retrieve semantically similar images in response to a query image submitted by the end user. CBIR is a hard problem because of the phenomenon known as $\\textit {semantic gap}$.\n",
        "submission_date": "2014-09-02T00:00:00",
        "last_modified_date": "2014-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1062",
        "title": "Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations",
        "authors": [
            "Fanhua Shang",
            "Yuanyuan Liu",
            "Hanghang Tong",
            "James Cheng",
            "Hong Cheng"
        ],
        "abstract": "Recovering low-rank and sparse matrices from incomplete or corrupted observations is an important problem in machine learning, statistics, bioinformatics, computer vision, as well as signal and image processing. In theory, this problem can be solved by the natural convex joint/mixed relaxations (i.e., l_{1}-norm and trace norm) under certain conditions. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i.e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our method compared with the state-of-the-art methods.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1199",
        "title": "Focused Proofreading: Efficiently Extracting Connectomes from Segmented EM Images",
        "authors": [
            "Stephen M. Plaza"
        ],
        "abstract": "Identifying complex neural circuitry from electron microscopic (EM) images may help unlock the mysteries of the brain. However, identifying this circuitry requires time-consuming, manual tracing (proofreading) due to the size and intricacy of these image datasets, thus limiting state-of-the-art analysis to very small brain regions. Potential avenues to improve scalability include automatic image segmentation and crowd sourcing, but current efforts have had limited success. In this paper, we propose a new strategy, focused proofreading, that works with automatic segmentation and aims to limit proofreading to the regions of a dataset that are most impactful to the resulting circuit. We then introduce a novel workflow, which exploits biological information such as synapses, and apply it to a large dataset in the fly optic lobe. With our techniques, we achieve significant tracing speedups of 3-5x without sacrificing the quality of the resulting circuit. Furthermore, our methodology makes the task of proofreading much more accessible and hence potentially enhances the effectiveness of crowd sourcing.\n    ",
        "submission_date": "2014-09-03T00:00:00",
        "last_modified_date": "2014-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1801",
        "title": "Annotating Synapses in Large EM Datasets",
        "authors": [
            "Stephen M. Plaza",
            "Toufiq Parag",
            "Gary B. Huang",
            "Donald J. Olbris",
            "Mathew A. Saunders",
            "Patricia K. Rivlin"
        ],
        "abstract": "Reconstructing neuronal circuits at the level of synapses is a central problem in neuroscience and becoming a focus of the emerging field of connectomics. To date, electron microscopy (EM) is the most proven technique for identifying and quantifying synaptic connections. As advances in EM make acquiring larger datasets possible, subsequent manual synapse identification ({\\em i.e.}, proofreading) for deciphering a connectome becomes a major time bottleneck. Here we introduce a large-scale, high-throughput, and semi-automated methodology to efficiently identify synapses. We successfully applied our methodology to the Drosophila medulla optic lobe, annotating many more synapses than previous connectome efforts. Our approaches are extensible and will make the often complicated process of synapse identification accessible to a wider-community of potential proofreaders.\n    ",
        "submission_date": "2014-09-05T00:00:00",
        "last_modified_date": "2014-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.1892",
        "title": "Automatic Neuron Type Identification by Neurite Localization in the Drosophila Medulla",
        "authors": [
            "Ting Zhao",
            "Stephen M Plaza"
        ],
        "abstract": "Mapping the connectivity of neurons in the brain (i.e., connectomics) is a challenging problem due to both the number of connections in even the smallest organisms and the nanometer resolution required to resolve them. Because of this, previous connectomes contain only hundreds of neurons, such as in the ",
        "submission_date": "2014-09-05T00:00:00",
        "last_modified_date": "2014-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2080",
        "title": "Multiscale statistical testing for connectome-wide association studies in fMRI",
        "authors": [
            "P. Bellec",
            "Y. Benhajali",
            "F. Carbonell",
            "C. Dansereau",
            "G. Albouy",
            "M. Pelland",
            "C. Craddock",
            "O. Collignon",
            "J. Doyon",
            "E. Stip",
            "P. Orban"
        ],
        "abstract": "Alterations in brain connectivity have been associated with a variety of clinical disorders using functional magnetic resonance imaging (fMRI). We investigated empirically how the number of brain parcels (or scale) impacted the results of a mass univariate general linear model (GLM) on connectomes. The brain parcels used as nodes in the connectome analysis were functionnally defined by a group cluster analysis. We first validated that a classic Benjamini-Hochberg procedure with parametric GLM tests did control appropriately the false-discovery rate (FDR) at a given scale. We then observed on realistic simulations that there was no substantial inflation of the FDR across scales, as long as the FDR was controlled independently within each scale, and the presence of true associations could be established using an omnibus permutation test combining all scales. Second, we observed both on simulations and on three real resting-state fMRI datasets (schizophrenia, congenital blindness, motor practice) that the rate of discovery varied markedly as a function of scales, and was relatively higher for low scales, below 25. Despite the differences in discovery rate, the statistical maps derived at different scales were generally very consistent in the three real datasets. Some seeds still showed effects better observed around 50, illustrating the potential benefits of multiscale analysis. On real data, the statistical maps agreed well with the existing literature. Overall, our results support that the multiscale GLM connectome analysis with FDR is statistically valid and can capture biologically meaningful effects in a variety of experimental conditions.\n    ",
        "submission_date": "2014-09-07T00:00:00",
        "last_modified_date": "2015-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2287",
        "title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process Models",
        "authors": [
            "Andreas C. Damianou",
            "Michalis K. Titsias",
            "Neil D. Lawrence"
        ],
        "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.\n    ",
        "submission_date": "2014-09-08T00:00:00",
        "last_modified_date": "2014-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2579",
        "title": "A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices",
        "authors": [
            "Ting-ting Feng",
            "Gang Wu"
        ],
        "abstract": "The null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.2821",
        "title": "Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain Clustered Records",
        "authors": [
            "Meysam Ghaffari",
            "Nasser Ghadiri"
        ],
        "abstract": "As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each input sample to belong to more than one cluster, providing more flexibility than non-fuzzy clustering methods. However, the accuracy of FCM is subject to false detections caused by noisy records, weak feature selection and low certainty of the algorithm in some cases. The false detections are very important in some decision-making application domains like network security and medical diagnosis, where weak decisions based on such false detections may lead to catastrophic outcomes. They are mainly emerged from making decisions about a subset of records that do not provide enough evidence to make a good decision. In this paper, we propose a method for detecting such ambiguous records in FCM by introducing a certainty factor to decrease invalid detections. This approach enables us to send the detected ambiguous records to another discrimination method for a deeper investigation, thus increasing the accuracy by lowering the error rate. Most of the records are still processed quickly and with low error rate which prevents performance loss compared to similar hybrid methods. Experimental results of applying the proposed method on several datasets from different domains show a significant decrease in error rate as well as improved sensitivity of the algorithm.\n    ",
        "submission_date": "2014-09-09T00:00:00",
        "last_modified_date": "2014-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3660",
        "title": "10,000+ Times Accelerated Robust Subset Selection (ARSS)",
        "authors": [
            "Feiyun Zhu",
            "Bin Fan",
            "Xinliang Zhu",
            "Ying Wang",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "abstract": "Subset selection from massive data with noised information is increasingly popular for various applications. This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers. To address the above two issues, we propose an accelerated robust subset selection (ARSS) method. Specifically in the subset selection area, this is the first attempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outlier elements is greatly enhanced. Actually, data size is generally much larger than feature length, i.e. $N\\gg L$. Based on this observation, we propose a speedup solver (via ALM and equivalent derivations) to highly reduce the computational cost, theoretically from $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods, but also runs 10,000+ times faster than the most related method.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3714",
        "title": "Time-domain multiscale shape identification in electro-sensing",
        "authors": [
            "Habib Ammari",
            "Han Wang"
        ],
        "abstract": "This paper presents premier and innovative time-domain multi-scale method for shape identification in electro-sensing using pulse-type signals. The method is based on transform-invariant shape descriptors computed from filtered polarization tensors at multi-scales. The proposed algorithm enjoys a remarkable noise robustness even with far-field measurements at very limited angle of view. It opens a door for pulsed imaging using echolocation and induction data.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.3854",
        "title": "Linear, Deterministic, and Order-Invariant Initialization Methods for the K-Means Clustering Algorithm",
        "authors": [
            "M. Emre Celebi",
            "Hassan A. Kingravi"
        ],
        "abstract": "Over the past five decades, k-means has become the clustering algorithm of choice in many application domains primarily due to its simplicity, time/space efficiency, and invariance to the ordering of the data points. Unfortunately, the algorithm's sensitivity to the initial selection of the cluster centers remains to be its most serious drawback. Numerous initialization methods have been proposed to address this drawback. Many of these methods, however, have time complexity superlinear in the number of data points, which makes them impractical for large data sets. On the other hand, linear methods are often random and/or sensitive to the ordering of the data points. These methods are generally unreliable in that the quality of their results is unpredictable. Therefore, it is common practice to perform multiple runs of such methods and take the output of the run that produces the best results. Such a practice, however, greatly increases the computational requirements of the otherwise highly efficient k-means algorithm. In this chapter, we investigate the empirical performance of six linear, deterministic (non-random), and order-invariant k-means initialization methods on a large and diverse collection of data sets from the UCI Machine Learning Repository. The results demonstrate that two relatively unknown hierarchical initialization methods due to Su and Dy outperform the remaining four methods with respect to two objective effectiveness criteria. In addition, a recent method due to Erisoglu et al. performs surprisingly poorly.\n    ",
        "submission_date": "2014-09-12T00:00:00",
        "last_modified_date": "2014-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4043",
        "title": "Design of Novel Algorithm and Architecture for Gaussian Based Color Image Enhancement System for Real Time Applications",
        "authors": [
            "M. C. Hanumantharaju",
            "M. Ravishankar",
            "D. R. Rameshbabu"
        ],
        "abstract": "This paper presents the development of a new algorithm for Gaussian based color image enhancement system. The algorithm has been designed into architecture suitable for FPGA/ASIC implementation. The color image enhancement is achieved by first convolving an original image with a Gaussian kernel since Gaussian distribution is a point spread function which smoothen the image. Further, logarithm-domain processing and gain/offset corrections are employed in order to enhance and translate pixels into the display range of 0 to 255. The proposed algorithm not only provides better dynamic range compression and color rendition effect but also achieves color constancy in an image. The design exploits high degrees of pipelining and parallel processing to achieve real time performance. The design has been realized by RTL compliant Verilog coding and fits into a single FPGA with a gate count utilization of 321,804. The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148 FPGA device and is capable of processing high resolution color motion pictures of sizes of up to 1600x1200 pixels at the real time video rate of 116 frames per second. This shows that the proposed design would work for not only still images but also for high resolution video sequences.\n    ",
        "submission_date": "2014-09-14T00:00:00",
        "last_modified_date": "2014-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4139",
        "title": "A feasible roadmap for developing volumetric probability atlas of localized prostate cancer",
        "authors": [
            "Liang Zhao",
            "Jianhua Xuan",
            "Yue Wang"
        ],
        "abstract": "A statistical volumetric model, showing the probability map of localized prostate cancer within the host anatomical structure, has been developed from 90 optically-imaged surgical specimens. This master model permits an accurate characterization of prostate cancer distribution patterns and an atlas-informed biopsy sampling strategy. The model is constructed by mapping individual prostate models onto a site model, together with localized tumors. An accurate multi-object non-rigid warping scheme is developed based on a mixture of principal-axis registrations. We report our evaluation and pilot studies on the effectiveness of the method and its application to optimizing needle biopsy strategies.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4271",
        "title": "The Ordered Weighted $\\ell_1$ Norm: Atomic Formulation, Projections, and Algorithms",
        "authors": [
            "Xiangrong Zeng",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "The ordered weighted $\\ell_1$ norm (OWL) was recently proposed, with two different motivations: its good statistical properties as a sparsity promoting regularizer; the fact that it generalizes the so-called {\\it octagonal shrinkage and clustering algorithm for regression} (OSCAR), which has the ability to cluster/group regression variables that are highly correlated. This paper contains several contributions to the study and application of OWL regularization: the derivation of the atomic formulation of the OWL norm; the derivation of the dual of the OWL norm, based on its atomic formulation; a new and simpler derivation of the proximity operator of the OWL norm; an efficient scheme to compute the Euclidean projection onto an OWL ball; the instantiation of the conditional gradient (CG, also known as Frank-Wolfe) algorithm for linear regression problems under OWL regularization; the instantiation of accelerated projected gradient algorithms for the same class of problems. Finally, a set of experiments give evidence that accelerated projected gradient algorithms are considerably faster than CG, for the class of problems considered.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4469",
        "title": "Convolutional Networks for Image Processing by Coupled Oscillator Arrays",
        "authors": [
            "Dmitri E. Nikonov",
            "Ian A. Young",
            "George I. Bourianoff"
        ],
        "abstract": "A coupled oscillator array is shown to approximate convolutions with Gabor filters for image processing tasks. Pixelated image fragments and filter functions are converted to voltages, differenced, and input into a corresponding array of weakly coupled Voltage Controlled Oscillators (VCOs). This is referred to as Frequency Shift Keying (FSK). Upon synchronization of the array, the common node amplitude provides a metric for the degree of match between the image fragment and the filter function. The optimal oscillator parameters for synchronization are determined and favor a moderate value of the Q-factor.\n    ",
        "submission_date": "2014-09-15T00:00:00",
        "last_modified_date": "2014-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4627",
        "title": "DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF Features",
        "authors": [
            "Petra Budikova",
            "Jan Botorek",
            "Michal Batko",
            "Pavel Zezula"
        ],
        "abstract": "This paper constitutes an extension to the report on DISA-MU team participation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as published in [3]. Specifically, we introduce a new similarity search component that was implemented into the system, report on the results achieved by utilizing this component, and analyze the influence of different similarity search parameters on the annotation quality.\n    ",
        "submission_date": "2014-09-16T00:00:00",
        "last_modified_date": "2014-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.4958",
        "title": "Tensity Research Based on the Information of Eye Movement",
        "authors": [
            "Yi Wang"
        ],
        "abstract": "User's mental state is concerned gradually, during the interaction course of human robot. As the measurement and identification method of psychological state, tension, has certain practical significance role. At presents there is no suitable method of measuring the tension. Firstly, sum up some availability of eye movement index. And then parameters extraction on eye movement characteristics of normal illumination is studied, including the location of the face, eyes location, access to the pupil diameter, the eye pupil center characteristic parameters. And with the judgment of the tension in eye images, extract exact information of gaze direction. Finally, through the experiment to prove the proposed method is effective.\n    ",
        "submission_date": "2014-09-17T00:00:00",
        "last_modified_date": "2014-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.5185",
        "title": "Deeply-Supervised Nets",
        "authors": [
            "Chen-Yu Lee",
            "Saining Xie",
            "Patrick Gallagher",
            "Zhengyou Zhang",
            "Zhuowen Tu"
        ],
        "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).\n    ",
        "submission_date": "2014-09-18T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6045",
        "title": "Analyzing sparse dictionaries for online learning with kernels",
        "authors": [
            "Paul Honeine"
        ],
        "abstract": "Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2014-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.6046",
        "title": "Approximation errors of online sparsification criteria",
        "authors": [
            "Paul Honeine"
        ],
        "abstract": "Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient sparsification criteria have been introduced in the literature, such as the distance, the coherence and the Babel criteria. In this paper, we provide a framework that connects these sparsification criteria to the issue of approximating samples, by deriving theoretical bounds on the approximation errors. Moreover, we investigate the error of approximating any feature, by proposing upper-bounds on the approximation error for each of the aforementioned sparsification criteria. Two classes of features are described in detail, the empirical mean and the principal axes in the kernel principal component analysis.\n    ",
        "submission_date": "2014-09-21T00:00:00",
        "last_modified_date": "2014-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7272",
        "title": "Ctrax extensions for tracking in difficult lighting conditions",
        "authors": [
            "Ulrich Stern",
            "Chung-Hui Yang"
        ],
        "abstract": "The fly tracking software Ctrax by Branson et al. is popular for positional tracking of animals both within and beyond the fly community. Ctrax was not designed to handle tracking in difficult lighting conditions with strong shadows or recurring \"on\"/\"off\" changes in lighting - a condition that will likely become increasingly common due to the advent of red-shifted channelrhodopsin. We describe Ctrax extensions we developed that address this problem. The extensions enabled good tracking accuracy in three types of difficult lighting conditions in our lab. Our technique handling shadows relies on \"single animal tracking\"; the other techniques should be widely applicable.\n    ",
        "submission_date": "2014-09-25T00:00:00",
        "last_modified_date": "2014-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7450",
        "title": "Two-stage Geometric Information Guided Image Reconstruction",
        "authors": [
            "Jing Qin",
            "Weihong Guo"
        ],
        "abstract": "In compressive sensing, it is challenging to reconstruct image of high quality from very few noisy linear projections. Existing methods mostly work well on piecewise constant images but not so well on piecewise smooth images such as natural images, medical images that contain a lot of details. We propose a two-stage method called GeoCS to recover images with rich geometric information from very limited amount of noisy measurements. The method adopts the shearlet transform that is mathematically proven to be optimal in sparsely representing images containing anisotropic features such as edges, corners, spikes etc. It also uses the weighted total variation (TV) sparsity with spatially variant weights to preserve sharp edges but to reduce the staircase effects of TV. Geometric information extracted from the results of stage I serves as an initial prior for stage II which alternates image reconstruction and geometric information update in a mutually beneficial way. GeoCS has been tested on incomplete spectral Fourier samples. It is applicable to other types of measurements as well. Experimental results on various complicated images show that GeoCS is efficient and generates high-quality images.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2021-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7480",
        "title": "Generalized Twin Gaussian Processes using Sharma-Mittal Divergence",
        "authors": [
            "Mohamed Elhoseiny",
            "Ahmed Elgammal"
        ],
        "abstract": "There has been a growing interest in mutual information measures due to their wide range of applications in Machine Learning and Computer Vision. In this paper, we present a generalized structured regression framework based on Shama-Mittal divergence, a relative entropy measure, which is introduced to the Machine Learning community in this work. Sharma-Mittal (SM) divergence is a generalized mutual information measure for the widely used R\u00e9nyi, Tsallis, Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we study Sharma-Mittal divergence as a cost function in the context of the Twin Gaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the KL-divergence without computational penalty. We show interesting properties of Sharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing insights in the traditional TGP formulation. However, we generalize this theory based on SM-divergence instead of KL-divergence which is a special case. Experimentally, we evaluated the proposed SMTGP framework on several datasets. The results show that SMTGP reaches better predictions than KL-based TGP, since it offers a bigger class of models through its parameters that we learn from the data.\n    ",
        "submission_date": "2014-09-26T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7787",
        "title": "Audio Surveillance: a Systematic Review",
        "authors": [
            "Marco Crocco",
            "Marco Cristani",
            "Andrea Trucco",
            "Vittorio Murino"
        ],
        "abstract": "Despite surveillance systems are becoming increasingly ubiquitous in our living environment, automated surveillance, currently based on video sensory modality and machine intelligence, lacks most of the time the robustness and reliability required in several real applications. To tackle this issue, audio sensory devices have been taken into account, both alone or in combination with video, giving birth, in the last decade, to a considerable amount of research. In this paper audio-based automated surveillance methods are organized into a comprehensive survey: a general taxonomy, inspired by the more widespread video surveillance field, is proposed in order to systematically describe the methods covering background subtraction, event classification, object tracking and situation analysis. For each of these tasks, all the significant works are reviewed, detailing their pros and cons and the context for which they have been proposed. Moreover, a specific section is devoted to audio features, discussing their expressiveness and their employment in the above described tasks. Differently, from other surveys on audio processing and analysis, the present one is specifically targeted to automated surveillance, highlighting the target applications of each described methods and providing the reader tables and schemes useful to retrieve the most suited algorithms for a specific requirement.\n    ",
        "submission_date": "2014-09-27T00:00:00",
        "last_modified_date": "2014-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7794",
        "title": "Large-scale Online Feature Selection for Ultra-high Dimensional Sparse Data",
        "authors": [
            "Yue Wu",
            "Steven C. H. Hoi",
            "Tao Mei",
            "Nenghai Yu"
        ],
        "abstract": "Feature selection with large-scale high-dimensional data is important yet very challenging in machine learning and data mining. Online feature selection is a promising new paradigm that is more efficient and scalable than batch feature section methods, but the existing online approaches usually fall short in their inferior efficacy as compared with batch approaches. In this paper, we present a novel second-order online feature selection scheme that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to improve the existing first-order online feature selection methods by exploiting second-order information for choosing the subset of important features with high confidence weights. However, unlike many second-order learning methods that often suffer from extra high computational cost, we devise a novel smart algorithm for second-order online feature selection using a MaxHeap-based approach, which is not only more effective than the existing first-order approaches, but also significantly more efficient and scalable for large-scale feature selection with ultra-high dimensional sparse data, as validated from our extensive experiments. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion nonzero features, and 1-million samples), our new algorithm took only 8 minutes on a single PC, which is orders of magnitudes faster than traditional batch approaches. \\url{",
        "submission_date": "2014-09-27T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.7935",
        "title": "Combining human and machine learning for morphological analysis of galaxy images",
        "authors": [
            "Evan Kuminski",
            "Joe George",
            "John Wallin",
            "Lior Shamir"
        ],
        "abstract": "The increasing importance of digital sky surveys collecting many millions of galaxy images has reinforced the need for robust methods that can perform morphological analysis of large galaxy image databases. Citizen science initiatives such as Galaxy Zoo showed that large datasets of galaxy images can be analyzed effectively by non-scientist volunteers, but since databases generated by robotic telescopes grow much faster than the processing power of any group of citizen scientists, it is clear that computer analysis is required. Here we propose to use citizen science data for training machine learning systems, and show experimental results demonstrating that machine learning systems can be trained with citizen science data. Our findings show that the performance of machine learning depends on the quality of the data, which can be improved by using samples that have a high degree of agreement between the citizen scientists. The source code of the method is publicly available.\n    ",
        "submission_date": "2014-09-28T00:00:00",
        "last_modified_date": "2014-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1409.8500",
        "title": "Hyper-Spectral Image Analysis with Partially-Latent Regression and Spatial Markov Dependencies",
        "authors": [
            "Antoine Deleforge",
            "Florence Forbes",
            "Sileye Ba",
            "Radu Horaud"
        ],
        "abstract": "Hyper-spectral data can be analyzed to recover physical properties at large planetary scales. This involves resolving inverse problems which can be addressed within machine learning, with the advantage that, once a relationship between physical parameters and spectra has been established in a data-driven fashion, the learned relationship can be used to estimate physical parameters for new hyper-spectral observations. Within this framework, we propose a spatially-constrained and partially-latent regression method which maps high-dimensional inputs (hyper-spectral images) onto low-dimensional responses (physical parameters such as the local chemical composition of the soil). The proposed regression model comprises two key features. Firstly, it combines a Gaussian mixture of locally-linear mappings (GLLiM) with a partially-latent response model. While the former makes high-dimensional regression tractable, the latter enables to deal with physical parameters that cannot be observed or, more generally, with data contaminated by experimental artifacts that cannot be explained with noise models. Secondly, spatial constraints are introduced in the model through a Markov random field (MRF) prior which provides a spatial structure to the Gaussian-mixture hidden variables. Experiments conducted on a database composed of remotely sensed observations collected from the Mars planet by the Mars Express orbiter demonstrate the effectiveness of the proposed model.\n    ",
        "submission_date": "2014-09-30T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0095",
        "title": "Riemannian Multi-Manifold Modeling",
        "authors": [
            "Xu Wang",
            "Konstantinos Slavakis",
            "Gilad Lerman"
        ],
        "abstract": "This paper advocates a novel framework for segmenting a dataset in a Riemannian manifold $M$ into clusters lying around low-dimensional submanifolds of $M$. Important examples of $M$, for which the proposed clustering algorithm is computationally efficient, are the sphere, the set of positive definite matrices, and the Grassmannian. The clustering problem with these examples of $M$ is already useful for numerous application domains such as action identification in video sequences, dynamic texture clustering, brain fiber segmentation in medical imaging, and clustering of deformed images. The proposed clustering algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering. The intrinsic local geometry is encoded by local sparse coding and more importantly by directional information of local tangent spaces and geodesics. Theoretical guarantees are established for a simplified variant of the algorithm even when the clusters intersect. To avoid complication, these guarantees assume that the underlying submanifolds are geodesic. Extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical model as well as its superior performance over state-of-the-art techniques.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2014-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0210",
        "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.\n    ",
        "submission_date": "2014-10-01T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0719",
        "title": "Proceedings of the second \"international Traveling Workshop on Interactions between Sparse models and Technology\" (iTWIST'14)",
        "authors": [
            "L. Jacques",
            "C. De Vleeschouwer",
            "Y. Boursier",
            "P. Sudhakar",
            "C. De Mol",
            "A. Pizurica",
            "S. Anthoine",
            "P. Vandergheynst",
            "P. Frossard",
            "C. Bilen",
            "S. Kitic",
            "N. Bertin",
            "R. Gribonval",
            "N. Boumal",
            "B. Mishra",
            "P.-A. Absil",
            "R. Sepulchre",
            "S. Bundervoet",
            "C. Schretter",
            "A. Dooms",
            "P. Schelkens",
            "O. Chabiron",
            "F. Malgouyres",
            "J.-Y. Tourneret",
            "N. Dobigeon",
            "P. Chainais",
            "C. Richard",
            "B. Cornelis",
            "I. Daubechies",
            "D. Dunson",
            "M. Dankova",
            "P. Rajmic",
            "K. Degraux",
            "V. Cambareri",
            "B. Geelen",
            "G. Lafruit",
            "G. Setti",
            "J.-F. Determe",
            "J. Louveaux",
            "F. Horlin",
            "A. Dr\u00e9meau",
            "P. Heas",
            "C. Herzet",
            "V. Duval",
            "G. Peyr\u00e9",
            "A. Fawzi",
            "M. Davies",
            "N. Gillis",
            "S. A. Vavasis",
            "C. Soussen",
            "L. Le Magoarou",
            "J. Liang",
            "J. Fadili",
            "A. Liutkus",
            "D. Martina",
            "S. Gigan",
            "L. Daudet",
            "M. Maggioni",
            "S. Minsker",
            "N. Strawn",
            "C. Mory",
            "F. Ngole",
            "J.-L. Starck",
            "I. Loris",
            "S. Vaiter",
            "M. Golbabaee",
            "D. Vukobratovic"
        ],
        "abstract": "The implicit objective of the biennial \"international - Traveling Workshop on Interactions between Sparse models and Technology\" (iTWIST) is to foster collaboration between international scientific teams by disseminating ideas through both specific oral/poster presentations and free discussions. For its second edition, the iTWIST workshop took place in the medieval and picturesque town of Namur in Belgium, from Wednesday August 27th till Friday August 29th, 2014. The workshop was conveniently located in \"The Arsenal\" building within walking distance of both hotels and town center. iTWIST'14 has gathered about 70 international participants and has featured 9 invited talks, 10 oral presentations, and 14 posters on the following themes, all related to the theory, application and generalization of the \"sparsity paradigm\": Sparsity-driven data sensing and processing; Union of low dimensional subspaces; Beyond linear and convex inverse problem; Matrix/manifold/graph sensing/processing; Blind inverse problems and dictionary learning; Sparsity and computational neuroscience; Information theory, geometry and randomness; Complexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?; Sparse machine learning and inference.\n    ",
        "submission_date": "2014-10-02T00:00:00",
        "last_modified_date": "2014-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.0868",
        "title": "Group Orbit Optimization: A Unified Approach to Data Normalization",
        "authors": [
            "Shuchang Zhou",
            "Zhihua Zhang",
            "Xiaobing Feng"
        ],
        "abstract": "In this paper we propose and study an optimization problem over a matrix group orbit that we call \\emph{Group Orbit Optimization} (GOO). We prove that GOO can be used to induce matrix decomposition techniques such as singular value decomposition (SVD), LU decomposition, QR decomposition, Schur decomposition and Cholesky decomposition, etc. This gives rise to a unified framework for matrix decomposition and allows us to bridge these matrix decomposition methods. Moreover, we generalize GOO for tensor decomposition. As a concrete application of GOO, we devise a new data decomposition method over a special linear group to normalize point cloud data. Experiment results show that our normalization method is able to obtain recovery well from distortions like shearing, rotation and squeezing.\n    ",
        "submission_date": "2014-10-03T00:00:00",
        "last_modified_date": "2014-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.1699",
        "title": "Mumford-Shah and Potts Regularization for Manifold-Valued Data with Applications to DTI and Q-Ball Imaging",
        "authors": [
            "Andreas Weinmann",
            "Laurent Demaret",
            "Martin Storath"
        ],
        "abstract": "Mumford-Shah and Potts functionals are powerful variational models for regularization which are widely used in signal and image processing; typical applications are edge-preserving denoising and segmentation. Being both non-smooth and non-convex, they are computationally challenging even for scalar data. For manifold-valued data, the problem becomes even more involved since typical features of vector spaces are not available. In this paper, we propose algorithms for Mumford-Shah and for Potts regularization of manifold-valued signals and images. For the univariate problems, we derive solvers based on dynamic programming combined with (convex) optimization techniques for manifold-valued data. For the class of Cartan-Hadamard manifolds (which includes the data space in diffusion tensor imaging), we show that our algorithms compute global minimizers for any starting point. For the multivariate Mumford-Shah and Potts problems (for image regularization) we propose a splitting into suitable subproblems which we can solve exactly using the techniques developed for the corresponding univariate problems. Our method does not require any a priori restrictions on the edge set and we do not have to discretize the data space. We apply our method to diffusion tensor imaging (DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentation of the corpus callosum.\n    ",
        "submission_date": "2014-10-07T00:00:00",
        "last_modified_date": "2014-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.2167",
        "title": "Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM",
        "authors": [
            "Luigi Nardi",
            "Bruno Bodin",
            "M. Zeeshan Zia",
            "John Mawer",
            "Andy Nisbet",
            "Paul H. J. Kelly",
            "Andrew J. Davison",
            "Mikel Luj\u00e1n",
            "Michael F. P. O'Boyle",
            "Graham Riley",
            "Nigel Topham",
            "Steve Furber"
        ],
        "abstract": "Real-time dense computer vision and SLAM offer great potential for a new level of scene modelling, tracking and real environmental interaction for many types of robot, but their high computational requirements mean that use on mass market embedded platforms is challenging. Meanwhile, trends in low-cost, low-power processing are towards massive parallelism and heterogeneity, making it difficult for robotics and vision researchers to implement their algorithms in a performance-portable way. In this paper we introduce SLAMBench, a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs in performance, accuracy and energy consumption of a dense RGB-D SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP, OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D sequences with trajectory and scene ground truth for reliable accuracy comparison of different implementation and algorithms. We present an analysis and breakdown of the constituent algorithmic elements of KinectFusion, and experimentally investigate their execution time on a variety of multicore and GPUaccelerated platforms. For a popular embedded platform, we also present an analysis of energy efficiency for different configuration alternatives.\n    ",
        "submission_date": "2014-10-08T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3426",
        "title": "Computing Topology Preservation of RBF Transformations for Landmark-Based Image Registration",
        "authors": [
            "R. Cavoretto",
            "A. De Rossi",
            "H. Qiao",
            "B. Quatember",
            "W. Recheis",
            "M. Mayr"
        ],
        "abstract": "In image registration, a proper transformation should be topology preserving. Especially for landmark-based image registration, if the displacement of one landmark is larger enough than those of neighbourhood landmarks, topology violation will be occurred. This paper aim to analyse the topology preservation of some Radial Basis Functions (RBFs) which are used to model deformations in image registration. Mat\u00e9rn functions are quite common in the statistic literature (see, e.g. \\cite{Matern86,Stein99}). In this paper, we use them to solve the landmark-based image registration problem. We present the topology preservation properties of RBFs in one landmark and four landmarks model respectively. Numerical results of three kinds of Mat\u00e9rn transformations are compared with results of Gaussian, Wendland's, and Wu's functions.\n    ",
        "submission_date": "2014-10-13T00:00:00",
        "last_modified_date": "2014-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.3462",
        "title": "Tag Relevance Fusion for Social Image Retrieval",
        "authors": [
            "Xirong Li"
        ],
        "abstract": "Due to the subjective nature of social tagging, measuring the relevance of social tags with respect to the visual content is crucial for retrieving the increasing amounts of social-networked images. Witnessing the limit of a single measurement of tag relevance, we introduce in this paper tag relevance fusion as an extension to methods for tag relevance estimation. We present a systematic study, covering tag relevance fusion in early and late stages, and in supervised and unsupervised settings. Experiments on a large present-day benchmark set show that tag relevance fusion leads to better image retrieval. Moreover, unsupervised tag relevance fusion is found to be practically as effective as supervised tag relevance fusion, but without the need of any training efforts. This finding suggests the potential of tag relevance fusion for real-world deployment.\n    ",
        "submission_date": "2014-10-13T00:00:00",
        "last_modified_date": "2014-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4341",
        "title": "Implicit segmentation of Kannada characters in offline handwriting recognition using hidden Markov models",
        "authors": [
            "Manasij Venkatesh",
            "Vikas Majjagi",
            "Deepu Vijayasenan"
        ],
        "abstract": "We describe a method for classification of handwritten Kannada characters using Hidden Markov Models (HMMs). Kannada script is agglutinative, where simple shapes are concatenated horizontally to form a character. This results in a large number of characters making the task of classification difficult. Character segmentation plays a significant role in reducing the number of classes. Explicit segmentation techniques suffer when overlapping shapes are present, which is common in the case of handwritten text. We use HMMs to take advantage of the agglutinative nature of Kannada script, which allows us to perform implicit segmentation of characters along with recognition. All the experiments are performed on the Chars74k dataset that consists of 657 handwritten characters collected across multiple users. Gradient-based features are extracted from individual characters and are used to train character HMMs. The use of implicit segmentation technique at the character level resulted in an improvement of around 10%. This system also outperformed an existing system tested on the same dataset by around 16%. Analysis based on learning curves showed that increasing the training data could result in better accuracy. Accordingly, we collected additional data and obtained an improvement of 4% with 6 additional samples.\n    ",
        "submission_date": "2014-10-16T00:00:00",
        "last_modified_date": "2014-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.4871",
        "title": "Bayesian estimation of the multifractality parameter for image texture using a Whittle approximation",
        "authors": [
            "S\u00e9bastien Combrexelle",
            "Herwig Wendt",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret",
            "Steve McLaughlin",
            "Patrice Abry"
        ],
        "abstract": "Texture characterization is a central element in many image processing applications. Multifractal analysis is a useful signal and image processing tool, yet, the accurate estimation of multifractal parameters for image texture remains a challenge. This is due in the main to the fact that current estimation procedures consist of performing linear regressions across frequency scales of the two-dimensional (2D) dyadic wavelet transform, for which only a few such scales are computable for images. The strongly non-Gaussian nature of multifractal processes, combined with their complicated dependence structure, makes it difficult to develop suitable models for parameter estimation. Here, we propose a Bayesian procedure that addresses the difficulties in the estimation of the multifractality parameter. The originality of the procedure is threefold: The construction of a generic semi-parametric statistical model for the logarithm of wavelet leaders; the formulation of Bayesian estimators that are associated with this model and the set of parameter values admitted by multifractal theory; the exploitation of a suitable Whittle approximation within the Bayesian model which enables the otherwise infeasible evaluation of the posterior distribution associated with the model. Performance is assessed numerically for several 2D multifractal processes, for several image sizes and a large range of process parameters. The procedure yields significant benefits over current benchmark estimators in terms of estimation performance and ability to discriminate between the two most commonly used classes of multifractal process models. The gains in performance are particularly pronounced for small image sizes, notably enabling for the first time the analysis of image patches as small as 64x64 pixels.\n    ",
        "submission_date": "2014-10-17T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.6996",
        "title": "Improved depth imaging by constrained full-waveform inversion",
        "authors": [
            "Musa Maharramov",
            "Biondo Biondi"
        ],
        "abstract": "We propose a formulation of full-wavefield inversion (FWI) as a constrained optimization problem, and describe a computationally efficient technique for solving constrained full-wavefield inversion (CFWI). The technique is based on using a total-variation regularization method, with the regularization weighted in favor of constraining deeper subsurface model sections. The method helps to promote \"edge-preserving\" blocky model inversion where fitting the seismic data alone fails to adequately constrain the model. The method is demonstrated on synthetic datasets with added noise, and is shown to enhance the sharpness of the inverted model and correctly reposition mispositioned reflectors by better constraining the velocity model at depth.\n    ",
        "submission_date": "2014-10-26T00:00:00",
        "last_modified_date": "2014-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7100",
        "title": "Estimating the intrinsic dimension in fMRI space via dataset fractal analysis - Counting the `cpu cores' of the human brain",
        "authors": [
            "Harris V. Georgiou"
        ],
        "abstract": "Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool for localizing and analyzing brain activity. This study focuses on one very important aspect of the functional properties of human brain, specifically the estimation of the level of parallelism when performing complex cognitive tasks. Using fMRI as the main modality, the human brain activity is investigated through a purely data-driven signal processing and dimensionality analysis approach. Specifically, the fMRI signal is treated as a multi-dimensional data space and its intrinsic `complexity' is studied via dataset fractal analysis and blind-source separation (BSS) methods. One simulated and two real fMRI datasets are used in combination with Independent Component Analysis (ICA) and fractal analysis for estimating the intrinsic (true) dimensionality, in order to provide data-driven experimental evidence on the number of independent brain processes that run in parallel when visual or visuo-motor tasks are performed. Although this number is can not be defined as a strict threshold but rather as a continuous range, when a specific activation level is defined, a corresponding number of parallel processes or the casual equivalent of `cpu cores' can be detected in normal human brain activity.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2014-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7328",
        "title": "Exact Expression For Information Distance",
        "authors": [
            "P.M.B. Vitanyi"
        ],
        "abstract": "Information distance can be defined not only between two strings but also in a finite multiset of strings of cardinality greater than two. We give an elementary proof for expressing the information distance in terms of plain Kolmogorov complexity. It is exact since for each cardinality of the multiset the lower bound for some multiset equals the upper bound for all multisets up to a constant additive term.\n    ",
        "submission_date": "2014-10-27T00:00:00",
        "last_modified_date": "2017-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7613",
        "title": "A Short Image Series Based Scheme for Time Series Digital Image Correlation",
        "authors": [
            "Xian Wang",
            "Shaopeng Ma"
        ],
        "abstract": "A new scheme for digital image correlation, i.e., short time series DIC (STS-DIC) is proposed. Instead of processing the original deformed speckle images individually, STS-DIC combines several adjacent deformed speckle images from a short time series and then processes the averaged image, for which deformation continuity over time is introduced. The deformation of several adjacent images is assumed to be linear in time and a new spatial-temporal displacement representation method with eight unknowns is presented based on the subset-based representation method. Then, the model of STS-DIC is created and a solving scheme is developed based on the Newton-Raphson iteration. The proposed method is verified for numerical and experimental cases. The results show that the proposed STS-DIC greatly improves the accuracy of traditional DIC, both under simple and complicated deformation conditions, while retaining acceptable actual computational cost.\n    ",
        "submission_date": "2014-10-28T00:00:00",
        "last_modified_date": "2014-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.7795",
        "title": "Classification of Autism Spectrum Disorder Using Supervised Learning of Brain Connectivity Measures Extracted from Synchrostates",
        "authors": [
            "Wasifa Jamal",
            "Saptarshi Das",
            "Ioana-Anastasia Oprescu",
            "Koushik Maharatna",
            "Fabio Apicella",
            "Federico Sicca"
        ],
        "abstract": "Objective. The paper investigates the presence of autism using the functional brain connectivity measures derived from electro-encephalogram (EEG) of children during face perception tasks. Approach. Phase synchronized patterns from 128-channel EEG signals are obtained for typical children and children with autism spectrum disorder (ASD). The phase synchronized states or synchrostates temporally switch amongst themselves as an underlying process for the completion of a particular cognitive task. We used 12 subjects in each group (ASD and typical) for analyzing their EEG while processing fearful, happy and neutral faces. The minimal and maximally occurring synchrostates for each subject are chosen for extraction of brain connectivity features, which are used for classification between these two groups of subjects. Among different supervised learning techniques, we here explored the discriminant analysis and support vector machine both with polynomial kernels for the classification task. Main results. The leave one out cross-validation of the classification algorithm gives 94.7% accuracy as the best performance with corresponding sensitivity and specificity values as 85.7% and 100% respectively. Significance. The proposed method gives high classification accuracies and outperforms other contemporary research results. The effectiveness of the proposed method for classification of autistic and typical children suggests the possibility of using it on a larger population to validate it for clinical practice.\n    ",
        "submission_date": "2014-10-20T00:00:00",
        "last_modified_date": "2014-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1410.8027",
        "title": "Towards a Visual Turing Challenge",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.\n    ",
        "submission_date": "2014-10-29T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0161",
        "title": "Entropy of Overcomplete Kernel Dictionaries",
        "authors": [
            "Paul Honeine"
        ],
        "abstract": "In signal analysis and synthesis, linear approximation theory considers a linear decomposition of any given signal in a set of atoms, collected into a so-called dictionary. Relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms, yielding overcomplete dictionaries with an extended number of atoms. More generally than the linear decomposition, overcomplete kernel dictionaries provide an elegant nonlinear extension by defining the atoms through a mapping kernel function (e.g., the gaussian kernel). Models based on such kernel dictionaries are used in neural networks, gaussian processes and online learning with kernels.\n",
        "submission_date": "2014-11-01T00:00:00",
        "last_modified_date": "2014-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0296",
        "title": "Geodesic Exponential Kernels: When Curvature and Linearity Conflict",
        "authors": [
            "Aasa Feragen",
            "Francois Lauze",
            "S\u00f8ren Hauberg"
        ],
        "abstract": "We consider kernel methods on general geodesic metric spaces and provide both negative and positive results. First we show that the common Gaussian kernel can only be generalized to a positive definite kernel on a geodesic metric space if the space is flat. As a result, for data on a Riemannian manifold, the geodesic Gaussian kernel is only positive definite if the Riemannian manifold is Euclidean. This implies that any attempt to design geodesic Gaussian kernels on curved Riemannian manifolds is futile. However, we show that for spaces with conditionally negative definite distances the geodesic Laplacian kernel can be generalized while retaining positive definiteness. This implies that geodesic Laplacian kernels can be generalized to some curved spaces, including spheres and hyperbolic spaces. Our theoretical results are verified empirically.\n    ",
        "submission_date": "2014-11-02T00:00:00",
        "last_modified_date": "2014-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0763",
        "title": "A Weighted Common Subgraph Matching Algorithm",
        "authors": [
            "Xu Yang",
            "Hong Qiao",
            "Zhi-Yong Liu"
        ],
        "abstract": "We propose a weighted common subgraph (WCS) matching algorithm to find the most similar subgraphs in two labeled weighted graphs. WCS matching, as a natural generalization of the equal-sized graph matching or subgraph matching, finds wide applications in many computer vision and machine learning tasks. In this paper, the WCS matching is first formulated as a combinatorial optimization problem over the set of partial permutation matrices. Then it is approximately solved by a recently proposed combinatorial optimization framework - Graduated NonConvexity and Concavity Procedure (GNCCP). Experimental comparisons on both synthetic graphs and real world images validate its robustness against noise level, problem size, outlier number, and edge density.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0802",
        "title": "Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery",
        "authors": [
            "Lu Ma",
            "Mahsa Ghafarianzadeh",
            "Dave Coleman",
            "Nikolaus Correll",
            "Gabe Sibley"
        ],
        "abstract": "We present an unsupervised framework for simultaneous appearance-based object discovery, detection, tracking and reconstruction using RGBD cameras and a robot manipulator. The system performs dense 3D simultaneous localization and mapping concurrently with unsupervised object discovery. Putative objects that are spatially and visually coherent are manipulated by the robot to gain additional motion-cues. The robot uses appearance alone, followed by structure and motion cues, to jointly discover, verify, learn and improve models of objects. Induced motion segmentation reinforces learned models which are represented implicitly as 2D and 3D level sets to capture both shape and appearance. We compare three different approaches for appearance-based object discovery and find that a novel form of spatio-temporal super-pixels gives the highest quality candidate object models in terms of precision and recall. Live experiments with a Baxter robot demonstrate a holistic pipeline capable of automatic discovery, verification, detection, tracking and reconstruction of unknown objects.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.0814",
        "title": "A random algorithm for low-rank decomposition of large-scale matrices with missing entries",
        "authors": [
            "Yiguang Liu"
        ],
        "abstract": "A Random SubMatrix method (RSM) is proposed to calculate the low-rank decomposition of large-scale matrices with known entry percentage \\rho. RSM is very fast as the floating-point operations (flops) required are compared favorably with the state-of-the-art algorithms. Meanwhile RSM is very memory-saving. With known entries homogeneously distributed in the given matrix, sub-matrices formed by known entries are randomly selected. According to the just proved theorem that subspace related to smaller singular values is less perturbed by noise, the null vectors or the right singular vectors associated with the minor singular values are calculated for each submatrix. The vectors are the null vectors of the corresponding submatrix in the ground truth of the given large-scale matrix. If enough sub-matrices are randomly chosen, the low-rank decomposition is estimated. The experimental results on random synthetical matrices with sizes such as 131072X1024 and on real data sets indicate that RSM is much faster and memory-saving, and, meanwhile, has considerable high precision achieving or approximating to the best.\n    ",
        "submission_date": "2014-11-04T00:00:00",
        "last_modified_date": "2014-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1537",
        "title": "Large-Margin Determinantal Point Processes",
        "authors": [
            "Boqing Gong",
            "Wei-lun Chao",
            "Kristen Grauman",
            "Fei Sha"
        ],
        "abstract": "Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1668",
        "title": "On Chord and Sagitta in ${\\mathbb Z}^2$: An Analysis towards Fast and Robust Circular Arc Detection",
        "authors": [
            "Sahadev Bera",
            "Shyamosree Pal",
            "Partha Bhowmick",
            "Bhargab B. Bhattacharya"
        ],
        "abstract": "Although chord and sagitta, when considered in tandem, may reflect many underlying geometric properties of circles on the Euclidean plane, their implications on the digital plane are not yet well-understood. In this paper, we explore some of their fundamental properties on the digital plane that have a strong bearing on the unsupervised detection of circles and circular arcs in a digital image. We show that although the chord-and-sagitta properties of a real circle do not readily migrate to the digital plane, they can indeed be used for the analysis in the discrete domain based on certain bounds on their deviations, which are derived from the real domain. In particular, we derive an upper bound on the circumferential angular deviation of a point in the context of chord property, and an upper bound on the relative error in radius estimation with regard to the sagitta property. Using these two bounds, we design a novel algorithm for the detection and parameterization of circles and circular arcs, which does not require any heuristic initialization or manual tuning. The chord property is deployed for the detection of circular arcs, whereas the sagitta property is used to estimate their centers and radii. Finally, to improve the accuracy of estimation, the notion of restricted Hough transform is used. Experimental results demonstrate superior efficiency and robustness of the proposed methodology compared to existing techniques.\n    ",
        "submission_date": "2014-10-26T00:00:00",
        "last_modified_date": "2014-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1752",
        "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets",
        "authors": [
            "Adarsh Prasad",
            "Stefanie Jegelka",
            "Dhruv Batra"
        ],
        "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.1784",
        "title": "Conditional Generative Adversarial Nets",
        "authors": [
            "Mehdi Mirza",
            "Simon Osindero"
        ],
        "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.\n    ",
        "submission_date": "2014-11-06T00:00:00",
        "last_modified_date": "2014-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.2539",
        "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
        "authors": [
            "Ryan Kiros",
            "Ruslan Salakhutdinov",
            "Richard S. Zemel"
        ],
        "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.\n    ",
        "submission_date": "2014-11-10T00:00:00",
        "last_modified_date": "2014-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3525",
        "title": "Gaze Stabilization for Humanoid Robots: a Comprehensive Framework",
        "authors": [
            "Alessandro Roncone",
            "Ugo Pattacini",
            "Giorgio Metta",
            "Lorenzo Natale"
        ],
        "abstract": "Gaze stabilization is an important requisite for humanoid robots. Previous work on this topic has focused on the integration of inertial and visual information. Little attention has been given to a third component, which is the knowledge that the robot has about its own movement. In this work we propose a comprehensive framework for gaze stabilization in a humanoid robot. We focus on the problem of compensating for disturbances induced in the cameras due to self-generated movements of the robot. In this work we employ two separate signals for stabilization: (1) an anticipatory term obtained from the velocity commands sent to the joints while the robot moves autonomously; (2) a feedback term from the on board gyroscope, which compensates unpredicted external disturbances. We first provide the mathematical formulation to derive the forward and the differential kinematics of the fixation point of the stereo system. We finally test our method on the iCub robot. We show that the stabilization consistently reduces the residual optical flow during the movement of the robot and in presence of external disturbances. We also demonstrate that proper integration of the neck DoF is crucial to achieve correct stabilization.\n    ",
        "submission_date": "2014-11-13T00:00:00",
        "last_modified_date": "2014-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.3815",
        "title": "Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction",
        "authors": [
            "Mingmin Zhao",
            "Chengxu Zhuang",
            "Yizhou Wang",
            "Tai Sing Lee"
        ],
        "abstract": "We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthesis process in a predictive coding framework. The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes. In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects. We show that our model can outperform state-of-art performances of gated Boltzmann machines (GBM) in estimation of contextual information. Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information. We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames, a function that is lacking in GBM.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4046",
        "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy",
        "authors": [
            "Mohammad Ali Keyvanrad",
            "Mohammad Mehdi Homayounpour"
        ],
        "abstract": "Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at \"",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4080",
        "title": "6 Seconds of Sound and Vision: Creativity in Micro-Videos",
        "authors": [
            "Miriam Redi",
            "Neil O Hare",
            "Rossano Schifanella",
            "Michele Trevisiol",
            "Alejandro Jaimes"
        ],
        "abstract": "The notion of creativity, as opposed to related concepts such as beauty or interestingness, has not been studied from the perspective of automatic analysis of multimedia content. Meanwhile, short online videos shared on social media platforms, or micro-videos, have arisen as a new medium for creative expression. In this paper we study creative micro-videos in an effort to understand the features that make a video creative, and to address the problem of automatic detection of creative content. Defining creative videos as those that are novel and have aesthetic value, we conduct a crowdsourcing experiment to create a dataset of over 3,800 micro-videos labelled as creative and non-creative. We propose a set of computational features that we map to the components of our definition of creativity, and conduct an analysis to determine which of these features correlate most with creative video. Finally, we evaluate a supervised approach to automatically detect creative video, with promising results, showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy.\n    ",
        "submission_date": "2014-11-14T00:00:00",
        "last_modified_date": "2014-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4101",
        "title": "Deep Deconvolutional Networks for Scene Parsing",
        "authors": [
            "Rahul Mohan"
        ],
        "abstract": "Scene parsing is an important and challenging prob- lem in computer vision. It requires labeling each pixel in an image with the category it belongs to. Tradition- ally, it has been approached with hand-engineered features from color information in images. Recently convolutional neural networks (CNNs), which automatically learn hierar- chies of features, have achieved record performance on the task. These approaches typically include a post-processing technique, such as superpixels, to produce the final label- ing. In this paper, we propose a novel network architecture that combines deep deconvolutional neural networks with CNNs. Our experiments show that deconvolutional neu- ral networks are capable of learning higher order image structure beyond edge primitives in comparison to CNNs. The new network architecture is employed for multi-patch training, introduced as part of this work. Multi-patch train- ing makes it possible to effectively learn spatial priors from scenes. The proposed approach yields state-of-the-art per- formance on four scene parsing datasets, namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our system has the added advantage of having a training system that can be completely automated end-to-end with- out requiring any post-processing.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.4114",
        "title": "Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading",
        "authors": [
            "Ha Jong Won",
            "Li Gwang Chol",
            "Kim Hyok Chol",
            "Li Kum Song"
        ],
        "abstract": "In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency.\n    ",
        "submission_date": "2014-11-15T00:00:00",
        "last_modified_date": "2014-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5307",
        "title": "Efficient Media Retrieval from Non-Cooperative Queries",
        "authors": [
            "Kevin Shih",
            "Wei Di",
            "Vignesh Jagadeesh",
            "Robinson Piramuthu"
        ],
        "abstract": "Text is ubiquitous in the artificial world and easily attainable when it comes to book title and author names. Using the images from the book cover set from the Stanford Mobile Visual Search dataset and additional book covers and metadata from ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.5340",
        "title": "Affordances Provide a Fundamental Categorization Principle for Visual Scenes",
        "authors": [
            "Michelle R. Greene",
            "Christopher Baldassano",
            "Andre Esteva",
            "Diane M. Beck",
            "Li Fei-Fei"
        ],
        "abstract": "How do we know that a kitchen is a kitchen by looking? Relatively little is known about how we conceptualize and categorize different visual environments. Traditional models of visual perception posit that scene categorization is achieved through the recognition of a scene's objects, yet these models cannot account for the mounting evidence that human observers are relatively insensitive to the local details in an image. Psychologists have long theorized that the affordances, or actionable possibilities of a stimulus are pivotal to its perception. To what extent are scene categories created from similar affordances? Using a large-scale experiment using hundreds of scene categories, we show that the activities afforded by a visual scene provide a fundamental categorization principle. Affordance-based similarity explained the majority of the structure in the human scene categorization patterns, outperforming alternative similarities based on objects or visual features. We all models were combined, affordances provided the majority of the predictive power in the combined model, and nearly half of the total explained variance is captured only by affordances. These results challenge many existing models of high-level visual perception, and provide immediately testable hypotheses for the functional organization of the human perceptual system.\n    ",
        "submission_date": "2014-11-19T00:00:00",
        "last_modified_date": "2014-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6326",
        "title": "Vision and Learning for Deliberative Monocular Cluttered Flight",
        "authors": [
            "Debadeepta Dey",
            "Kumar Shaurya Shankar",
            "Sam Zeng",
            "Rupesh Mehta",
            "M. Talha Agcayazi",
            "Christopher Eriksen",
            "Shreyansh Daftry",
            "Martial Hebert",
            "J. Andrew Bagnell"
        ],
        "abstract": "Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.\n    ",
        "submission_date": "2014-11-24T00:00:00",
        "last_modified_date": "2014-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.6880",
        "title": "An Automated Images-to-Graphs Framework for High Resolution Connectomics",
        "authors": [
            "William Gray Roncal",
            "Dean M. Kleissas",
            "Joshua T. Vogelstein",
            "Priya Manavalan",
            "Kunal Lillaney",
            "Michael Pekala",
            "Randal Burns",
            "R. Jacob Vogelstein",
            "Carey E. Priebe",
            "Mark A. Chevillet",
            "Gregory D. Hager"
        ],
        "abstract": "Reconstructing a map of neuronal connectivity is a critical challenge in contemporary neuroscience. Recent advances in high-throughput serial section electron microscopy (EM) have produced massive 3D image volumes of nanoscale brain tissue for the first time. The resolution of EM allows for individual neurons and their synaptic connections to be directly observed. Recovering neuronal networks by manually tracing each neuronal process at this scale is unmanageable, and therefore researchers are developing automated image processing modules. Thus far, state-of-the-art algorithms focus only on the solution to a particular task (e.g., neuron segmentation or synapse identification).\n",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7445",
        "title": "Bi-objective Optimization for Robust RGB-D Visual Odometry",
        "authors": [
            "Tao Han",
            "Chao Xu",
            "Ryan Loxton",
            "Lei Xie"
        ],
        "abstract": "This paper considers a new bi-objective optimization formulation for robust RGB-D visual odometry. We investigate two methods for solving the proposed bi-objective optimization problem: the weighted sum method (in which the objective functions are combined into a single objective function) and the bounded objective method (in which one of the objective functions is optimized and the value of the other objective function is bounded via a constraint). Our experimental results for the open source TUM RGB-D dataset show that the new bi-objective optimization formulation is superior to several existing RGB-D odometry methods. In particular, the new formulation yields more accurate motion estimates and is more robust when textural or structural features in the image sequence are lacking.\n    ",
        "submission_date": "2014-11-27T00:00:00",
        "last_modified_date": "2014-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1411.7889",
        "title": "Open-source code for manifold-based 3D rotation recovery of X-ray scattering patterns",
        "authors": [
            "Aliakbar Jafarpour"
        ],
        "abstract": "Single particle 3D imaging with ultrashort X-ray laser pulses is based on collecting and combining the information content of 2D scattering patterns of an object at different orientations. Typical sample-delivery schemes leave little or no room for controlling the orientations. As such, the orientation associated with a given snapshot should be estimated after the experiment. Here we present an open-source code for the most rigorous technique having been reported in this context. Some practical issues along with proposed solutions are also discussed.\n    ",
        "submission_date": "2014-11-26T00:00:00",
        "last_modified_date": "2014-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0218",
        "title": "Simple pairs of points in digital spaces. Topology-preserving transformations of digital spaces by contracting simple pairs of points",
        "authors": [
            "Alexander V. Evako"
        ],
        "abstract": "Transformations of digital spaces preserving local and global topology play an important role in thinning, skeletonization and simplification of digital images. In the present paper, we introduce and study contractions of simple pair of points based on the notions of a digital contractible space and contractible transformations of digital spaces. We show that the contraction of a simple pair of points preserves local and global topology of a digital space. Relying on the obtained results, we study properties if digital manifolds. In particular, we show that a digital n-manifold can be transformed to its compressed form with the minimal number of points by sequential contractions of simple pairs.\n",
        "submission_date": "2014-11-30T00:00:00",
        "last_modified_date": "2014-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.0614",
        "title": "Classification and Reconstruction of High-Dimensional Signals from Low-Dimensional Features in the Presence of Side Information",
        "authors": [
            "Francesco Renna",
            "Liming Wang",
            "Xin Yuan",
            "Jianbo Yang",
            "Galen Reeves",
            "Robert Calderbank",
            "Lawrence Carin",
            "Miguel R. D. Rodrigues"
        ],
        "abstract": "This paper offers a characterization of fundamental limits on the classification and reconstruction of high-dimensional signals from low-dimensional features, in the presence of side information. We consider a scenario where a decoder has access both to linear features of the signal of interest and to linear features of the side information signal; while the side information may be in a compressed form, the objective is recovery or classification of the primary signal, not the side information. The signal of interest and the side information are each assumed to have (distinct) latent discrete labels; conditioned on these two labels, the signal of interest and side information are drawn from a multivariate Gaussian distribution. With joint probabilities on the latent labels, the overall signal-(side information) representation is defined by a Gaussian mixture model. We then provide sharp sufficient and/or necessary conditions for these quantities to approach zero when the covariance matrices of the Gaussians are nearly low-rank. These conditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv conditions, are a function of the number of linear features extracted from the signal of interest, the number of linear features extracted from the side information signal, and the geometry of these signals and their interplay. Moreover, on assuming that the signal of interest and the side information obey such an approximately low-rank model, we derive expansions of the reconstruction error as a function of the deviation from an exactly low-rank model; such expansions also allow identification of operational regimes where the impact of side information on signal reconstruction is most relevant. Our framework, which offers a principled mechanism to integrate side information in high-dimensional data problems, is also tested in the context of imaging applications.\n    ",
        "submission_date": "2014-12-01T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1219",
        "title": "Colorisation et texturation temps r\u00e9el d'environnements urbains par syst\u00e8me mobile avec scanner laser et cam\u00e9ra fish-eye",
        "authors": [
            "Jean-Emmanuel Deschaud",
            "Xavier Brun",
            "Fran\u00e7ois Goulette"
        ],
        "abstract": "We present here a real time mobile mapping system mounted on a vehicle. The terrestrial acquisition system is based on a geolocation system and two sensors, namely, a laser scanner and a camera with a fish-eye lens. We produce 3D colored points cloud and textured models of the environment. Once the system has been calibrated, the data acquisition and processing are done \"on the way\". This article mainly presents our methods of colorization of point cloud, triangulation and texture mapping.\n    ",
        "submission_date": "2014-12-03T00:00:00",
        "last_modified_date": "2014-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1732",
        "title": "Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey",
        "authors": [
            "Hao Zhang",
            "Jing Wang",
            "Jianhua Ma",
            "Hongbing Lu",
            "Zhengrong Liang"
        ],
        "abstract": "Statistical image reconstruction (SIR) methods have shown potential to substantially improve the image quality of low-dose X-ray computed tomography (CT) as compared to the conventional filtered back-projection (FBP) method for various clinical tasks. According to the maximum a posterior (MAP) estimation, the SIR methods can be typically formulated by an objective function consisting of two terms: (1) data-fidelity (or equivalently, data-fitting or data-mismatch) term modeling the statistics of projection measurements, and (2) regularization (or equivalently, prior or penalty) term reflecting prior knowledge or expectation on the characteristics of the image to be reconstructed. Existing SIR methods for low-dose CT can be divided into two groups: (1) those that use calibrated transmitted photon counts (before log-transform) with penalized maximum likelihood (pML) criterion, and (2) those that use calibrated line-integrals (after log-transform) with penalized weighted least-squares (PWLS) criterion. Accurate statistical modeling of the projection measurements is a prerequisite for SIR, while the regularization term in the objective function also plays a critical role for successful image reconstruction. This paper reviews several statistical models on CT projection measurements and various regularization strategies incorporating prior knowledge or expected properties of the image to be reconstructed, which together formulate the objective function of the SIR methods for low-dose X-ray CT.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1740",
        "title": "Image Data Compression for Covariance and Histogram Descriptors",
        "authors": [
            "Matt J. Kusner",
            "Nicholas I. Kolkin",
            "Stephen Tyree",
            "Kilian Q. Weinberger"
        ],
        "abstract": "Covariance and histogram image descriptors provide an effective way to capture information about images. Both excel when used in combination with special purpose distance metrics. For covariance descriptors these metrics measure the distance along the non-Euclidean Riemannian manifold of symmetric positive definite matrices. For histogram descriptors the Earth Mover's distance measures the optimal transport between two histograms. Although more precise, these distance metrics are very expensive to compute, making them impractical in many applications, even for data sets of only a few thousand examples. In this paper we present two methods to compress the size of covariance and histogram datasets with only marginal increases in test error for k-nearest neighbor classification. Specifically, we show that we can reduce data sets to 16% and in some cases as little as 2% of their original size, while approximately matching the test error of kNN classification on the full training set. In fact, because the compressed set is learned in a supervised fashion, it sometimes even outperforms the full data set, while requiring only a fraction of the space and drastically reducing test-time computation.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2015-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.1871",
        "title": "A higher homotopic extension of persistent (co)homology",
        "authors": [
            "Estanislao Herscovich"
        ],
        "abstract": "Our objective in this article is to show a possibly interesting structure of homotopic nature appearing in persistent (co)homology. Assuming that the filtration of the (say) simplicial set embedded in a finite dimensional vector space induces a multiplicative filtration (which would not be a so harsh hypothesis in our setting) on the dg algebra given by the complex of simplicial cochains, we may use a result by T. Kadeishvili to get a unique (up to noncanonical equivalence) A_infinity-algebra structure on the complete persistent cohomology of the filtered simplicial (or topological) set. We then provide a construction of a (pseudo)metric on the set of all (generalized) barcodes (that is, of all cohomological degrees) enriched with the A_infinity-algebra structure stated before, refining the usual bottleneck metric, and which is also independent of the particular A_infinity-algebra structure chosen (among those equivalent to each other). We think that this distance might deserve some attention for topological data analysis, for it in particular can recognize different linking or foldings patterns, as in the Borromean rings. As an aside, we give a simple proof of a result relating the barcode structure between persistent homology and cohomology. This result was observed in a recent article by V. de Silva, D. Morozov and M. Vejdemo-Johansson under some restricted assumptions, which we do not suppose.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2014-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2032",
        "title": "On using the Microsoft Kinect$^{\\rm TM}$ sensors in the analysis of human motion",
        "authors": [
            "M.J. Malinowski",
            "E. Matsinos",
            "S. Roth"
        ],
        "abstract": "The present paper aims at providing the theoretical background required for investigating the use of the Microsoft Kinect$^{\\rm TM}$ (`Kinect', for short) sensors (original and upgraded) in the analysis of human motion. Our methodology is developed in such a way that its application be easily adaptable to comparative studies of other systems used in capturing human-motion data. Our future plans include the application of this methodology to two situations: first, in a comparative study of the performance of the two Kinect sensors; second, in pursuing their validation on the basis of comparisons with a marker-based system (MBS). One important feature in our approach is the transformation of the MBS output into Kinect-output format, thus enabling the analysis of the measurements, obtained from different systems, with the same software application, i.e., the one we use in the analysis of Kinect-captured data; one example of such a transformation, for one popular marker-placement scheme (`Plug-in Gait'), is detailed. We propose that the similarity of the output, obtained from the different systems, be assessed on the basis of the comparison of a number of waveforms, representing the variation within the gait cycle of quantities which are commonly used in the modelling of the human motion. The data acquisition may involve commercially-available treadmills and a number of velocity settings: for instance, walking-motion data may be acquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommend that particular attention be called to systematic effects associated with the subject's knee and lower leg, as well as to the ability of the Kinect sensors in reliably capturing the details in the asymmetry of the motion for the left and right parts of the human body. The previous versions of the study have been withdrawn due to the use of a non-representative database.\n    ",
        "submission_date": "2014-12-04T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2291",
        "title": "Adjusted least squares fitting of algebraic hypersurfaces",
        "authors": [
            "Konstantin Usevich",
            "Ivan Markovsky"
        ],
        "abstract": "We consider the problem of fitting a set of points in Euclidean space by an algebraic hypersurface. We assume that points on a true hypersurface, described by a polynomial equation, are corrupted by zero mean independent Gaussian noise, and we estimate the coefficients of the true polynomial equation. The adjusted least squares estimator accounts for the bias present in the ordinary least squares estimator. The adjusted least squares estimator is based on constructing a quasi-Hankel matrix, which is a bias-corrected matrix of moments. For the case of unknown noise variance, the estimator is defined as a solution of a polynomial eigenvalue problem. In this paper, we present new results on invariance properties of the adjusted least squares estimator and an improved algorithm for computing the estimator for an arbitrary set of monomials in the polynomial equation.\n    ",
        "submission_date": "2014-12-06T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2309",
        "title": "Visual Causal Feature Learning",
        "authors": [
            "Krzysztof Chalupka",
            "Pietro Perona",
            "Frederick Eberhardt"
        ],
        "abstract": "We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort. The theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with, but may not cause, the target behavior. Finally, we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. We illustrate our inference and learning algorithms in experiments based on both synthetic and real data.\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2672",
        "title": "When Computer Vision Gazes at Cognition",
        "authors": [
            "Tao Gao",
            "Daniel Harari",
            "Joshua Tenenbaum",
            "Shimon Ullman"
        ],
        "abstract": "Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8\u00b0-10\u00b0 of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2684",
        "title": "HyperSpectral classification with adaptively weighted L1-norm regularization and spatial postprocessing",
        "authors": [
            "Victor Stefan Aldea"
        ],
        "abstract": "Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using an RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. It is shown that the proposed method is competitive with state of the art algorithms such as SVM-CK, KSOMP-CK and KSSP-CK.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2018-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.2700",
        "title": "Subspace based low rank and joint sparse matrix recovery",
        "authors": [
            "Sampurna Biswas",
            "Sunrita Poddar",
            "Soura Dasgupta",
            "Raghuraman Mudumbai",
            "Mathews Jacob"
        ],
        "abstract": "We consider the recovery of a low rank and jointly sparse matrix from under sampled measurements of its columns. This problem is highly relevant in the recovery of dynamic MRI data with high spatio-temporal resolution, where each column of the matrix corresponds to a frame in the image time series; the matrix is highly low-rank since the frames are highly correlated. Similarly the non-zero locations of the matrix in appropriate transform/frame domains (e.g. wavelet, gradient) are roughly the same in different frame. The superset of the support can be safely assumed to be jointly sparse. Unlike the classical multiple measurement vector (MMV) setup that measures all the snapshots using the same matrix, we consider each snapshot to be measured using a different measurement matrix. We show that this approach reduces the total number of measurements, especially when the rank of the matrix is much smaller than than its sparsity. Our experiments in the context of dynamic imaging shows that this approach is very useful in realizing free breathing cardiac MRI.\n    ",
        "submission_date": "2014-12-05T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3128",
        "title": "Real-Time Grasp Detection Using Convolutional Neural Networks",
        "authors": [
            "Joseph Redmon",
            "Anelia Angelova"
        ],
        "abstract": "We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.\n    ",
        "submission_date": "2014-12-09T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3919",
        "title": "Machine Learning for Neuroimaging with Scikit-Learn",
        "authors": [
            "Alexandre Abraham",
            "Fabian Pedregosa",
            "Michael Eickenberg",
            "Philippe Gervais",
            "Andreas Muller",
            "Jean Kossaifi",
            "Alexandre Gramfort",
            "Bertrand Thirion",
            "G\u00e4el Varoquaux"
        ],
        "abstract": "Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g. multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g. resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.3925",
        "title": "Region segmentation for sparse decompositions: better brain parcellations from rest fMRI",
        "authors": [
            "Alexandre Abraham",
            "Elvis Dohmatob",
            "Bertrand Thirion",
            "Dimitris Samaras",
            "Gael Varoquaux"
        ],
        "abstract": "Functional Magnetic Resonance Images acquired during resting-state provide information about the functional organization of the brain through measuring correlations between brain areas. Independent components analysis is the reference approach to estimate spatial components from weakly structured data such as brain signal time courses; each of these components may be referred to as a brain network and the whole set of components can be conceptualized as a brain functional atlas. Recently, new methods using a sparsity prior have emerged to deal with low signal-to-noise ratio data. However, even when using sophisticated priors, the results may not be very sparse and most often do not separate the spatial components into brain regions. This work presents post-processing techniques that automatically sparsify brain maps and separate regions properly using geometric operations, and compares these techniques according to faithfulness to data and stability metrics. In particular, among threshold-based approaches, hysteresis thresholding and random walker segmentation, the latter improves significantly the stability of both dense and sparse models.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2014-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4031",
        "title": "High-level numerical simulations of noise in CCD and CMOS photosensors: review and tutorial",
        "authors": [
            "Mikhail Konnik",
            "James Welsh"
        ],
        "abstract": "In many applications, such as development and testing of image processing algorithms, it is often necessary to simulate images containing realistic noise from solid-state photosensors. A high-level model of CCD and CMOS photosensors based on a literature review is formulated in this paper. The model includes photo-response non-uniformity, photon shot noise, dark current Fixed Pattern Noise, dark current shot noise, offset Fixed Pattern Noise, source follower noise, sense node reset noise, and quantisation noise. The model also includes voltage-to-voltage, voltage-to-electrons, and analogue-to-digital converter non-linearities. The formulated model can be used to create synthetic images for testing and validation of image processing algorithms in the presence of realistic images noise. An example of the simulated CMOS photosensor and a comparison with a custom-made CMOS hardware sensor is presented. Procedures for characterisation from both light and dark noises are described. Experimental results that confirm the validity of the numerical model are provided. The paper addresses the issue of the lack of comprehensive high-level photosensor models that enable engineers to simulate realistic effects of noise on the images obtained from solid-state photosensors.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2014-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4044",
        "title": "Adaptive Stochastic Gradient Descent on the Grassmannian for Robust Low-Rank Subspace Recovery and Clustering",
        "authors": [
            "Jun He",
            "Yue Zhang"
        ],
        "abstract": "In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient for $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to robustly recover the low-rank subspace from a large matrix. In the presence of column outliers, we reformulate the batch mode matrix $L_{2,1}$ norm minimization with rank constraint problem as a stochastic optimization approach constrained on Grassmann manifold. For each observed data vector, the low-rank subspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic of Grassmannian. In order to accelerate the convergence rate of the stochastic gradient method, we choose to adaptively tune the constant step-size by leveraging the consecutive gradients. Furthermore, we demonstrate that with proper initialization, the K-subspaces extension, K-GASG21, can robustly cluster a large number of corrupted data vectors into a union of subspaces. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed algorithms even with heavy column outliers corruption.\n    ",
        "submission_date": "2014-12-12T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4237",
        "title": "First order algorithms in variational image processing",
        "authors": [
            "Martin Burger",
            "Alex Sawatzky",
            "Gabriele Steidl"
        ],
        "abstract": "Variational methods in imaging are nowadays developing towards a quite universal and flexible tool, allowing for highly successful approaches on tasks like denoising, deblurring, inpainting, segmentation, super-resolution, disparity, and optical flow estimation. The overall structure of such approaches is of the form ${\\cal D}(Ku) + \\alpha {\\cal R} (u) \\rightarrow \\min_u$ ; where the functional ${\\cal D}$ is a data fidelity term also depending on some input data $f$ and measuring the deviation of $Ku$ from such and ${\\cal R}$ is a regularization functional. Moreover $K$ is a (often linear) forward operator modeling the dependence of data on an underlying image, and $\\alpha$ is a positive regularization parameter. While ${\\cal D}$ is often smooth and (strictly) convex, the current practice almost exclusively uses nonsmooth regularization functionals. The majority of successful techniques is using nonsmooth and convex functionals like the total variation and generalizations thereof or $\\ell_1$-norms of coefficients arising from scalar products with some frame system. The efficient solution of such variational problems in imaging demands for appropriate algorithms. Taking into account the specific structure as a sum of two very different terms to be minimized, splitting algorithms are a quite canonical choice. Consequently this field has revived the interest in techniques like operator splittings or augmented Lagrangians. Here we shall provide an overview of methods currently developed and recent results as well as some computational studies providing a comparison of different methods and also illustrating their success in applications.\n    ",
        "submission_date": "2014-12-13T00:00:00",
        "last_modified_date": "2014-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.4659",
        "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
        "authors": [
            "Qing Qu",
            "Ju Sun",
            "John Wright"
        ],
        "abstract": "Is it possible to find the sparsest vector (direction) in a generic subspace $\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$? This problem can be considered a homogeneous variant of the sparse recovery problem, and finds connections to sparse dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper, we focus on a **planted sparse model** for the subspace: the target sparse vector is embedded in an otherwise random subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the first practical algorithm to achieve linear scaling under the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g., sparse dictionary learning.\n    ",
        "submission_date": "2014-12-15T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5068",
        "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples",
        "authors": [
            "Shixiang Gu",
            "Luca Rigazio"
        ],
        "abstract": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty.\n    ",
        "submission_date": "2014-12-11T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.5902",
        "title": "Nearest Descent, In-Tree, and Clustering",
        "authors": [
            "Teng Qiu",
            "Kaifu Yang",
            "Chaoyi Li",
            "Yongjie Li"
        ],
        "abstract": "In this paper, we propose a physically inspired graph-theoretical clustering method, which first makes the data points organized into an attractive graph, called In-Tree, via a physically inspired rule, called Nearest Descent (ND). In particular, the rule of ND works to select the nearest node in the descending direction of potential as the parent node of each node, which is in essence different from the classical Gradient Descent or Steepest Descent. The constructed In-Tree proves a very good candidate for clustering due to its particular features and properties. In the In-Tree, the original clustering problem is reduced to a problem of removing a very few of undesired edges from this graph. Pleasingly, the undesired edges in In-Tree are so distinguishable that they can be easily determined in either automatic or interactive way, which is in stark contrast to the cases in the widely used Minimal Spanning Tree and k-nearest-neighbor graph. The cluster number in the proposed method can be easily determined based on some intermediate plots, and the cluster assignment for each node is easily made by quickly searching its root node in each sub-graph (also an In-Tree). The proposed method is extensively evaluated on both synthetic and real-world datasets. Overall, the proposed clustering method is a density-based one, but shows significant differences and advantages in comparison to the traditional ones. The proposed method is simple yet efficient and reliable, and is applicable to various datasets with diverse shapes, attributes and any high dimensionality\n    ",
        "submission_date": "2014-12-07T00:00:00",
        "last_modified_date": "2018-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6149",
        "title": "Design, Implementation and Simulation of a Cloud Computing System for Enhancing Real-time Video Services by using VANET and Onboard Navigation Systems",
        "authors": [
            "Karim Hammoudi",
            "Nabil Ajam",
            "Mohamed Kasraoui",
            "Fadi Dornaika",
            "Karan Radhakrishnan",
            "Karthik Bandi",
            "Qing Cai",
            "Sai Liu"
        ],
        "abstract": "In this paper, we propose a design for novel and experimental cloud computing systems. The proposed system aims at enhancing computational, communicational and annalistic capabilities of road navigation services by merging several independent technologies, namely vision-based embedded navigation systems, prominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET). This work presents our initial investigations by describing the design of a global generic system. The designed system has been experimented with various scenarios of video-based road services. Moreover, the associated architecture has been implemented on a small-scale simulator of an in-vehicle embedded system. The implemented architecture has been experimented in the case of a simulated road service to aid the police agency. The goal of this service is to recognize and track searched individuals and vehicles in a real-time monitoring system remotely connected to moving cars. The presented work demonstrates the potential of our system for efficiently enhancing and diversifying real-time video services in road environments.\n    ",
        "submission_date": "2014-11-25T00:00:00",
        "last_modified_date": "2014-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6153",
        "title": "Intelligent Indoor Mobile Robot Navigation Using Stereo Vision",
        "authors": [
            "Arjun B. Krishnan",
            "Jayaram Kollipara"
        ],
        "abstract": "Majority of the existing robot navigation systems, which facilitate the use of laser range finders, sonar sensors or artificial landmarks, has the ability to locate itself in an unknown environment and then build a map of the corresponding environment. Stereo vision, while still being a rapidly developing technique in the field of autonomous mobile robots, are currently less preferable due to its high implementation cost. This paper aims at describing an experimental approach for the building of a stereo vision system that helps the robots to avoid obstacles and navigate through indoor environments and at the same time remaining very much cost effective. This paper discusses the fusion techniques of stereo vision and ultrasound sensors which helps in the successful navigation through different types of complex environments. The data from the sensor enables the robot to create the two dimensional topological map of unknown environments and stereo vision systems models the three dimension model of the same environment.\n    ",
        "submission_date": "2014-09-10T00:00:00",
        "last_modified_date": "2014-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6464",
        "title": "Simplified firefly algorithm for 2D image key-points search",
        "authors": [
            "Christian Napoli",
            "Giuseppe Pappalardo",
            "Emiliano Tramontana",
            "Zbigniew Marsza\u0142ek",
            "Dawid Po\u0142ap",
            "Marcin Wo\u017aniak"
        ],
        "abstract": "In order to identify an object, human eyes firstly search the field of view for points or areas which have particular properties. These properties are used to recognise an image or an object. Then this process could be taken as a model to develop computer algorithms for images identification. This paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2D images. For a set of input test images the proposed version of firefly algorithm has been examined. Research results are presented and discussed to show the efficiency of this evolutionary computation method.\n    ",
        "submission_date": "2014-12-19T00:00:00",
        "last_modified_date": "2014-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6563",
        "title": "Self-informed neural network structure learning",
        "authors": [
            "David Warde-Farley",
            "Andrew Rabinovich",
            "Dragomir Anguelov"
        ],
        "abstract": "We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6583",
        "title": "Discovering Hidden Factors of Variation in Deep Networks",
        "authors": [
            "Brian Cheung",
            "Jesse A. Livezey",
            "Arjun K. Bansal",
            "Bruno A. Olshausen"
        ],
        "abstract": "Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6604",
        "title": "Video (language) modeling: a baseline for generative models of natural videos",
        "authors": [
            "MarcAurelio Ranzato",
            "Arthur Szlam",
            "Joan Bruna",
            "Michael Mathieu",
            "Ronan Collobert",
            "Sumit Chopra"
        ],
        "abstract": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6614",
        "title": "In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning",
        "authors": [
            "Behnam Neyshabur",
            "Ryota Tomioka",
            "Nathan Srebro"
        ],
        "abstract": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6622",
        "title": "Deep metric learning using Triplet network",
        "authors": [
            "Elad Hoffer",
            "Nir Ailon"
        ],
        "abstract": "Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.\n    ",
        "submission_date": "2014-12-20T00:00:00",
        "last_modified_date": "2018-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6806",
        "title": "Striving for Simplicity: The All Convolutional Net",
        "authors": [
            "Jost Tobias Springenberg",
            "Alexey Dosovitskiy",
            "Thomas Brox",
            "Martin Riedmiller"
        ],
        "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6808",
        "title": "Learning the nonlinear geometry of high-dimensional data: Models and algorithms",
        "authors": [
            "Tong Wu",
            "Waheed U. Bajwa"
        ],
        "abstract": "Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing \"related\" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6821",
        "title": "A Stable Multi-Scale Kernel for Topological Machine Learning",
        "authors": [
            "Jan Reininghaus",
            "Stefan Huber",
            "Ulrich Bauer",
            "Roland Kwitt"
        ],
        "abstract": "Topological data analysis offers a rich source of valuable information to study vision problems. Yet, so far we lack a theoretically sound connection to popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In this work, we establish such a connection by designing a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. We show that this kernel is positive definite and prove its stability with respect to the 1-Wasserstein distance. Experiments on two benchmark datasets for 3D shape classification/retrieval and texture recognition show considerable performance gains of the proposed method compared to an alternative approach that is based on the recently introduced persistence landscapes.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2014-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.6830",
        "title": "Learning Activation Functions to Improve Deep Neural Networks",
        "authors": [
            "Forest Agostinelli",
            "Matthew Hoffman",
            "Peter Sadowski",
            "Pierre Baldi"
        ],
        "abstract": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.\n    ",
        "submission_date": "2014-12-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7012",
        "title": "Boltzmann-Machine Learning of Prior Distributions of Binarized Natural Images",
        "authors": [
            "Tomoyuki Obuchi",
            "Hirokazu Koma",
            "Muneki Yasuda"
        ],
        "abstract": "Prior distributions of binarized natural images are learned by using a Boltzmann machine. According the results of this study, there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects the individual characteristics of the three sets of pictures that we process. Meanwhile, in a longer spatial scale, a longer-range, although still rapidly decaying, ferromagnetic interaction commonly appears in all cases. The characteristic length scale of the interactions is universally up to approximately four lattice spacings $\\xi \\approx 4$. These results are derived by using the mean-field method, which effectively reduces the computational time required in a Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same results, as well as the Monte Carlo method does for small size images. These reinforce the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed.\n    ",
        "submission_date": "2014-12-16T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7024",
        "title": "Training deep neural networks with low precision multiplications",
        "authors": [
            "Matthieu Courbariaux",
            "Yoshua Bengio",
            "Jean-Pierre David"
        ],
        "abstract": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7056",
        "title": "Clustering multi-way data: a novel algebraic approach",
        "authors": [
            "Eric Kernfeld",
            "Shuchin Aeron",
            "Misha Kilmer"
        ],
        "abstract": "In this paper, we develop a method for unsupervised clustering of two-way (matrix) data by combining two recent innovations from different fields: the Sparse Subspace Clustering (SSC) algorithm [10], which groups points coming from a union of subspaces into their respective subspaces, and the t-product [18], which was introduced to provide a matrix-like multiplication for third order tensors. Our algorithm is analogous to SSC in that an \"affinity\" between different data points is built using a sparse self-representation of the data. Unlike SSC, we employ the t-product in the self-representation. This allows us more flexibility in modeling; infact, SSC is a special case of our method. When using the t-product, three-way arrays are treated as matrices whose elements (scalars) are n-tuples or tubes. Convolutions take the place of scalar multiplication. This framework allows us to embed the 2-D data into a vector-space-like structure called a free module over a commutative ring. These free modules retain many properties of complex inner-product spaces, and we leverage that to provide theoretical guarantees on our algorithm. We show that compared to vector-space counterparts, SSmC achieves higher accuracy and better able to cluster data with less preprocessing in some image clustering problems. In particular we show the performance of the proposed method on Weizmann face database, the Extended Yale B Face database and the MNIST handwritten digits database.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7210",
        "title": "Denoising autoencoder with modulated lateral connections learns invariant representations of natural images",
        "authors": [
            "Antti Rasmus",
            "Tapani Raiko",
            "Harri Valpola"
        ],
        "abstract": "Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.\n    ",
        "submission_date": "2014-12-22T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7625",
        "title": "An Effective Semi-supervised Divisive Clustering Algorithm",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Nowadays, data are generated massively and rapidly from scientific fields as bioinformatics, neuroscience and astronomy to business and engineering fields. Cluster analysis, as one of the major data analysis tools, is therefore more significant than ever. We propose in this work an effective Semi-supervised Divisive Clustering algorithm (SDC). Data points are first organized by a minimal spanning tree. Next, this tree structure is transitioned to the in-tree structure, and then divided into sub-trees under the supervision of the labeled data, and in the end, all points in the sub-trees are directly associated with specific cluster centers. SDC is fully automatic, non-iterative, involving no free parameter, insensitive to noise, able to detect irregularly shaped cluster structures, applicable to the data sets of high dimensionality and different attributes. The power of SDC is demonstrated on several datasets.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7659",
        "title": "Transformation Properties of Learned Visual Representations",
        "authors": [
            "Taco S. Cohen",
            "Max Welling"
        ],
        "abstract": "When a three-dimensional object moves relative to an observer, a change occurs on the observer's image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non-commutative 3D rotation group SO(3).\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7755",
        "title": "Multiple Object Recognition with Visual Attention",
        "authors": [
            "Jimmy Ba",
            "Volodymyr Mnih",
            "Koray Kavukcuoglu"
        ],
        "abstract": "We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.\n    ",
        "submission_date": "2014-12-24T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7880",
        "title": "Enhancing fractal descriptors on images by combining boundary and interior of Minkowski dilation",
        "authors": [
            "Marcos W. S. Oliveira",
            "Dalcimar Casanova",
            "Jo\u00e3o B. Florindo",
            "Odemir Martinez Bruno"
        ],
        "abstract": "This work proposes to obtain novel fractal descriptors from gray-level texture images by combining information from interior and boundary measures of the Minkowski dilation applied to the texture surface. At first, the image is converted into a surface where the height of each point is the gray intensity of the respective pixel in that position in the image. Thus, this surface is morphologically dilated by spheres. The radius of such spheres is ranged within an interval and the volume and the external area of the dilated structure are computed for each radius. The final descriptors are given by such measures concatenated and subject to a canonical transform to reduce the dimensionality. The proposal is an enhancement to the classical Bouligand-Minkowski fractal descriptors, where only the volume (interior) information is considered. As different structures may have the same volume, but not the same area, the proposal yields to more rich descriptors as confirmed by results on the classification of benchmark databases.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.7934",
        "title": "A Novel Feature Selection and Extraction Technique for Classification",
        "authors": [
            "Kratarth Goel",
            "Raunaq Vohra",
            "Ainesh Bakshi"
        ],
        "abstract": "This paper presents a versatile technique for the purpose of feature selection and extraction - Class Dependent Features (CDFs). We use CDFs to improve the accuracy of classification and at the same time control computational expense by tackling the curse of dimensionality. In order to demonstrate the generality of this technique, it is applied to handwritten digit recognition and text categorization.\n    ",
        "submission_date": "2014-12-26T00:00:00",
        "last_modified_date": "2014-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8307",
        "title": "Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm",
        "authors": [
            "Mark D. McDonnell",
            "Migel D. Tissera",
            "Tony Vladusich",
            "Andr\u00e9 van Schaik",
            "Jonathan Tapson"
        ],
        "abstract": "Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8380",
        "title": "A simple coding for cross-domain matching with dimension reduction via spectral graph embedding",
        "authors": [
            "Hidetoshi Shimodaira"
        ],
        "abstract": "Data vectors are obtained from multiple domains. They are feature vectors of images or vector representations of words. Domains may have different numbers of data vectors with different dimensions. These data vectors from multiple domains are projected to a common space by linear transformations in order to search closely related vectors across domains. We would like to find projection matrices to minimize distances between closely related data vectors. This formulation of cross-domain matching is regarded as an extension of the spectral graph embedding to multi-domain setting, and it includes several multivariate analysis methods of statistics such as multiset canonical correlation analysis, correspondence analysis, and principal component analysis. Similar approaches are very popular recently in pattern recognition and vision. In this paper, instead of proposing a novel method, we will introduce an embarrassingly simple idea of coding the data vectors for explaining all the above mentioned approaches. A data vector is concatenated with zero vectors from all other domains to make an augmented vector. The cross-domain matching is solved by applying the single-domain version of spectral graph embedding to these augmented vectors of all the domains. An interesting connection to the classical associative memory model of neural networks is also discussed by noticing a coding for association. A cross-validation method for choosing the dimension of the common space and a regularization parameter will be discussed in an illustrative numerical example.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2015-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1412.8419",
        "title": "Simple Image Description Generator via a Linear Phrase-Based Approach",
        "authors": [
            "Remi Lebret",
            "Pedro O. Pinheiro",
            "Ronan Collobert"
        ],
        "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.\n    ",
        "submission_date": "2014-12-29T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    }
]