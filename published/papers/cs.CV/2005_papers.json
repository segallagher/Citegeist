[
    {
        "url": "https://arxiv.org/abs/cs/0502095",
        "title": "Gradient Vector Flow Models for Boundary Extraction in 2D Images",
        "authors": [
            "Gilson A. Giraldi",
            "Leandro S. Marturelli",
            "Paulo S. Rodrigues"
        ],
        "abstract": "  The Gradient Vector Flow (GVF) is a vector diffusion approach based on Partial Differential Equations (PDEs). This method has been applied together with snake models for boundary extraction medical images segmentation. The key idea is to use a diffusion-reaction PDE to generate a new external force field that makes snake models less sensitivity to initialization as well as improves the snake's ability to move into boundary concavities. In this paper, we firstly review basic results about convergence and numerical analysis of usual GVF schemes. We point out that GVF presents numerical problems due to discontinuities image intensity. This point is considered from a practical viewpoint from which the GVF parameters must follow a relationship in order to improve numerical convergence. Besides, we present an analytical analysis of the GVF dependency from the parameters values. Also, we observe that the method can be used for multiply connected domains by just imposing the suitable boundary condition. In the experimental results we verify these theoretical points and demonstrate the utility of GVF on a segmentation approach that we have developed based on snakes.\n    ",
        "submission_date": "2005-02-28T00:00:00",
        "last_modified_date": "2005-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0503001",
        "title": "Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but actually it is not)",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  Pattern recognition is generally assumed as an interaction of two inversely directed image-processing streams: the bottom-up information details gathering and localization (segmentation) stream, and the top-down information features aggregation, association and interpretation (recognition) stream. Inspired by recent evidence from biological vision research and by the insights of Kolmogorov Complexity theory, we propose a new, just top-down evolving, procedure of initial image segmentation. We claim that traditional top-down cognitive reasoning, which is supposed to guide the segmentation process to its final result, is not at all a part of the image information content evaluation. And that initial image segmentation is certainly an unsupervised process. We present some illustrative examples, which support our claims.\n    ",
        "submission_date": "2005-03-01T00:00:00",
        "last_modified_date": "2005-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0503053",
        "title": "A hybrid MLP-PNN architecture for fast image superresolution",
        "authors": [
            "Carlos Miravet",
            "Francisco B. Rodriguez"
        ],
        "abstract": "  Image superresolution methods process an input image sequence of a scene to obtain a still image with increased resolution. Classical approaches to this problem involve complex iterative minimization procedures, typically with high computational costs. In this paper is proposed a novel algorithm for super-resolution that enables a substantial decrease in computer load. First, a probabilistic neural network architecture is used to perform a scattered-point interpolation of the image sequence data. The network kernel function is optimally determined for this problem by a multi-layer perceptron trained on synthetic data. Network parameters dependence on sequence noise level is quantitatively analyzed. This super-sampled image is spatially filtered to correct finite pixel size effects, to yield the final high-resolution estimate. Results on a real outdoor sequence are presented, showing the quality of the proposed method.\n    ",
        "submission_date": "2005-03-22T00:00:00",
        "last_modified_date": "2005-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0503056",
        "title": "Semi-automatic vectorization of linear networks on rasterized cartographic maps",
        "authors": [
            "Carlos Miravet",
            "Enrique Coiras",
            "Javier Santamaria"
        ],
        "abstract": "  A system for semi-automatic vectorization of linear networks (roads, rivers, etc.) on rasterized cartographic maps is presented. In this system, human intervention is limited to a graphic, interactive selection of the color attributes of the information to be obtained. Using this data, the system performs a preliminary extraction of the linear network, which is subsequently completed, refined and vectorized by means of an automatic procedure. Results on maps of different sources and scales are included.\n",
        "submission_date": "2005-03-22T00:00:00",
        "last_modified_date": "2005-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0503076",
        "title": "Geometric Models of Rolling-Shutter Cameras",
        "authors": [
            "Marci Meingast",
            "Christopher Geyer",
            "Shankar Sastry"
        ],
        "abstract": "  Cameras with rolling shutters are becoming more common as low-power, low-cost CMOS sensors are being used more frequently in cameras. The rolling shutter means that not all scanlines are exposed over the same time interval. The effects of a rolling shutter are noticeable when either the camera or objects in the scene are moving and can lead to systematic biases in projection estimation. We develop a general projection equation for a rolling shutter camera and show how it is affected by different types of camera motion. In the case of fronto-parallel motion, we show how that camera can be modeled as an X-slit camera. We also develop approximate projection equations for a non-zero angular velocity about the optical axis and approximate the projection equation for a constant velocity screw motion. We demonstrate how the rolling shutter effects the projective geometry of the camera and in turn the structure-from-motion.\n    ",
        "submission_date": "2005-03-29T00:00:00",
        "last_modified_date": "2005-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0504031",
        "title": "Convexity Analysis of Snake Models Based on Hamiltonian Formulation",
        "authors": [
            "Gilson Antonio Giraldi",
            "Antonio Alberto Fernandes de Oliveira"
        ],
        "abstract": "  This paper presents a convexity analysis for the dynamic snake model based on the Potential Energy functional and the Hamiltonian formulation of the classical mechanics. First we see the snake model as a dynamical system whose singular points are the borders we seek. Next we show that a necessary condition for a singular point to be an attractor is that the energy functional is strictly convex in a neighborhood of it, that means, if the singular point is a local minimum of the potential energy. As a consequence of this analysis, a local expression relating the dynamic parameters and the rate of convergence arises. Such results link the convexity analysis of the potential energy and the dynamic snake model and point forward to the necessity of a physical quantity whose convexity analysis is related to the dynamic and which incorporate the velocity space. Such a quantity is exactly the (conservative) Hamiltonian of the system.\n    ",
        "submission_date": "2005-04-08T00:00:00",
        "last_modified_date": "2005-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0504037",
        "title": "Bayesian Restoration of Digital Images Employing Markov Chain Monte Carlo a Review",
        "authors": [
            "K. P. N. Murthy",
            "M. Janani",
            "B. Shenbga Priya"
        ],
        "abstract": "  A review of Bayesian restoration of digital images based on Monte Carlo techniques is presented. The topics covered include Likelihood, Prior and Posterior distributions, Poisson, Binay symmetric channel, and Gaussian channel models of Likelihood distribution,Ising and Potts spin models of Prior distribution, restoration of an image through Posterior maximization, statistical estimation of a true image from Posterior ensembles, Markov Chain Monte Carlo methods and cluster algorithms.\n    ",
        "submission_date": "2005-04-11T00:00:00",
        "last_modified_date": "2006-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0505006",
        "title": "Searching for image information content, its discovery, extraction, and representation",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  Image information content is known to be a complicated and controvercial problem. This paper posits a new image information content definition. Following the theory of Solomonoff-Kolmogorov-Chaitin's complexity, we define image information content as a set of descriptions of imafe data structures. Three levels of such description can be generally distinguished: 1)the global level, where the coarse structure of the entire scene is initially outlined; 2) the intermediate level, where structures of separate, non-overlapping image regions usually associated with individual scene objects are deliniated; and 3) the low-level description, where local image structures observed in a limited and restricted field of view are resolved. A technique for creating such image information content descriptors is developed. Its algorithm is presented and elucidated with some examples, which demonstrate the effectiveness of the proposed approach.\n    ",
        "submission_date": "2005-05-02T00:00:00",
        "last_modified_date": "2005-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0505058",
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "abstract": "  The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.\n",
        "submission_date": "2005-05-23T00:00:00",
        "last_modified_date": "2005-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0506019",
        "title": "An Efficient Approximation Algorithm for Point Pattern Matching Under Noise",
        "authors": [
            "Vicky Choi",
            "Navin Goyal"
        ],
        "abstract": "  Point pattern matching problems are of fundamental importance in various areas including computer vision and structural bioinformatics. In this paper, we study one of the more general problems, known as LCP (largest common point set problem): Let $\\PP$ and $\\QQ$ be two point sets in $\\mathbb{R}^3$, and let $\\epsilon \\geq 0$ be a tolerance parameter, the problem is to find a rigid motion $\\mu$ that maximizes the cardinality of subset $\\II$ of $Q$, such that the Hausdorff distance $\\distance(\\PP,\\mu(\\II)) \\leq \\epsilon$. We denote the size of the optimal solution to the above problem by $\\LCP(P,Q)$. The problem is called exact-LCP for $\\epsilon=0$, and \\tolerant-LCP when $\\epsilon>0$ and the minimum interpoint distance is greater than $2\\epsilon$. A $\\beta$-distance-approximation algorithm for tolerant-LCP finds a subset $I \\subseteq \\QQ$ such that $|I|\\geq \\LCP(P,Q)$ and $\\distance(\\PP,\\mu(\\II)) \\leq \\beta \\epsilon$ for some $\\beta \\ge 1$.\n",
        "submission_date": "2005-06-07T00:00:00",
        "last_modified_date": "2006-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0506089",
        "title": "Field geology with a wearable computer: 1st results of the Cyborg Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup",
            "Helge Ritter"
        ],
        "abstract": "  We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.\n    ",
        "submission_date": "2005-06-24T00:00:00",
        "last_modified_date": "2005-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0507058",
        "title": "Paving the Way for Image Understanding: A New Kind of Image Decomposition is Desired",
        "authors": [
            "Emanuel Diamant"
        ],
        "abstract": "  In this paper we present an unconventional image segmentation approach which is devised to meet the requirements of image understanding and pattern recognition tasks. Generally image understanding assumes interplay of two sub-processes: image information content discovery and image information content interpretation. Despite of its widespread use, the notion of \"image information content\" is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannon's sense, which means information content assessment averaged over the whole signal ensemble. Humans, however,rarely resort to such estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image parts. We posit that following the latest findings in human attention vision studies and the concepts of Kolmogorov's complexity theory an unorthodox segmentation approach can be proposed that provides effective image decomposition to information preserving image fragments well suited for subsequent image interpretation. We provide some illustrative examples, demonstrating effectiveness of this approach.\n    ",
        "submission_date": "2005-07-22T00:00:00",
        "last_modified_date": "2005-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0508007",
        "title": "Regularity of Position Sequences",
        "authors": [
            "Manfred Harringer"
        ],
        "abstract": "A person is given a numbered sequence of positions on a sheet of paper. The person is asked, \"Which will be the next (or the next after that) position?\" Everyone has an opinion as to how he or she would proceed. There are regular sequences for which there is general agreement on how to continue. However, there are less regular sequences for which this assessment is less certain. There are sequences for which every continuation is perceived to be arbitrary. I would like to present a mathematical model that reflects these opinions and perceptions with the aid of a valuation function. It is necessary to apply a rich set of invariant features of position sequences to ensure the quality of this model. All other properties of the model are arbitrary.\n    ",
        "submission_date": "2005-08-01T00:00:00",
        "last_modified_date": "2010-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0509081",
        "title": "Automatic Face Recognition System Based on Local Fourier-Bessel Features",
        "authors": [
            "Yossi Zana",
            "Roberto M. Cesar-Jr",
            "Regis de A. Barbosa"
        ],
        "abstract": "  We present an automatic face verification system inspired by known properties of biological systems. In the proposed algorithm the whole image is converted from the spatial to polar frequency domain by a Fourier-Bessel Transform (FBT). Using the whole image is compared to the case where only face image regions (local analysis) are considered. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images, and a Pseudo-Fisher discriminator is built. Verification test results on the FERET database showed that the local-based algorithm outperforms the global-FBT version. The local-FBT algorithm performed as state-of-the-art methods under different testing conditions, indicating that the proposed system is highly robust for expression, age, and illumination variations. We also evaluated the performance of the proposed system under strong occlusion conditions and found that it is highly robust for up to 50% of face occlusion. Finally, we automated completely the verification system by implementing face and eye detection algorithms. Under this condition, the local approach was only slightly superior to the global approach.\n    ",
        "submission_date": "2005-09-27T00:00:00",
        "last_modified_date": "2005-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0509082",
        "title": "Face Recognition Based on Polar Frequency Features",
        "authors": [
            "Yossi Zana",
            "Roberto M. Cesar-JR"
        ],
        "abstract": "  A novel biologically motivated face recognition algorithm based on polar frequency is presented. Polar frequency descriptors are extracted from face images by Fourier-Bessel transform (FBT). Next, the Euclidean distance between all images is computed and each image is now represented by its dissimilarity to the other images. A Pseudo-Fisher Linear Discriminant was built on this dissimilarity space. The performance of Discrete Fourier transform (DFT) descriptors, and a combination of both feature types was also evaluated. The algorithms were tested on a 40- and 1196-subjects face database (ORL and FERET, respectively). With 5 images per subject in the training and test datasets, error rate on the ORL database was 3.8, 1.25 and 0.2% for the FBT, DFT, and the combined classifier, respectively, as compared to 2.6% achieved by the best previous algorithm. The most informative polar frequency features were concentrated at low-to-medium angular frequencies coupled to low radial frequencies. On the FERET database, where an affine normalization pre-processing was applied, the FBT algorithm outperformed only the PCA in a rank recognition test. However, it achieved performance comparable to state-of-the-art methods when evaluated by verification tests. These results indicate the high informative value of the polar frequency content of face images in relation to recognition and verification tasks, and that the Cartesian frequency content can complement information about the subjects' identity, but possibly only when the images are not pre-normalized. Possible implications for human face recognition are discussed.\n    ",
        "submission_date": "2005-09-27T00:00:00",
        "last_modified_date": "2005-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0509083",
        "title": "Face Verification in Polar Frequency Domain: a Biologically Motivated Approach",
        "authors": [
            "Yossi Zana",
            "Roberto M. Cesar-Jr",
            "Rogerio S. Feris",
            "Matthew Turk"
        ],
        "abstract": "  We present a novel local-based face verification system whose components are analogous to those of biological systems. In the proposed system, after global registration and normalization, three eye regions are converted from the spatial to polar frequency domain by a Fourier-Bessel Transform. The resulting representations are embedded in a dissimilarity space, where each image is represented by its distance to all the other images. In this dissimilarity space a Pseudo-Fisher discriminator is built. ROC and equal error rate verification test results on the FERET database showed that the system performed at least as state-of-the-art methods and better than a system based on polar Fourier features. The local-based system is especially robust to facial expression and age variations, but sensitive to registration errors.\n    ",
        "submission_date": "2005-09-27T00:00:00",
        "last_modified_date": "2005-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0510001",
        "title": "Retinal Vessel Segmentation Using the 2-D Morlet Wavelet and Supervised Classification",
        "authors": [
            "Jo\u00e3o V. B. Soares",
            "Jorge J. G. Leandro",
            "Roberto M. Cesar Jr.",
            "Herbert F. Jelinek",
            "Michael J. Cree"
        ],
        "abstract": "  We present a method for automated segmentation of the vasculature in retinal images. The method produces segmentations by classifying each image pixel as vessel or non-vessel, based on the pixel's feature vector. Feature vectors are composed of the pixel's intensity and continuous two-dimensional Morlet wavelet transform responses taken at multiple scales. The Morlet wavelet is capable of tuning to specific frequencies, thus allowing noise filtering and vessel enhancement in a single step. We use a Bayesian classifier with class-conditional probability density functions (likelihoods) described as Gaussian mixtures, yielding a fast classification, while being able to model complex decision surfaces and compare its performance with the linear minimum squared error classifier. The probability distributions are estimated based on a training set of labeled pixels obtained from manual segmentations. The method's performance is evaluated on publicly available DRIVE and STARE databases of manually labeled non-mydriatic images. On the DRIVE database, it achieves an area under the receiver operating characteristic (ROC) curve of 0.9598, being slightly superior than that presented by the method of Staal et al.\n    ",
        "submission_date": "2005-09-30T00:00:00",
        "last_modified_date": "2006-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0510008",
        "title": "Accurate and robust image superresolution by neural processing of local image representations",
        "authors": [
            "Carlos Miravet",
            "Francisco B. Rodriguez"
        ],
        "abstract": "  Image superresolution involves the processing of an image sequence to generate a still image with higher resolution. Classical approaches, such as bayesian MAP methods, require iterative minimization procedures, with high computational costs. Recently, the authors proposed a method to tackle this problem, based on the use of a hybrid MLP-PNN architecture. In this paper, we present a novel superresolution method, based on an evolution of this concept, to incorporate the use of local image models. A neural processing stage receives as input the value of model coefficients on local windows. The data dimensionality is firstly reduced by application of PCA. An MLP, trained on synthetic sequences with various amounts of noise, estimates the high-resolution image data. The effect of varying the dimension of the network input space is examined, showing a complex, structured behavior. Quantitative results are presented showing the accuracy and robustness of the proposed method.\n    ",
        "submission_date": "2005-10-03T00:00:00",
        "last_modified_date": "2005-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0510026",
        "title": "A decision support system for ship identification based on the curvature scale space representation",
        "authors": [
            "Alvaro Enriquez de Luna",
            "Carlos Miravet",
            "Deitze Otaduy",
            "Carlos Dorronsoro"
        ],
        "abstract": "  In this paper, a decision support system for ship identification is presented. The system receives as input a silhouette of the vessel to be identified, previously extracted from a side view of the object. This view could have been acquired with imaging sensors operating at different spectral ranges (CCD, FLIR, image intensifier). The input silhouette is preprocessed and compared to those stored in a database, retrieving a small number of potential matches ranked by their similarity to the target silhouette. This set of potential matches is presented to the system operator, who makes the final ship identification. This system makes use of an evolved version of the Curvature Scale Space (CSS) representation. In the proposed approach, it is curvature extrema, instead of zero crossings, that are tracked during silhouette evolution, hence improving robustness and enabling to cope successfully with cases where the standard CCS representation is found to be unstable. Also, the use of local curvature was replaced with the more robust concept of lobe concavity, with significant additional gains in performance. Experimental results on actual operational imagery prove the excellent performance and robustness of the developed method.\n    ",
        "submission_date": "2005-10-11T00:00:00",
        "last_modified_date": "2005-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0510056",
        "title": "First-Order Modeling and Stability Analysis of Illusory Contours",
        "authors": [
            "Yoon-Mo Jung",
            "Jianhong Shen"
        ],
        "abstract": "  In visual cognition, illusions help elucidate certain intriguing latent perceptual functions of the human vision system, and their proper mathematical modeling and computational simulation are therefore deeply beneficial to both biological and computer vision. Inspired by existent prior works, the current paper proposes a first-order energy-based model for analyzing and simulating illusory contours. The lower complexity of the proposed model facilitates rigorous mathematical analysis on the detailed geometric structures of illusory contours. After being asymptotically approximated by classical active contours, the proposed model is then robustly computed using the celebrated level-set method of Osher and Sethian (J. Comput. Phys., 79:12-49, 1988) with a natural supervising scheme. Potential cognitive implications of the mathematical results are addressed, and generic computational examples are demonstrated and discussed.\n    ",
        "submission_date": "2005-10-19T00:00:00",
        "last_modified_date": "2005-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0511064",
        "title": "The consistency principle for a digitization procedure. An algorithm for building normal digital spaces of continuous n-dimensional objects",
        "authors": [
            "Alexander V. Evako"
        ],
        "abstract": "  This paper considers conditions, which allow to preserve important topological and geometric properties in the process of digitization. For this purpose, we introduce a triplet {C,M,D} consisting of a continuous object C, an intermediate model M, which is a collection of subregions whose union is C, a digital model D, which is the intersection graph of M, and apply the consistency principle and criteria of similarity to M in order to make its mathematical structure consistent with the natural structure of D. Specifically, this paper introduces a locally centered lump collection of subregions and shows that for any locally centered lump cover of an n-dimensional continuous manifold, the digital model of the manifold is a digital normal n-dimensional space. In addition, we give examples of locally centered lump tilings of two-manifolds. We propose an algorithm for constructing normal digital models of continuous objects.\n    ",
        "submission_date": "2005-11-16T00:00:00",
        "last_modified_date": "2005-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0512084",
        "title": "Understanding physics from interconnected data",
        "authors": [
            "Nikita Sakhanenko",
            "Hanna Makaruk"
        ],
        "abstract": "  Metal melting on release after explosion is a physical system far from quilibrium. A complete physical model of this system does not exist, because many interrelated effects have to be considered. General methodology needs to be developed so as to describe and understand physical phenomena involved.\n",
        "submission_date": "2005-12-21T00:00:00",
        "last_modified_date": "2005-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0502017",
        "title": "Estimating mutual information and multi--information in large networks",
        "authors": [
            "Noam Slonim",
            "Gurinder S. Atwal",
            "Gasper Tkacik",
            "William Bialek"
        ],
        "abstract": "  We address the practical problems of estimating the information relations that characterize large networks. Building on methods developed for analysis of the neural code, we show that reliable estimates of mutual information can be obtained with manageable computational effort. The same methods allow estimation of higher order, multi--information terms. These ideas are illustrated by analyses of gene expression, financial markets, and consumer preferences. In each case, information theoretic measures correlate with independent, intuitive measures of the underlying structures in the system.\n    ",
        "submission_date": "2005-02-03T00:00:00",
        "last_modified_date": "2005-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0505064",
        "title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping Tasks",
        "authors": [
            "P.C. McGuire",
            "J. Fritsch",
            "J. J. Steil",
            "F. Roethling",
            "G. A. Fink",
            "S. Wachsmuth",
            "G. Sagerer",
            "H. Ritter"
        ],
        "abstract": "  A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.\n    ",
        "submission_date": "2005-05-24T00:00:00",
        "last_modified_date": "2005-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0505084",
        "title": "An explicit formula for the number of tunnels in digital objects",
        "authors": [
            "Valentin Brimkov",
            "Angelo Maimone",
            "Giorgio Nordo"
        ],
        "abstract": "  An important concept in digital geometry for computer imagery is that of tunnel. In this paper we obtain a formula for the number of tunnels as a function of the number of the object vertices, pixels, holes, connected components, and 2x2 grid squares. It can be used to test for tunnel-freedom a digital object, in particular a digital curve.\n    ",
        "submission_date": "2005-05-31T00:00:00",
        "last_modified_date": "2005-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0507039",
        "title": "Distributed Regression in Sensor Networks: Training Distributively with Alternating Projections",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "abstract": "  Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.\n    ",
        "submission_date": "2005-07-18T00:00:00",
        "last_modified_date": "2005-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0507040",
        "title": "Pattern Recognition for Conditionally Independent Data",
        "authors": [
            "Daniil Ryabko"
        ],
        "abstract": "  In this work we consider the task of relaxing the i.i.d assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.\n",
        "submission_date": "2005-07-18T00:00:00",
        "last_modified_date": "2005-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0508099",
        "title": "Search Process and Probabilistic Bifix Approach",
        "authors": [
            "Dragana Bajic",
            "Cedomir Stefanovic",
            "Dejan Vukobratovic"
        ],
        "abstract": "  An analytical approach to a search process is a mathematical prerequisite for digital synchronization acquisition analysis and optimization. A search is performed for an arbitrary set of sequences within random but not equiprobable L-ary data. This paper derives in detail an expression for probability distribution function, from which other statistical parameters - expected value and variance - can be obtained. The probabilistic nature of (cross-) bifix indicators is shown and application examples are outlined, ranging beyond the usual telecommunication field.\n    ",
        "submission_date": "2005-08-23T00:00:00",
        "last_modified_date": "2005-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0509022",
        "title": "Achievable Rates for Pattern Recognition",
        "authors": [
            "M. Brandon Westover",
            "Joseph A. O'Sullivan"
        ],
        "abstract": "  Biological and machine pattern recognition systems face a common challenge: Given sensory data about an unknown object, classify the object by comparing the sensory data with a library of internal representations stored in memory. In many cases of interest, the number of patterns to be discriminated and the richness of the raw data force recognition systems to internally represent memory and sensory information in a compressed format. However, these representations must preserve enough information to accommodate the variability and complexity of the environment, or else recognition will be unreliable. Thus, there is an intrinsic tradeoff between the amount of resources devoted to data representation and the complexity of the environment in which a recognition system may reliably operate.\n",
        "submission_date": "2005-09-08T00:00:00",
        "last_modified_date": "2005-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/cs/0511032",
        "title": "Spatiotemporal sensistivity and visual attention for efficient rendering of dynamic environments",
        "authors": [
            "Yang Li Hector Yee"
        ],
        "abstract": "  We present a method to accelerate global illumination computation in dynamic environments by taking advantage of limitations of the human visual system. A model of visual attention is used to locate regions of interest in a scene and to modulate spatiotemporal sensitivity. The method is applied in the form of a spatiotemporal error tolerance map. Perceptual acceleration combined with good sampling protocols provide a global illumination solution feasible for use in animation. Results indicate an order of magnitude improvement in computational speed. The method is adaptable and can also be used in image-based rendering, geometry level of detail selection, realistic image synthesis, video telephony and video compression.\n    ",
        "submission_date": "2005-11-08T00:00:00",
        "last_modified_date": "2005-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/math/0508171",
        "title": "Matrices of Forests and the Analysis of Digraphs",
        "authors": [
            "Pavel Chebotarev",
            "Rafig Agaev"
        ],
        "abstract": "  The matrices of spanning rooted forests are studied as a tool for analysing the structure of digraphs and measuring their characteristics. The problems of revealing the basis bicomponents, measuring vertex proximity, and ranking from preference relations / sports competitions are considered. It is shown that the vertex accessibility measure based on spanning forests has a number of desirable properties. An interpretation for the normalized matrix of out-forests in terms of information dissemination is given.\n",
        "submission_date": "2005-08-09T00:00:00",
        "last_modified_date": "2006-02-04T00:00:00"
    }
]