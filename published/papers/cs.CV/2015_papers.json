[
    {
        "url": "https://arxiv.org/abs/1501.00092",
        "title": "Image Super-Resolution Using Deep Convolutional Networks",
        "authors": [
            "Chao Dong",
            "Chen Change Loy",
            "Kaiming He",
            "Xiaoou Tang"
        ],
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\n    ",
        "submission_date": "2014-12-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00102",
        "title": "ModDrop: adaptive multi-modal gesture recognition",
        "authors": [
            "Natalia Neverova",
            "Christian Wolf",
            "Graham W. Taylor",
            "Florian Nebout"
        ],
        "abstract": "We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.\n    ",
        "submission_date": "2014-12-31T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00105",
        "title": "Face recognition using color local binary pattern from mutually independent color channels",
        "authors": [
            "Gholamreza Anbarjafari"
        ],
        "abstract": "In this paper, a high performance face recognition system based on local binary pattern (LBP) using the probability distribution functions (PDF) of pixels in different mutually independent color channels which are robust to frontal homogenous illumination and planer rotation is proposed. The illumination of faces is enhanced by using the state-of-the-art technique which is using discrete wavelet transform (DWT) and singular value decomposition (SVD). After equalization, face images are segmented by use of local Successive Mean Quantization Transform (SMQT) followed by skin color based face detection system. Kullback-Leibler Distance (KLD) between the concatenated PDFs of a given face obtained by LBP and the concatenated PDFs of each face in the database is used as a metric in the recognition process. Various decision fusion techniques have been used in order to improve the recognition rate. The proposed system has been tested on the FERET, HP, and Bosphorus face databases. The proposed system is compared with conventional and thestate-of-the-art techniques. The recognition rates obtained using FVF approach for FERET database is 99.78% compared with 79.60% and 68.80% for conventional gray scale LBP and Principle Component Analysis (PCA) based face recognition techniques respectively.\n    ",
        "submission_date": "2014-12-31T00:00:00",
        "last_modified_date": "2014-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00108",
        "title": "HSI based colour image equalization using iterative nth root and nth power",
        "authors": [
            "Gholamreza Anbarjafari"
        ],
        "abstract": "In this paper an equalization technique for colour images is introduced. The method is based on nth root and nth power equalization approach but with optimization of the mean of the image in different colour channels such as RGB and HSI. The performance of the proposed method has been measured by the means of peak signal to noise ratio. The proposed algorithm has been compared with conventional histogram equalization and the visual and quantitative experimental results are showing that the proposed method over perform the histogram equalization.\n    ",
        "submission_date": "2014-12-31T00:00:00",
        "last_modified_date": "2014-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00614",
        "title": "Understanding Trajectory Behavior: A Motion Pattern Approach",
        "authors": [
            "Mahdi M. Kalayeh",
            "Stephen Mussmann",
            "Alla Petrakova",
            "Niels da Vitoria Lobo",
            "Mubarak Shah"
        ],
        "abstract": "Mining the underlying patterns in gigantic and complex data is of great importance to data analysts. In this paper, we propose a motion pattern approach to mine frequent behaviors in trajectory data. Motion patterns, defined by a set of highly similar flow vector groups in a spatial locality, have been shown to be very effective in extracting dominant motion behaviors in video sequences. Inspired by applications and properties of motion patterns, we have designed a framework that successfully solves the general task of trajectory clustering. Our proposed algorithm consists of four phases: flow vector computation, motion component extraction, motion component's reachability set creation, and motion pattern formation. For the first phase, we break down trajectories into flow vectors that indicate instantaneous movements. In the second phase, via a Kmeans clustering approach, we create motion components by clustering the flow vectors with respect to their location and velocity. Next, we create motion components' reachability set in terms of spatial proximity and motion similarity. Finally, for the fourth phase, we cluster motion components using agglomerative clustering with the weighted Jaccard distance between the motion components' signatures, a set created using path reachability. We have evaluated the effectiveness of our proposed method in an extensive set of experiments on diverse datasets. Further, we have shown how our proposed method handles difficulties in the general task of trajectory clustering that challenge the existing state-of-the-art methods.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2015-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00630",
        "title": "Non-iterative rigid 2D/3D point-set registration using semidefinite programming",
        "authors": [
            "Yuehaw Khoo",
            "Ankur Kapoor"
        ],
        "abstract": "We describe a convex programming framework for pose estimation in 2D/3D point-set registration with unknown point correspondences. We give two mixed-integer nonlinear program (MINP) formulations of the 2D/3D registration problem when there are multiple 2D images, and propose convex relaxations for both of the MINPs to semidefinite programs (SDP) that can be solved efficiently by interior point methods. Our approach to the 2D/3D registration problem is non-iterative in nature as we jointly solve for pose and correspondence. Furthermore, these convex programs can readily incorporate feature descriptors of points to enhance registration results. We prove that the convex programs exactly recover the solution to the original nonconvex 2D/3D registration problem under noiseless condition. We apply these formulations to the registration of 3D models of coronary vessels to their 2D projections obtained from multiple intra-operative fluoroscopic images. For this application, we experimentally corroborate the exact recovery property in the absence of noise and further demonstrate robustness of the convex programs in the presence of noise.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00642",
        "title": "Unsupervised Feature Learning for Dense Correspondences across Scenes",
        "authors": [
            "Chao Zhang",
            "Chunhua Shen",
            "Tingzhi Shen"
        ],
        "abstract": "We propose a fast, accurate matching method for estimating dense pixel correspondences across scenes. It is a challenging problem to estimate dense pixel correspondences between images depicting different scenes or instances of the same object category. While most such matching methods rely on hand-crafted features such as SIFT, we learn features from a large amount of unlabeled image patches using unsupervised learning. Pixel-layer features are obtained by encoding over the dictionary, followed by spatial pooling to obtain patch-layer features. The learned features are then seamlessly embedded into a multi-layer match- ing framework. We experimentally demonstrate that the learned features, together with our matching model, outperforms state-of-the-art methods such as the SIFT flow, coherency sensitive hashing and the recent deformable spatial pyramid matching methods both in terms of accuracy and computation efficiency. Furthermore, we evaluate the performance of a few different dictionary learning and feature encoding methods in the proposed pixel correspondences estimation framework, and analyse the impact of dictionary learning and feature encoding with respect to the final matching performance.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00752",
        "title": "A Deep-structured Conditional Random Field Model for Object Silhouette Tracking",
        "authors": [
            "Mohammad Shafiee",
            "Zohreh Azimifar",
            "Alexander Wong"
        ],
        "abstract": "In this work, we introduce a deep-structured conditional random field (DS-CRF) model for the purpose of state-based object silhouette tracking. The proposed DS-CRF model consists of a series of state layers, where each state layer spatially characterizes the object silhouette at a particular point in time. The interactions between adjacent state layers are established by inter-layer connectivity dynamically determined based on inter-frame optical flow. By incorporate both spatial and temporal context in a dynamic fashion within such a deep-structured probabilistic graphical model, the proposed DS-CRF model allows us to develop a framework that can accurately and efficiently track object silhouettes that can change greatly over time, as well as under different situations such as occlusion and multiple targets within the scene. Experiment results using video surveillance datasets containing different scenarios such as occlusion and multiple targets showed that the proposed DS-CRF approach provides strong object silhouette tracking performance when compared to baseline methods such as mean-shift tracking, as well as state-of-the-art methods such as context tracking and boosted particle filtering.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00777",
        "title": "Sparse Deep Stacking Network for Image Classification",
        "authors": [
            "Jun Li",
            "Heyou Chang",
            "Jian Yang"
        ],
        "abstract": "Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00825",
        "title": "Group $K$-Means",
        "authors": [
            "Jianfeng Wang",
            "Shuicheng Yan",
            "Yi Yang",
            "Mohan S Kankanhalli",
            "Shipeng Li",
            "Jingdong Wang"
        ],
        "abstract": "We study how to learn multiple dictionaries from a dataset, and approximate any data point by the sum of the codewords each chosen from the corresponding dictionary. Although theoretically low approximation errors can be achieved by the global solution, an effective solution has not been well studied in practice. To solve the problem, we propose a simple yet effective algorithm \\textit{Group $K$-Means}. Specifically, we take each dictionary, or any two selected dictionaries, as a group of $K$-means cluster centers, and then deal with the approximation issue by minimizing the approximation errors. Besides, we propose a hierarchical initialization for such a non-convex problem. Experimental results well validate the effectiveness of the approach.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00834",
        "title": "Inverse Renormalization Group Transformation in Bayesian Image Segmentations",
        "authors": [
            "Kazuyuki Tanaka",
            "Shun Kataoka",
            "Muneki Yasuda",
            "Masayuki Ohzeki"
        ],
        "abstract": "A new Bayesian image segmentation algorithm is proposed by combining a loopy belief propagation with an inverse real space renormalization group transformation to reduce the computational time. In results of our experiment, we observe that the proposed method can reduce the computational time to less than one-tenth of that taken by conventional Bayesian approaches.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00857",
        "title": "Fast forward feature selection for the nonlinear classification of hyperspectral images",
        "authors": [
            "Mathieu Fauvel",
            "Clement Dechesne",
            "Anthony Zullo",
            "Fr\u00e9d\u00e9ric Ferraty"
        ],
        "abstract": "A fast forward feature selection algorithm is presented in this paper. It is based on a Gaussian mixture model (GMM) classifier. GMM are used for classifying hyperspectral images. The algorithm selects iteratively spectral features that maximizes an estimation of the classification rate. The estimation is done using the k-fold cross validation. In order to perform fast in terms of computing time, an efficient implementation is proposed. First, the GMM can be updated when the estimation of the classification rate is computed, rather than re-estimate the full model. Secondly, using marginalization of the GMM, sub models can be directly obtained from the full model learned with all the spectral features. Experimental results for two real hyperspectral data sets show that the method performs very well in terms of classification accuracy and processing time. Furthermore, the extracted model contains very few spectral channels.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00901",
        "title": "Learning to Recognize Pedestrian Attribute",
        "authors": [
            "Yubin Deng",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Learning to recognize pedestrian attributes at far distance is a challenging problem in visual surveillance since face and body close-shots are hardly available; instead, only far-view image frames of pedestrian are given. In this study, we present an alternative approach that exploits the context of neighboring pedestrian images for improved attribute inference compared to the conventional SVM-based method. In addition, we conduct extensive experiments to evaluate the informativeness of background and foreground features for attribute recognition. Experiments are based on our newly released pedestrian attribute dataset, which is by far the largest and most diverse of its kind.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00909",
        "title": "Adaptive Objectness for Object Tracking",
        "authors": [
            "Pengpeng Liang",
            "Chunyuan Liao",
            "Xue Mei",
            "Haibin Ling"
        ],
        "abstract": "Object tracking is a long standing problem in vision. While great efforts have been spent to improve tracking performance, a simple yet reliable prior knowledge is left unexploited: the target object in tracking must be an object other than non-object. The recently proposed and popularized objectness measure provides a natural way to model such prior in visual tracking. Thus motivated, in this paper we propose to adapt objectness for visual object tracking. Instead of directly applying an existing objectness measure that is generic and handles various objects and environments, we adapt it to be compatible to the specific tracking sequence and object. More specifically, we use the newly proposed BING objectness as the base, and then train an object-adaptive objectness for each tracking task. The training is implemented by using an adaptive support vector machine that integrates information from the specific tracking target into the BING measure. We emphasize that the benefit of the proposed adaptive objectness, named ADOBING, is generic. To show this, we combine ADOBING with seven top performed trackers in recent evaluations. We run the ADOBING-enhanced trackers with their base trackers on two popular benchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking Benchmark (100 sequences). On both benchmarks, our methods not only consistently improve the base trackers, but also achieve the best known performances. Noting that the way we integrate objectness in visual tracking is generic and straightforward, we expect even more improvement by using tracker-specific objectness.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01075",
        "title": "Skincure: An Innovative Smart Phone-Based Application To Assist In Melanoma Early Detection And Prevention",
        "authors": [
            "Omar Abuzaghleh",
            "Miad Faezipour",
            "Buket D. Barkana"
        ],
        "abstract": "Melanoma spreads through metastasis, and therefore it has been proven to be very fatal. Statistical evidence has revealed that the majority of deaths resulting from skin cancer are as a result of melanoma. Further investigations have shown that the survival rates in patients depend on the stage of the infection; early detection and intervention of melanoma implicates higher chances of cure. Clinical diagnosis and prognosis of melanoma is challenging since the processes are prone to misdiagnosis and inaccuracies due to doctors subjectivity. This paper proposes an innovative and fully functional smart-phone based application to assist in melanoma early detection and prevention. The application has two major components; the first component is a real-time alert to help users prevent skin burn caused by sunlight; a novel equation to compute the time for skin to burn is thereby introduced. The second component is an automated image analysis module which contains image acquisition, hair detection and exclusion, lesion segmentation, feature extraction, and classification. The proposed system exploits PH2 Dermoscopy image database from Pedro Hispano Hospital for development and testing purposes. The image database contains a total of 200 dermoscopy images of lesions, including normal, atypical, and melanoma cases. The experimental results show that the proposed system is efficient, achieving classification of the normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and 97.5%, respectively.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01083",
        "title": "Stem-Calyx Recognition of an Apple using Shape Descriptors",
        "authors": [
            "S.H. Mohana",
            "C.J. Prabhakar"
        ],
        "abstract": "This paper presents a novel method to recognize stem - calyx of an apple using shape descriptors. The main drawback of existing apple grading techniques is that stem - calyx part of an apple is treated as defects, this leads to poor grading of apples. In order to overcome this drawback, we proposed an approach to recognize stem-calyx and differentiated from true defects based on shape features. Our method comprises of steps such as segmentation of apple using grow-cut method, candidate objects such as stem-calyx and small defects are detected using multi-threshold segmentation. The shape features are extracted from detected objects using Multifractal, Fourier and Radon descriptor and finally stem-calyx regions are recognized and differentiated from true defects using SVM classifier. The proposed algorithm is evaluated using experiments conducted on apple image dataset and results exhibit considerable improvement in recognition of stem-calyx region compared to other techniques.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01090",
        "title": "A Novel Technique for Grading of Dates using Shape and Texture Features",
        "authors": [
            "S.H. Mohana",
            "C.J. Prabhakar"
        ],
        "abstract": "This paper presents a novel method to grade the date fruits based on the combination of shape and texture features. The method begins with reducing the specular reflection and small noise using a bilateral filter. Threshold based segmentation is performed for background removal and fruit part selection from the given image. Shape features is extracted using the contour of the date fruit and texture features are extracted using Curvelet transform and Local Binary Pattern (LBP) from the selected date fruit region. Finally, combinations of shape and texture features are fused to grade the dates into six grades. k-Nearest Neighbour(k-NN) classifier yields the best grading rate compared to other two classifiers such as Support Vector Machine (SVM) and Linear Discriminant(LDA) classifiers. The experiment result shows that our technique achieves highest accuracy.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01106",
        "title": "A Study on Clustering for Clustering Based Image De-Noising",
        "authors": [
            "Hossein Bakhshi Golestani",
            "Mohsen Joneidi",
            "Mostafa Sadeghi"
        ],
        "abstract": "In this paper, the problem of de-noising of an image contaminated with Additive White Gaussian Noise (AWGN) is studied. This subject is an open problem in signal processing for more than 50 years. Local methods suggested in recent years, have obtained better results than global methods. However by more intelligent training in such a way that first, important data is more effective for training, second, clustering in such way that training blocks lie in low-rank subspaces, we can design a dictionary applicable for image de-noising and obtain results near the state of the art local methods. In the present paper, we suggest a method based on global clustering of image constructing blocks. As the type of clustering plays an important role in clustering-based de-noising methods, we address two questions about the clustering. The first, which parts of the data should be considered for clustering? and the second, what data clustering method is suitable for de-noising.? Then clustering is exploited to learn an over complete dictionary. By obtaining sparse decomposition of the noisy image blocks in terms of the dictionary atoms, the de-noised version is achieved. In addition to our framework, 7 popular dictionary learning methods are simulated and compared. The results are compared based on two major factors: (1) de-noising performance and (2) execution time. Experimental results show that our dictionary learning framework outperforms its competitors in terms of both factors.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01181",
        "title": "Object localization in ImageNet by looking out of the window",
        "authors": [
            "Alexander Vezhnevets",
            "Vittorio Ferrari"
        ],
        "abstract": "We propose a method for annotating the location of objects in ImageNet. Traditionally, this is cast as an image window classification problem, where each window is considered independently and scored based on its appearance alone. Instead, we propose a method which scores each candidate window in the context of all other windows in the image, taking into account their similarity in appearance space as well as their spatial relations in the image plane. We devise a fast and exact procedure to optimize our scoring function over all candidate windows in an image, and we learn its parameters using structured output regression. We demonstrate on 92000 images from ImageNet that this significantly improves localization over recent techniques that score windows in isolation.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01186",
        "title": "Analysing domain shift factors between videos and images for object detection",
        "authors": [
            "Vicky Kalogeiton",
            "Vittorio Ferrari",
            "Cordelia Schmid"
        ],
        "abstract": "Object detection is one of the most important challenges in computer vision. Object detectors are usually trained on bounding-boxes from still images. Recently, video has been used as an alternative source of data. Yet, for a given test domain (image or video), the performance of the detector depends on the domain it was trained on. In this paper, we examine the reasons behind this performance gap. We define and evaluate different domain shift factors: spatial location accuracy, appearance diversity, image quality and aspect distribution. We examine the impact of these factors by comparing performance before and after factoring them out. The results show that all four factors affect the performance of the detectors and their combined effect explains nearly the whole performance gap.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01364",
        "title": "Leader Follower Formation Control of Ground Vehicles Using Camshift Based Guidance",
        "authors": [
            "S.M. Vaitheeswaran",
            "Bharath M.K.",
            "Gokul M"
        ],
        "abstract": "Autonomous ground vehicles have been designed for the purpose of that relies on ranging and bearing information received from forward looking camera on the Formation control . A visual guidance control algorithm is designed where real time image processing is used to provide feedback signals. The vision subsystem and control subsystem work in parallel to accomplish formation control. A proportional navigation and line of sight guidance laws are used to estimate the range and bearing information from the leader vehicle using the vision subsystem. The algorithms for vision detection and localization used here are similar to approaches for many computer vision tasks such as face tracking and detection that are based color-and texture based features, and non-parametric Continuously Adaptive Mean-shift algorithms to keep track of the leader. This is being proposed for the first time in the leader follower framework. The algorithms are simple but effective for real time and provide an alternate approach to traditional based approaches like the Viola Jones algorithm. Further to stabilize the follower to the leader trajectory, the sliding mode controller is used to dynamically track the leader. The performance of the results is demonstrated in simulation and in practical experiments.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01372",
        "title": "Weighted Schatten $p$-Norm Minimization for Image Denoising with Local and Nonlocal Regularization",
        "authors": [
            "Yuan Xie"
        ],
        "abstract": "This paper presents a patch-wise low-rank based image denoising method with constrained variational model involving local and nonlocal regularization. On one hand, recent patch-wise methods can be represented as a low-rank matrix approximation problem whose convex relaxation usually depends on nuclear norm minimization (NNM). Here, we extend the NNM to the nonconvex schatten p-norm minimization with additional weights assigned to different singular values, which is referred to as the Weighted Schatten p-Norm Minimization (WSNM). An efficient algorithm is also proposed to solve the WSNM problem. The proposed WSNM not only gives better approximation to the original low-rank assumption, but also considers physical meanings of different data components. On the other hand, due to the naive aggregation schema which integrates all the denoised patches into a whole image, current patch-wise denoising methods always produce various degree of artifacts in denoised results. Therefore, to further reduce artifacts, a data-driven regularizer called Steering Total Variation (STV) combined with nonlocal TV is derived for a variational model, which imposes local and nonlocal consistency constraints on the patch-wise denoised image. A highly simple but efficient algorithm is proposed to solve this variational model with convergence guarantee. Both WSNM and local \\& nonlocal consistent regularization are integrated into an iterative restoration framework to produce final results. Extensive experimental testing shows, both qualitatively and quantitatively, that the proposed method can effectively remove noise, as well as reduce artifacts compared with state-of-the-art methods.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01548",
        "title": "Implementation of Auto Monitoring and Short-Message-Service System via GSM Modem",
        "authors": [
            "Akilan Thangarajah",
            "Buddhapala Wongkaew",
            "Mongkol Ekpanyapong"
        ],
        "abstract": "Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring system for any critical operational environments. It detects an undesired event occurring in the environment, generates an alert with detailed message and sends it to the user to prevent hazards. This system employs a Friendly ARM as main controller while, sensors and terminals to interact with the real world. A GSM network is utilized to bridge the communication between monitoring system and user. This paper presents details of prototyping the system.\n    ",
        "submission_date": "2015-01-07T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01697",
        "title": "Super-resolution MRI Using Finite Rate of Innovation Curves",
        "authors": [
            "Greg Ongie",
            "Mathews Jacob"
        ],
        "abstract": "We propose a two-stage algorithm for the super-resolution of MR images from their low-frequency k-space samples. In the first stage we estimate a resolution-independent mask whose zeros represent the edges of the image. This builds off recent work extending the theory of sampling signals of finite rate of innovation (FRI) to two-dimensional curves. We enable its application to MRI by proposing extensions of the signal models allowed by FRI theory, and by developing a more robust and efficient means to determine the edge mask. In the second stage of the scheme, we recover the super-resolved MR image using the discretized edge mask as an image prior. We evaluate our scheme on simulated single-coil MR data obtained from analytical phantoms, and compare against total variation reconstructions. Our experiments show improved performance in both noiseless and noisy settings.\n    ",
        "submission_date": "2015-01-08T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01723",
        "title": "An Effective Image Feature Classiffication using an improved SOM",
        "authors": [
            "M. Abdelsamea",
            "Marghny H. Mohamed",
            "Mohamed Bamatraf"
        ],
        "abstract": "Image feature classification is a challenging problem in many computer vision applications, specifically, in the fields of remote sensing, image analysis and pattern recognition. In this paper, a novel Self Organizing Map, termed improved SOM (iSOM), is proposed with the aim of effectively classifying Mammographic images based on their texture feature representation. The main contribution of the iSOM is to introduce a new node structure for the map representation and adopting a learning technique based on Kohonen SOM accordingly. The main idea is to control, in an unsupervised fashion, the weight updating procedure depending on the class reliability of the node, during the weight update time. Experiments held on a real Mammographic images. Results showed high accuracy compared to classical SOM and other state-of-art classifiers.\n    ",
        "submission_date": "2015-01-08T00:00:00",
        "last_modified_date": "2015-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01744",
        "title": "Optimal Radiometric Calibration for Camera-Display Communication",
        "authors": [
            "Wenjia Yuan",
            "Eric Wengrowski",
            "Kristin J. Dana",
            "Ashwin Ashok",
            "Marco Gruteser",
            "Narayan Mandayam"
        ],
        "abstract": "We present a novel method for communicating between a camera and display by embedding and recovering hidden and dynamic information within a displayed image. A handheld camera pointed at the display can receive not only the display image, but also the underlying message. These active scenes are fundamentally different from traditional passive scenes like QR codes because image formation is based on display emittance, not surface reflectance. Detecting and decoding the message requires careful photometric modeling for computational message recovery. Unlike standard watermarking and steganography methods that lie outside the domain of computer vision, our message recovery algorithm uses illumination to optically communicate hidden messages in real world scenes. The key innovation of our approach is an algorithm that performs simultaneous radiometric calibration and message recovery in one convex optimization problem. By modeling the photometry of the system using a camera-display transfer function (CDTF), we derive a physics-based kernel function for support vector machine classification. We demonstrate that our method of optimal online radiometric calibration (OORC) leads to an efficient and robust algorithm for computational messaging between nine commercial cameras and displays.\n    ",
        "submission_date": "2015-01-08T00:00:00",
        "last_modified_date": "2015-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02113",
        "title": "Filter Design and Performance Evaluation for Fingerprint Image Segmentation",
        "authors": [
            "Duy Hoang Thai",
            "Stephan Huckemann",
            "Carsten Gottschlich"
        ],
        "abstract": "Fingerprint recognition plays an important role in many commercial applications and is used by millions of people every day, e.g. for unlocking mobile phones. Fingerprint image segmentation is typically the first processing step of most fingerprint algorithms and it divides an image into foreground, the region of interest, and background. Two types of error can occur during this step which both have a negative impact on the recognition performance: 'true' foreground can be labeled as background and features like minutiae can be lost, or conversely 'true' background can be misclassified as foreground and spurious features can be introduced. The contribution of this paper is threefold: firstly, we propose a novel factorized directional bandpass (FDB) segmentation method for texture extraction based on the directional Hilbert transform of a Butterworth bandpass (DHBB) filter interwoven with soft-thresholding. Secondly, we provide a manually marked ground truth segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a systematic performance comparison between the FDB method and four of the most often cited fingerprint segmentation algorithms showing that the FDB segmentation method clearly outperforms these four widely used methods. The benchmark and the implementation of the FDB method are made publicly available.\n    ",
        "submission_date": "2015-01-09T00:00:00",
        "last_modified_date": "2015-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02372",
        "title": "Efficient Rotation-Scaling-Translation Parameters Estimation Based on Fractal Image Model",
        "authors": [
            "M. Uss",
            "B. Vozel",
            "V.Lukin",
            "K. Chehdi"
        ],
        "abstract": "This paper deals with area-based subpixel image registration under rotation-isometric scaling-translation transformation hypothesis. Our approach is based on a parametrical modeling of geometrically transformed textural image fragments and maximum likelihood estimation of transformation vector between them. Due to the parametrical approach based on the fractional Brownian motion modeling of the local fragments texture, the proposed estimator MLfBm (ML stands for \"Maximum Likelihood\" and fBm for \"Fractal Brownian motion\") has the ability to better adapt to real image texture content compared to other methods relying on universal similarity measures like mutual information or normalized correlation. The main benefits are observed when assumptions underlying the fBm model are fully satisfied, e.g. for isotropic normally distributed textures with stationary increments. Experiments on both simulated and real images and for high and weak correlation between registered images show that the MLfBm estimator offers significant improvement compared to other state-of-the-art methods. It reduces translation vector, rotation angle and scaling factor estimation errors by a factor of about 1.75...2 and it decreases probability of false match by up to 5 times. Besides, an accurate confidence interval for MLfBm estimates can be obtained from the Cramer-Rao lower bound on rotation-scaling-translation parameters estimation error. This bound depends on texture roughness, noise level in reference and template images, correlation between these images and geometrical transformation parameters.\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02393",
        "title": "Riemannian Metric Learning for Symmetric Positive Definite Matrices",
        "authors": [
            "Raviteja Vemulapalli",
            "David W. Jacobs"
        ],
        "abstract": "Over the past few years, symmetric positive definite (SPD) matrices have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks.\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02530",
        "title": "A Dataset for Movie Description",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Niket Tandon",
            "Bernt Schiele"
        ],
        "abstract": "Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed DVS, which is temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing DVS to scripts, we find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02555",
        "title": "Tri-Subject Kinship Verification: Understanding the Core of A Family",
        "authors": [
            "Xiaoqian Qin",
            "Xiaoyang Tan",
            "Songcan Chen"
        ],
        "abstract": "One major challenge in computer vision is to go beyond the modeling of individual objects and to investigate the bi- (one-versus-one) or tri- (one-versus-two) relationship among multiple visual entities, answering such questions as whether a child in a photo belongs to given parents. The child-parents relationship plays a core role in a family and understanding such kin relationship would have fundamental impact on the behavior of an artificial intelligent agent working in the human world. In this work, we tackle the problem of one-versus-two (tri-subject) kinship verification and our contributions are three folds: 1) a novel relative symmetric bilinear model (RSBM) introduced to model the similarity between the child and the parents, by incorporating the prior knowledge that a child may resemble a particular parent more than the other; 2) a spatially voted method for feature selection, which jointly selects the most discriminative features for the child-parents pair, while taking local spatial information into account; 3) a large scale tri-subject kinship database characterized by over 1,000 child-parents families. Extensive experiments on KinFaceW, Family101 and our newly released kinship database show that the proposed method outperforms several previous state of the art methods, while could also be used to significantly boost the performance of one-versus-one kinship verification when the information about both parents are available.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02565",
        "title": "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow",
        "authors": [
            "Jerome Revaud",
            "Philippe Weinzaepfel",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02741",
        "title": "Salient Object Detection: A Benchmark",
        "authors": [
            "Ali Borji",
            "Ming-Ming Cheng",
            "Huaizu Jiang",
            "Jia Li"
        ],
        "abstract": "We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2018-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02825",
        "title": "A Survey on Recent Advances of Computer Vision Algorithms for Egocentric Video",
        "authors": [
            "Sven Bambach"
        ],
        "abstract": "Recent technological advances have made lightweight, head mounted cameras both practical and affordable and products like Google Glass show first approaches to introduce the idea of egocentric (first-person) video to the mainstream. Interestingly, the computer vision community has only recently started to explore this new domain of egocentric vision, where research can roughly be categorized into three areas: Object recognition, activity detection/recognition, video summarization. In this paper, we try to give a broad overview about the different problems that have been addressed and collect and compare evaluation results. Moreover, along with the emergence of this new domain came the introduction of numerous new and versatile benchmark datasets, which we summarize and compare as well.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02876",
        "title": "Deep Image: Scaling up Image Recognition",
        "authors": [
            "Ren Wu",
            "Shengen Yan",
            "Yi Shan",
            "Qingqing Dang",
            "Gang Sun"
        ],
        "abstract": "We present a state-of-the-art image recognition system, Deep Image, developed using end-to-end deep learning. The key components are a custom-built supercomputer dedicated to deep learning, a highly optimized parallel algorithm using new strategies for data partitioning and communication, larger deep neural network models, novel data augmentation approaches, and usage of multi-scale high-resolution images. Our method achieves excellent results on multiple challenging computer vision benchmarks.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02887",
        "title": "Online Handwritten Devanagari Stroke Recognition Using Extended Directional Features",
        "authors": [
            "Lajish VL",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "This paper describes a new feature set, called the extended directional features (EDF) for use in the recognition of online handwritten strokes. We use EDF specifically to recognize strokes that form a basis for producing Devanagari script, which is the most widely used Indian language script. It should be noted that stroke recognition in handwritten script is equivalent to phoneme recognition in speech signals and is generally very poor and of the order of 20% for singing voice. Experiments are conducted for the automatic recognition of isolated handwritten strokes. Initially we describe the proposed feature set, namely EDF and then show how this feature can be effectively utilized for writer independent script recognition through stroke recognition. Experimental results show that the extended directional feature set performs well with about 65+% stroke level recognition accuracy for writer independent data set.\n    ",
        "submission_date": "2015-01-11T00:00:00",
        "last_modified_date": "2015-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02894",
        "title": "A Modified No Search Algorithm for Fractal Image Compression",
        "authors": [
            "Mehdi. Salarian",
            "Babak. Mohamadinia",
            "Jalil Rasekhi"
        ],
        "abstract": "Fractal image compression has some desirable properties like high quality at high compression ratio, fast decoding, and resolution independence. Therefore it can be used for many applications such as texture mapping and pattern recognition and image watermarking. But it suffers from long encoding time due to its need to find the best match between sub blocks. This time is related to the approach that is used. In this paper we present a fast encoding Algorithm based on no search method. Our goal is that more blocks are covered in initial step of quad tree algorithm. Experimental result has been compared with other new fast fractal coding methods, showing it is better in term of bit rate in same condition while the other parameters are fixed.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03058",
        "title": "An Adaptive Neuro-Fuzzy Inference System Modeling for Grid-Adaptive Interpolation over Depth Images",
        "authors": [
            "Arbaaz Singh Sidhu"
        ],
        "abstract": "A suitable interpolation method is essential to keep the noise level minimum along with the time-delay. In recent years, many different interpolation filters have been developed for instance H.264-6 tap filter, and AVS- 4 tap filter. The present work uses Adaptive Neuro-Fuzzy Inference System (ANFIS) technique to model and investigate the effects of a four-tap low-pass tap filter (Grid-adaptive filter) on a hole-filled depth image. The work demonstrates the general form of uniform interpolations for both integer and sub-pixel locations in terms of the sampling interval and filter length of depth-images via diverse finite impulse response filtering schemes. The demonstrated model combined modelling function of fuzzy inference with the learning ability of artificial neural network.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03069",
        "title": "Learning from Multiple Sources for Video Summarisation",
        "authors": [
            "Xiatian Zhu",
            "Chen Change Loy",
            "Shaogang Gong"
        ],
        "abstract": "Many visual surveillance tasks, ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03124",
        "title": "Robust and Real Time Detection of Curvy Lanes (Curves) with Desired Slopes for Driving Assistance and Autonomous Vehicles",
        "authors": [
            "Amartansh Dubey",
            "K. M. Bhurchandi"
        ],
        "abstract": "One of the biggest reasons for road accidents is curvy lanes and blind turns. Even one of the biggest hurdles for new autonomous vehicles is to detect curvy lanes, multiple lanes and lanes with a lot of discontinuity and noise. This paper presents very efficient and advanced algorithm for detecting curves having desired slopes (especially for detecting curvy lanes in real time) and detection of curves (lanes) with a lot of noise, discontinuity and disturbances. Overall aim is to develop robust method for this task which is applicable even in adverse conditions. Even in some of most famous and useful libraries like OpenCV and Matlab, there is no function available for detecting curves having desired slopes , shapes, discontinuities. Only few predefined shapes like circle, ellipse, etc, can be detected using presently available functions. Proposed algorithm can not only detect curves with discontinuity, noise, desired slope but also it can perform shadow and illumination correction and detect/ differentiate between different curves.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03271",
        "title": "Higher dimensional homodyne filtering for suppression of incidental phase artifacts in multichannel MRI",
        "authors": [
            "Joseph Suresh Paul",
            "Uma Krishna Swamy Pillai"
        ],
        "abstract": "The aim of this paper is to introduce procedural steps for extension of the 1D homodyne phase correction for k-space truncation in all gradient encoding directions. Compared to the existing method applied to 2D partial k-space, signal losses introduced by the phase correction filter is observed to be minimal for the extended approach. In addition, the modified form of phase correction mitigates Incidental Phase Artifacts (IPA) due to truncation. For parallel imaging with undersampling along phase encode direction, the extended homodyne filtering is shown to be effective for minimizing these artifacts when each of the channel k-spaces are truncated along both phase and frequency encode directions. This is illustrated with 2D partial k-space for flow compensated multichannel Susceptibility Weighted Imaging (SWI). Extension of our method to 3D partial k-space shows improved reconstruction of flow information in phase contrast angiography.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03320",
        "title": "Image enhancement in intensity projected multichannel MRI using spatially adaptive directional anisotropic diffusion",
        "authors": [
            "P. K. Akshara",
            "J. S. Paul"
        ],
        "abstract": "Anisotropic Diffusion is widely used for noise reduction with simultaneous preservation of vascular structures in maximum intensity projected (MIP) angiograms. However, extension to minimum intensity projected (mIP) venograms in Susceptibility Weighted Imaging (SWI) poses difficulties due to spatially varying baseline. Here, we introduce a modified version of the directional anisotropic diffusion which allows us to simultaneously reduce the noise and enhance vascular structures reconstructed using both M/mIP angiograms. This method is based on spatial adaptation of the diffusion function, separately in the directions of the gradient, and along those of the minimum and maximum curvatures. The existing approach of directional anisotropic diffusion uses binary switched diffusion function to ensure diffusion along the direction of maximum curvature stopped near the vessel borders. Here, the choice of a threshold for detecting the upper limit of diffusion becomes difficult in the presence of spatially varying baseline. Also, the approach of using vesselness measure to steer the diffusion process results in structural discontinuities due to junction suppression in mIP. The merits of the proposed method include elimination of the need for an apriori choice of a threshold to detect the vessel, and problems due to junction suppression. The proposed method is also extended to multi-channel phase contrast angiogram.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03383",
        "title": "On the Distribution of Salient Objects in Web Images and its Influence on Salient Object Detection",
        "authors": [
            "Boris Schauerte",
            "Rainer Stiefelhagen"
        ],
        "abstract": "It has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer's tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. We investigate the influence of the photographer's center bias on salient object detection, extending our previous work. We show that the centroid locations of salient objects in photographs of Achanta and Liu's data set in fact correlate strongly with a Gaussian model. This is an important insight, because it provides an empirical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two state-of-the-art salient object detection algorithms. This way, first, we quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection. Second, we improve the performance in terms of F1 score, Fb score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu. Third, by debiasing Cheng et al.'s region contrast model, we exemplarily demonstrate that implicit center biases are partially responsible for the outstanding performance of state-of-the-art algorithms. Last but not least, as a result of debiasing Cheng et al.'s algorithm, we introduce a non-biased salient object detection method, which is of interest for applications in which the image data is not likely to have a photographer's center bias (e.g., image data of surveillance cameras or autonomous robots).\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03719",
        "title": "LATCH: Learned Arrangements of Three Patch Codes",
        "authors": [
            "Gil Levi",
            "Tal Hassner"
        ],
        "abstract": "We present a novel means of describing local image appearances using binary strings. Binary descriptors have drawn increasing interest in recent years due to their speed and low memory footprint. A known shortcoming of these representations is their inferior performance compared to larger, histogram based descriptors such as the SIFT. Our goal is to close this performance gap while maintaining the benefits attributed to binary representations. To this end we propose the Learned Arrangements of Three Patch Codes descriptors, or LATCH. Our key observation is that existing binary descriptors are at an increased risk from noise and local appearance variations. This, as they compare the values of pixel pairs; changes to either of the pixels can easily lead to changes in descriptor values, hence damaging its performance. In order to provide more robustness, we instead propose a novel means of comparing pixel patches. This ostensibly small change, requires a substantial redesign of the descriptors themselves and how they are produced. Our resulting LATCH representation is rigorously compared to state-of-the-art binary descriptors and shown to provide far better performance for similar computation and space requirements.\n    ",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03755",
        "title": "Screen Content Image Segmentation Using Least Absolute Deviation Fitting",
        "authors": [
            "Shervin Minaee",
            "Yao Wang"
        ],
        "abstract": "We propose an algorithm for separating the foreground (mainly text and line graphics) from the smoothly varying background in screen content images. The proposed method is designed based on the assumption that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity and cannot be modeled by this smooth representation. The algorithm separates the background and foreground using a least absolute deviation method to fit the smooth model to the image pixels. This algorithm has been tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other popular methods, such as k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. Such background/foreground segmentation are important pre-processing steps for text extraction and separate coding of background and foreground for compression of screen content images.\n    ",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03771",
        "title": "Submodular relaxation for inference in Markov random fields",
        "authors": [
            "Anton Osokin",
            "Dmitry Vetrov"
        ],
        "abstract": "In this paper we address the problem of finding the most probable state of a discrete Markov random field (MRF), also known as the MRF energy minimization problem. The task is known to be NP-hard in general and its practical importance motivates numerous approximate algorithms. We propose a submodular relaxation approach (SMR) based on a Lagrangian relaxation of the initial problem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMR does not decompose the graph structure of the initial problem but constructs a submodular energy that is minimized within the Lagrangian relaxation. Our approach is applicable to both pairwise and high-order MRFs and allows to take into account global potentials of certain types. We study theoretical properties of the proposed approach and evaluate it experimentally.\n    ",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03779",
        "title": "Computer-assisted polyp matching between optical colonoscopy and CT colonography: a phantom study",
        "authors": [
            "Holger R. Roth",
            "Thomas E. Hampshire",
            "Emma Helbren",
            "Mingxing Hu",
            "Roser Vega",
            "Steve Halligan",
            "David J. Hawkes"
        ],
        "abstract": "Potentially precancerous polyps detected with CT colonography (CTC) need to be removed subsequently, using an optical colonoscope (OC). Due to large colonic deformations induced by the colonoscope, even very experienced colonoscopists find it difficult to pinpoint the exact location of the colonoscope tip in relation to polyps reported on CTC. This can cause unduly prolonged OC examinations that are stressful for the patient, colonoscopist and supporting staff.\n",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03879",
        "title": "A new ADMM algorithm for the Euclidean median and its application to robust patch regression",
        "authors": [
            "Kunal N. Chaudhury",
            "K. R. Ramakrishnan"
        ],
        "abstract": "The Euclidean Median (EM) of a set of points $\\Omega$ in an Euclidean space is the point x minimizing the (weighted) sum of the Euclidean distances of x to the points in $\\Omega$. While there exits no closed-form expression for the EM, it can nevertheless be computed using iterative methods such as the Wieszfeld algorithm. The EM has classically been used as a robust estimator of centrality for multivariate data. It was recently demonstrated that the EM can be used to perform robust patch-based denoising of images by generalizing the popular Non-Local Means algorithm. In this paper, we propose a novel algorithm for computing the EM (and its box-constrained counterpart) using variable splitting and the method of augmented Lagrangian. The attractive feature of this approach is that the subproblems involved in the ADMM-based optimization of the augmented Lagrangian can be resolved using simple closed-form projections. The proposed ADMM solver is used for robust patch-based image denoising and is shown to exhibit faster convergence compared to an existing solver.\n    ",
        "submission_date": "2015-01-16T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03952",
        "title": "Mind the Gap: Subspace based Hierarchical Domain Adaptation",
        "authors": [
            "Anant Raj",
            "Vinay P. Namboodiri",
            "Tinne Tuytelaars"
        ],
        "abstract": "Domain adaptation techniques aim at adapting a classifier learnt on a source domain to work on the target domain. Exploiting the subspaces spanned by features of the source and target domains respectively is one approach that has been investigated towards solving this problem. These techniques normally assume the existence of a single subspace for the entire source / target domain. In this work, we consider the hierarchical organization of the data and consider multiple subspaces for the source and target domain based on the hierarchy. We evaluate different subspace based domain adaptation techniques under this setting and observe that using different subspaces based on the hierarchy yields consistent improvement over a non-hierarchical baseline\n    ",
        "submission_date": "2015-01-16T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04009",
        "title": "Visual Analytics of Image-Centric Cohort Studies in Epidemiology",
        "authors": [
            "Bernhard Preim",
            "Paul Klemm",
            "Helwig Hauser",
            "Katrin Hegenscheid",
            "Steffen Oeltze",
            "Klaus Toennies",
            "Henry V\u00f6lzke"
        ],
        "abstract": "Epidemiology characterizes the influence of causes to disease and health conditions of defined populations. Cohort studies are population-based studies involving usually large numbers of randomly selected individuals and comprising numerous attributes, ranging from self-reported interview data to results from various medical examinations, e.g., blood and urine samples. Since recently, medical imaging has been used as an additional instrument to assess risk factors and potential prognostic information. In this chapter, we discuss such studies and how the evaluation may benefit from visual analytics. Cluster analysis to define groups, reliable image analysis of organs in medical imaging data and shape space exploration to characterize anatomical shapes are among the visual analytics tools that may enable epidemiologists to fully exploit the potential of their huge and complex data. To gain acceptance, visual analytics tools need to complement more classical epidemiologic tools, primarily hypothesis-driven statistical analysis.\n    ",
        "submission_date": "2015-01-15T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04140",
        "title": "A Fast Fractal Image Compression Algorithm Using Predefined Values for Contrast Scaling",
        "authors": [
            "H. Miar Naimi",
            "M. Salarian"
        ],
        "abstract": "In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of scanning the parameter space [0,1]. Within this approach only domain blocks with entropies greater than a threshold are considered. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above. The algorithm has been examined for some well-known images. This result shows that our proposed algorithm considerably reduces the encoding time producing images that are approximately the same in quality.\n    ",
        "submission_date": "2015-01-17T00:00:00",
        "last_modified_date": "2015-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04163",
        "title": "Meaningful Objects Segmentation from SAR Images via A Multi-Scale Non-Local Active Contour Model",
        "authors": [
            "Gui-Song Xia",
            "Gang Liu",
            "Wen Yang"
        ],
        "abstract": "The segmentation of synthetic aperture radar (SAR) images is a longstanding yet challenging task, not only because of the presence of speckle, but also due to the variations of surface backscattering properties in the images. Tremendous investigations have been made to eliminate the speckle effects for the segmentation of SAR images, while few work devotes to dealing with the variations of backscattering coefficients in the images. In order to overcome both the two difficulties, this paper presents a novel SAR image segmentation method by exploiting a multi-scale active contour model based on the non-local processing principle. More precisely, we first formulize the SAR segmentation problem with an active contour model by integrating the non-local interactions between pairs of patches inside and outside the segmented regions. Secondly, a multi-scale strategy is proposed to speed up the non-local active contour segmentation procedure and to avoid falling into local minimum for achieving more accurate segmentation results. Experimental results on simulated and real SAR images demonstrate the efficiency and feasibility of the proposed method: it can not only achieve precise segmentations for images with heavy speckles and non-local intensity variations, but also can be used for SAR images from different types of sensors.\n    ",
        "submission_date": "2015-01-17T00:00:00",
        "last_modified_date": "2015-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04276",
        "title": "Correlation Adaptive Subspace Segmentation by Trace Lasso",
        "authors": [
            "Canyi Lu",
            "Jiashi Feng",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "abstract": "This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using $\\ell^1$-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-Rank Representation (LRR), by rank minimization, and Least Squares Regression (LSR), by $\\ell^2$-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR.\n",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04277",
        "title": "Correntropy Induced L2 Graph for Robust Subspace Clustering",
        "authors": [
            "Canyi Lu",
            "Jinhui Tang",
            "Min Lin",
            "Liang Lin",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the non-convex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04284",
        "title": "Pairwise Constraint Propagation on Multi-View Data",
        "authors": [
            "Zhiwu Lu",
            "Liwei Wang"
        ],
        "abstract": "This paper presents a graph-based learning approach to pairwise constraint propagation on multi-view data. Although pairwise constraint propagation has been studied extensively, pairwise constraints are usually defined over pairs of data points from a single view, i.e., only intra-view constraint propagation is considered for multi-view tasks. In fact, very little attention has been paid to inter-view constraint propagation, which is more challenging since pairwise constraints are now defined over pairs of data points from different views. In this paper, we propose to decompose the challenging inter-view constraint propagation problem into semi-supervised learning subproblems so that they can be efficiently solved based on graph-based label propagation. To the best of our knowledge, this is the first attempt to give an efficient solution to inter-view constraint propagation from a semi-supervised learning viewpoint. Moreover, since graph-based label propagation has been adopted for basic optimization, we develop two constrained graph construction methods for interview constraint propagation, which only differ in how the intra-view pairwise constraints are exploited. The experimental results in cross-view retrieval have shown the promising performance of our inter-view constraint propagation.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04292",
        "title": "Image classification by visual bag-of-words refinement and reduction",
        "authors": [
            "Zhiwu Lu",
            "Liwei Wang",
            "Ji-Rong Wen"
        ],
        "abstract": "This paper presents a new framework for visual bag-of-words (BOW) refinement and reduction to overcome the drawbacks associated with the visual BOW model which has been widely used for image classification. Although very influential in the literature, the traditional visual BOW model has two distinct drawbacks. Firstly, for efficiency purposes, the visual vocabulary is commonly constructed by directly clustering the low-level visual feature vectors extracted from local keypoints, without considering the high-level semantics of images. That is, the visual BOW model still suffers from the semantic gap, and thus may lead to significant performance degradation in more challenging tasks (e.g. social image classification). Secondly, typically thousands of visual words are generated to obtain better performance on a relatively large image dataset. Due to such large vocabulary size, the subsequent image classification may take sheer amount of time. To overcome the first drawback, we develop a graph-based method for visual BOW refinement by exploiting the tags (easy to access although noisy) of social images. More notably, for efficient image classification, we further reduce the refined visual BOW model to a much smaller size through semantic spectral clustering. Extensive experimental results show the promising performance of the proposed framework for visual BOW refinement and reduction.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04367",
        "title": "Reconstruction-free action inference from compressive imagers",
        "authors": [
            "Kuldeep Kulkarni",
            "Pavan Turaga"
        ],
        "abstract": "Persistent surveillance from camera networks, such as at parking lots, UAVs, etc., often results in large amounts of video data, resulting in significant challenges for inference in terms of storage, communication and computation. Compressive cameras have emerged as a potential solution to deal with the data deluge issues in such applications. However, inference tasks such as action recognition require high quality features which implies reconstructing the original video data. Much work in compressive sensing (CS) theory is geared towards solving the reconstruction problem, where state-of-the-art methods are computationally intensive and provide low-quality results at high compression rates. Thus, reconstruction-free methods for inference are much desired. In this paper, we propose reconstruction-free methods for action recognition from compressive cameras at high compression ratios of 100 and above. Recognizing actions directly from CS measurements requires features which are mostly nonlinear and thus not easily applicable. This leads us to search for such properties that are preserved in compressive measurements. To this end, we propose the use of spatio-temporal smashed filters, which are compressive domain versions of pixel-domain matched filters. We conduct experiments on publicly available databases and show that one can obtain recognition rates that are comparable to the oracle method in uncompressed setup, even for high compression ratios.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2015-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04378",
        "title": "Instance Significance Guided Multiple Instance Boosting for Robust Visual Tracking",
        "authors": [
            "Jinwu Liu",
            "Yao Lu",
            "Tianfei Zhou"
        ],
        "abstract": "Multiple Instance Learning (MIL) recently provides an appealing way to alleviate the drifting problem in visual tracking. Following the tracking-by-detection framework, an online MILBoost approach is developed that sequentially chooses weak classifiers by maximizing the bag likelihood. In this paper, we extend this idea towards incorporating the instance significance estimation into the online MILBoost framework. First, instead of treating all instances equally, with each instance we associate a significance-coefficient that represents its contribution to the bag likelihood. The coefficients are estimated by a simple Bayesian formula that jointly considers the predictions from several standard MILBoost classifiers. Next, we follow the online boosting framework, and propose a new criterion for the selection of weak classifiers. Experiments with challenging public datasets show that the proposed method outperforms both existing MIL based and boosting based trackers.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2020-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04505",
        "title": "Robust Visual Tracking via Convolutional Networks",
        "authors": [
            "Kaihua Zhang",
            "Qingshan Liu",
            "Yi Wu",
            "Ming-Hsuan Yang"
        ],
        "abstract": "Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to develop a robust representation for visual tracking. In the first frame, we employ the k-means algorithm to extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and the useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps form together a global representation, which is built on mid-level features, thereby remaining close to image-level information, and hence the inner geometric layout of the target is also well preserved. A simple soft shrinkage method with an adaptive threshold is employed to de-noise the global representation, resulting in a robust sparse representation. The representation is updated via a simple and effective online strategy, allowing it to robustly adapt to target appearance variations. Our convolution networks have surprisingly lightweight structure, yet perform favorably against several state-of-the-art methods on the CVPR2013 tracking benchmark dataset with 50 challenging videos.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04537",
        "title": "Coupled Depth Learning",
        "authors": [
            "Mohammad Haris Baig",
            "Lorenzo Torresani"
        ],
        "abstract": "In this paper we propose a method for estimating depth from a single image using a coarse to fine approach. We argue that modeling the fine depth details is easier after a coarse depth map has been computed. We express a global (coarse) depth map of an image as a linear combination of a depth basis learned from training examples. The depth basis captures spatial and statistical regularities and reduces the problem of global depth estimation to the task of predicting the input-specific coefficients in the linear combination. This is formulated as a regression problem from a holistic representation of the image. Crucially, the depth basis and the regression function are {\\bf coupled} and jointly optimized by our learning scheme. We demonstrate that this results in a significant improvement in accuracy compared to direct regression of depth pixel values or approaches learning the depth basis disjointly from the regression function. The global depth estimate is then used as a guidance by a local refinement method that introduces depth details that were not captured at the global level. Experiments on the NYUv2 and KITTI datasets show that our method outperforms the existing state-of-the-art at a considerably lower computational cost for both training and testing.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2016-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04560",
        "title": "Transductive Multi-view Zero-Shot Learning",
        "authors": [
            "Yanwei Fu",
            "Timothy M. Hospedales",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "Most existing zero-shot learning approaches exploit transfer learning via an intermediate-level semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and is applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04587",
        "title": "Transferring Rich Feature Hierarchies for Robust Visual Tracking",
        "authors": [
            "Naiyan Wang",
            "Siyi Li",
            "Abhinav Gupta",
            "Dit-Yan Yeung"
        ],
        "abstract": "Convolutional neural network (CNN) models have demonstrated great success in various computer vision tasks including image classification and object detection. However, some equally important tasks such as visual tracking remain relatively unexplored. We believe that a major hurdle that hinders the application of CNN to visual tracking is the lack of properly labeled training data. While existing applications that liberate the power of CNN often need an enormous amount of training data in the order of millions, visual tracking applications typically have only one labeled example in the first frame of each video. We address this research issue here by pre-training a CNN offline and then transferring the rich feature hierarchies learned to online tracking. The CNN is also fine-tuned during online tracking to adapt to the appearance of the tracked target specified in the first video frame. To fit the characteristics of object tracking, we first pre-train the CNN to recognize what is an object, and then propose to generate a probability map instead of producing a simple class label. Using two challenging open benchmarks for performance evaluation, our proposed tracker has demonstrated substantial improvement over other state-of-the-art trackers.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04686",
        "title": "Deep Convolutional Neural Networks for Action Recognition Using Depth Map Sequences",
        "authors": [
            "Pichao Wang",
            "Wanqing Li",
            "Zhimin Gao",
            "Jing Zhang",
            "Chang Tang",
            "Philip Ogunbona"
        ],
        "abstract": "Recently, deep learning approach has achieved promising results in various fields of computer vision. In this paper, a new framework called Hierarchical Depth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks (3ConvNets) is proposed for human action recognition using depth map sequences. Firstly, we rotate the original depth data in 3D pointclouds to mimic the rotation of cameras, so that our algorithms can handle view variant cases. Secondly, in order to effectively extract the body shape and motion information, we generate weighted depth motion maps (DMM) at several temporal scales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, three channels of ConvNets are trained on the HDMMs from three projected orthogonal planes separately. The proposed algorithms are evaluated on MSRAction3D, MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively. We also combine the last three datasets into a larger one (called Combined Dataset) and test the proposed method on it. The results show that our approach can achieve state-of-the-art results on the individual datasets and without dramatical performance degradation on the Combined Dataset.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04690",
        "title": "Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?",
        "authors": [
            "Erjin Zhou",
            "Zhimin Cao",
            "Qi Yin"
        ],
        "abstract": "Face recognition performance improves rapidly with the recent deep learning technique developing and underlying large training dataset accumulating. In this paper, we report our observations on how big data impacts the recognition performance. According to these observations, we build our Megvii Face Recognition System, which achieves 99.50% accuracy on the LFW benchmark, outperforming the previous state-of-the-art. Furthermore, we report the performance in a real-world security certification scenario. There still exists a clear gap between machine recognition and human performance. We summarize our experiments and present three challenges lying ahead in recent face recognition. And we indicate several possible solutions towards these challenges. We hope our work will stimulate the community's discussion of the difference between research benchmark and real-world applications.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04691",
        "title": "Tracing the boundaries of materials in transparent vessels using computer vision",
        "authors": [
            "Sagi Eppel"
        ],
        "abstract": "Visual recognition of material boundaries in transparent vessels is valuable for numerous applications. Such recognition is essential for estimation of fill-level, volume and phase-boundaries as well as for tracking of such chemical processes as precipitation, crystallization, condensation, evaporation and phase-separation. The problem of material boundary recognition in images is particularly complex for materials with non-flat surfaces, i.e., solids, powders and viscous fluids, in which the material interfaces have unpredictable shapes. This work demonstrates a general method for finding the boundaries of materials inside transparent containers in images. The method uses an image of the transparent vessel containing the material and the boundary of the vessel in this image. The recognition is based on the assumption that the material boundary appears in the image in the form of a curve (with various constraints) whose endpoints are both positioned on the vessel contour. The probability that a curve matches the material boundary in the image is evaluated using a cost function based on some image properties along this curve. Several image properties were examined as indicators for the material boundary. The optimal boundary curve was found using Dijkstra's algorithm. The method was successfully examined for recognition of various types of phase-boundaries, including liquid-air, solid-air and solid-liquid interfaces, as well as for various types of glassware containers from everyday life and the chemistry laboratory (i.e., bottles, beakers, flasks, jars, columns, vials and separation-funnels). In addition, the method can be easily extended to materials carried on top of carrier vessels (i.e., plates, spoons, spatulas).\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04711",
        "title": "DeepHash: Getting Regularization, Depth and Fine-Tuning Right",
        "authors": [
            "Jie Lin",
            "Olivier Morere",
            "Vijay Chandrasekhar",
            "Antoine Veillard",
            "Hanlin Goh"
        ],
        "abstract": "This work focuses on representing very high-dimensional global image descriptors using very compact 64-1024 bit binary hashes for instance retrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to making DeepHash work at extremely low bitrates are three important considerations -- regularization, depth and fine-tuning -- each requiring solutions specific to the hashing problem. In-depth evaluation shows that our scheme consistently outperforms state-of-the-art methods across all data sets for both Fisher Vectors and Deep Convolutional Neural Network features, by up to 20 percent over other schemes. The retrieval performance with 256-bit hashes is close to that of the uncompressed floating point features -- a remarkable 512 times compression.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04717",
        "title": "Robust Face Recognition by Constrained Part-based Alignment",
        "authors": [
            "Yuting Zhang",
            "Kui Jia",
            "Yueming Wang",
            "Gang Pan",
            "Tsung-Han Chan",
            "Yi Ma"
        ],
        "abstract": "Developing a reliable and practical face recognition system is a long-standing goal in computer vision research. Existing literature suggests that pixel-wise face alignment is the key to achieve high-accuracy face recognition. By assuming a human face as piece-wise planar surfaces, where each surface corresponds to a facial part, we develop in this paper a Constrained Part-based Alignment (CPA) algorithm for face recognition across pose and/or expression. Our proposed algorithm is based on a trainable CPA model, which learns appearance evidence of individual parts and a tree-structured shape configuration among different parts. Given a probe face, CPA simultaneously aligns all its parts by fitting them to the appearance evidence with consideration of the constraint from the tree-structured shape configuration. This objective is formulated as a norm minimization problem regularized by graph likelihoods. CPA can be easily integrated with many existing classifiers to perform part-based face recognition. Extensive experiments on benchmark face datasets show that CPA outperforms or is on par with existing methods for robust face recognition across pose, expression, and/or illumination changes.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04754",
        "title": "Distributed Data Association in Smart Camera Networks via Dual Decomposition",
        "authors": [
            "Jiuqing Wan",
            "Yuting Nie",
            "Li Liu"
        ],
        "abstract": "One of the fundamental requirements for visual surveillance using smart camera networks is the correct association of each persons observations generated on different cameras. Recently, distributed data association that involves only local information processing on each camera node and mutual information exchanging between neighboring cameras has attracted many research interests due to its superiority in large scale applications. In this paper, we formulate the problem of data association in smart camera networks as an Integer Programming problem by introducing a set of linking variables, and propose two distributed algorithms, namely L-DD and Q-DD, to solve the Integer Programming problem using dual decomposition technique. In our algorithms, the original IP problem is decomposed into several sub-problems, which can be solved locally and efficiently on each smart camera, and then different sub-problems reach consensus on their solutions in a rigorous way by adjusting their parameters based on projected sub-gradient optimization. The proposed methods are simple and flexible, in that (i) we can incorporate any feature extraction and matching technique into our framework to measure the similarity between two observations, which is used to define the cost of each link, and (ii) we can decompose the original problem in any way as long as the resulting sub-problem can be solved independently on individual camera. We show the competitiveness of our methods in both accuracy and speed by theoretical analysis and experimental comparison with state of the art algorithms on two real data sets collected by camera networks in our campus garden and office building.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04782",
        "title": "Constructing Binary Descriptors with a Stochastic Hill Climbing Search",
        "authors": [
            "Nenad Marku\u0161",
            "Igor S. Pand\u017ei\u0107",
            "J\u00f6rgen Ahlberg"
        ],
        "abstract": "Binary descriptors of image patches provide processing speed advantages and require less storage than methods that encode the patch appearance with a vector of real numbers. We provide evidence that, despite its simplicity, a stochastic hill climbing bit selection procedure for descriptor construction defeats recently proposed alternatives on a standard discriminative power benchmark. The method is easy to implement and understand, has no free parameters that need fine tuning, and runs fast.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04878",
        "title": "A Light Transport Model for Mitigating Multipath Interference in TOF Sensors",
        "authors": [
            "Nikhil Naik",
            "Achuta Kadambi",
            "Christoph Rhemann",
            "Shahram Izadi",
            "Ramesh Raskar",
            "Sing Bing Kang"
        ],
        "abstract": "Continuous-wave Time-of-flight (TOF) range imaging has become a commercially viable technology with many applications in computer vision and graphics. However, the depth images obtained from TOF cameras contain scene dependent errors due to multipath interference (MPI). Specifically, MPI occurs when multiple optical reflections return to a single spatial location on the imaging sensor. Many prior approaches to rectifying MPI rely on sparsity in optical reflections, which is an extreme simplification. In this paper, we correct MPI by combining the standard measurements from a TOF camera with information from direct and global light transport. We report results on both simulated experiments and physical experiments (using the Kinect sensor). Our results, evaluated against ground truth, demonstrate a quantitative improvement in depth accuracy.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05152",
        "title": "Mirror, mirror on the wall, tell me, is the error small?",
        "authors": [
            "Heng Yang",
            "Ioannis Patras"
        ],
        "abstract": "Do object part localization methods produce bilaterally symmetric results on mirror images? Surprisingly not, even though state of the art methods augment the training set with mirrored images. In this paper we take a closer look into this issue. We first introduce the concept of mirrorability as the ability of a model to produce symmetric results in mirrored images and introduce a corresponding measure, namely the \\textit{mirror error} that is defined as the difference between the detection result on an image and the mirror of the detection result on its mirror image. We evaluate the mirrorability of several state of the art algorithms in two of the most intensively studied problems, namely human pose estimation and face alignment. Our experiments lead to several interesting findings: 1) Surprisingly, most of state of the art methods struggle to preserve the mirror symmetry, despite the fact that they do have very similar overall performance on the original and mirror images; 2) the low mirrorability is not caused by training or testing sample bias - all algorithms are trained on both the original images and their mirrored versions; 3) the mirror error is strongly correlated to the localization/alignment error (with correlation coefficients around 0.7). Since the mirror error is calculated without knowledge of the ground truth, we show two interesting applications - in the first it is used to guide the selection of difficult samples and in the second to give feedback in a popular Cascaded Pose Regression method for face alignment.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05192",
        "title": "A Graph Theoretic Approach for Object Shape Representation in Compositional Hierarchies Using a Hybrid Generative-Descriptive Model",
        "authors": [
            "Umit Rusen Aktas",
            "Mete Ozay",
            "Ales Leonardis",
            "Jeremy L. Wyatt"
        ],
        "abstract": "A graph theoretic approach is proposed for object shape representation in a hierarchical compositional architecture called Compositional Hierarchy of Parts (CHOP). In the proposed approach, vocabulary learning is performed using a hybrid generative-descriptive model. First, statistical relationships between parts are learned using a Minimum Conditional Entropy Clustering algorithm. Then, selection of descriptive parts is defined as a frequent subgraph discovery problem, and solved using a Minimum Description Length (MDL) principle. Finally, part compositions are constructed by compressing the internal data representation with discovered substructures. Shape representation and computational complexity properties of the proposed approach and algorithms are examined using six benchmark two-dimensional shape image datasets. Experiments show that CHOP can employ part shareability and indexing mechanisms for fast inference of part compositions using learned shape vocabularies. Additionally, CHOP provides better shape retrieval performance than the state-of-the-art shape retrieval methods.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05382",
        "title": "Enhanced Mixtures of Part Model for Human Pose Estimation",
        "authors": [
            "Wenjuan Gong",
            "Yongzhen Huang",
            "Jordi Gonzalez",
            "and Liang Wang"
        ],
        "abstract": "Mixture of parts model has been successfully applied to 2D human pose estimation problem either as explicitly trained body part model or as latent variables for the whole human body model. Mixture of parts model usually utilize tree structure for representing relations between body parts. Tree structures facilitate training and referencing of the model but could not deal with double counting problems, which hinder its applications in 3D pose estimation. While most of work targeted to solve these problems tend to modify the tree models or the optimization target. We incorporate other cues from input features. For example, in surveillance environments, human silhouettes can be extracted relative easily although not flawlessly. In this condition, we can combine extracted human blobs with histogram of gradient feature, which is commonly used in mixture of parts model for training body part templates. The method can be easily extend to other candidate features under our generalized framework. We show 2D body part detection results on a public available dataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trained with Gaussian process regression model and 2D body part detections from the proposed method is fed to the estimator, thus 3D poses are predictable given new 2D body part detections. We also show results of 3D pose estimation on HumanEva dataset.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05432",
        "title": "Point Context: An Effective Shape Descriptor for RST-invariant Trajectory Recognition",
        "authors": [
            "Xingyu Wu",
            "Xia Mao",
            "Lijiang Chen",
            "Yuli Xue",
            "Angelo Compare"
        ],
        "abstract": "Motion trajectory recognition is important for characterizing the moving property of an object. The speed and accuracy of trajectory recognition rely on a compact and discriminative feature representation, and the situations of varying rotation, scaling and translation has to be specially considered. In this paper we propose a novel feature extraction method for trajectories. Firstly a trajectory is represented by a proposed point context, which is a rotation-scale-translation (RST) invariant shape descriptor with a flexible tradeoff between computational complexity and discrimination, yet we prove that it is a complete shape descriptor. Secondly, the shape context is nonlinearly mapped to a subspace by kernel nonparametric discriminant analysis (KNDA) to get a compact feature representation, and thus a trajectory is projected to a single point in a low-dimensional feature space. Experimental results show that, the proposed trajectory feature shows encouraging improvement than state-of-art methods.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05472",
        "title": "Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach",
        "authors": [
            "Ram Sarkar",
            "Bibhash Sen",
            "Nibaran Das",
            "Subhadip Basu"
        ],
        "abstract": "The paper concentrates on improvement of segmentation accuracy by addressing some of the key challenges of handwritten Devanagari word image segmentation technique. In the present work, we have developed a new feature based approach for identification of Matra pixels from a word image, design of a non-linear fuzzy membership functions for headline estimation and finally design of a non-linear fuzzy functions for identifying segmentation points on the Matra. The segmentation accuracy achieved by the current technique is 94.8%. This shows an improvement of performance by 1.8% over the previous technique [1] on a 300-word dataset, used for the current experiment.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05494",
        "title": "Design of a novel convex hull based feature set for recognition of isolated handwritten Roman numerals",
        "authors": [
            "Nibaran Das",
            "Sandip Pramanik",
            "Subhadip Basu",
            "Punam Kumar Saha",
            "Ram Sarkar",
            "Mahantapas Kundu"
        ],
        "abstract": "In this paper, convex hull based features are used for recognition of isolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier. Experiments of convex hull based features for handwritten character recognition are few in numbers. Convex hull of a pattern and the centroid of the convex hull both are affine invariant attributes. In this work, 25 features are extracted based on different bays attributes of the convex hull of the digit patterns. Then these patterns are divided into four sub-images with respect to the centroid of the convex hull boundary. From each such sub-image 25 bays features are also calculated. In all 125 convex hull based features are extracted for each numeric digit patterns under the current experiment. The performance of the designed feature set is tested on the standard MNIST data set, consisting of 60000 training and 10000 test images of handwritten Roman using an MLP based classifier a maximum success rate of 97.44% is achieved on the test data.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05495",
        "title": "A GA Based approach for selection of local features for recognition of handwritten Bangla numerals",
        "authors": [
            "Nibaran Das",
            "Subhadip Basu",
            "Punam Kumar Saha",
            "Ram Sarkar",
            "Mahantapas Kundu",
            "Mita Nasipuri"
        ],
        "abstract": "Soft computing approaches are mainly designed to address the real world ill-defined, imprecisely formulated problems, combining different kind of novel models of computation, such as neural networks, genetic algorithms (GAs. Handwritten digit recognition is a typical example of one such problem. In the current work we have developed a two-pass approach where the first pass classifier performs a coarse classification, based on some global features of the input pattern by restricting the possibility of classification decisions within a group of classes, smaller than the number of classes considered initially. In the second pass, the group specific classifiers concentrate on the features extracted from the selected local regions, and refine the earlier decision by combining the local and the global features for selecting the true class of the input pattern from the group of candidate classes selected in the first pass. To optimize the selection of local regions a GA based approach has been developed here. The maximum recognition performance on Bangla digit samples as achieved on the test set, during the first pass of the two pass approach is 93.35%. After combining the results of the two stage classifiers, an overall success rate of 95.25% is achieved.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05497",
        "title": "An Improved Feature Descriptor for Recognition of Handwritten Bangla Alphabet",
        "authors": [
            "Nibaran Das",
            "Subhadip Basu",
            "Ram Sarkar",
            "Mahantapas Kundu",
            "Mita Nasipuri",
            "Dipak kumar Basu"
        ],
        "abstract": "Appropriate feature set for representation of pattern classes is one of the most important aspects of handwritten character recognition. The effectiveness of features depends on the discriminating power of the features chosen to represent patterns of different classes. However, discriminatory features are not easily measurable. Investigative experimentation is necessary for identifying discriminatory features. In the present work we have identified a new variation of feature set which significantly outperforms on handwritten Bangla alphabet from the previously used feature set. 132 number of features in all viz. modified shadow features, octant and centroid features, distance based features, quad tree based longest run features are used here. Using this feature set the recognition performance increases sharply from the 75.05% observed in our previous work [7], to 85.40% on 50 character classes with MLP based classifier on the same dataset.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05499",
        "title": "Globally Optimal Cell Tracking using Integer Programming",
        "authors": [
            "Engin T\u00fcretken",
            "Xinchao Wang",
            "Carlos Becker",
            "Carsten Haubold",
            "Pascal Fua"
        ],
        "abstract": "We propose a novel approach to automatically tracking cell populations in time-lapse images. To account for cell occlusions and overlaps, we introduce a robust method that generates an over-complete set of competing detection hypotheses. We then perform detection and tracking simultaneously on these hypotheses by solving to optimality an integer program with only one type of flow variables. This eliminates the need for heuristics to handle missed detections due to occlusions and complex morphology. We demonstrate the effectiveness of our approach on a range of challenging sequences consisting of clumped cells and show that it outperforms state-of-the-art techniques.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05617",
        "title": "Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks",
        "authors": [
            "Mohamed Ali Mahjoub",
            "Mohamed Mhiri"
        ],
        "abstract": "Today Bayesian networks are more used in many areas of decision support and image processing. In this way, our proposed approach uses Bayesian Network to modelize the segmented image quality. This quality is calculated on a set of attributes that represent local evaluation measures. The idea is to have these local levels chosen in a way to be intersected into them to keep the overall appearance of segmentation. The approach operates in two phases: the first phase is to make an over-segmentation which gives superpixels card. In the second phase, we model the superpixels by a Bayesian Network. To find the segmented image with the best overall quality we used two approximate inference methods, the first using ICM algorithm which is widely used in Markov Models and a second is a recursive method called algorithm of model decomposition based on max-product algorithm which is very popular in the recent works of image segmentation. For our model, we have shown that the composition of these two algorithms leads to good segmentation performance.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05680",
        "title": "Active Mean Fields for Probabilistic Image Segmentation: Connections with Chan-Vese and Rudin-Osher-Fatemi Models",
        "authors": [
            "Marc Niethammer",
            "Kilian M. Pohl",
            "Firdaus Janoos",
            "William M. Wells III"
        ],
        "abstract": "Segmentation is a fundamental task for extracting semantically meaningful regions from an image. The goal of segmentation algorithms is to accurately assign object labels to each image location. However, image-noise, shortcomings of algorithms, and image ambiguities cause uncertainty in label assignment. Estimating the uncertainty in label assignment is important in multiple application domains, such as segmenting tumors from medical images for radiation treatment planning. One way to estimate these uncertainties is through the computation of posteriors of Bayesian models, which is computationally prohibitive for many practical applications. On the other hand, most computationally efficient methods fail to estimate label uncertainty. We therefore propose in this paper the Active Mean Fields (AMF) approach, a technique based on Bayesian modeling that uses a mean-field approximation to efficiently compute a segmentation and its corresponding uncertainty. Based on a variational formulation, the resulting convex model combines any label-likelihood measure with a prior on the length of the segmentation boundary. A specific implementation of that model is the Chan-Vese segmentation model (CV), in which the binary segmentation task is defined by a Gaussian likelihood and a prior regularizing the length of the segmentation boundary. Furthermore, the Euler-Lagrange equations derived from the AMF model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image denoising. Solutions to the AMF model can thus be implemented by directly utilizing highly-efficient ROF solvers on log-likelihood ratio fields. We qualitatively assess the approach on synthetic data as well as on real natural and medical images. For a quantitative evaluation, we apply our approach to the icgbench dataset.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05703",
        "title": "Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues",
        "authors": [
            "Ning Zhang",
            "Manohar Paluri",
            "Yaniv Taigman",
            "Rob Fergus",
            "Lubomir Bourdev"
        ],
        "abstract": "We explore the task of recognizing peoples' identities in photo albums in an unconstrained setting. To facilitate this, we introduce the new People In Photo Albums (PIPA) dataset, consisting of over 60000 instances of 2000 individuals collected from public Flickr photo albums. With only about half of the person images containing a frontal face, the recognition task is very challenging due to the large variations in pose, clothing, camera viewpoint, image resolution and illumination. We propose the Pose Invariant PErson Recognition (PIPER) method, which accumulates the cues of poselet-level person recognizers trained by deep convolutional networks to discount for the pose variations, combined with a face recognizer and a global recognizer. Experiments on three different settings confirm that in our unconstrained setup PIPER significantly improves on the performance of DeepFace, which is one of the best face recognizers as measured on the LFW dataset.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05759",
        "title": "Filtered Channel Features for Pedestrian Detection",
        "authors": [
            "Shanshan Zhang",
            "Rodrigo Benenson",
            "Bernt Schiele"
        ],
        "abstract": "This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis.\n",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05790",
        "title": "Taking a Deeper Look at Pedestrians",
        "authors": [
            "Jan Hosang",
            "Mohamed Omran",
            "Rodrigo Benenson",
            "Bernt Schiele"
        ],
        "abstract": "In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pre-training on surrogate tasks.\n",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05854",
        "title": "Unsupervised Segmentation of Multispectral Images with Cellular Automata",
        "authors": [
            "Wuilian Torres",
            "Antonio Rueda-Toicen"
        ],
        "abstract": "Multispectral images acquired by satellites are used to study phenomena on the Earth's surface. Unsupervised classification techniques analyze multispectral image content without considering prior knowledge of the observed terrain; this is done using techniques which group pixels that have similar statistics of digital level distribution in the various image channels. In this paper, we propose a methodology for unsupervised classification based on a deterministic cellular automaton. The automaton is initialized in an unsupervised manner by setting seed cells, selected according to two criteria: to be representative of the spatial distribution of the dominant elements in the image, and to take into account the diversity of spectral signatures in the image. The automaton's evolution is based on an attack rule that is applied simultaneously to all its cells. Among the noteworthy advantages of deterministic cellular automata for multispectral processing of satellite imagery is the consideration of topological information in the image via seed positioning, and the ability to modify the scale of the study.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05964",
        "title": "Advances in Human Action Recognition: A Survey",
        "authors": [
            "Guangchun Cheng",
            "Yiwen Wan",
            "Abdullah N. Saudagar",
            "Kamesh Namuduri",
            "Bill P. Buckles"
        ],
        "abstract": "Human action recognition has been an important topic in computer vision due to its many applications such as video surveillance, human machine interaction and video retrieval. One core problem behind these applications is automatically recognizing low-level actions and high-level activities of interest. The former is usually the basis for the latter. This survey gives an overview of the most recent advances in human action recognition during the past several years, following a well-formed taxonomy proposed by a previous survey. From this state-of-the-art survey, researchers can view a panorama of progress in this area for future research.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05970",
        "title": "Automatic Objects Removal for Scene Completion",
        "authors": [
            "Jianjun Yang",
            "Yin Wang",
            "Honggang Wang",
            "Kun Hua",
            "Wei Wang",
            "Ju Shen"
        ],
        "abstract": "With the explosive growth of web-based cameras and mobile devices, billions of photographs are uploaded to the internet. We can trivially collect a huge number of photo streams for various goals, such as 3D scene reconstruction and other big data applications. However, this is not an easy task due to the fact the retrieved photos are neither aligned nor calibrated. Furthermore, with the occlusion of unexpected foreground objects like people, vehicles, it is even more challenging to find feature correspondences and reconstruct realistic scenes. In this paper, we propose a structure based image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture. We use an edge matching technique to infer the potential structure of the unknown region. Driven by the estimated structure, texture synthesis is performed automatically along the estimated curves. We evaluate the proposed method on different types of images: from highly structured indoor environment to the natural scenes. Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing: 3D scene reconstruction and location recognition.\n    ",
        "submission_date": "2015-01-23T00:00:00",
        "last_modified_date": "2015-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06114",
        "title": "Accurate automatic segmentation of retina layers with emphasis on first layer",
        "authors": [
            "Mahdi Salarian"
        ],
        "abstract": "Quantification of intra-retinal boundaries in optical coherence tomography (OCT) is a crucial task for studying and diagnosing neurological and ocular diseases. Since manual segmentation of layers is usually a time consuming task and relay on user, a lot of attempts done to do it automatically and without interference of user. Although for extracting all layers usually same procedure is applied but finding the first layer is usually more difficult due to vanishing it in some region specially close to Fobia. To have a general software, beside using common methods like applying shortest path algorithm on global gradient of image, some extra steps are used here to confine search area for Dijstra algorithm especially for the second layer. Results demonstrates high accuracy in segmenting all present layers, especially the first one that is important for diagnosing issue.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06129",
        "title": "An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in Dynamic Scenes",
        "authors": [
            "Sourav Garg",
            "Swagat Kumar",
            "Rajesh Ratnakaram",
            "Prithwijit Guha"
        ],
        "abstract": "This paper looks into the problem of pedestrian tracking using a monocular, potentially moving, uncalibrated camera. The pedestrians are located in each frame using a standard human detector, which are then tracked in subsequent frames. This is a challenging problem as one has to deal with complex situations like changing background, partial or full occlusion and camera motion. In order to carry out successful tracking, it is necessary to resolve associations between the detected windows in the current frame with those obtained from the previous frame. Compared to methods that use temporal windows incorporating past as well as future information, we attempt to make decision on a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolve the association problem between a pair of consecutive frames by using an affinity matrix that defines the closeness between a pair of windows and then, uses a binary integer programming to obtain unique association between them. A second stage of verification based on SURF matching is used to deal with those cases where the above optimization scheme might yield wrong associations. The efficacy of the approach is demonstrated through experiments on several standard pedestrian datasets.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06170",
        "title": "Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals",
        "authors": [
            "Minsu Cho",
            "Suha Kwak",
            "Cordelia Schmid",
            "Jean Ponce"
        ],
        "abstract": "This paper addresses unsupervised discovery and localization of dominant objects from a noisy image collection with multiple object classes. The setting of this problem is fully unsupervised, without even image-level annotations or any assumption of a single dominant class. This is far more general than typical colocalization, cosegmentation, or weakly-supervised localization tasks. We tackle the discovery and localization problem using a part-based region matching approach: We use off-the-shelf region proposals to form a set of candidate bounding boxes for objects and object parts. These regions are efficiently matched across images using a probabilistic Hough transform that evaluates the confidence for each candidate correspondence considering both appearance and spatial consistency. Dominant objects are discovered and localized by comparing the scores of candidate regions and selecting those that stand out over other regions containing them. Extensive experimental evaluations on standard benchmarks demonstrate that the proposed approach significantly outperforms the current state of the art in colocalization, and achieves robust object discovery in challenging mixed-class datasets.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06180",
        "title": "Exploring Human Vision Driven Features for Pedestrian Detection",
        "authors": [
            "Shanshan Zhang",
            "Christian Bauckhage",
            "Dominik A. Klein",
            "Armin B. Cremers"
        ],
        "abstract": "Motivated by the center-surround mechanism in the human visual attention system, we propose to use average contrast maps for the challenge of pedestrian detection in street scenes due to the observation that pedestrians indeed exhibit discriminative contrast texture. Our main contributions are first to design a local, statistical multi-channel descriptorin order to incorporate both color and gradient information. Second, we introduce a multi-direction and multi-scale contrast scheme based on grid-cells in order to integrate expressive local variations. Contributing to the issue of selecting most discriminative features for assessing and classification, we perform extensive comparisons w.r.t. statistical descriptors, contrast measurements, and scale structures. This way, we obtain reasonable results under various configurations. Empirical findings from applying our optimized detector on the INRIA and Caltech pedestrian datasets show that our features yield state-of-the-art performance in pedestrian detection.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06202",
        "title": "Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels",
        "authors": [
            "Yanwei Fu",
            "Timothy M. Hospedales",
            "Tao Xiang",
            "Jiechao Xiong",
            "Shaogang Gong",
            "Yizhou Wang",
            "Yuan Yao"
        ],
        "abstract": "The problem of estimating subjective visual properties from image and video has attracted increasing interest. A subjective visual property is useful either on its own (e.g. image and video interestingness) or as an intermediate representation for visual recognition (e.g. a relative attribute). Due to its ambiguous nature, annotating the value of a subjective visual property for learning a prediction model is challenging. To make the annotation more reliable, recent studies employ crowdsourcing tools to collect pairwise comparison labels because human annotators are much better at ranking two images/videos (e.g. which one is more interesting) than giving an absolute value to each of them separately. However, using crowdsourced data also introduces outliers. Existing methods rely on majority voting to prune the annotation outliers/errors. They thus require large amount of pairwise labels to be collected. More importantly as a local outlier detection method, majority voting is ineffective in identifying outliers that can cause global ranking inconsistencies. In this paper, we propose a more principled way to identify annotation outliers by formulating the subjective visual property prediction task as a unified robust learning to rank problem, tackling both the outlier detection and learning to rank jointly. Differing from existing methods, the proposed method integrates local pairwise comparison labels together to minimise a cost that corresponds to global inconsistency of ranking order. This not only leads to better detection of annotation outliers but also enables learning with extremely sparse annotations. Extensive experiments on various benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-arts alternatives.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06262",
        "title": "3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks",
        "authors": [
            "Keze Wang",
            "Xiaolong Wang",
            "Liang Lin",
            "Meng Wang",
            "Wangmeng Zuo"
        ],
        "abstract": "Human activity understanding with 3D/depth sensors has received increasing attention in multimedia processing and interactions. This work targets on developing a novel deep model for automatic activity recognition from RGB-D videos. We represent each human activity as an ensemble of cubic-like video segments, and learn to discover the temporal structures for a category of activities, i.e. how the activities to be decomposed in terms of classification. Our model can be regarded as a structured deep architecture, as it extends the convolutional neural networks (CNNs) by incorporating structure alternatives. Specifically, we build the network consisting of 3D convolutions and max-pooling operators over the video segments, and introduce the latent variables in each convolutional layer manipulating the activation of neurons. Our model thus advances existing approaches in two aspects: (i) it acts directly on the raw inputs (grayscale-depth data) to conduct recognition instead of relying on hand-crafted features, and (ii) the model structure can be dynamically adjusted accounting for the temporal variations of human activities, i.e. the network configuration is allowed to be partially activated during inference. For model training, we propose an EM-type optimization method that iteratively (i) discovers the latent structure by determining the decomposed actions for each training example, and (ii) learns the network parameters by using the back-propagation algorithm. Our approach is validated in challenging scenarios, and outperforms state-of-the-art methods. A large human activity database of RGB-D videos is presented in addition.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06272",
        "title": "Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval",
        "authors": [
            "Fang Zhao",
            "Yongzhen Huang",
            "Liang Wang",
            "Tieniu Tan"
        ],
        "abstract": "With the rapid growth of web images, hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However, most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach, deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes, which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile, a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06297",
        "title": "Geodesic convolutional neural networks on Riemannian manifolds",
        "authors": [
            "Jonathan Masci",
            "Davide Boscaini",
            "Michael M. Bronstein",
            "Pierre Vandergheynst"
        ],
        "abstract": "Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract \"patches\", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2018-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06716",
        "title": "A General Preprocessing Method for Improved Performance of Epipolar Geometry Estimation Algorithms",
        "authors": [
            "Maria Kushnir",
            "Ilan Shimshoni"
        ],
        "abstract": "In this paper a deterministic preprocessing algorithm is presented, whose output can be given as input to most state-of-the-art epipolar geometry estimation algorithms, improving their results considerably. They are now able to succeed on hard cases for which they failed before. The algorithm consists of three steps, whose scope changes from local to global. In the local step it extracts from a pair of images local features (e.g. SIFT). Similar features from each image are clustered and the clusters are matched yielding a large number of putative matches. In the second step pairs of spatially close features (called 2keypoints) are matched and ranked by a classifier. The 2keypoint matches with the highest ranks are selected. In the global step, from each two 2keypoint matches a fundamental matrix is computed. As quite a few of the matrices are generated from correct matches they are used to rank the putative matches found in the first step. For each match the number of fundamental matrices, for which it approximately satisfies the epipolar constraint, is calculated. This set of matches is combined with the putative matches generated by standard methods and their probabilities to be correct are estimated by a classifier. These are then given as input to state-of-the-art epipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yielding much better results than the original algorithms. This was shown in extensive testing performed on almost 900 image pairs from six publicly available data-sets.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06722",
        "title": "Parametric Image Segmentation of Humans with Structural Shape Priors",
        "authors": [
            "Alin-Ionut Popa",
            "Cristian Sminchisescu"
        ],
        "abstract": "The figure-ground segmentation of humans in images captured in natural environments is an outstanding open problem due to the presence of complex backgrounds, articulation, varying body proportions, partial views and viewpoint changes. In this work we propose class-specific segmentation models that leverage parametric max-flow image segmentation and a large dataset of human shapes. Our contributions are as follows: (1) formulation of a sub-modular energy model that combines class-specific structural constraints and data-driven shape priors, within a parametric max-flow optimization methodology that systematically computes all breakpoints of the model in polynomial time; (2) design of a data-driven class-specific fusion methodology, based on matching against a large training set of exemplar human shapes (100,000 in our experiments), that allows the shape prior to be constructed on-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of state of the art results, in two challenging datasets, H3D and MPII (where figure-ground segmentation annotations have been added by us), where we substantially improve on the first ranked hypothesis estimates of mid-level segmentation methods, by 20%, with hypothesis set sizes that are up to one order of magnitude smaller.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06751",
        "title": "A Cheap System for Vehicle Speed Detection",
        "authors": [
            "Chaim Ginzburg",
            "Amit Raphael",
            "Daphna Weinshall"
        ],
        "abstract": "The reliable detection of speed of moving vehicles is considered key to traffic law enforcement in most countries, and is seen by many as an important tool to reduce the number of traffic accidents and fatalities. Many automatic systems and different methods are employed in different countries, but as a rule they tend to be expensive and/or labor intensive, often employing outdated technology due to the long development time. Here we describe a speed detection system that relies on simple everyday equipment - a laptop and a consumer web camera. Our method is based on tracking the license plates of cars, which gives the relative movement of the cars in the image. This image displacement is translated to actual motion by using the method of projection to a reference plane, where the reference plane is the road itself. However, since license plates do not touch the road, we must compensate for the entailed distortion in speed measurement. We show how to compute the compensation factor using knowledge of the license plate standard dimensions. Consequently our system computes the true speed of moving vehicles fast and accurately. We show promising results on videos obtained in a number of scenes and with different car models.\n    ",
        "submission_date": "2015-01-27T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06993",
        "title": "Feature Sampling Strategies for Action Recognition",
        "authors": [
            "Youjie Zhou",
            "Hongkai Yu",
            "Song Wang"
        ],
        "abstract": "Although dense local spatial-temporal features with bag-of-features representation achieve state-of-the-art performance for action recognition, the huge feature number and feature size prevent current methods from scaling up to real size problems. In this work, we investigate different types of feature sampling strategies for action recognition, namely dense sampling, uniformly random sampling and selective sampling. We propose two effective selective sampling methods using object proposal techniques. Experiments conducted on a large video dataset show that we are able to achieve better average recognition accuracy using 25% less features, through one of proposed selective sampling methods, and even remain comparable accuracy while discarding 70% features.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07180",
        "title": "End-to-End Photo-Sketch Generation via Fully Convolutional Representation Learning",
        "authors": [
            "Liliang Zhang",
            "Liang Lin",
            "Xian Wu",
            "Shengyong Ding",
            "Lei Zhang"
        ],
        "abstract": "Sketch-based face recognition is an interesting task in vision and multimedia research, yet it is quite challenging due to the great difference between face photos and sketches. In this paper, we propose a novel approach for photo-sketch generation, aiming to automatically transform face photos into detail-preserving personal sketches. Unlike the traditional models synthesizing sketches based on a dictionary of exemplars, we develop a fully convolutional network to learn the end-to-end photo-sketch mapping. Our approach takes whole face photos as inputs and directly generates the corresponding sketch images with efficient inference and learning, in which the architecture are stacked by only convolutional kernels of very small sizes. To well capture the person identity during the photo-sketch transformation, we define our optimization objective in the form of joint generative-discriminative minimization. In particular, a discriminative regularization term is incorporated into the photo-sketch generation, enhancing the discriminability of the generated person sketches against other individuals. Extensive experiments on several standard benchmarks suggest that our approach outperforms other state-of-the-art methods in both photo-sketch generation and face sketch verification.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07304",
        "title": "The Beauty of Capturing Faces: Rating the Quality of Digital Portraits",
        "authors": [
            "Miriam Redi",
            "Nikhil Rasiwasia",
            "Gaurav Aggarwal",
            "Alejandro Jaimes"
        ],
        "abstract": "Digital portrait photographs are everywhere, and while the number of face pictures keeps growing, not much work has been done to on automatic portrait beauty assessment. In this paper, we design a specific framework to automatically evaluate the beauty of digital portraits. To this end, we procure a large dataset of face images annotated not only with aesthetic scores but also with information about the traits of the subject portrayed. We design a set of visual features based on portrait photography literature, and extensively analyze their relation with portrait beauty, exposing interesting findings about what makes a portrait beautiful. We find that the beauty of a portrait is linked to its artistic value, and independent from age, race and gender of the subject. We also show that a classifier trained with our features to separate beautiful portraits from non-beautiful portraits outperforms generic aesthetic classifiers.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07338",
        "title": "On Vectorization of Deep Convolutional Neural Networks for Vision Tasks",
        "authors": [
            "Jimmy SJ. Ren",
            "Li Xu"
        ],
        "abstract": "We recently have witnessed many ground-breaking results in machine learning and computer vision, generated by using deep convolutional neural networks (CNN). While the success mainly stems from the large volume of training data and the deep network architectures, the vector processing hardware (e.g. GPU) undisputedly plays a vital role in modern CNN implementations to support massive computation. Though much attention was paid in the extent literature to understand the algorithmic side of deep CNN, little research was dedicated to the vectorization for scaling up CNNs. In this paper, we studied the vectorization process of key building blocks in deep CNNs, in order to better understand and facilitate parallel implementation. Key steps in training and testing deep CNNs are abstracted as matrix and vector operators, upon which parallelism can be easily achieved. We developed and compared six implementations with various degrees of vectorization with which we illustrated the impact of vectorization on the speed of model training and testing. Besides, a unified CNN framework for both high-level and low-level vision tasks is provided, along with a vectorized Matlab implementation with state-of-the-art speed performance.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07359",
        "title": "Learning And-Or Models to Represent Context and Occlusion for Car Detection and Viewpoint Estimation",
        "authors": [
            "Tianfu Wu",
            "Bo Li",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a method for learning And-Or models to represent context and occlusion for car detection and viewpoint estimation. The learned And-Or model represents car-to-car context and occlusion configurations at three levels: (i) spatially-aligned cars, (ii) single car under different occlusion configurations, and (iii) a small number of parts. The And-Or model embeds a grammar for representing large structural and appearance variations in a reconfigurable hierarchy. The learning process consists of two stages in a weakly supervised way (i.e., only bounding boxes of single cars are annotated). Firstly, the structure of the And-Or model is learned with three components: (a) mining multi-car contextual patterns based on layouts of annotated single car bounding boxes, (b) mining occlusion configurations between single cars, and (c) learning different combinations of part visibility based on car 3D CAD simulation. The And-Or model is organized in a directed and acyclic graph which can be inferred by Dynamic Programming. Secondly, the model parameters (for appearance, deformation and bias) are jointly trained using Weak-Label Structural SVM. In experiments, we test our model on four car detection datasets --- the KITTI dataset \\cite{Geiger12}, the PASCAL VOC2007 car dataset~\\cite{pascal}, and two self-collected car datasets, namely the Street-Parking car dataset and the Parking-Lot car dataset, and three datasets for car viewpoint estimation --- the PASCAL VOC2006 car dataset~\\cite{pascal}, the 3D car dataset~\\cite{savarese}, and the PASCAL3D+ car dataset~\\cite{xiang_wacv14}. Compared with state-of-the-art variants of deformable part-based models and other methods, our model achieves significant improvement consistently on the four detection datasets, and comparable performance on car viewpoint estimation.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07422",
        "title": "Pairwise Rotation Hashing for High-dimensional Features",
        "authors": [
            "Kohta Ishikawa",
            "Ikuro Sato",
            "Mitsuru Ambai"
        ],
        "abstract": "Binary Hashing is widely used for effective approximate nearest neighbors search. Even though various binary hashing methods have been proposed, very few methods are feasible for extremely high-dimensional features often used in visual tasks today. We propose a novel highly sparse linear hashing method based on pairwise rotations. The encoding cost of the proposed algorithm is $\\mathrm{O}(n \\log n)$ for n-dimensional features, whereas that of the existing state-of-the-art method is typically $\\mathrm{O}(n^2)$. The proposed method is also remarkably faster in the learning phase. Along with the efficiency, the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art. Pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes. Although these hashing criteria are widely used in previous researches, its analytical behavior is rarely studied. All building blocks of our algorithm are based on the analytical solution, and it thus provides a fairly simple and efficient procedure.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07492",
        "title": "Weakly Supervised Learning for Salient Object Detection",
        "authors": [
            "Huaizu Jiang"
        ],
        "abstract": "Recent advances in supervised salient object detection has resulted in significant performance on benchmark datasets. Training such models, however, requires expensive pixel-wise annotations of salient objects. Moreover, many existing salient object detection models assume that at least one salient object exists in the input image. Such an assumption often leads to less appealing saliency maps on the background images, which contain no salient object at all. To avoid the requirement of expensive pixel-wise salient region annotations, in this paper, we study weakly supervised learning approaches for salient object detection. Given a set of background images and salient object images, we propose a solution toward jointly addressing the salient object existence and detection tasks. We adopt the latent SVM framework and formulate the two problems together in a single integrated objective function: saliency labels of superpixels are modeled as hidden variables and involved in a classification term conditioned to the salient object existence variable, which in turn depends on both global image and regional saliency features and saliency label assignment. Experimental results on benchmark datasets validate the effectiveness of our proposed approach.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07645",
        "title": "Hyper-parameter optimization of Deep Convolutional Networks for object recognition",
        "authors": [
            "Sachin S. Talathi"
        ],
        "abstract": "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07680",
        "title": "Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous Landscapes using Holistic Structure based Models",
        "authors": [
            "Subit Chakrabarti",
            "Jasmeet Judge",
            "Anand Rangarajan",
            "Sanjay Ranka"
        ],
        "abstract": "In this study, a novel machine learning algorithm is presented for disaggregation of satellite soil moisture (SM) based on self-regularized regressive models (SRRM) using high-resolution correlated information from auxiliary sources. It includes regularized clustering that assigns soft memberships to each pixel at fine-scale followed by a kernel regression that computes the value of the desired variable at all pixels. Coarse-scale remotely sensed SM were disaggregated from 10km to 1km using land cover, precipitation, land surface temperature, leaf area index, and in-situ observations of SM. This algorithm was evaluated using multi-scale synthetic observations in NC Florida for heterogeneous agricultural land covers. It was found that the root mean square error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. The clusters generated represented the data well and reduced the RMSE by upto 40% during periods of high heterogeneity in land-cover and meteorological conditions. The Kullback Leibler divergence (KLD) between the true SM and the disaggregated estimates is close to 0, for both vegetated and baresoil landcovers. The disaggregated estimates were compared to those generated by the Principle of Relevant Information (PRI) method. The RMSE for the PRI disaggregated estimates is higher than the RMSE for the SRRM on each day of the season. The KLD of the disaggregated estimates generated by the SRRM is at least four orders of magnitude lower than those for the PRI disaggregated estimates, while the computational time needed was reduced by three times. The results indicate that the SRRM can be used for disaggregating SM with complex non-linear correlations on a grid with high accuracy.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07681",
        "title": "Vector Quantization by Minimizing Kullback-Leibler Divergence",
        "authors": [
            "Lan Yang",
            "Jingbin Wang",
            "Yujin Tu",
            "Prarthana Mahapatra",
            "Nelson Cardoso"
        ],
        "abstract": "This paper proposes a new method for vector quantization by minimizing the Kullback-Leibler Divergence between the class label distributions over the quantization inputs, which are original vectors, and the output, which is the quantization subsets of the vector set. In this way, the vector quantization output can keep as much information of the class label as possible. An objective function is constructed and we also developed an iterative algorithm to minimize it. The new method is evaluated on bag-of-features based image classification problem.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07683",
        "title": "Downscaling Microwave Brightness Temperatures Using Self Regularized Regressive Models",
        "authors": [
            "Subit Chakrabarti",
            "Jasmeet Judge",
            "Anand Rangarajan",
            "Sanjay Ranka"
        ],
        "abstract": "A novel algorithm is proposed to downscale microwave brightness temperatures ($\\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil Moisture Active Passive mission to a resolution meaningful for hydrological and agricultural applications. This algorithm, called Self-Regularized Regressive Models (SRRM), uses auxiliary variables correlated to $\\mathrm{T_B}$ along-with a limited set of \\textit{in-situ} SM observations, which are converted to high resolution $\\mathrm{T_B}$ observations using biophysical models. It includes an information-theoretic clustering step based on all auxiliary variables to identify areas of similarity, followed by a kernel regression step that produces downscaled $\\mathrm{T_B}$. This was implemented on a multi-scale synthetic data-set over NC-Florida for one year. An RMSE of 5.76~K with standard deviation of 2.8~k was achieved during the vegetated season and an RMSE of 1.2~K with a standard deviation of 0.9~K during periods of no vegetation.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07692",
        "title": "Blob indentation identification via curvature measurement",
        "authors": [
            "Matthew Sottile"
        ],
        "abstract": "This paper presents a novel method for identifying indentations on the boundary of solid 2D shape. It uses the signed curvature at a set of points along the boundary to identify indentations and provides one parameter for tuning the selection mechanism for discriminating indentations from other boundary irregularities. An efficient implementation is described based on the Fourier transform for calculating curvature from a sequence of points obtained from the boundary of a binary blob.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07738",
        "title": "Co-Regularized Deep Representations for Video Summarization",
        "authors": [
            "Olivier Mor\u00e8re",
            "Hanlin Goh",
            "Antoine Veillard",
            "Vijay Chandrasekhar",
            "Jie Lin"
        ],
        "abstract": "Compact keyframe-based video summaries are a popular way of generating viewership on video sharing platforms. Yet, creating relevant and compelling summaries for arbitrarily long videos with a small number of keyframes is a challenging task. We propose a comprehensive keyframe-based summarization framework combining deep convolutional neural networks and restricted Boltzmann machines. An original co-regularization scheme is used to discover meaningful subject-scene associations. The resulting multimodal representations are then used to select highly-relevant keyframes. A comprehensive user study is conducted comparing our proposed method to a variety of schemes, including the summarization currently in use by one of the most popular video sharing websites. The results show that our method consistently outperforms the baseline schemes for any given amount of keyframes both in terms of attractiveness and informativeness. The lead is even more significant for smaller summaries.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07844",
        "title": "A Proximal Bregman Projection Approach to Continuous Max-Flow Problems Using Entropic Distances",
        "authors": [
            "John S.H. Baxter",
            "Martin Rajchl",
            "Jing Yuan",
            "Terry M. Peters"
        ],
        "abstract": "One issue limiting the adaption of large-scale multi-region segmentation is the sometimes prohibitive memory requirements. This is especially troubling considering advances in massively parallel computing and commercial graphics processing units because of their already limited memory compared to the current random access memory used in more traditional computation. To address this issue in the field of continuous max-flow segmentation, we have developed a \\textit{pseudo-flow} framework using the theory of Bregman proximal projections and entropic distances which implicitly represents flow variables between labels and designated source and sink nodes. This reduces the memory requirements for max-flow segmentation by approximately 20\\% for Potts models and approximately 30\\% for hierarchical max-flow (HMF) and directed acyclic graph max-flow (DAGMF) models. This represents a great improvement in the state-of-the-art in max-flow segmentation, allowing for much larger problems to be addressed and accelerated using commercially available graphics processing hardware.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07862",
        "title": "An Analytical Study of different Document Image Binarization Methods",
        "authors": [
            "Mahua Nandy",
            "Satadal Saha"
        ],
        "abstract": "Document image has been the area of research for a couple of decades because of its potential application in the area of text recognition, line recognition or any other shape recognition from the image. For most of these purposes binarization of image becomes mandatory as far as recognition is concerned. Throughout couple decades standard algorithms have already been developed for this purpose. Some of these algorithms are applicable to degraded image also. Our objective behind this work is to study the existing techniques, compare them in view of advantages and disadvantages and modify some of these algorithms to optimize time or performance.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07867",
        "title": "Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors",
        "authors": [
            "Hojjat Seyed Mousavi",
            "Umamahesh Srinivas",
            "Vishal Monga",
            "Yuanming Suo",
            "Minh Dao",
            "Trac. D. Tran"
        ],
        "abstract": "Promising results have been achieved in image classification problems by exploiting the discriminative power of sparse representations for classification (SRC). Recently, it has been shown that the use of \\emph{class-specific} spike-and-slab priors in conjunction with the class-specific dictionaries from SRC is particularly effective in low training scenarios. As a logical extension, we build on this framework for multitask scenarios, wherein multiple representations of the same physical phenomena are available. We experimentally demonstrate the benefits of mining joint information from different camera views for multi-view face recognition.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07873",
        "title": "Sketch-a-Net that Beats Humans",
        "authors": [
            "Qian Yu",
            "Yongxin Yang",
            "Yi-Zhe Song",
            "Tao Xiang",
            "Timothy Hospedales"
        ],
        "abstract": "We propose a multi-scale multi-channel deep neural network framework that, for the first time, yields sketch recognition performance surpassing that of humans. Our superior performance is a result of explicitly embedding the unique characteristics of sketches in our model: (i) a network architecture designed for sketch rather than natural photo statistics, (ii) a multi-channel generalisation that encodes sequential ordering in the sketching process, and (iii) a multi-scale network ensemble with joint Bayesian fusion that accounts for the different levels of abstraction exhibited in free-hand sketches. We show that state-of-the-art deep networks specifically engineered for photos of natural objects fail to perform well on sketch recognition, regardless whether they are trained using photo or sketch. Our network on the other hand not only delivers the best performance on the largest human sketch dataset to date, but also is small in size making efficient training possible using just CPUs.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00030",
        "title": "SHOE: Supervised Hashing with Output Embeddings",
        "authors": [
            "Sravanthi Bondugula",
            "Varun Manjunatha",
            "Larry S. Davis",
            "David Doermann"
        ],
        "abstract": "We present a supervised binary encoding scheme for image retrieval that learns projections by taking into account similarity between classes obtained from output embeddings. Our motivation is that binary hash codes learned in this way improve both the visual quality of retrieval results and existing supervised hashing schemes. We employ a sequential greedy optimization that learns relationship aware projections by minimizing the difference between inner products of binary codes and output embedding vectors. We develop a joint optimization framework to learn projections which improve the accuracy of supervised hashing over the current state of the art with respect to standard and sibling evaluation metrics. We further boost performance by applying the supervised dimensionality reduction technique on kernelized input CNN features. Experiments are performed on three datasets: CUB-2011, SUN-Attribute and ImageNet ILSVRC 2010. As a by-product of our method, we show that using a simple k-nn pooling classifier with our discriminative codes improves over the complex classification models on fine grained datasets like CUB and offer an impressive compression ratio of 1024 on CNN features.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00046",
        "title": "Max-Margin Object Detection",
        "authors": [
            "Davis E. King"
        ],
        "abstract": "Most object detection methods operate by applying a binary classifier to sub-windows of an image, followed by a non-maximum suppression step where detections on overlapping sub-windows are removed. Since the number of possible sub-windows in even moderately sized image datasets is extremely large, the classifier is typically learned from only a subset of the windows. This avoids the computational difficulty of dealing with the entire set of sub-windows, however, as we will show in this paper, it leads to sub-optimal detector performance.\n",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00082",
        "title": "Category-Epitomes : Discriminatively Minimalist Representations for Object Categories",
        "authors": [
            "Ravi Kiran Sarvadevabhatla",
            "R. Venkatesh Babu"
        ],
        "abstract": "Freehand line sketches are an interesting and unique form of visual representation. Typically, such sketches are studied and utilized as an end product of the sketching process. However, we have found it instructive to study the sketches as sequentially accumulated composition of drawing strokes added over time. Studying sketches in this manner has enabled us to create novel sparse yet discriminative sketch-based representations for object categories which we term category-epitomes. Our procedure for obtaining these epitomes concurrently provides a natural measure for quantifying the sparseness underlying the original sketch, which we term epitome-score. We construct and analyze category-epitomes and epitome-scores for freehand sketches belonging to various object categories. Our analysis provides a novel viewpoint for studying the semantic nature of object categories.\n    ",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00115",
        "title": "Optimized Projection for Sparse Representation Based Classification",
        "authors": [
            "Can-Yi Lu",
            "De-Shuang Huang"
        ],
        "abstract": "Dimensionality reduction (DR) methods have been commonly used as a principled way to understand the high-dimensional data such as facial images. In this paper, we propose a new supervised DR method called Optimized Projection for Sparse Representation based Classification (OP-SRC), which is based on the recent face recognition method, Sparse Representation based Classification (SRC). SRC seeks a sparse linear combination on all the training data for a given query image, and make the decision by the minimal reconstruction residual. OP-SRC is designed on the decision rule of SRC, it aims to reduce the within-class reconstruction residual and simultaneously increase the between-class reconstruction residual on the training data. The projections are optimized and match well with the mechanism of SRC. Therefore, SRC performs well in the OP-SRC transformed space. The feasibility and effectiveness of the proposed method is verified on the Yale, ORL and UMIST databases with promising results.\n    ",
        "submission_date": "2015-01-31T00:00:00",
        "last_modified_date": "2015-01-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00192",
        "title": "Pose and Shape Estimation with Discriminatively Learned Parts",
        "authors": [
            "Menglong Zhu",
            "Xiaowei Zhou",
            "Kostas Daniilidis"
        ],
        "abstract": "We introduce a new approach for estimating the 3D pose and the 3D shape of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model from the training set through a facil- ity location optimization. The training set of 3D models is summarized into a sparse set of shapes from which we can generalize by linear combination. Given a test picture, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that minimizes simultaneously the geometric reprojection error as well as the appearance matching of the parts. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. We evaluate our approach on the Fine Grained 3D Car dataset with superior performance in shape and pose errors. Our main and novel contribution is the simultaneous solution for part localization, 3D pose and shape by maximizing both geometric and appearance compatibility.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00250",
        "title": "Driver distraction detection and recognition using RGB-D sensor",
        "authors": [
            "C\u00e9line Craye",
            "Fakhri Karray"
        ],
        "abstract": "Driver inattention assessment has become a very active field in intelligent transportation systems. Based on active sensor Kinect and computer vision tools, we have built an efficient module for detecting driver distraction and recognizing the type of distraction. Based on color and depth map data from the Kinect, our system is composed of four sub-modules. We call them eye behavior (detecting gaze and blinking), arm position (is the right arm up, down, right of forward), head orientation, and facial expressions. Each module produces relevant information for assessing driver inattention. They are merged together later on using two different classification strategies: AdaBoost classifier and Hidden Markov Model. Evaluation is done using a driving simulator and 8 drivers of different gender, age and nationality for a total of more than 8 hours of recording. Qualitative and quantitative results show strong and accurate detection and recognition capacity (85% accuracy for the type of distraction and 90% for distraction detection). Moreover, each module is obtained independently and could be used for other types of inference, such as fatigue detection, and could be implemented for real cars systems.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00254",
        "title": "Freehand Sketch Recognition Using Deep Features",
        "authors": [
            "Ravi Kiran Sarvadevabhatla",
            "R. Venkatesh Babu"
        ],
        "abstract": "Freehand sketches often contain sparse visual detail. In spite of the sparsity, they are easily and consistently recognized by humans across cultures, languages and age groups. Therefore, analyzing such sparse sketches can aid our understanding of the neuro-cognitive processes involved in visual representation and recognition. In the recent past, Convolutional Neural Networks (CNNs) have emerged as a powerful framework for feature representation and recognition for a variety of image domains. However, the domain of sketch images has not been explored. This paper introduces a freehand sketch recognition framework based on \"deep\" features extracted from CNNs. We use two popular CNNs for our experiments -- Imagenet CNN and a modified version of LeNet CNN. We evaluate our recognition framework on a publicly available benchmark database containing thousands of freehand sketches depicting everyday objects. Our results are an improvement over the existing state-of-the-art accuracies by 3% - 11%. The effectiveness and relative compactness of our deep features also make them an ideal candidate for related problems such as sketch-based image retrieval. In addition, we provide a preliminary glimpse of how such features can help identify crucial attributes (e.g. object-parts) of the sketched objects.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00256",
        "title": "Human Re-identification by Matching Compositional Template with Cluster Sampling",
        "authors": [
            "Yuanlu Xu",
            "Liang Lin",
            "Wei-Shi Zheng",
            "Xiaobai Liu"
        ],
        "abstract": "This paper aims at a newly raising task in visual surveillance: re-identifying people at a distance by matching body information, given several reference examples. Most of existing works solve this task by matching a reference template with the target individual, but often suffer from large human appearance variability (e.g. different poses/views, illumination) and high false positives in matching caused by conjunctions, occlusions or surrounding clutters. Addressing these problems, we construct a simple yet expressive template from a few reference images of a certain individual, which represents the body as an articulated assembly of compositional and alternative parts, and propose an effective matching algorithm with cluster sampling. This algorithm is designed within a candidacy graph whose vertices are matching candidates (i.e. a pair of source and target body parts), and iterates in two steps for convergence. (i) It generates possible partial matches based on compatible and competitive relations among body parts. (ii) It confirms the partial matches to generate a new matching solution, which is accepted by the Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate the superior performance of our approach on three public databases compared to existing methods.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00258",
        "title": "Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition",
        "authors": [
            "Xiaodan Liang",
            "Liang Lin",
            "Liangliang Cao"
        ],
        "abstract": "Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions e.g. triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration (e.g. the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (e.g. different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other competing methods.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00303",
        "title": "Dynamic texture and scene classification by transferring deep image features",
        "authors": [
            "Xianbiao Qi",
            "Chun-Guang Li",
            "Guoying Zhao",
            "Xiaopeng Hong",
            "Matti Pietik\u00e4inen"
        ],
        "abstract": "Dynamic texture and scene classification are two fundamental problems in understanding natural video content. Extracting robust and effective features is a crucial step towards solving these problems. However the existing approaches suffer from the sensitivity to either varying illumination, or viewpoint changing, or even camera motion, and/or the lack of spatial information. Inspired by the success of deep structures in image classification, we attempt to leverage a deep structure to extract feature for dynamic texture and scene classification. To tackle with the challenges in training a deep structure, we propose to transfer some prior knowledge from image domain to video domain. To be specific, we propose to apply a well-trained Convolutional Neural Network (ConvNet) as a mid-level feature extractor to extract features from each frame, and then form a representation of a video by concatenating the first and the second order statistics over the mid-level features. We term this two-level feature extraction scheme as a Transferred ConvNet Feature (TCoF). Moreover we explore two different implementations of the TCoF scheme, i.e., the \\textit{spatial} TCoF and the \\textit{temporal} TCoF, in which the mean-removed frames and the difference between two adjacent frames are used as the inputs of the ConvNet, respectively. We evaluate systematically the proposed spatial TCoF and the temporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN, and Maryland, and demonstrate that the proposed approach yields superior performance.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00319",
        "title": "Efficient refinement of GPS-based localization in urban areas using visual information and sensor parameter",
        "authors": [
            "Mahdi Salarian",
            "Rashid Ansari"
        ],
        "abstract": "An efficient method is proposed for refining GPS-acquired location coordinates in urban areas using camera images, Google Street View (GSV) and sensor parameters. The main goal is to compensate for GPS location imprecision in dense area of cities due to proximity to walls and buildings. Avail-able methods for better localization often use visual information by using query images acquired with camera-equipped mobile devices and applying image retrieval techniques to find the closest match in a GPS-referenced image data set. The search areas required for reliable search are about 1-2 sq. Km and the accuracy is typically 25-100 meters. Here we describe a method based on image retrieval where a reliable search can be confined to areas of 0.01 sq. Km and the accuracy in our experiments is less than 10 meters. To test our procedure we created a database by acquiring all Google Street View images close to what is seen by a pedestrian in a large region of downtown Chicago and saved all coordinates and orientation data to be used for confining our search region. Prior knowledge from approximate position of query image is leveraged to address complexity and accuracy issues of our search in a large scale geo-tagged data set. One key aspect that differentiates our work is that it utilizes the sensor information of GPS SOS and the camera orientation in improving localization. Finally we demonstrate retrieval-based technique are less accurate in sparse open areas compared with purely GPS measurement. The effectiveness of our approach is discussed in detail and experimental results show improved performance when compared with regular approaches.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00324",
        "title": "Modified Fast Fractal Image Compression Algorithm in spatial domain",
        "authors": [
            "M.Salarian",
            "H. Miar Naimi"
        ],
        "abstract": "In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of searching it across [0,1]. Only the domain blocks with entropy greater than a threshold are considered as domain pool. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above, the remaining range blocks are split into four new smaller range blocks and the algorithm must be iterated for them, considered as the other step of encoding process. The algorithm has been examined for some of the well-known images and the results have been compared with the state-of-the-art algorithms. The experiments show that our proposed algorithm has considerably lower encoding time than the other where the encoded images are approximately the same in quality.\n    ",
        "submission_date": "2015-02-01T00:00:00",
        "last_modified_date": "2015-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00341",
        "title": "Discriminatively Trained And-Or Graph Models for Object Shape Detection",
        "authors": [
            "Liang Lin",
            "Xiaolong Wang",
            "Wei Yang",
            "Jian-Huang Lai"
        ],
        "abstract": "In this paper, we investigate a novel reconfigurable part-based model, namely And-Or graph model, to recognize object shapes in images. Our proposed model consists of four layers: leaf-nodes at the bottom are local classifiers for detecting contour fragments; or-nodes above the leaf-nodes function as the switches to activate their child leaf-nodes, making the model reconfigurable during inference; and-nodes in a higher layer capture holistic shape deformations; one root-node on the top, which is also an or-node, activates one of its child and-nodes to deal with large global variations (e.g. different poses and views). We propose a novel structural optimization algorithm to discriminatively train the And-Or model from weakly annotated data. This algorithm iteratively determines the model structures (e.g. the nodes and their layouts) along with the parameter learning. On several challenging datasets, our model demonstrates the effectiveness to perform robust shape-based object detection against background clutter and outperforms the other state-of-the-art approaches. We also release a new shape database with annotations, which includes more than 1500 challenging shape instances, for recognition and detection.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00344",
        "title": "Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal Models",
        "authors": [
            "Liang Lin",
            "Yuanlu Xu",
            "Xiaodan Liang",
            "Jianhuang Lai"
        ],
        "abstract": "Although it has been widely discussed in video surveillance, background subtraction is still an open problem in the context of complex scenarios, e.g., dynamic backgrounds, illumination variations, and indistinct foreground objects. To address these challenges, we propose an effective background subtraction method by learning and maintaining an array of dynamic texture models within the spatio-temporal representations. At any location of the scene, we extract a sequence of regular video bricks, i.e. video volumes spanning over both spatial and temporal domain. The background modeling is thus posed as pursuing subspaces within the video bricks while adapting the scene variations. For each sequence of video bricks, we pursue the subspace by employing the ARMA (Auto Regressive Moving Average) Model that jointly characterizes the appearance consistency and temporal coherence of the observations. During online processing, we incrementally update the subspaces to cope with disturbances from foreground objects and scene changes. In the experiments, we validate the proposed method in several complex scenarios, and show superior performances over other state-of-the-art approaches of background subtraction. The empirical studies of parameter setting and component analysis are presented as well.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00374",
        "title": "Adaptive Scene Category Discovery with Generative Learning and Compositional Sampling",
        "authors": [
            "Liang Lin",
            "Ruimao Zhang",
            "Xiaohua Duan"
        ],
        "abstract": "This paper investigates a general framework to discover categories of unlabeled scene images according to their appearances (i.e., textures and structures). We jointly solve the two coupled tasks in an unsupervised manner: (i) classifying images without pre-determining the number of categories, and (ii) pursuing generative model for each category. In our method, each image is represented by two types of image descriptors that are effective to capture image appearances from different aspects. By treating each image as a graph vertex, we build up an graph, and pose the image categorization as a graph partition process. Specifically, a partitioned sub-graph can be regarded as a category of scenes, and we define the probabilistic model of graph partition by accumulating the generative models of all separated categories. For efficient inference with the graph, we employ a stochastic cluster sampling algorithm, which is designed based on the Metropolis-Hasting mechanism. During the iterations of inference, the model of each category is analytically updated by a generative learning algorithm. In the experiments, our approach is validated on several challenging databases, and it outperforms other popular state-of-the-art methods. The implementation details and empirical analysis are presented as well.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00377",
        "title": "Integrating Graph Partitioning and Matching for Trajectory Analysis in Video Surveillance",
        "authors": [
            "Liang Lin",
            "Yongyi Lu",
            "Yan Pan",
            "Xiaowu Chen"
        ],
        "abstract": "In order to track the moving objects in long range against occlusion, interruption, and background clutter, this paper proposes a unified approach for global trajectory analysis. Instead of the traditional frame-by-frame tracking, our method recovers target trajectories based on a short sequence of video frames, e.g. $15$ frames. We initially calculate a foreground map at each frame, as obtained from a state-of-the-art background model. An attribute graph is then extracted from the foreground map, where the graph vertices are image primitives represented by the composite features. With this graph representation, we pose trajectory analysis as a joint task of spatial graph partitioning and temporal graph matching. The task can be formulated by maximizing a posteriori under the Bayesian framework, in which we integrate the spatio-temporal contexts and the appearance models. The probabilistic inference is achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a peroid of observed frames, the algorithm simulates a ergodic and aperiodic Markov Chain, and it visits a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. In the experiments, our method is tested on several challenging videos from the public datasets of visual surveillance, and it outperforms the state-of-the-art methods.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00416",
        "title": "Towards a solid solution of real-time fire and flame detection",
        "authors": [
            "Bo Jiang",
            "Yongyi Lu",
            "Xiying Li",
            "Liang Lin"
        ],
        "abstract": "Although the object detection and recognition has received growing attention for decades, a robust fire and flame detection method is rarely explored. This paper presents an empirical study, towards a general and solid approach to fast detect fire and flame in videos, with the applications in video surveillance and event retrieval. Our system consists of three cascaded steps: (1) candidate regions proposing by a background model, (2) fire region classifying with color-texture features and a dictionary of visual words, and (3) temporal verifying. The experimental evaluation and analysis are done for each step. We believe that it is a useful service to both academic research and real-world application. In addition, we release the software of the proposed system with the source code, as well as a public benchmark and data set, including 64 video clips covered both indoor and outdoor scenes under different conditions. We achieve an 82% Recall with 93% Precision on the data set, and greatly improve the performance by state-of-the-arts methods.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00478",
        "title": "Structured Occlusion Coding for Robust Face Recognition",
        "authors": [
            "Yandong Wen",
            "Weiyang Liu",
            "Meng Yang",
            "Yuli Fu",
            "Youjun Xiang",
            "Rui Hu"
        ],
        "abstract": "Occlusion in face recognition is a common yet challenging problem. While sparse representation based classification (SRC) has been shown promising performance in laboratory conditions (i.e. noiseless or random pixel corrupted), it performs much worse in practical scenarios. In this paper, we consider the practical face recognition problem, where the occlusions are predictable and available for sampling. We propose the structured occlusion coding (SOC) to address occlusion problems. The structured coding here lies in two folds. On one hand, we employ a structured dictionary for recognition. On the other hand, we propose to use the structured sparsity in this formulation. Specifically, SOC simultaneously separates the occlusion and classifies the image. In this way, the problem of recognizing an occluded image is turned into seeking a structured sparse solution on occlusion-appended dictionary. In order to construct a well-performing occlusion dictionary, we propose an occlusion mask estimating technique via locality constrained dictionary (LCD), showing striking improvement in occlusion sample. On a category-specific occlusion dictionary, we replace norm sparsity with the structured sparsity which is shown more robust, further enhancing the robustness of our approach. Moreover, SOC achieves significant improvement in handling large occlusion in real world. Extensive experiments are conducted on public data sets to validate the superiority of the proposed algorithm.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00500",
        "title": "Fast and Robust Feature Matching for RGB-D Based Localization",
        "authors": [
            "Miguel Heredia",
            "Felix Endres",
            "Wolfram Burgard",
            "Rafael Sanz"
        ],
        "abstract": "In this paper we present a novel approach to global localization using an RGB-D camera in maps of visual features. For large maps, the performance of pure image matching techniques decays in terms of robustness and computational cost. Particularly, repeated occurrences of similar features due to repeating structure in the world (e.g., doorways, chairs, etc.) or missing associations between observations pose critical challenges to visual localization. We address these challenges using a two-step approach. We first estimate a candidate pose using few correspondences between features of the current camera frame and the feature map. The initial set of correspondences is established by proximity in feature space. The initial pose estimate is used in the second step to guide spatial matching of features in 3D, i.e., searching for associations where the image features are expected to be found in the map. A RANSAC algorithm is used to compute a fine estimation of the pose from the correspondences. Our approach clearly outperforms localization based on feature matching exclusively in feature space, both in terms of estimation accuracy and robustness to failure and allows for global localization in real time (30Hz).\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00501",
        "title": "An Expressive Deep Model for Human Action Parsing from A Single Image",
        "authors": [
            "Zhujin Liang",
            "Xiaolong Wang",
            "Rui Huang",
            "Liang Lin"
        ],
        "abstract": "This paper aims at one newly raising task in vision and multimedia research: recognizing human actions from still images. Its main challenges lie in the large variations in human poses and appearances, as well as the lack of temporal motion information. Addressing these problems, we propose to develop an expressive deep model to naturally integrate human layout and surrounding contexts for higher level action understanding from still images. In particular, a Deep Belief Net is trained to fuse information from different noisy sources such as body part detection and object detection. To bridge the semantic gap, we used manually labeled data to greatly improve the effectiveness and efficiency of the pre-training and fine-tuning stages of the DBN training. The resulting framework is shown to be robust to sometimes unreliable inputs (e.g., imprecise detections of human parts and objects), and outperforms the state-of-the-art approaches.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00558",
        "title": "Complex-Valued Hough Transforms for Circles",
        "authors": [
            "Marcelo Cicconet",
            "Davi Geiger",
            "Michael Werman"
        ],
        "abstract": "This paper advocates the use of complex variables to represent votes in the Hough transform for circle detection. Replacing the positive numbers classically used in the parameter space of the Hough transforms by complex numbers allows cancellation effects when adding up the votes. Cancellation and the computation of shape likelihood via a complex number's magnitude square lead to more robust solutions than the \"classic\" algorithms, as shown by computational experiments on synthetic and real datasets.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00561",
        "title": "Quantum Pairwise Symmetry: Applications in 2D Shape Analysis",
        "authors": [
            "Marcelo Cicconet",
            "Davi Geiger",
            "Michael Werman"
        ],
        "abstract": "A pair of rooted tangents -- defining a quantum triangle -- with an associated quantum wave of spin 1/2 is proposed as the primitive to represent and compute symmetry. Measures of the spin characterize how \"isosceles\" or how \"degenerate\" these triangles are -- which corresponds to their mirror or parallel symmetry. We also introduce a complex-valued kernel to model probability errors in the parameter space, which is more robust to noise and clutter than the classical model.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00652",
        "title": "Learning the Matching Function",
        "authors": [
            "\u013dubor Ladick\u00fd",
            "Christian H\u00e4ne",
            "Marc Pollefeys"
        ],
        "abstract": "The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time.\n",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00705",
        "title": "Recovery of Piecewise Smooth Images from Few Fourier Samples",
        "authors": [
            "Greg Ongie",
            "Mathews Jacob"
        ],
        "abstract": "We introduce a Prony-like method to recover a continuous domain 2-D piecewise smooth image from few of its Fourier samples. Assuming the discontinuity set of the image is localized to the zero level-set of a trigonometric polynomial, we show the Fourier transform coefficients of partial derivatives of the signal satisfy an annihilation relation. We present necessary and sufficient conditions for unique recovery of piecewise constant images using the above annihilation relation. We pose the recovery of the Fourier coefficients of the signal from the measurements as a convex matrix completion algorithm, which relies on the lifting of the Fourier data to a structured low-rank matrix; this approach jointly estimates the signal and the annihilating filter. Finally, we demonstrate our algorithm on the recovery of MRI phantoms from few low-resolution Fourier samples.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00712",
        "title": "Deep Boosting: Layered Feature Mining for General Image Classification",
        "authors": [
            "Zhanglin Peng",
            "Liang Lin",
            "Ruimao Zhang",
            "Jing Xu"
        ],
        "abstract": "Constructing effective representations is a critical but challenging problem in multimedia understanding. The traditional handcraft features often rely on domain knowledge, limiting the performances of exiting methods. This paper discusses a novel computational architecture for general image feature mining, which assembles the primitive filters (i.e. Gabor wavelets) into compositional features in a layer-wise manner. In each layer, we produce a number of base classifiers (i.e. regression stumps) associated with the generated features, and discover informative compositions by using the boosting algorithm. The output compositional features of each layer are treated as the base components to build up the next layer. Our framework is able to generate expressive image representations while inducing very discriminate functions for image classification. The experiments are conducted on several public datasets, and we demonstrate superior performances over state-of-the-art approaches.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00717",
        "title": "Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation",
        "authors": [
            "Hongyuan Zhu",
            "Fanman Meng",
            "Jianfei Cai",
            "Shijian Lu"
        ],
        "abstract": "Image segmentation refers to the process to divide an image into nonoverlapping meaningful regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. However, while many segmentation algorithms exist, yet there are only a few sparse and outdated summarizations available, an overview of the recent achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. Covering 180 publications, we give an overview of broad areas of segmentation topics including not only the classic bottom-up approaches, but also the recent development in superpixel, interactive methods, object proposals, semantic image parsing and image cosegmentation. In addition, we also review the existing influential datasets and evaluation metrics. Finally, we suggest some design flavors and research directions for future research in image segmentation.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00723",
        "title": "Learning Contour-Fragment-based Shape Model with And-Or Tree Representation",
        "authors": [
            "Liang Lin",
            "Xiaolong Wang",
            "Wei Yang",
            "Jianhuang Lai"
        ],
        "abstract": "This paper proposes a simple yet effective method to learn the hierarchical object shape model consisting of local contour fragments, which represents a category of shapes in the form of an And-Or tree. This model extends the traditional hierarchical tree structures by introducing the \"switch\" variables (i.e. the or-nodes) that explicitly specify production rules to capture shape variations. We thus define the model with three layers: the leaf-nodes for detecting local contour fragments, the or-nodes specifying selection of leaf-nodes, and the root-node encoding the holistic distortion. In the training stage, for optimization of the And-Or tree learning, we extend the concave-convex procedure (CCCP) by embedding the structural clustering during the iterative learning steps. The inference of shape detection is consistent with the model optimization, which integrates the local testings via the leaf-nodes and or-nodes with the global verification via the root-node. The advantages of our approach are validated on the challenging shape databases (i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method is able to accurately localize shape contours against unreliable edge detection and edge tracing. (2) The And-Or tree model enables us to well capture the intraclass variance.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00739",
        "title": "Clothing Co-Parsing by Joint Image Segmentation and Labeling",
        "authors": [
            "Wei Yang",
            "Ping Luo",
            "Liang Lin"
        ],
        "abstract": "This paper aims at developing an integrated system of clothing co-parsing, in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. We propose a data-driven framework consisting of two phases of inference. The first phase, referred as \"image co-segmentation\", iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM (E-SVM) technique [23]. In the second phase (i.e. \"region co-labeling\"), we construct a multi-image graphical model by taking the segmented regions as vertices, and incorporate several contexts of clothing configuration (e.g., item location and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [30], we construct a dataset called CCP consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89% recognition rate on the Fashionista and the CCP datasets, respectively, which are superior compared with state-of-the-art methods.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00741",
        "title": "Dynamical And-Or Graph Learning for Object Shape Modeling and Detection",
        "authors": [
            "Xiaolong Wang",
            "Liang Lin"
        ],
        "abstract": "This paper studies a novel discriminative part-based model to represent and recognize object shapes with an \"And-Or graph\". We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00743",
        "title": "Deep Joint Task Learning for Generic Object Extraction",
        "authors": [
            "Xiaolong Wang",
            "Liliang Zhang",
            "Liang Lin",
            "Zhujin Liang",
            "Wangmeng Zuo"
        ],
        "abstract": "This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches).\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00744",
        "title": "Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection",
        "authors": [
            "Xiaolong Wang",
            "Liang Lin",
            "Lichao Huang",
            "Shuicheng Yan"
        ],
        "abstract": "This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) \"switch\" variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the and-nodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achieves state-of-the-arts performance.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00749",
        "title": "Data-Driven Scene Understanding with Adaptively Retrieved Exemplars",
        "authors": [
            "Xionghao Liu",
            "Wei Yang",
            "Liang Lin",
            "Qing Wang",
            "Zhaoquan Cai",
            "Jianhuang Lai"
        ],
        "abstract": "This article investigates a data-driven approach for semantically scene understanding, without pixelwise annotation and classifier training. Our framework parses a target image with two steps: (i) retrieving its exemplars (i.e. references) from an image database, where all images are unsegmented but annotated with tags; (ii) recovering its pixel labels by propagating semantics from the references. We present a novel framework making the two steps mutually conditional and bootstrapped under the probabilistic Expectation-Maximization (EM) formulation. In the first step, the references are selected by jointly matching their appearances with the target as well as the semantics (i.e. the assigned labels of the target and the references). We process the second step via a combinatorial graphical representation, in which the vertices are superpixels extracted from the target and its selected references. Then we derive the potentials of assigning labels to one vertex of the target, which depend upon the graph edges that connect the vertex to its spatial neighbors of the target and to its similar vertices of the references. Besides, the proposed framework can be naturally applied to perform image annotation on new test images. In the experiments, we validate our approach on two public databases, and demonstrate superior performances over the state-of-the-art methods in both semantic segmentation and image annotation tasks.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00750",
        "title": "Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model",
        "authors": [
            "Xiaodan Liang",
            "Qingxing Cao",
            "Rui Huang",
            "Liang Lin"
        ],
        "abstract": "The aim of this study is to provide an automatic computational framework to assist clinicians in diagnosing Focal Liver Lesions (FLLs) in Contrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clip as an ensemble of Region-of-Interests (ROIs), whose locations are modeled as latent variables in a discriminative model. Different types of FLLs are characterized by both spatial and temporal enhancement patterns of the ROIs. The model is learned by iteratively inferring the optimal ROI locations and optimizing the model parameters. To efficiently search the optimal spatial and temporal locations of the ROIs, we propose a data-driven inference algorithm by combining effective spatial and temporal pruning. The experiments show that our method achieves promising results on the largest dataset in the literature (to the best of our knowledge), which we have made publicly available.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00836",
        "title": "Task-Driven Dictionary Learning for Hyperspectral Image Classification with Structured Sparsity Constraints",
        "authors": [
            "Xiaoxia Sun",
            "Nasser M. Nasrabadi",
            "Trac D. Tran"
        ],
        "abstract": "Sparse representation models a signal as a linear combination of a small number of dictionary atoms. As a generative model, it requires the dictionary to be highly redundant in order to ensure both a stable high sparsity level and a low reconstruction error for the signal. However, in practice, this requirement is usually impaired by the lack of labelled training samples. Fortunately, previous research has shown that the requirement for a redundant dictionary can be less rigorous if simultaneous sparse approximation is employed, which can be carried out by enforcing various structured sparsity constraints on the sparse codes of the neighboring pixels. In addition, numerous works have shown that applying a variety of dictionary learning methods for the sparse representation model can also improve the classification performance. In this paper, we highlight the task-driven dictionary learning algorithm, which is a general framework for the supervised dictionary learning method. We propose to enforce structured sparsity priors on the task-driven dictionary learning method in order to improve the performance of the hyperspectral classification. Our approach is able to benefit from both the advantages of the simultaneous sparse representation and those of the supervised dictionary learning. We enforce two different structured sparsity priors, the joint and Laplacian sparsity, on the task-driven dictionary learning method and provide the details of the corresponding optimization algorithms. Experiments on numerous popular hyperspectral images demonstrate that the classification performance of our approach is superior to sparse representation classifier with structured priors or the task-driven dictionary learning method.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00852",
        "title": "Face frontalization for Alignment and Recognition",
        "authors": [
            "Christos Sagonas",
            "Yannis Panagakis",
            "Stefanos Zafeiriou",
            "Maja Pantic"
        ],
        "abstract": "Recently, it was shown that excellent results can be achieved in both face landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D faces data. In this paper, we propose a novel method for joint face landmark localization and frontal face reconstruction (pose correction) using a small set of frontal images only. By observing that the frontal facial image is the one with the minimum rank from all different poses we formulate an appropriate model which is able to jointly recover the facial landmarks as well as the frontalized version of the face. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix $\\ell_1$ norm, is solved. The proposed method is assessed in frontal face reconstruction (pose correction), face landmark localization, and pose-invariant face recognition and verification by conducting experiments on $6$ facial images databases. The experimental results demonstrate the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00873",
        "title": "DeepID3: Face Recognition with Very Deep Neural Networks",
        "authors": [
            "Yi Sun",
            "Ding Liang",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00946",
        "title": "Classification of Hyperspectral Imagery on Embedded Grassmannians",
        "authors": [
            "Sofya Chepushtanova",
            "Michael Kirby"
        ],
        "abstract": "We propose an approach for capturing the signal variability in hyperspectral imagery using the framework of the Grassmann manifold. Labeled points from each class are sampled and used to form abstract points on the Grassmannian. The resulting points on the Grassmannian have representations as orthonormal matrices and as such do not reside in Euclidean space in the usual sense. There are a variety of metrics which allow us to determine a distance matrices that can be used to realize the Grassmannian as an embedding in Euclidean space. We illustrate that we can achieve an approximately isometric embedding of the Grassmann manifold using the chordal metric while this is not the case with geodesic distances. However, non-isometric embeddings generated by using a pseudometric on the Grassmannian lead to the best classification results. We observe that as the dimension of the Grassmannian grows, the accuracy of the classification grows to 100% on two illustrative examples. We also observe a decrease in classification rates if the dimension of the points on the Grassmannian is too large for the dimension of the Euclidean space. We use sparse support vector machines to perform additional model reduction. The resulting classifier selects a subset of dimensions of the embedding without loss in classification performance.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01032",
        "title": "DFDL: Discriminative Feature-oriented Dictionary Learning for Histopathological Image Classification",
        "authors": [
            "Tiep H. Vu",
            "Hojjat S. Mousavi",
            "Vishal Monga",
            "UK Arvind Rao",
            "Ganesh Rao"
        ],
        "abstract": "In histopathological image analysis, feature extraction for classification is a challenging task due to the diversity of histology features suitable for each problem as well as presence of rich geometrical structure. In this paper, we propose an automatic feature discovery framework for extracting discriminative class-specific features and present a low-complexity method for classification and disease grading in histopathology. Essentially, our Discriminative Feature-oriented Dictionary Learning (DFDL) method learns class-specific features which are suitable for representing samples from the same class while are poorly capable of representing samples from other classes. Experiments on three challenging real-world image databases: 1) histopathological images of intraductal breast lesions, 2) mammalian lung images provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, show the significance of DFDL model in a variety problems over state-of-the-art methods\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01097",
        "title": "Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene Classification of High-Resolution Remote Sensing Imagery",
        "authors": [
            "Jingwen Hu",
            "Gui-Song Xia",
            "Fan Hu",
            "Liangpei Zhang"
        ],
        "abstract": "Scene classification is a key problem in the interpretation of high-resolution remote sensing imagery. Many state-of-the-art methods, e.g. bag-of-visual-words model and its variants, the topic models as well as deep learning-based approaches, share similar procedures: patch sampling, feature description/learning and classification. Patch sampling is the first and a key procedure which has a great influence on the results. In the literature, many different sampling strategies have been used, {e.g. dense sampling, random sampling, keypoint-based sampling and saliency-based sampling, etc. However, it is still not clear which sampling strategy is suitable for the scene classification of high-resolution remote sensing images. In this paper, we comparatively study the effects of different sampling strategies under the scenario of scene classification of high-resolution remote sensing images. We divide the existing sampling methods into two types: dense sampling and sparse sampling, the later of which includes random sampling, keypoint-based sampling and various saliency-based sampling proposed recently. In order to compare their performances, we rely on a standard bag-of-visual-words model to construct our testing scheme, owing to their simplicity, robustness and efficiency. The experimental results on two commonly used datasets show that dense sampling has the best performance among all the strategies but with high spatial and computational complexity, random sampling gives better or comparable results than other sparse sampling methods, like the sophisticated multi-scale key-point operators and the saliency-based methods which are intensively studied and commonly used recently.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01199",
        "title": "A Multiple-Expert Binarization Framework for Multispectral Images",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "abstract": "In this work, a multiple-expert binarization framework for multispectral images is proposed. The framework is based on a constrained subspace selection limited to the spectral bands combined with state-of-the-art gray-level binarization methods. The framework uses a binarization wrapper to enhance the performance of the gray-level binarization. Nonlinear preprocessing of the individual spectral bands is used to enhance the textual information. An evolutionary optimizer is considered to obtain the optimal and some suboptimal 3-band subspaces from which an ensemble of experts is then formed. The framework is applied to a ground truth multispectral dataset with promising results. In addition, a generalization to the cross-validation approach is developed that not only evaluates generalizability of the framework, it also provides a practical instance of the selected experts that could be then applied to unseen inputs despite the small size of the given ground truth dataset.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01228",
        "title": "Linear-time Online Action Detection From 3D Skeletal Data Using Bags of Gesturelets",
        "authors": [
            "Moustafa Meshry",
            "Mohamed E. Hussein",
            "Marwan Torki"
        ],
        "abstract": "Sliding window is one direct way to extend a successful recognition system to handle the more challenging detection problem. While action recognition decides only whether or not an action is present in a pre-segmented video sequence, action detection identifies the time interval where the action occurred in an unsegmented video stream. Sliding window approaches for action detection can however be slow as they maximize a classifier score over all possible sub-intervals. Even though new schemes utilize dynamic programming to speed up the search for the optimal sub-interval, they require offline processing on the whole video sequence. In this paper, we propose a novel approach for online action detection based on 3D skeleton sequences extracted from depth data. It identifies the sub-interval with the maximum classifier score in linear time. Furthermore, it is invariant to temporal scale variations and is suitable for real-time applications with low latency.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01423",
        "title": "Collaborative Feature Learning from Social Media",
        "authors": [
            "Chen Fang",
            "Hailin Jin",
            "Jianchao Yang",
            "Zhe Lin"
        ],
        "abstract": "Image feature representation plays an essential role in image recognition and related tasks. The current state-of-the-art feature learning paradigm is supervised learning from labeled data. However, this paradigm requires large-scale category labels, which limits its applicability to domains where labels are hard to obtain. In this paper, we propose a new data-driven feature learning paradigm which does not rely on category labels. Instead, we learn from user behavior data collected on social media. Concretely, we use the image relationship discovered in the latent space from the user behavior data to guide the image feature learning. We collect a large-scale image and user behavior dataset from ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01475",
        "title": "Fast Constraint Propagation for Image Segmentation",
        "authors": [
            "Peng Han"
        ],
        "abstract": "This paper presents a novel selective constraint propagation method for constrained image segmentation. In the literature, many pairwise constraint propagation methods have been developed to exploit pairwise constraints for cluster analysis. However, since most of these methods have a polynomial time complexity, they are not much suitable for segmentation of images even with a moderate size, which is actually equivalent to cluster analysis with a large data size. Considering the local homogeneousness of a natural image, we choose to perform pairwise constraint propagation only over a selected subset of pixels, but not over the whole image. Such a selective constraint propagation problem is then solved by an efficient graph-based learning algorithm. To further speed up our selective constraint propagation, we also discard those less important propagated constraints during graph-based learning. Finally, the selectively propagated constraints are exploited based on $L_1$-minimization for normalized cuts over the whole image. The experimental results demonstrate the promising performance of the proposed method for segmentation with selectively propagated constraints.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01526",
        "title": "Object Proposal with Kernelized Partial Ranking",
        "authors": [
            "Jing Wang",
            "Jie Shen",
            "Ping Li"
        ],
        "abstract": "Object proposals are an ensemble of bounding boxes with high potential to contain objects. In order to determine a small set of proposals with a high recall, a common scheme is extracting multiple features followed by a ranking algorithm which however, incurs two major challenges: {\\bf 1)} The ranking model often imposes pairwise constraints between each proposal, rendering the problem away from an efficient training/testing phase; {\\bf 2)} Linear kernels are utilized due to the computational and memory bottleneck of training a kernelized model.\n",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2017-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01540",
        "title": "Semantic Embedding Space for Zero-Shot Action Recognition",
        "authors": [
            "Xun Xu",
            "Timothy Hospedales",
            "Shaogang Gong"
        ],
        "abstract": "The number of categories for action recognition is growing rapidly. It is thus becoming increasingly hard to collect sufficient training data to learn conventional models for each category. This issue may be ameliorated by the increasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework a mapping is constructed between visual features and a human interpretable semantic description of each category, allowing categories to be recognised in the absence of any training data. Existing ZSL studies focus primarily on image data, and attribute-based semantic representations. In this paper, we address zero-shot recognition in contemporary video action recognition tasks, using semantic word vector space as the common space to embed videos and category labels. This is more challenging because the mapping between the semantic space and space-time features of videos containing complex actions is more complex and harder to learn. We demonstrate that a simple self-training and data augmentation strategy can significantly improve the efficacy of this mapping. Experiments on human action datasets including HMDB51 and UCF101 demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01761",
        "title": "A Framework for Symmetric Part Detection in Cluttered Scenes",
        "authors": [
            "Tom Lee",
            "Sanja Fidler",
            "Alex Levinshtein",
            "Cristian Sminchisescu",
            "Sven Dickinson"
        ],
        "abstract": "The role of symmetry in computer vision has waxed and waned in importance during the evolution of the field from its earliest days. At first figuring prominently in support of bottom-up indexing, it fell out of favor as shape gave way to appearance and recognition gave way to detection. With a strong prior in the form of a target object, the role of the weaker priors offered by perceptual grouping was greatly diminished. However, as the field returns to the problem of recognition from a large database, the bottom-up recovery of the parts that make up the objects in a cluttered scene is critical for their recognition. The medial axis community has long exploited the ubiquitous regularity of symmetry as a basis for the decomposition of a closed contour into medial parts. However, today's recognition systems are faced with cluttered scenes, and the assumption that a closed contour exists, i.e. that figure-ground segmentation has been solved, renders much of the medial axis community's work inapplicable. In this article, we review a computational framework, previously reported in Lee et al. (2013), Levinshtein et al. (2009, 2013), that bridges the representation power of the medial axis and the need to recover and group an object's parts in a cluttered scene. Our framework is rooted in the idea that a maximally inscribed disc, the building block of a medial axis, can be modeled as a compact superpixel in the image. We evaluate the method on images of cluttered scenes.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01782",
        "title": "Multi-Action Recognition via Stochastic Modelling of Optical Flow and Gradients",
        "authors": [
            "Johanna Carvajal",
            "Conrad Sanderson",
            "Chris McCool",
            "Brian C. Lovell"
        ],
        "abstract": "In this paper we propose a novel approach to multi-action recognition that performs joint segmentation and classification. This approach models each action using a Gaussian mixture using robust low-dimensional action features. Segmentation is achieved by performing classification on overlapping temporal windows, which are then merged to produce the final result. This approach is considerably less complicated than previous methods which use dynamic programming or computationally expensive hidden Markov models (HMMs). Initial experiments on a stitched version of the KTH dataset show that the proposed approach achieves an accuracy of 78.3%, outperforming a recent HMM-based approach which obtained 71.2%.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01812",
        "title": "Crowded Scene Analysis: A Survey",
        "authors": [
            "Teng Li",
            "Huan Chang",
            "Meng Wang",
            "Bingbing Ni",
            "Richang Hong",
            "Shuicheng Yan"
        ],
        "abstract": "Automated scene analysis has been a topic of great interest in computer vision and cognitive science. Recently, with the growth of crowd phenomena in the real world, crowded scene analysis has attracted much attention. However, the visual occlusions and ambiguities in crowded scenes, as well as the complex behaviors and scene semantics, make the analysis a challenging task. In the past few years, an increasing number of works on crowded scene analysis have been reported, covering different aspects including crowd motion pattern learning, crowd behavior and activity analysis, and anomaly detection in crowds. This paper surveys the state-of-the-art techniques on this topic. We first provide the background knowledge and the available features related to crowded scenes. Then, existing models, popular algorithms, evaluation protocols, as well as system performance are provided corresponding to different aspects of crowded scene analysis. We also outline the available datasets for performance evaluation. Finally, some research problems and promising future directions are presented with discussions.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01852",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01853",
        "title": "Generalized Inpainting Method for Hyperspectral Image Acquisition",
        "authors": [
            "K. Degraux",
            "V. Cambareri",
            "L. Jacques",
            "B. Geelen",
            "C. Blanch",
            "G. Lafruit"
        ],
        "abstract": "A recently designed hyperspectral imaging device enables multiplexed acquisition of an entire data volume in a single snapshot thanks to monolithically-integrated spectral filters. Such an agile imaging technique comes at the cost of a reduced spatial resolution and the need for a demosaicing procedure on its interleaved data. In this work, we address both issues and propose an approach inspired by recent developments in compressed sensing and analysis sparse models. We formulate our superresolution and demosaicing task as a 3-D generalized inpainting problem. Interestingly, the target spatial resolution can be adjusted for mitigating the compression level of our sensing. The reconstruction procedure uses a fast greedy method called Pseudo-inverse IHT. We also show on simulations that a random arrangement of the spectral filters on the sensor is preferable to regular mosaic layout as it improves the quality of the reconstruction. The efficiency of our technique is demonstrated through numerical experiments on both synthetic and real data as acquired by the snapshot imager.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01880",
        "title": "A Fingerprint-based Access Control using Principal Component Analysis and Edge Detection",
        "authors": [
            "E.F. Melo",
            "H.M. de Oliveira"
        ],
        "abstract": "This paper presents a novel approach for deciding on the appropriateness or not of an acquired fingerprint image into a given database. The process begins with the assembly of a training base in an image space constructed by combining Principal Component Analysis (PCA) and edge detection. Then, the parameter H, a new feature that helps in the decision making about the relevance of a fingerprint image in databases, is derived from a relationship between Euclidean and Mahalanobian distances. This procedure ends with the lifting of the curve of the Receiver Operating Characteristic (ROC), where the thresholds defined on the parameter H are chosen according to the acceptable rates of false positives and false negatives.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02063",
        "title": "Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel",
        "authors": [
            "Hossein Hajimirsadeghi",
            "Wang Yan",
            "Arash Vahdat",
            "Greg Mori"
        ],
        "abstract": "Many visual recognition problems can be approached by counting instances. To determine whether an event is present in a long internet video, one could count how many frames seem to contain the activity. Classifying the activity of a group of people can be done by counting the actions of individual people. Encoding these cardinality relationships can reduce sensitivity to clutter, in the form of irrelevant frames or individuals not involved in a group activity. Learned parameters can encode how many instances tend to occur in a class of interest. To this end, this paper develops a powerful and flexible framework to infer any cardinality relation between latent labels in a multi-instance model. Hard or soft cardinality relations can be encoded to tackle diverse levels of ambiguity. Experiments on tasks such as human activity recognition, video event detection, and video summarization demonstrate the effectiveness of using cardinality relations for improving recognition results.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02092",
        "title": "Reflectance Hashing for Material Recognition",
        "authors": [
            "Hang Zhang",
            "Kristin Dana",
            "Ko Nishino"
        ],
        "abstract": "We introduce a novel method for using reflectance to identify materials. Reflectance offers a unique signature of the material but is challenging to measure and use for recognizing materials due to its high-dimensionality. In this work, one-shot reflectance is captured using a unique optical camera measuring {\\it reflectance disks} where the pixel coordinates correspond to surface viewing angles. The reflectance has class-specific stucture and angular gradients computed in this reflectance space reveal the material class.\n",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02160",
        "title": "A Survey on Hough Transform, Theory, Techniques and Applications",
        "authors": [
            "Allam Shehata Hassanein",
            "Sherien Mohammad",
            "Mohamed Sameer",
            "Mohammad Ehab Ragab"
        ],
        "abstract": "For more than half a century, the Hough transform is ever-expanding for new frontiers. Thousands of research papers and numerous applications have evolved over the decades. Carrying out an all-inclusive survey is hardly possible and enormously space-demanding. What we care about here is emphasizing some of the most crucial milestones of the transform. We describe its variations elaborating on the basic ones such as the line and circle Hough transforms. The high demand for storage and computation time is clarified with different solution approaches. Since most uses of the transform take place on binary images, we have been concerned with the work done directly on gray or color images. The myriad applications of the standard transform and its variations have been classified highlighting the up-to-date and the unconventional ones. Due to its merits such as noise-immunity and expandability, the transform has an excellent history, and a bright future as well.\n    ",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02171",
        "title": "Person Re-identification Meets Image Search",
        "authors": [
            "Liang Zheng",
            "Liyue Shen",
            "Lu Tian",
            "Shengjin Wang",
            "Jiahao Bu",
            "Qi Tian"
        ],
        "abstract": "For long time, person re-identification and image search are two separately studied tasks. However, for person re-identification, the effectiveness of local features and the \"query-search\" mode make it well posed for image search techniques.\n",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02182",
        "title": "Comparison of Algorithms for Compressed Sensing of Magnetic Resonance Images",
        "authors": [
            "Jelena Badnjar"
        ],
        "abstract": "Magnetic resonance imaging (MRI) is an essential medical tool with inherently slow data acquisition process. Slow acquisition process requires patient to be long time exposed to scanning apparatus. In recent years significant efforts are made towards the applying Compressive Sensing technique to the acquisition process of MRI and biomedical images. Compressive Sensing is an emerging theory in signal processing. It aims to reduce the amount of acquired data required for successful signal reconstruction. Reducing the amount of acquired image coefficients leads to lower acquisition time, i.e. time of exposition to the MRI apparatus. Using optimization algorithms, satisfactory image quality can be obtained from the small set of acquired samples. A number of optimization algorithms for the reconstruction of the biomedical images is proposed in the literature. In this paper, three commonly used optimization algorithms are compared and results are presented on the several MRI images.\n    ",
        "submission_date": "2015-02-07T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02410",
        "title": "Out-of-sample generalizations for supervised manifold learning for classification",
        "authors": [
            "Elif Vural",
            "Christine Guillemot"
        ],
        "abstract": "Supervised manifold learning methods for data classification map data samples residing in a high-dimensional ambient space to a lower-dimensional domain in a structure-preserving way, while enhancing the separation between different classes in the learned embedding. Most nonlinear supervised manifold learning methods compute the embedding of the manifolds only at the initially available training points, while the generalization of the embedding to novel points, known as the out-of-sample extension problem in manifold learning, becomes especially important in classification applications. In this work, we propose a semi-supervised method for building an interpolation function that provides an out-of-sample extension for general supervised manifold learning algorithms studied in the context of classification. The proposed algorithm computes a radial basis function (RBF) interpolator that minimizes an objective function consisting of the total embedding error of unlabeled test samples, defined as their distance to the embeddings of the manifolds of their own class, as well as a regularization term that controls the smoothness of the interpolation function in a direction-dependent way. The class labels of test data and the interpolation function parameters are estimated jointly with a progressive procedure. Experimental results on face and object images demonstrate the potential of the proposed out-of-sample extension algorithm for the classification of manifold-modeled data sets.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02445",
        "title": "Deep Neural Networks for Anatomical Brain Segmentation",
        "authors": [
            "Alexandre de Brebisson",
            "Giovanni Montana"
        ],
        "abstract": "We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture the local spatial context while large, compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02506",
        "title": "Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks",
        "authors": [
            "Adrien Payan",
            "Giovanni Montana"
        ],
        "abstract": "Pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease have been the subject of extensive research in recent years. In this paper, we use deep learning methods, and in particular sparse autoencoders and 3D convolutional neural networks, to build an algorithm that can predict the disease status of a patient, based on an MRI scan of the brain. We report on experiments using the ADNI data set involving 2,265 historical scans. We demonstrate that 3D convolutional neural networks outperform several other classifiers reported in the literature and produce state-of-art results.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02734",
        "title": "Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation",
        "authors": [
            "George Papandreou",
            "Liang-Chieh Chen",
            "Kevin Murphy",
            "Alan L. Yuille"
        ],
        "abstract": "Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02766",
        "title": "Multi-view Face Detection Using Deep Convolutional Neural Networks",
        "authors": [
            "Sachin Sudhakar Farfade",
            "Mohammad Saberian",
            "Li-Jia Li"
        ],
        "abstract": "In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between dis- tribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02772",
        "title": "A HMAX with LLC for visual recognition",
        "authors": [
            "Kean Hong Lau",
            "Yong Haur Tay",
            "Fook Loong Lo"
        ],
        "abstract": "Today's high performance deep artificial neural networks (ANNs) rely heavily on parameter optimization, which is sequential in nature and even with a powerful GPU, would have taken weeks to train them up for solving challenging tasks [22]. HMAX [17] has demonstrated that a simple high performing network could be obtained without heavy optimization. In this paper, we had improved on the existing best HMAX neural network [12] in terms of structural simplicity and performance. Our design replaces the L1 minimization sparse coding (SC) with a locality-constrained linear coding (LLC) [20] which has a lower computational demand. We also put the simple orientation filter bank back into the front layer of the network replacing PCA. Our system's performance has improved over the existing architecture and reached 79.0% on the challenging Caltech-101 [7] dataset, which is state-of-the-art for ANNs (without transfer learning). From our empirical data, the main contributors to our system's performance include an introduction of partial signal whitening, a spot detector, and a spatial pyramid matching (SPM) [14] layer.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02905",
        "title": "Real Time Implementation of Spatial Filtering On FPGA",
        "authors": [
            "Chaitannya Supe"
        ],
        "abstract": "Field Programmable Gate Array (FPGA) technology has gained vital importance mainly because of its parallel processing hardware which makes it ideal for image and video processing. In this paper, a step by step approach to apply a linear spatial filter on real time video frame sent by Omnivision OV7670 camera using Zynq Evaluation and Development board based on Xilinx XC7Z020 has been discussed. Face detection application was chosen to explain above procedure. This procedure is applicable to most of the complex image processing algorithms which needs to be implemented using FPGA.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02965",
        "title": "Video Primal Sketch: A Unified Middle-Level Representation for Video",
        "authors": [
            "Zhi Han",
            "Zongben Xu",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a middle-level video representation named Video Primal Sketch (VPS), which integrates two regimes of models: i) sparse coding model using static or moving primitives to explicitly represent moving corners, lines, feature points, etc., ii) FRAME /MRF model reproducing feature statistics extracted from input video to implicitly represent textured motion, such as water and fire. The feature statistics include histograms of spatio-temporal filters and velocity distributions. This paper makes three contributions to the literature: i) Learning a dictionary of video primitives using parametric generative models; ii) Proposing the Spatio-Temporal FRAME (ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling and synthesizing textured motion; and iii) Developing a parsimonious hybrid model for generic video representation. Given an input video, VPS selects the proper models automatically for different motion patterns and is compatible with high-level action representations. In the experiments, we synthesize a number of textured motion; reconstruct real videos using the VPS; report a series of human perception experiments to verify the quality of reconstructed videos; demonstrate how the VPS changes over the scale transition in videos; and present the close connection between VPS and high-level action models.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03121",
        "title": "Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation",
        "authors": [
            "Qi Wei",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "abstract": "This paper proposes a fast multi-band image fusion algorithm, which combines a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. The well admitted forward model is explored to form the likelihoods of the observations. Maximizing the likelihoods leads to solving a Sylvester equation. By exploiting the properties of the circulant and downsampling matrices associated with the fusion problem, a closed-form solution for the corresponding Sylvester equation is obtained explicitly, getting rid of any iterative update step. Coupled with the alternating direction method of multipliers and the block coordinate descent method, the proposed algorithm can be easily generalized to incorporate prior information for the fusion problem, allowing a Bayesian estimator. Simulation results show that the proposed algorithm achieves the same performance as existing algorithms with the advantage of significantly decreasing the computational complexity of these algorithms.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03240",
        "title": "Conditional Random Fields as Recurrent Neural Networks",
        "authors": [
            "Shuai Zheng",
            "Sadeep Jayasumana",
            "Bernardino Romera-Paredes",
            "Vibhav Vineet",
            "Zhizhong Su",
            "Dalong Du",
            "Chang Huang",
            "Philip H. S. Torr"
        ],
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03273",
        "title": "Image denoising based on improved data-driven sparse representation",
        "authors": [
            "Dai-Qiang Chen"
        ],
        "abstract": "Sparse representation of images under certain transform domain has been playing a fundamental role in image restoration tasks. One such representative method is the widely used wavelet tight frame systems. Instead of adopting fixed filters for constructing a tight frame to sparsely model any input image, a data-driven tight frame was proposed for the sparse representation of images, and shown to be very efficient for image denoising very recently. However, in this method the number of framelet filters used for constructing a tight frame is the same as the length of filters. In fact, through further investigation it is found that part of these filters are unnecessary and even harmful to the recovery effect due to the influence of noise. Therefore, an improved data-driven sparse representation systems constructed with much less number of filters are proposed. Numerical results on denoising experiments demonstrate that the proposed algorithm overall outperforms the original data-driven tight frame construction scheme on both the recovery quality and computational time.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03436",
        "title": "An exploration of parameter redundancy in deep networks with circulant projections",
        "authors": [
            "Yu Cheng",
            "Felix X. Yu",
            "Rogerio S. Feris",
            "Sanjiv Kumar",
            "Alok Choudhary",
            "Shih-Fu Chang"
        ],
        "abstract": "We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d^2) to O(dlogd) and space complexity from O(d^2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03532",
        "title": "An equalised global graphical model-based approach for multi-camera object tracking",
        "authors": [
            "Weihua Chen",
            "Lijun Cao",
            "Xiaotang Chen",
            "Kaiqi Huang"
        ],
        "abstract": "Non-overlapping multi-camera visual object tracking typically consists of two steps: single camera object tracking and inter-camera object tracking. Most of tracking methods focus on single camera object tracking, which happens in the same scene, while for real surveillance scenes, inter-camera object tracking is needed and single camera tracking methods can not work effectively. In this paper, we try to improve the overall multi-camera object tracking performance by a global graph model with an improved similarity metric. Our method treats the similarities of single camera tracking and inter-camera tracking differently and obtains the optimization in a global graph model. The results show that our method can work better even in the condition of poor single camera object tracking.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2016-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03596",
        "title": "Towards zero-configuration condition monitoring based on dictionary learning",
        "authors": [
            "Sergio Martin-del-Campo",
            "Fredrik Sandin"
        ],
        "abstract": "Condition-based predictive maintenance can significantly improve overall equipment effectiveness provided that appropriate monitoring methods are used. Online condition monitoring systems are customized to each type of machine and need to be reconfigured when conditions change, which is costly and requires expert knowledge. Basic feature extraction methods limited to signal distribution functions and spectra are commonly used, making it difficult to automatically analyze and compare machine conditions. In this paper, we investigate the possibility to automate the condition monitoring process by continuously learning a dictionary of optimized shift-invariant feature vectors using a well-known sparse approximation method. We study how the feature vectors learned from a vibration signal evolve over time when a fault develops within a ball bearing of a rotating machine. We quantify the adaptation rate of learned features and find that this quantity changes significantly in the transitions between normal and faulty states of operation of the ball bearing.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03851",
        "title": "Discovering Human Interactions in Videos with Limited Data Labeling",
        "authors": [
            "Mehran Khodabandeh",
            "Arash Vahdat",
            "Guang-Tong Zhou",
            "Hossein Hajimirsadeghi",
            "Mehrsan Javan Roshtkhari",
            "Greg Mori",
            "Stephen Se"
        ],
        "abstract": "We present a novel approach for discovering human interactions in videos. Activity understanding techniques usually require a large number of labeled examples, which are not available in many practical cases. Here, we focus on recovering semantically meaningful clusters of human-human and human-object interaction in an unsupervised fashion. A new iterative solution is introduced based on Maximum Margin Clustering (MMC), which also accepts user feedback to refine clusters. This is achieved by formulating the whole process as a unified constrained latent max-margin clustering problem. Extensive experiments have been carried out over three challenging datasets, Collective Activity, VIRAT, and UT-interaction. Empirical results demonstrate that the proposed algorithm can efficiently discover perfect semantic clusters of human interactions with only a small amount of labeling effort.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03913",
        "title": "Skeleton Matching based approach for Text Localization in Scene Images",
        "authors": [
            "B.H. Shekar",
            "Smitha M.L"
        ],
        "abstract": "In this paper, we propose a skeleton matching based approach which aids in text localization in scene images. The input image is preprocessed and segmented into blocks using connected component analysis. We obtain the skeleton of the segmented block using morphology based approach. The skeletonized images are compared with the trained templates in the database to categorize into text and non-text blocks. Further, the newly designed geometrical rules and morphological operations are employed on the detected text blocks for scene text localization. The experimental results obtained on publicly available standard datasets illustrate that the proposed method can detect and localize the texts of various sizes, fonts and colors.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03918",
        "title": "Gradient Difference based approach for Text Localization in Compressed domain",
        "authors": [
            "B.H. Shekar",
            "Smitha M.L"
        ],
        "abstract": "In this paper, we propose a gradient difference based approach to text localization in videos and scene images. The input video frame/ image is first compressed using multilevel 2-D wavelet transform. The edge information of the reconstructed image is found which is further used for finding the maximum gradient difference between the pixels and then the boundaries of the detected text blocks are computed using zero crossing technique. We perform logical AND operation of the text blocks obtained by gradient difference and the zero crossing technique followed by connected component analysis to eliminate the false positives. Finally, the morphological dilation operation is employed on the detected text blocks for scene text localization. The experimental results obtained on publicly available standard datasets illustrate that the proposed method can detect and localize the texts of various sizes, fonts and colors.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04110",
        "title": "Modeling Brain Circuitry over a Wide Range of Scales",
        "authors": [
            "Pascal Fua",
            "Graham Knott"
        ],
        "abstract": "If we are ever to unravel the mysteries of brain function at its most fundamental level, we will need a precise understanding of how its component neurons connect to each other. Electron Microscopes (EM) can now provide the nanometer resolution that is needed to image synapses, and therefore connections, while Light Microscopes (LM) see at the micrometer resolution required to model the 3D structure of the dendritic network. Since both the topology and the connection strength are integral parts of the brain's wiring diagram, being able to combine these two modalities is critically important.\n",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04132",
        "title": "Long-short Term Motion Feature for Action Classification and Retrieval",
        "authors": [
            "Zhenzhong Lan",
            "Xuanchong Li",
            "Ming Lin",
            "Alexander G. Hauptmann"
        ],
        "abstract": "We propose a method for representing motion information for video classification and retrieval. We improve upon local descriptor based methods that have been among the most popular and successful models for representing videos. The desired local descriptors need to satisfy two requirements: 1) to be representative, 2) to be discriminative. Therefore, they need to occur frequently enough in the videos and to be be able to tell the difference among different types of motions. To generate such local descriptors, the video blocks they are based on must contain just the right amount of motion information. However, current state-of-the-art local descriptor methods use video blocks with a single fixed size, which is insufficient for covering actions with varying speeds. In this paper, we introduce a long-short term motion feature that generates descriptors from video blocks with multiple lengths, thus covering motions with large speed variance. Experimental results show that, albeit simple, our model achieves state-of-the-arts results on several benchmark datasets.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04204",
        "title": "Gray-Level Image Transitions Driven by Tsallis Entropic Index",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "The maximum entropy principle is largely used in thresholding and segmentation of images. Among the several formulations of this principle, the most effectively applied is that based on Tsallis non-extensive entropy. Here, we discuss the role of its entropic index in determining the thresholds. When this index is spanning the interval (0,1), for some images, the values of thresholds can have large leaps. In this manner, we observe abrupt transitions in the appearance of corresponding bi-level or multi-level images. These gray-level image transitions are analogous to order or texture transitions observed in physical systems, transitions which are driven by the temperature or by other physical quantities.\n    ",
        "submission_date": "2015-02-14T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04252",
        "title": "Cardiac MR Image Segmentation Techniques: an overview",
        "authors": [
            "Tizita Nesibu Shewaye"
        ],
        "abstract": "Broadly speaking, the objective in cardiac image segmentation is to delineate the outer and inner walls of the heart to segment out either the entire or parts of the organ boundaries. This paper will focus on MR images as they are the most widely used in cardiac segmentation -- as a result of the accurate morphological information and better soft tissue contrast they provide. This cardiac segmentation information is very useful as it eases physical measurements that provides useful metrics for cardiac diagnosis such as infracted volumes, ventricular volumes, ejection fraction, myocardial mass, cardiac movement, and the like. But, this task is difficult due to the intensity and texture similarities amongst the different cardiac and background structures on top of some noisy artifacts present in MR images. Thus far, various researchers have proposed different techniques to solve some of the pressing issues. This seminar paper presents an overview of representative medical image segmentation techniques. The paper also highlights preferred approaches for segmentation of the four cardiac chambers: the left ventricle (LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on short axis image planes.\n    ",
        "submission_date": "2015-02-14T00:00:00",
        "last_modified_date": "2015-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04272",
        "title": "Spatial Stimuli Gradient Sketch Model",
        "authors": [
            "Joshin John Mathew",
            "Alex Pappachen James"
        ],
        "abstract": "The inability of automated edge detection methods inspired from primal sketch models to accurately calculate object edges under the influence of pixel noise is an open problem. Extending the principles of image perception i.e. Weber-Fechner law, and Sheperd similarity law, we propose a new edge detection method and formulation that use perceived brightness and neighbourhood similarity calculations in the determination of robust object edges. The robustness of the detected edges is benchmark against Sobel, SIS, Kirsch, and Prewitt edge detection methods in an example face recognition problem showing statistically significant improvement in recognition accuracy and pixel noise tolerance.\n    ",
        "submission_date": "2015-02-15T00:00:00",
        "last_modified_date": "2015-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04275",
        "title": "segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection",
        "authors": [
            "Yukun Zhu",
            "Raquel Urtasun",
            "Ruslan Salakhutdinov",
            "Sanja Fidler"
        ],
        "abstract": "In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current state-of-the-art, demonstrating the power of our approach.\n    ",
        "submission_date": "2015-02-15T00:00:00",
        "last_modified_date": "2015-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04383",
        "title": "A Comprehensive Survey on Pose-Invariant Face Recognition",
        "authors": [
            "Changxing Ding",
            "Dacheng Tao"
        ],
        "abstract": "The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, pose-invariant face recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this paper, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, i.e., pose-robust feature extraction approaches, multi-view subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed.\n    ",
        "submission_date": "2015-02-15T00:00:00",
        "last_modified_date": "2016-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04492",
        "title": "Towards Building Deep Networks with Bayesian Factor Graphs",
        "authors": [
            "Amedeo Buonanno",
            "Francesco A.N. Palmieri"
        ],
        "abstract": "We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04499",
        "title": "Color Image Enhancement Using the lrgb Coordinates in the Context of Support Fuzzification",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "Image enhancement is an important stage in the image-processing domain. The most known image enhancement method is the histogram equalization. This method is an automated one, and realizes a simultaneous modification for brightness and contrast in the case of monochrome images and for brightness, contrast, saturation and hue in the case of color images. Simple and efficient methods can be obtained if affine transforms within logarithmic models are used. A very important thing in the affine transform determination for color images is the coordinate system that is used for color space representation. Thus, the using of the RGB coordinates leads to a simultaneous modification of luminosity and saturation. In this paper using the lrgb perceptual coordinates one can define affine transforms, which allow a separated modification of luminosity l and saturation s (saturation being calculated with the component rgb in the chromatic plane). Better results can be obtained if partitions are defined on the image support and then the pixels are separately processed in each window belonging to the defined partition. Classical partitions frequently lead to the appearance of some discontinuities at the boundaries between these windows. In order to avoid all these drawbacks the classical partitions may be replaced by fuzzy partitions. Their elements will be fuzzy windows and in each of them there will be defined an affine transform induced by parameters using the fuzzy mean, fuzzy variance and fuzzy saturation computed for the pixels that belong to the analyzed window. The final image is obtained by summing up in a weight way the images of every fuzzy window.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04500",
        "title": "Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "In this paper we are proposing the use of Kaniadakis entropy in the bi-level thresholding of images, in the framework of a maximum entropy principle. We discuss the role of its entropic index in determining the threshold and in driving an \"image transition\", that is, an abrupt transition in the appearance of the corresponding bi-level image. Some examples are proposed to illustrate the method and for comparing it to the approach which is using the Tsallis entropy.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04569",
        "title": "Image Specificity",
        "authors": [
            "Mainak Jas",
            "Devi Parikh"
        ],
        "abstract": "For some images, descriptions written by multiple people are consistent with each other. But for other images, descriptions across people vary considerably. In other words, some images are specific $-$ they elicit consistent descriptions from different people $-$ while other images are ambiguous. Applications involving images and text can benefit from an understanding of which images are specific and which ones are ambiguous. For instance, consider text-based image retrieval. If a query description is moderately similar to the caption (or reference description) of an ambiguous image, that query may be considered a decent match to the image. But if the image is very specific, a moderate similarity between the query and the reference description may not be sufficient to retrieve the image.\n",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04623",
        "title": "DRAW: A Recurrent Neural Network For Image Generation",
        "authors": [
            "Karol Gregor",
            "Ivo Danihelka",
            "Alex Graves",
            "Danilo Jimenez Rezende",
            "Daan Wierstra"
        ],
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04652",
        "title": "Inferring 3D Object Pose in RGB-D Images",
        "authors": [
            "Saurabh Gupta",
            "Pablo Arbel\u00e1ez",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "The goal of this work is to replace objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene using the approach from Gupta et al. [13]. We use a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel normals in images containing rendered synthetic objects. When tested on real data, it outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place the model that fits the best into the scene. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art [33], while being an order of magnitude faster at the same time.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04658",
        "title": "HEp-2 Cell Classification via Fusing Texture and Shape Information",
        "authors": [
            "Xianbiao Qi",
            "Guoying Zhao",
            "Chun-Guang Li",
            "Jun Guo",
            "Matti Pietik\u00e4inen"
        ],
        "abstract": "Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidence for diagnosis of autoimmune diseases. Recently computer-aided diagnosis of autoimmune diseases by IIF HEp-2 cell classification has attracted great attention. However the HEp-2 cell classification task is quite challenging due to large intra-class variation and small between-class variation. In this paper we propose an effective and efficient approach for the automatic classification of IIF HEp-2 cell image by fusing multi-resolution texture information and richer shape information. To be specific, we propose to: a) capture the multi-resolution texture information by a novel Pairwise Rotation Invariant Co-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depict the richer shape information by using an Improved Fisher Vector (IFV) model with RootSIFT features which are sampled from large image patches in multiple scales, and c) combine them properly. We evaluate systematically the proposed approach on the IEEE International Conference on Pattern Recognition (ICPR) 2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR 2014 contest data sets. The experimental results for the proposed methods significantly outperform the winners of ICPR 2012 and ICIP 2013 contest, and achieve comparable performance with the winner of the newly released ICPR 2014 contest.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04754",
        "title": "3D Pose from Detections",
        "authors": [
            "Cosimo Rubino",
            "Marco Crocco",
            "Alessandro Perina",
            "Vittorio Murino",
            "Alessio Del Bue"
        ],
        "abstract": "We present a novel method to infer, in closed-form, a general 3D spatial occupancy and orientation of a collection of rigid objects given 2D image detections from a sequence of images. In particular, starting from 2D ellipses fitted to bounding boxes, this novel multi-view problem can be reformulated as the estimation of a quadric (ellipsoid) in 3D. We show that an efficient solution exists in the dual-space using a minimum of three views while a solution with two views is possible through the use of regularization. However, this algebraic solution can be negatively affected in the presence of gross inaccuracies in the bounding boxes estimation. To this end, we also propose a robust ellipse fitting algorithm able to improve performance in the presence of errors in the detected objects. Results on synthetic tests and on different real datasets, involving real challenging scenarios, demonstrate the applicability and potential of our method.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04824",
        "title": "Randomized LU decomposition: An Algorithm for Dictionaries Construction",
        "authors": [
            "Aviv Rotbart",
            "Gil Shabat",
            "Yaniv Shmueli",
            "Amir Averbuch"
        ],
        "abstract": "In recent years, distinctive-dictionary construction has gained importance due to his usefulness in data processing. Usually, one or more dictionaries are constructed from a training data and then they are used to classify signals that did not participate in the training process. A new dictionary construction algorithm is introduced. It is based on a low-rank matrix factorization being achieved by the application of the randomized LU decomposition to a training data. This method is fast, scalable, parallelizable, consumes low memory, outperforms SVD in these categories and works also extremely well on large sparse matrices. In contrast to existing methods, the randomized LU decomposition constructs an under-complete dictionary, which simplifies both the construction and the classification processes of newly arrived signals. The dictionary construction is generic and general that fits different applications. We demonstrate the capabilities of this algorithm for file type identification, which is a fundamental task in digital security arena, performed nowadays for example by sandboxing mechanism, deep packet inspection, firewalls and anti-virus systems. We propose a content-based method that detects file types that neither depend on file extension nor on metadata. Such approach is harder to deceive and we show that only a few file fragments from a whole file are needed for a successful classification. Based on the constructed dictionaries, we show that the proposed method can effectively identify execution code fragments in PDF files.\n",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2018-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04981",
        "title": "Semi-supervised Segmentation Fusion of Multi-spectral and Aerial Images",
        "authors": [
            "Mete Ozay"
        ],
        "abstract": "A Semi-supervised Segmentation Fusion algorithm is proposed using consensus and distributed learning. The aim of Unsupervised Segmentation Fusion (USF) is to achieve a consensus among different segmentation outputs obtained from different segmentation algorithms by computing an approximate solution to the NP problem with less computational complexity. Semi-supervision is incorporated in USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF). In SSSF, side information about the co-occurrence of pixels in the same or different segments is formulated as the constraints of a convex optimization problem. The results of the experiments employed on artificial and real-world benchmark multi-spectral and aerial images show that the proposed algorithms perform better than the individual state-of-the art segmentation algorithms.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04983",
        "title": "Context Tricks for Cheap Semantic Segmentation",
        "authors": [
            "Thanapong Intharah",
            "Gabriel J. Brostow"
        ],
        "abstract": "Accurate semantic labeling of image pixels is difficult because intra-class variability is often greater than inter-class variability. In turn, fast semantic segmentation is hard because accurate models are usually too complicated to also run quickly at test-time. Our experience with building and running semantic segmentation systems has also shown a reasonably obvious bottleneck on model complexity, imposed by small training datasets. We therefore propose two simple complementary strategies that leverage context to give better semantic segmentation, while scaling up or down to train on different-sized datasets.\n",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05082",
        "title": "What makes for effective detection proposals?",
        "authors": [
            "Jan Hosang",
            "Rodrigo Benenson",
            "Piotr Doll\u00e1r",
            "Bernt Schiele"
        ],
        "abstract": "Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05137",
        "title": "Prediction of Search Targets From Fixations in Open-World Settings",
        "authors": [
            "Hosnieh Sattar",
            "Sabine M\u00fcller",
            "Mario Fritz",
            "Andreas Bulling"
        ],
        "abstract": "Previous work on predicting the target of visual search from human fixations only considered closed-world settings in which training labels are available and predictions are performed for a known set of potential targets. In this work we go beyond the state of the art by studying search target prediction in an open-world setting in which we no longer assume that we have fixation data to train for the search targets. We present a dataset containing fixation data of 18 users searching for natural images from three image categories within synthesised image collages of about 80 images. In a closed-world baseline experiment we show that we can predict the correct target image out of a candidate set of five images. We then present a new problem formulation for search target prediction in the open-world setting that is based on learning compatibilities between fixations and potential targets.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05212",
        "title": "IAT - Image Annotation Tool: Manual",
        "authors": [
            "Gianluigi Ciocca",
            "Paolo Napoletano",
            "Raimondo Schettini"
        ],
        "abstract": "The annotation of image and video data of large datasets is a fundamental task in multimedia information retrieval and computer vision applications. In order to support the users during the image and video annotation process, several software tools have been developed to provide them with a graphical environment which helps drawing object contours, handling tracking information and specifying object metadata. Here we introduce a preliminary version of the image annotation tools developed at the Imaging and Vision Laboratory.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05224",
        "title": "Cross-Modality Hashing with Partial Correspondence",
        "authors": [
            "Yun Gu",
            "Haoyang Xue",
            "Jie Yang"
        ],
        "abstract": "Learning a hashing function for cross-media search is very desirable due to its low storage cost and fast query speed. However, the data crawled from Internet cannot always guarantee good correspondence among different modalities which affects the learning for hashing function. In this paper, we focus on cross-modal hashing with partially corresponded data. The data without full correspondence are made in use to enhance the hashing performance. The experiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method outperforms some state-of-the-art hashing approaches with fewer correspondence information.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05241",
        "title": "NEFI: Network Extraction From Images",
        "authors": [
            "Michael Dirnberger",
            "Adrian Neumann",
            "Tim Kehl"
        ],
        "abstract": "Networks and network-like structures are amongst the central building blocks of many technological and biological systems. Given a mathematical graph representation of a network, methods from graph theory enable a precise investigation of its properties. Software for the analysis of graphs is widely available and has been applied to graphs describing large scale networks such as social networks, protein-interaction networks, etc. In these applications, graph acquisition, i.e., the extraction of a mathematical graph from a network, is relatively simple. However, for many network-like structures, e.g. leaf venations, slime molds and mud cracks, data collection relies on images where graph extraction requires domain-specific solutions or even manual. Here we introduce Network Extraction From Images, NEFI, a software tool that automatically extracts accurate graphs from images of a wide range of networks originating in various domains. While there is previous work on graph extraction from images, theoretical results are fully accessible only to an expert audience and ready-to-use implementations for non-experts are rarely available or insufficiently documented. NEFI provides a novel platform allowing practitioners from many disciplines to easily extract graph representations from images by supplying flexible tools from image processing, computer vision and graph theory bundled in a convenient package. Thus, NEFI constitutes a scalable alternative to tedious and error-prone manual graph extraction and special purpose tools. We anticipate NEFI to enable the collection of larger datasets by reducing the time spent on graph extraction. The analysis of these new datasets may open up the possibility to gain new insights into the structure and function of various types of networks. NEFI is open source and available ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05243",
        "title": "SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks",
        "authors": [
            "Aalok Gangopadhyay",
            "Shivam Mani Tripathi",
            "Ishan Jindal",
            "Shanmuganathan Raman"
        ],
        "abstract": "The task of classifying videos of natural dynamic scenes into appropriate classes has gained lot of attention in recent years. The problem especially becomes challenging when the camera used to capture the video is dynamic. In this paper, we analyse the performance of statistical aggregation (SA) techniques on various pre-trained convolutional neural network(CNN) models to address this problem. The proposed approach works by extracting CNN activation features for a number of frames in a video and then uses an aggregation scheme in order to obtain a robust feature descriptor for the video. We show through results that the proposed approach performs better than the-state-of-the arts for the Maryland and YUPenn dataset. The final descriptor obtained is powerful enough to distinguish among dynamic scenes and is even capable of addressing the scenario where the camera motion is dominant and the scene dynamics are complex. Further, this paper shows an extensive study on the performance of various aggregation methods and their combinations. We compare the proposed approach with other dynamic scene classification algorithms on two publicly available datasets - Maryland and YUPenn to demonstrate the superior performance of the proposed approach.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05435",
        "title": "Fusion of Image Segmentation Algorithms using Consensus Clustering",
        "authors": [
            "Mete Ozay",
            "Fatos T. Yarman Vural",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "abstract": "A new segmentation fusion method is proposed that ensembles the output of several segmentation algorithms applied on a remotely sensed image. The candidate segmentation sets are processed to achieve a consensus segmentation using a stochastic optimization algorithm based on the Filtered Stochastic BOEM (Best One Element Move) method. For this purpose, Filtered Stochastic BOEM is reformulated as a segmentation fusion problem by designing a new distance learning approach. The proposed algorithm also embeds the computation of the optimum number of clusters into the segmentation fusion problem.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2015-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05461",
        "title": "Visualizing Object Detection Features",
        "authors": [
            "Carl Vondrick",
            "Aditya Khosla",
            "Hamed Pirsiavash",
            "Tomasz Malisiewicz",
            "Antonio Torralba"
        ],
        "abstract": "We introduce algorithms to visualize feature spaces used by object detectors. Our method works by inverting a visual feature back to multiple natural images. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and supports that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of recognition systems.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05565",
        "title": "Multi-valued Color Representation Based on Frank t-norm Properties",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper two knowledge representation models are proposed, FP4 and FP6. Both combine ideas from fuzzy sets and four-valued and hexa-valued logics. Both represent imprecise properties whose accomplished degree is unknown or contradictory for some objects. A possible application in the color analysis and color image processing is discussed.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05678",
        "title": "VIP: Finding Important People in Images",
        "authors": [
            "Clint Solomon Mathialagan",
            "Andrew C. Gallagher",
            "Dhruv Batra"
        ],
        "abstract": "People preserve memories of events such as birthdays, weddings, or vacations by capturing photos, often depicting groups of people. Invariably, some individuals in the image are more important than others given the context of the event. This paper analyzes the concept of the importance of individuals in group photographs. We address two specific questions -- Given an image, who are the most important individuals in it? Given multiple images of a person, which image depicts the person in the most important role? We introduce a measure of importance of people in images and investigate the correlation between importance and visual saliency. We find that not only can we automatically predict the importance of people from purely visual cues, incorporating this predicted importance results in significant improvement in applications such as im2text (generating sentences that describe images of groups of people).\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05689",
        "title": "Unsupervised Network Pretraining via Encoding Human Design",
        "authors": [
            "Ming-Yu Liu",
            "Arun Mallya",
            "Oncel C. Tuzel",
            "Xi Chen"
        ],
        "abstract": "Over the years, computer vision researchers have spent an immense amount of effort on designing image features for the visual object recognition task. We propose to incorporate this valuable experience to guide the task of training deep neural networks. Our idea is to pretrain the network through the task of replicating the process of hand-designed feature extraction. By learning to replicate the process, the neural network integrates previous research knowledge and learns to model visual objects in a way similar to the hand-designed features. In the succeeding finetuning step, it further learns object-specific representations from labeled data and this boosts its classification power. We pretrain two convolutional neural networks where one replicates the process of histogram of oriented gradients feature extraction, and the other replicates the process of region covariance feature extraction. After finetuning, we achieve substantially better performance than the baseline methods.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05742",
        "title": "Application of Independent Component Analysis Techniques in Speckle Noise Reduction of Retinal OCT Images",
        "authors": [
            "Ahmadreza Baghaie",
            "Roshan M. D'souza",
            "Zeyun Yu"
        ],
        "abstract": "Optical Coherence Tomography (OCT) is an emerging technique in the field of biomedical imaging, with applications in ophthalmology, dermatology, coronary imaging etc. OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation. Therefore the need for speckle noise reduction techniques is of high importance. To the best of our knowledge, use of Independent Component Analysis (ICA) techniques has never been explored for speckle reduction of OCT images. Here, a comparative study of several ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise reduction of retinal OCT images. Having multiple B-scans of the same location, the eye movements are compensated using a rigid registration technique. Then, different ICA techniques are applied to the aggregated set of B-scans for extracting the noise-free image. Signal-to-Noise-Ratio (SNR), Contrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as analysis on the computational complexity of the methods, are considered as metrics for comparison. The results show that use of ICA can be beneficial, especially in case of having fewer number of B-scans.\n    ",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05752",
        "title": "Pairwise Constraint Propagation: A Survey",
        "authors": [
            "Zhenyong Fu",
            "Zhiwu Lu"
        ],
        "abstract": "As one of the most important types of (weaker) supervised information in machine learning and pattern recognition, pairwise constraint, which specifies whether a pair of data points occur together, has recently received significant attention, especially the problem of pairwise constraint propagation. At least two reasons account for this trend: the first is that compared to the data label, pairwise constraints are more general and easily to collect, and the second is that since the available pairwise constraints are usually limited, the constraint propagation problem is thus important.\n",
        "submission_date": "2015-02-19T00:00:00",
        "last_modified_date": "2015-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05803",
        "title": "Visual object tracking performance measures revisited",
        "authors": [
            "Luka \u010cehovin",
            "Ale\u0161 Leonardis",
            "Matej Kristan"
        ],
        "abstract": "The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments. This makes the cross-paper tracker comparison difficult. Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased towards particular tracking aspects. In this paper we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing towards homogenization of the tracker evaluation methodology. These two measures can be intuitively interpreted and visualized and have been employed by the recent Visual Object Tracking (VOT) challenges as the foundation for the evaluation methodology.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05840",
        "title": "A General Multi-Graph Matching Approach via Graduated Consistency-regularized Boosting",
        "authors": [
            "Junchi Yan",
            "Minsu Cho",
            "Hongyuan Zha",
            "Xiaokang Yang",
            "Stephen Chu"
        ],
        "abstract": "This paper addresses the problem of matching $N$ weighted graphs referring to an identical object or category. More specifically, matching the common node correspondences among graphs. This multi-graph matching problem involves two ingredients affecting the overall accuracy: i) the local pairwise matching affinity score among graphs; ii) the global matching consistency that measures the uniqueness of the pairwise matching results by different chaining orders. Previous studies typically either enforce the matching consistency constraints in the beginning of iterative optimization, which may propagate matching error both over iterations and across graph pairs; or separate affinity optimizing and consistency regularization in two steps. This paper is motivated by the observation that matching consistency can serve as a regularizer in the affinity objective function when the function is biased due to noises or inappropriate modeling. We propose multi-graph matching methods to incorporate the two aspects by boosting the affinity score, meanwhile gradually infusing the consistency as a regularizer. Furthermore, we propose a node-wise consistency/affinity-driven mechanism to elicit the common inlier nodes out of the irrelevant outliers. Extensive results on both synthetic and public image datasets demonstrate the competency of the proposed algorithms.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05908",
        "title": "Learning Descriptors for Object Recognition and 3D Pose Estimation",
        "authors": [
            "Paul Wohlhart",
            "Vincent Lepetit"
        ],
        "abstract": "Detecting poorly textured objects and estimating their 3D pose reliably is still a very challenging problem. We introduce a simple but powerful approach to computing descriptors for object views that efficiently capture both the object identity and 3D pose. By contrast with previous manifold-based approaches, we can rely on the Euclidean distance to evaluate the similarity between descriptors, and therefore use scalable Nearest Neighbor search methods to efficiently handle a large number of objects under a large range of poses. To achieve this, we train a Convolutional Neural Network to compute these descriptors by enforcing simple similarity and dissimilarity constraints between the descriptors. We show that our constraints nicely untangle the images from different objects and different views into clusters that are not only well-separated but also structured as the corresponding sets of poses: The Euclidean distance between descriptors is large when the descriptors are from different objects, and directly related to the distance between the poses when the descriptors are from the same object. These important properties allow us to outperform state-of-the-art object views representations on challenging RGB and RGB-D data.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05928",
        "title": "Supervised Dictionary Learning and Sparse Representation-A Review",
        "authors": [
            "Mehrdad J. Gangeh",
            "Ahmed K. Farahat",
            "Ali Ghodsi",
            "Mohamed S. Kamel"
        ],
        "abstract": "Dictionary learning and sparse representation (DLSR) is a recent and successful mathematical model for data representation that achieves state-of-the-art performance in various fields such as pattern recognition, machine learning, computer vision, and medical imaging. The original formulation for DLSR is based on the minimization of the reconstruction error between the original signal and its sparse representation in the space of the learned dictionary. Although this formulation is optimal for solving problems such as denoising, inpainting, and coding, it may not lead to optimal solution in classification tasks, where the ultimate goal is to make the learned dictionary and corresponding sparse representation as discriminative as possible. This motivated the emergence of a new category of techniques, which is appropriately called supervised dictionary learning and sparse representation (S-DLSR), leading to more optimal dictionary and sparse representation in classification tasks. Despite many research efforts for S-DLSR, the literature lacks a comprehensive view of these techniques, their connections, advantages and shortcomings. In this paper, we address this gap and provide a review of the recently proposed algorithms for S-DLSR. We first present a taxonomy of these algorithms into six categories based on the approach taken to include label information into the learning of the dictionary and/or sparse representation. For each category, we draw connections between the algorithms in this category and present a unified framework for them. We then provide guidelines for applied researchers on how to represent and learn the building blocks of an S-DLSR solution based on the problem at hand. This review provides a broad, yet deep, view of the state-of-the-art methods for S-DLSR and allows for the advancement of research and development in this emerging area of research.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2015-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06073",
        "title": "Study on Sparse Representation based Classification for Biometric Verification",
        "authors": [
            "Zengxi Huang",
            "Yiguang Liu",
            "Xiaoming Wang",
            "Jinrong Hu"
        ],
        "abstract": "In this paper, we propose a multimodal verification system integrating face and ear based on sparse representation based classification (SRC). The face and ear query samples are first encoded separately to derive sparsity-based match scores, and which are then combined with sum-rule fusion for verification. Apart from validating the encouraging performance of SRC-based multimodal verification, this paper also dedicates to provide a clear understanding about the characteristics of SRC-based biometric verification. To this end, two sparsity-based metrics, i.e. spare coding error (SCE) and sparse contribution rate (SCR), are involved, together with face and ear unimodal SRC-based verification. As for the issue that SRC-based biometric verification may suffer from heavy computational burden and verification accuracy degradation with increase of enrolled subjects, we argue that it could be properly resolved by exploiting small random dictionary for sparsity-based score computation, which consists of training samples from a limited number of randomly selected subjects. Experimental results demonstrate the superiority of SRC-based multimodal verification compared to the state-of-the-art multimodal methods like likelihood ratio (LLR), support vector machine (SVM), and the sum-rule fusion methods using cosine similarity, meanwhile the idea of using small random dictionary is feasible in both effectiveness and efficiency.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06075",
        "title": "A new network-based algorithm for human activity recognition in video",
        "authors": [
            "Weiyao Lin",
            "Yuanzhe Chen",
            "Jianxin Wu",
            "Hanli Wang",
            "Bin Sheng",
            "Hongxiang Li"
        ],
        "abstract": "In this paper, a new network-transmission-based (NTB) algorithm is proposed for human activity recognition in videos. The proposed NTB algorithm models the entire scene as an error-free network. In this network, each node corresponds to a patch of the scene and each edge represents the activity correlation between the corresponding patches. Based on this network, we further model people in the scene as packages while human activities can be modeled as the process of package transmission in the network. By analyzing these specific \"package transmission\" processes, various activities can be effectively detected. The implementation of our NTB algorithm into abnormal activity detection and group activity recognition are described in detail in the paper. Experimental results demonstrate the effectiveness of our proposed algorithm.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06076",
        "title": "A Heat-Map-based Algorithm for Recognizing Group Activities in Videos",
        "authors": [
            "Weiyao Lin",
            "Hang Chu",
            "Jianxin Wu",
            "Bin Sheng",
            "Zhenzhong Chen"
        ],
        "abstract": "In this paper, a new heat-map-based (HMB) algorithm is proposed for group activity recognition. The proposed algorithm first models human trajectories as series of \"heat sources\" and then applies a thermal diffusion process to create a heat map (HM) for representing the group activities. Based on this heat map, a new key-point based (KPB) method is used for handling the alignments among heat maps with different scales and rotations. And a surface-fitting (SF) method is also proposed for recognizing group activities. Our proposed HM feature can efficiently embed the temporal motion information of the group activities while the proposed KPB and SF methods can effectively utilize the characteristics of the heat map for activity recognition. Experimental results demonstrate the effectiveness of our proposed algorithms.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06080",
        "title": "Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise Tone Mapping",
        "authors": [
            "Yuanzhe Chen",
            "Weiyao Lin",
            "Chongyang Zhang",
            "Zhenzhong Chen",
            "Ning Xu",
            "Jun Xie"
        ],
        "abstract": "Video enhancement plays an important role in various video applications. In this paper, we propose a new intra-and-inter-constraint-based video enhancement approach aiming to 1) achieve high intra-frame quality of the entire picture where multiple region-of-interests (ROIs) can be adaptively and simultaneously enhanced, and 2) guarantee the inter-frame quality consistencies among video frames. We first analyze features from different ROIs and create a piecewise tone mapping curve for the entire frame such that the intra-frame quality of a frame can be enhanced. We further introduce new inter-frame constraints to improve the temporal quality consistency. Experimental results show that the proposed algorithm obviously outperforms the state-of-the-art algorithms.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06081",
        "title": "Study of a Robust Algorithm Applied in the Optimal Position Tuning for the Camera Lens in Automated Visual Inspection Systems",
        "authors": [
            "Radu Arsinte"
        ],
        "abstract": "This paper present the mathematical fundaments and experimental study of an algorithm used to find the optimal position for the camera lens to obtain a maximum of details. This information can be further applied to a appropriate system to automatically correct this position. The algorithm is based on the evaluation of a so called resolution function who calculates the maximum of gradient in a certain zone of the image. The paper also presents alternative forms of the function, results of measurements and set up a set of practical rules for the right application of the algorithm.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06105",
        "title": "Regularization and Kernelization of the Maximin Correlation Approach",
        "authors": [
            "Taehoon Lee",
            "Taesup Moon",
            "Seung Jean Kim",
            "Sungroh Yoon"
        ],
        "abstract": "Robust classification becomes challenging when each class consists of multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables in the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06108",
        "title": "Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks",
        "authors": [
            "Xiao Lin",
            "Devi Parikh"
        ],
        "abstract": "Artificial agents today can answer factual questions. But they fall short on questions that require common sense reasoning. Perhaps this is because most existing common sense databases rely on text to learn and represent knowledge. But much of common sense knowledge is unwritten - partly because it tends not to be interesting enough to talk about, and partly because some common sense is unnatural to articulate in text. While unwritten, it is not unseen. In this paper we leverage semantic common sense knowledge learned from images - i.e. visual common sense - in two textual tasks: fill-in-the-blank and visual paraphrasing. We propose to \"imagine\" the scene behind the text, and leverage visual cues from the \"imagined\" scenes in addition to textual cues while answering these questions. We imagine the scenes as a visual abstraction. Our approach outperforms a strong text-only baseline on these tasks. Our proposed tasks can serve as benchmarks to quantitatively evaluate progress in solving tasks that go \"beyond recognition\". Our code and datasets are publicly available.\n    ",
        "submission_date": "2015-02-21T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06219",
        "title": "Video Text Localization with an emphasis on Edge Features",
        "authors": [
            "B.H. Shekar",
            "Smitha M.L."
        ],
        "abstract": "The text detection and localization plays a major role in video analysis and understanding. The scene text embedded in video consist of high-level semantics and hence contributes significantly to visual content analysis and retrieval. This paper proposes a novel method to robustly localize the texts in natural scene images and videos based on sobel edge emphasizing approach. The input image is preprocessed and edge emphasis is done to detect the text clusters. Further, a set of rules have been devised using morphological operators for false positive elimination and connected component analysis is performed to detect the text regions and hence text localization is performed. The experimental results obtained on publicly available standard datasets illustrate that the proposed method can detect and localize the texts of various sizes, fonts and colors.\n    ",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06220",
        "title": "Boosting of Image Denoising Algorithms",
        "authors": [
            "Yaniv Romano",
            "Michael Elad"
        ],
        "abstract": "In this paper we propose a generic recursive algorithm for improving image denoising methods. Given the initial denoised image, we suggest repeating the following \"SOS\" procedure: (i) (S)trengthen the signal by adding the previous denoised image to the degraded input image, (ii) (O)perate the denoising method on the strengthened image, and (iii) (S)ubtract the previous denoised image from the restored signal-strengthened outcome. The convergence of this process is studied for the K-SVD image denoising and related algorithms. Still in the context of K-SVD image denoising, we introduce an interesting interpretation of the SOS algorithm as a technique for closing the gap between the local patch-modeling and the global restoration task, thereby leading to improved performance. In a quest for the theoretical origin of the SOS algorithm, we provide a graph-based interpretation of our method, where the SOS recursive update effectively minimizes a penalty function that aims to denoise the image, while being regularized by the graph Laplacian. We demonstrate the SOS boosting algorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL), showing tendency to further improve denoising performance.\n    ",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06235",
        "title": "Spatio-temporal Video Parsing for Abnormality Detection",
        "authors": [
            "Borislav Anti\u0107",
            "Bj\u00f6rn Ommer"
        ],
        "abstract": "Abnormality detection in video poses particular challenges due to the infinite size of the class of all irregular objects and behaviors. Thus no (or by far not enough) abnormal training samples are available and we need to find abnormalities in test data without actually knowing what they are. Nevertheless, the prevailing concept of the field is to directly search for individual abnormal local patches or image regions independent of another. To address this problem, we propose a method for joint detection of abnormalities in videos by spatio-temporal video parsing. The goal of video parsing is to find a set of indispensable normal spatio-temporal object hypotheses that jointly explain all the foreground of a video, while, at the same time, being supported by normal training samples. Consequently, we avoid a direct detection of abnormalities and discover them indirectly as those hypotheses which are needed for covering the foreground without finding an explanation for themselves by normal samples. Abnormalities are localized by MAP inference in a graphical model and we solve it efficiently by formulating it as a convex optimization problem. We experimentally evaluate our approach on several challenging benchmark sets, improving over the state-of-the-art on all standard benchmarks both in terms of abnormality classification and localization.\n    ",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06260",
        "title": "Compressive Hyperspectral Imaging with Side Information",
        "authors": [
            "Xin Yuan",
            "Tsung-Han Tsai",
            "Ruoyu Zhu",
            "Patrick Llull",
            "David Brady",
            "Lawrence Carin"
        ],
        "abstract": "A blind compressive sensing algorithm is proposed to reconstruct hyperspectral images from spectrally-compressed ",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06344",
        "title": "Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding",
        "authors": [
            "Clemens-Alexander Brust",
            "Sven Sickert",
            "Marcel Simon",
            "Erik Rodner",
            "Joachim Denzler"
        ],
        "abstract": "Classifying single image patches is important in many different applications, such as road detection or scene understanding. In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and which can be used for pixel-wise labeling. We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for certain categories jointly with an appearance model. In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art results on the KITTI as well as on the LabelMeFacade dataset.\n",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06556",
        "title": "Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "The maximum entropy principle is often used for bi-level or multi-level thresholding of images. For this purpose, some methods are available based on Shannon and Tsallis entropies. In this paper, we discuss them and propose a method based on Kaniadakis entropy.\n    ",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06648",
        "title": "Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data",
        "authors": [
            "Marcus Rohrbach",
            "Anna Rohrbach",
            "Michaela Regneri",
            "Sikandar Amin",
            "Mykhaylo Andriluka",
            "Manfred Pinkal",
            "Bernt Schiele"
        ],
        "abstract": "Activity recognition has shown impressive progress in recent years. However, the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them. The first challenge is to detect fine-grained activities, which are defined by low inter-class variability and are typically characterized by fine-grained body motions. We explore how human pose and hands can help to approach this challenge by comparing two pose-based and two hand-centric features with state-of-the-art holistic features. To attack the second challenge, recognizing composite activities, we leverage the fact that these activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. We show the benefits of our hand-centric approach for fine-grained activity classification and detection. For composite activity recognition we find that decomposition into attributes allows sharing information across composites and is essential to attack this hard task. Using script data we can recognize novel composites without having training data for them.\n    ",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06703",
        "title": "Discrete Wavelet Transform and Gradient Difference based approach for text localization in videos",
        "authors": [
            "B.H. Shekar",
            "Smitha M.L.",
            "P. Shivakumara"
        ],
        "abstract": "The text detection and localization is important for video analysis and understanding. The scene text in video contains semantic information and thus can contribute significantly to video retrieval and understanding. However, most of the approaches detect scene text in still images or single video frame. Videos differ from images in temporal redundancy. This paper proposes a novel hybrid method to robustly localize the texts in natural scene images and videos based on fusion of discrete wavelet transform and gradient difference. A set of rules and geometric properties have been devised to localize the actual text regions. Then, morphological operation is performed to generate the text regions and finally the connected component analysis is employed to localize the text in a video frame. The experimental results obtained on publicly available standard ICDAR 2003 and Hua dataset illustrate that the proposed method can accurately detect and localize texts of various sizes, fonts and colors. The experimentation on huge collection of video databases reveal the suitability of the proposed method to video databases.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06796",
        "title": "Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network",
        "authors": [
            "Seunghoon Hong",
            "Tackgeun You",
            "Suha Kwak",
            "Bohyung Han"
        ],
        "abstract": "We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by backpropagating CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map visualizes spatial configuration of target effectively, it improves target localization accuracy and enable us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06807",
        "title": "Hands Deep in Deep Learning for Hand Pose Estimation",
        "authors": [
            "Markus Oberweger",
            "Paul Wohlhart",
            "Vincent Lepetit"
        ],
        "abstract": "We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map. We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions. We also show how to use context efficiently to deal with ambiguities between fingers. These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07058",
        "title": "Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval",
        "authors": [
            "Adam W. Harley",
            "Alex Ufkes",
            "Konstantinos G. Derpanis"
        ],
        "abstract": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07209",
        "title": "Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks",
        "authors": [
            "Yu-Gang Jiang",
            "Zuxuan Wu",
            "Jun Wang",
            "Xiangyang Xue",
            "Shih-Fu Chang"
        ],
        "abstract": "In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by rigorously imposing regularizations in the learning process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be efficiently realized using a GPU-based implementation with an affordable training cost. Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed rDNN is more suitable for modeling video semantics. With extensive experimental evaluations, we show that rDNN produces superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain very competitive results: 66.9\\% and 73.5\\% respectively in terms of mean average precision. In addition, to substantially evaluate our rDNN and stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2018-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07243",
        "title": "Real-Time System of Hand Detection And Gesture Recognition In Cyber Presence Interactive System For E-Learning",
        "authors": [
            "Bousaaid Mourad",
            "Ayaou Tarik",
            "Afdel Karim",
            "Estraillier Pascal"
        ],
        "abstract": "The development of technologies of multimedia, linked to that of Internet and democratization of high outflow, has made henceforth E-learning possible for learners being in virtual classes and geographically distributed. The quality and quantity of asynchronous and synchronous communications are the key elements for E-learning success. It is important to have a propitious supervision to reduce the feeling of isolation in E-learning. This feeling of isolation is among the main causes of loss and high rates of stalling in E-learning. The researches to be conducted in this domain aim to bring solutions of convergence coming from real time image for the capture and recognition of hand gestures. These gestures will be analyzed by the system and transformed as indicator of participation. This latter is displayed in the table of performance of the tutor as a curve according to the time. In case of isolation of learner, the indicator of participation will become red and the tutor will be informed of learners with difficulties to participate during learning session.\n    ",
        "submission_date": "2014-12-08T00:00:00",
        "last_modified_date": "2014-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07331",
        "title": "Highly corrupted image inpainting through hypoelliptic diffusion",
        "authors": [
            "Ugo Boscain",
            "Roman Chertovskih",
            "Jean-Paul Gauthier",
            "Dario Prandi",
            "Alexey Remizov"
        ],
        "abstract": "We present a new image inpainting algorithm, the Averaging and Hypoelliptic Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic diffusion and ad-hoc local averaging techniques. In particular, we focus on reconstructing highly corrupted images (i.e. where more than the 80% of the image is missing), for which we obtain reconstructions comparable with the state-of-the-art.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2018-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07411",
        "title": "Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields",
        "authors": [
            "Fayao Liu",
            "Chunhua Shen",
            "Guosheng Lin",
            "Ian Reid"
        ],
        "abstract": "In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is $\\sim 10$ times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07423",
        "title": "Connections Between Nuclear Norm and Frobenius Norm Based Representations",
        "authors": [
            "Xi Peng",
            "Canyi Lu",
            "Zhang Yi",
            "Huajin Tang"
        ],
        "abstract": "A lot of works have shown that frobenius-norm based representation (FNR) is competitive to sparse representation and nuclear-norm based representation (NNR) in numerous tasks such as subspace clustering. Despite the success of FNR in experimental studies, less theoretical analysis is provided to understand its working mechanism. In this paper, we fill this gap by building the theoretical connections between FNR and NNR. More specially, we prove that: 1) when the dictionary can provide enough representative capacity, FNR is exactly NNR even though the data set contains the Gaussian noise, Laplacian noise, or sample-specified corruption, 2) otherwise, FNR and NNR are two solutions on the column space of the dictionary.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07432",
        "title": "Coercive Region-level Registration for Multi-modal Images",
        "authors": [
            "Yu-Hui Chen",
            "Dennis Wei",
            "Gregory Newstadt",
            "Jeffrey Simmons",
            "Alfred Hero"
        ],
        "abstract": "We propose a coercive approach to simultaneously register and segment multi-modal images which share similar spatial structure. Registration is done at the region level to facilitate data fusion while avoiding the need for interpolation. The algorithm performs alternating minimization of an objective function informed by statistical models for pixel values in different modalities. Hypothesis tests are developed to determine whether to refine segmentations by splitting regions. We demonstrate that our approach has significantly better performance than the state-of-the-art registration and segmentation methods on microscopy images.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07436",
        "title": "A Dictionary Approach to EBSD Indexing",
        "authors": [
            "Yu-Hui Chen",
            "Se Un Park",
            "Dennis Wei",
            "Gregory Newstadt",
            "Michael Jackson",
            "Jeff P. Simmons",
            "Marc De Graef",
            "Alfred O. Hero"
        ],
        "abstract": "We propose a framework for indexing of grain and sub-grain structures in electron backscatter diffraction (EBSD) images of polycrystalline materials. The framework is based on a previously introduced physics-based forward model by Callahan and De Graef (2013) relating measured patterns to grain orientations (Euler angle). The forward model is tuned to the microscope and the sample symmetry group. We discretize the domain of the forward model onto a dense grid of Euler angles and for each measured pattern we identify the most similar patterns in the dictionary. These patterns are used to identify boundaries, detect anomalies, and index crystal orientations. The statistical distribution of these closest matches is used in an unsupervised binary decision tree (DT) classifier to identify grain boundaries and anomalous regions. The DT classifies a pattern as an anomaly if it has an abnormally low similarity to any pattern in the dictionary. It classifies a pixel as being near a grain boundary if the highly ranked patterns in the dictionary differ significantly over the pixels 3x3 neighborhood. Indexing is accomplished by computing the mean orientation of the closest dictionary matches to each pattern. The mean orientation is estimated using a maximum likelihood approach that models the orientation distribution as a mixture of Von Mises-Fisher distributions over the quaternionic 3-sphere. The proposed dictionary matching approach permits segmentation, anomaly detection, and indexing to be performed in a unified manner with the additional benefit of uncertainty quantification. We demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07446",
        "title": "Estimating the Potential Speedup of Computer Vision Applications on Embedded Multiprocessors",
        "authors": [
            "V\u00edtor Schwambach",
            "S\u00e9bastien Cleyet-Merle",
            "Alain Issard",
            "St\u00e9phane Mancini"
        ],
        "abstract": "Computer vision applications constitute one of the key drivers for embedded multicore architectures. Although the number of available cores is increasing in new architectures, designing an application to maximize the utilization of the platform is still a challenge. In this sense, parallel performance prediction tools can aid developers in understanding the characteristics of an application and finding the most adequate parallelization strategy. In this work, we present a method for early parallel performance estimation on embedded multiprocessors from sequential application traces. We describe its implementation in Parana, a fast trace-driven simulator targeting OpenMP applications on the STMicroelectronics' STxP70 Application-Specific Multiprocessor (ASMP). Results for the FAST key point detector application show an error margin of less than 10% compared to the reference cycle-approximate simulator, with lower modeling effort and up to 20x faster execution time.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07453",
        "title": "A Holistic Approach for Modeling and Synthesis of Image Processing Applications for Heterogeneous Computing Architectures",
        "authors": [
            "Christian Hartmann",
            "Anna Yupatova",
            "Marc Reichenbach",
            "Dietmar Fey",
            "Reinhard German"
        ],
        "abstract": "Image processing applications are common in every field of our daily life. However, most of them are very complex and contain several tasks with different complexities which result in varying requirements for computing architectures. Nevertheless, a general processing scheme in every image processing application has a similar structure, called image processing pipeline: (1) capturing an image, (2) pre-processing using local operators, (3) processing with global operators and (4) post-processing using complex operations. Therefore, application-specialized hardware solutions based on heterogeneous architectures are used for image processing. Unfortunately the development of applications for heterogeneous hardware architectures is challenging due to the distribution of computational tasks among processors and programmable logic units. Nowadays, image processing systems are started from scratch which is time-consuming, error-prone and inflexible. A new methodology for modeling and implementing is needed in order to reduce the development time of heterogenous image processing systems. This paper introduces a new holistic top down approach for image processing systems. Two challenges have to be investigated. First, designers ought to be able to model their complete image processing pipeline on an abstract layer using UML. Second, we want to close the gap between the abstract system and the system architecture.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07540",
        "title": "A hypothesize-and-verify framework for Text Recognition using Deep Recurrent Neural Networks",
        "authors": [
            "Anupama Ray",
            "Sai Rajeswar",
            "Santanu Chaudhury"
        ],
        "abstract": "Deep LSTM is an ideal candidate for text recognition. However text recognition involves some initial image processing steps like segmentation of lines and words which can induce error to the recognition system. Without segmentation, learning very long range context is difficult and becomes computationally intractable. Therefore, alternative soft decisions are needed at the pre-processing level. This paper proposes a hybrid text recognizer using a deep recurrent neural network with multiple layers of abstraction and long range context along with a language model to verify the performance of the deep neural network. In this paper we construct a multi-hypotheses tree architecture with candidate segments of line sequences from different segmentation algorithms at its different branches. The deep neural network is trained on perfectly segmented data and tests each of the candidate segments, generating unicode sequences. In the verification step, these unicode sequences are validated using a sub-string match with the language model and best first search is used to find the best possible combination of alternative hypothesis from the tree structure. Thus the verification framework using language models eliminates wrong segmentation outputs and filters recognition errors.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07643",
        "title": "Dynamic Belief Fusion for Object Detection",
        "authors": [
            "Ryan Robinson"
        ],
        "abstract": "A novel approach for the fusion of detection scores from disparate object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score (called \"uncertainty\") is estimated using the precision/recall relationship of the corresponding detector. The proposed fusion method, called Dynamic Belief Fusion (DBF), dynamically assigns basic probabilities to propositions (target, non-target, uncertain) based on confidence levels in the detection results of individual approaches. A joint basic probability assignment, containing information from all detectors, is determined using Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as state-of-the-art individual detectors.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07666",
        "title": "Landmark-Guided Elastic Shape Analysis of Human Character Motions",
        "authors": [
            "Martin Bauer",
            "Markus Eslitzbichler",
            "Markus Grasmair"
        ],
        "abstract": "Motions of virtual characters in movies or video games are typically generated by recording actors using motion capturing methods. Animations generated this way often need postprocessing, such as improving the periodicity of cyclic animations or generating entirely new motions by interpolation of existing ones. Furthermore, search and classification of recorded motions becomes more and more important as the amount of recorded motion data grows.\n",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07770",
        "title": "Total variation on a tree",
        "authors": [
            "Vladimir Kolmogorov",
            "Thomas Pock",
            "Michal Rolinek"
        ],
        "abstract": "We consider the problem of minimizing the continuous valued total variation subject to different unary terms on trees and propose fast direct algorithms based on dynamic programming to solve these problems. We treat both the convex and the non-convex case and derive worst case complexities that are equal or better than existing methods. We show applications to total variation based 2D image processing and computer vision problems based on a Lagrangian decomposition approach. The resulting algorithms are very efficient, offer a high degree of parallelism and come along with memory requirements which are only in the order of the number of image pixels.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07781",
        "title": "The conjugated null space method of blind PSF estimation and deconvolution optimization",
        "authors": [
            "Yuriy A. Bunyak",
            "Roman N. Kvetnyy",
            "Olga Yu. Sofina"
        ],
        "abstract": "We have shown that the vector of the point spread function (PSF) lexicographical presentation belongs to the left side conjugated null space (NS) of the autoregression (AR) matrix operator on condition the AR parameters are common for original and blurred images. The method of the PSF and inverse PSF (IPSF) evaluation in the basis of the NS eigenfunctions is offered. The optimization of the PSF and IPSF shape with the aim of fluctuation elimination is considered in NS spectral domain and image space domain. The function of surface area was used as the regularization functional. Two methods of original image estimate optimization were designed basing on maximum entropy generalization of sought and blurred images conditional probability density and regularization. The first method uses balanced variations of convolutions with the PSF and IPSF to obtaining iterative schema of image optimization. The variations balance is providing by dynamic regularization basing on condition of the iteration process convergence. The regularization has dynamic character because depends on current and previous image estimate variations. The second method implements the regularization of the deconvolution optimization in curved space with metric defined on image estimate surface. The given iterative schemas have fast convergence and therefore can be used for reconstruction of high resolution images series in real time. The NS can be used for design of denoising bilateral linear filter which does not introduce image smoothing.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07802",
        "title": "Modelling Local Deep Convolutional Neural Network Features to Improve Fine-Grained Image Classification",
        "authors": [
            "ZongYuan Ge",
            "Chris McCool",
            "Conrad Sanderson",
            "Peter Corke"
        ],
        "abstract": "We propose a local modelling approach using deep convolutional neural networks (CNNs) for fine-grained image classification. Recently, deep CNNs trained from large datasets have considerably improved the performance of object recognition. However, to date there has been limited work using these deep CNNs as local feature extractors. This partly stems from CNNs having internal representations which are high dimensional, thereby making such representations difficult to model using stochastic models. To overcome this issue, we propose to reduce the dimensionality of one of the internal fully connected layers, in conjunction with layer-restricted retraining to avoid retraining the entire network. The distribution of low-dimensional features obtained from the modified layer is then modelled using a Gaussian mixture model. Comparative experiments show that considerable performance improvements can be achieved on the challenging Fish and UEC FOOD-100 datasets.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07976",
        "title": "Error-Correcting Factorization",
        "authors": [
            "Miguel Angel Bautista",
            "Oriol Pujol",
            "Fernando de la Torre",
            "Sergio Escalera"
        ],
        "abstract": "  Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi- class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pair-wise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method, our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08040",
        "title": "DistancePPG: Robust non-contact vital signs monitoring using a camera",
        "authors": [
            "Mayank Kumar",
            "Ashok Veeraraghavan",
            "Ashutosh Sabharval"
        ],
        "abstract": "Vital signs such as pulse rate and breathing rate are currently measured using contact probes. But, non-contact methods for measuring vital signs are desirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situ health tracking (e.g. on mobile phone and computers with webcams). Recently, camera-based non-contact vital sign monitoring have been shown to be feasible. However, camera-based vital sign monitoring is challenging for people with darker skin tone, under low lighting conditions, and/or during movement of an individual in front of the camera. In this paper, we propose distancePPG, a new camera-based vital sign estimation algorithm which addresses these challenges. DistancePPG proposes a new method of combining skin-color change signals from different tracked regions of the face using a weighted average, where the weights depend on the blood perfusion and incident light intensity in the region, to improve the signal-to-noise ratio (SNR) of camera-based estimate. One of our key contributions is a new automatic method for determining the weights based only on the video recording of the subject. The gains in SNR of camera-based PPG estimated using distancePPG translate into reduction of the error in vital sign estimation, and thus expand the scope of camera-based vital sign monitoring to potentially challenging scenarios. Further, a dataset will be released, comprising of synchronized video recordings of face and pulse oximeter based ground truth recordings from the earlobe for people with different skin tones, under different lighting conditions and for various motion scenarios.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08046",
        "title": "Image Segmentation in Liquid Argon Time Projection Chamber Detector",
        "authors": [
            "Piotr P\u0142o\u0144ski",
            "Dorota Stefan",
            "Robert Sulej",
            "Krzysztof Zaremba"
        ],
        "abstract": "The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. An efficient and automatic reconstruction procedures are required to exploit potential of this imaging technology. Herein, a novel method for segmentation of images from LAr-TPC detectors is presented. The proposed approach computes a feature descriptor for each pixel in the image, which characterizes amplitude distribution in pixel and its neighbourhood. The supervised classifier is employed to distinguish between pixels representing particle's track and noise. The classifier is trained and evaluated on the hand-labeled dataset. The proposed approach can be a preprocessing step for reconstructing algorithms working directly on detector images.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00040",
        "title": "Efficient Upsampling of Natural Images",
        "authors": [
            "Chinmay Hegde",
            "Oncel Tuzel",
            "Fatih Porikli"
        ],
        "abstract": "We propose a novel method of efficient upsampling of a single natural image. Current methods for image upsampling tend to produce high-resolution images with either blurry salient edges, or loss of fine textural detail, or spurious noise artifacts.\n",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00064",
        "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes",
        "authors": [
            "Dahua Lin",
            "Chen Kong",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00072",
        "title": "DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking",
        "authors": [
            "Hanxi Li",
            "Yi Li",
            "Fatih Porikli"
        ],
        "abstract": "Deep neural networks, albeit their great success on feature learning in various computer vision tasks, are usually considered as impractical for online visual tracking because they require very long training time and a large number of training samples. In this work, we present an efficient and very robust tracking algorithm using a single Convolutional Neural Network (CNN) for learning effective feature representations of the target object, in a purely online manner. Our contributions are multifold: First, we introduce a novel truncated structural loss function that maintains as many training samples as possible and reduces the risk of tracking error accumulation. Second, we enhance the ordinary Stochastic Gradient Descent approach in CNN training with a robust sample selection mechanism. The sampling mechanism randomly generates positive and negative samples from different temporal distributions, which are generated by taking the temporal relations and label noise into account. Finally, a lazy yet effective updating scheme is designed for CNN training. Equipped with this novel updating algorithm, the CNN model is robust to some long-existing difficulties in visual tracking such as occlusion or incorrect detections, without loss of the effective adaption for significant appearance changes. In the experiment, our CNN tracker outperforms all compared state-of-the-art methods on two recently proposed benchmarks which in total involve over 60 video sequences. The remarkable performance improvement over the existing trackers illustrates the superiority of the feature representations which are learned\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00081",
        "title": "Activity Recognition Using A Combination of Category Components And Local Models for Video Surveillance",
        "authors": [
            "Weiyao Lin",
            "Ming-Ting Sun",
            "Radha Poovendran",
            "Zhengyou Zhang"
        ],
        "abstract": "This paper presents a novel approach for automatic recognition of human activities for video surveillance applications. We propose to represent an activity by a combination of category components, and demonstrate that this approach offers flexibility to add new activities to the system and an ability to deal with the problem of building models for activities lacking training data. For improving the recognition accuracy, a Confident-Frame- based Recognition algorithm is also proposed, where the video frames with high confidence for recognizing an activity are used as a specialized local model to help classify the remainder of the video frames. Experimental results show the effectiveness of the proposed approach.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00082",
        "title": "Group Event Detection with a Varying Number of Group Members for Video Surveillance",
        "authors": [
            "Weiyao Lin",
            "Ming-Ting Sun",
            "Radha Poovendran",
            "Zhengyou Zhang"
        ],
        "abstract": "This paper presents a novel approach for automatic recognition of group activities for video surveillance applications. We propose to use a group representative to handle the recognition with a varying number of group members, and use an Asynchronous Hidden Markov Model (AHMM) to model the relationship between people. Furthermore, we propose a group activity detection algorithm which can handle both symmetric and asymmetric group activities, and demonstrate that this approach enables the detection of hierarchical interactions between people. Experimental results show the effectiveness of our approach.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00090",
        "title": "Improved Image Deblurring based on Salient-region Segmentation",
        "authors": [
            "Chongyang Zhang",
            "Weiyao Lin",
            "Wei Li",
            "Bing Zhou",
            "Jun Xie",
            "Jijia Li"
        ],
        "abstract": "Image deblurring techniques play important roles in many image processing applications. As the blur varies spatially across the image plane, it calls for robust and effective methods to deal with the spatially-variant blur problem. In this paper, a Saliency-based Deblurring (SD) approach is proposed based on the saliency detection for salient-region segmentation and a corresponding compensate method for image deblurring. We also propose a PDE-based deblurring method which introduces an anisotropic Partial Differential Equation (PDE) model for latent image prediction and employs an adaptive optimization model in the kernel estimation and deconvolution steps. Experimental results demonstrate the effectiveness of the proposed algorithm.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00488",
        "title": "Graphical Representation for Heterogeneous Face Recognition",
        "authors": [
            "Chunlei Peng",
            "Xinbo Gao",
            "Nannan Wang",
            "Jie Li"
        ],
        "abstract": "Heterogeneous face recognition (HFR) refers to matching face images acquired from different sources (i.e., different sensors or different wavelengths) for identification. HFR plays an important role in both biometrics research and industry. In spite of promising progresses achieved in recent years, HFR is still a challenging problem due to the difficulty to represent two heterogeneous images in a homogeneous manner. Existing HFR methods either represent an image ignoring the spatial information, or rely on a transformation procedure which complicates the recognition task. Considering these problems, we propose a novel graphical representation based HFR method (G-HFR) in this paper. Markov networks are employed to represent heterogeneous image patches separately, which takes the spatial compatibility between neighboring image patches into consideration. A coupled representation similarity metric (CRSM) is designed to measure the similarity between obtained graphical representations. Extensive experiments conducted on multiple HFR scenarios (viewed sketch, forensic sketch, near infrared image, and thermal infrared image) show that the proposed method outperforms state-of-the-art methods.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2016-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00516",
        "title": "Matrix Product State for Feature Extraction of Higher-Order Tensors",
        "authors": [
            "Johann A. Bengua",
            "Ho N. Phien",
            "Hoang D. Tuan",
            "Minh N. Do"
        ],
        "abstract": "This paper introduces matrix product state (MPS) decomposition as a computational tool for extracting features of multidimensional data represented by higher-order tensors. Regardless of tensor order, MPS extracts its relevant features to the so-called core tensor of maximum order three which can be used for classification. Mainly based on a successive sequence of singular value decompositions (SVD), MPS is quite simple to implement without any recursive procedure needed for optimizing local tensors. Thus, it leads to substantial computational savings compared to other tensor feature extraction methods such as higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition (TD). Benchmark results show that MPS can reduce significantly the feature space of data while achieving better classification performance compared to HOOI.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00591",
        "title": "Deep Transfer Network: Unsupervised Domain Adaptation",
        "authors": [
            "Xu Zhang",
            "Felix Xinnan Yu",
            "Shih-Fu Chang",
            "Shengjin Wang"
        ],
        "abstract": "Domain adaptation aims at training a classifier in one dataset and applying it to a related but not identical dataset. One successfully used framework of domain adaptation is to learn a transformation to match both the distribution of the features (marginal distribution), and the distribution of the labels given features (conditional distribution). In this paper, we propose a new domain adaptation framework named Deep Transfer Network (DTN), where the highly flexible deep neural networks are used to implement such a distribution matching process.\n",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00593",
        "title": "Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal",
        "authors": [
            "Jian Sun",
            "Wenfei Cao",
            "Zongben Xu",
            "Jean Ponce"
        ],
        "abstract": "In this paper, we address the problem of estimating and removing non-uniform motion blur from a single blurry image. We propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations show that our approach can effectively estimate and remove complex non-uniform motion blur that is not handled well by previous approaches.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00769",
        "title": "Grouping and Recognition of Dot Patterns with Straight Offset Polygons",
        "authors": [
            "Toshiro Kubota"
        ],
        "abstract": "When the boundary of a familiar object is shown by a series of isolated dots, humans can often recognize the object with ease. This ability can be sustained with addition of distracting dots around the object. However, such capability has not been reproduced algorithmically on computers. We introduce a new algorithm that groups a set of dots into multiple non-disjoint subsets. It connects the dots into a spanning tree using the proximity cue. It then applies the straight polygon transformation to an initial polygon derived from the spanning tree. The straight polygon divides the space into polygons recursively and each polygon can be viewed as grouping of a subset of the dots. The number of polygons generated is O($n$). We also introduce simple shape selection and recognition algorithms that can be applied to the grouping result. We used both natural and synthetic images to show effectiveness of these algorithms.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00783",
        "title": "Joint calibration of Ensemble of Exemplar SVMs",
        "authors": [
            "Davide Modolo",
            "Alexander Vezhnevets",
            "Olga Russakovsky",
            "Vittorio Ferrari"
        ],
        "abstract": "We present a method for calibrating the Ensemble of Exemplar SVMs model. Unlike the standard approach, which calibrates each SVM independently, our method optimizes their joint performance as an ensemble. We formulate joint calibration as a constrained optimization problem and devise an efficient optimization algorithm to find its global optimum. The algorithm dynamically discards parts of the solution space that cannot contain the optimum early on, making the optimization computationally feasible. We experiment with EE-SVM trained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 and PASCAL VOC 2007 datasets show that (i) our joint calibration procedure outperforms independent calibration on the task of classifying windows as belonging to an object class or not; and (ii) this improved window classifier leads to better performance on the object detection task.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00787",
        "title": "Context Forest for efficient object detection with large mixture models",
        "authors": [
            "Davide Modolo",
            "Alexander Vezhnevets",
            "Vittorio Ferrari"
        ],
        "abstract": "We present Context Forest (ConF), a technique for predicting properties of the objects in an image based on its global appearance. Compared to standard nearest-neighbour techniques, ConF is more accurate, fast and memory efficient. We train ConF to predict which aspects of an object class are likely to appear in a given image (e.g. which viewpoint). This enables to speed-up multi-component object detectors, by automatically selecting the most relevant components to run on that image. This is particularly useful for detectors trained from large datasets, which typically need many components to fully absorb the data and reach their peak performance. ConF provides a speed-up of 2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To show ConF's generality, we also train it to predict at which locations objects are likely to appear in an image. Incorporating this information in the detector score improves mAP performance by about 2% by removing false positive detections in unlikely locations.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00848",
        "title": "Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation",
        "authors": [
            "Jordi Pont-Tuset",
            "Pablo Arbelaez",
            "Jonathan T. Barron",
            "Ferran Marques",
            "Jitendra Malik"
        ],
        "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five second per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00949",
        "title": "Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning",
        "authors": [
            "Ramazan Gokberk Cinbis",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "Object category localization is a challenging problem in computer vision. Standard supervised training requires bounding box annotations of object instances. This time-consuming annotation process is sidestepped in weakly supervised learning. In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations. We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images. Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations. This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features. We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior. We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00992",
        "title": "Anisotropic Diffusion in ITK",
        "authors": [
            "Jean-Marie Mirebeau",
            "J\u00e9r\u00f4me Fehrenbach",
            "Laurent Risser",
            "Shaza Tobji"
        ],
        "abstract": "Anisotropic Non-Linear Diffusion is a powerful image processing technique, which allows to simultaneously remove the noise and enhance sharp features in two or three dimensional images. Anisotropic Diffusion is understood here in the sense of Weickert, meaning that diffusion tensors are anisotropic and reflect the local orientation of image features. This is in contrast with the non-linear diffusion filter of Perona and Malik, which only involves scalar diffusion coefficients, in other words isotropic diffusion tensors. In this paper, we present an anisotropic non-linear diffusion technique we implemented in ITK. This technique is based on a recent adaptive scheme making the diffusion stable and requiring limited numerical resources. (See supplementary data.)\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01070",
        "title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research",
        "authors": [
            "Atousa Torabi",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01138",
        "title": "Learning Super-Resolution Jointly from External and Internal Examples",
        "authors": [
            "Zhangyang Wang",
            "Yingzhen Yang",
            "Zhaowen Wang",
            "Shiyu Chang",
            "Jianchao Yang",
            "Thomas S. Huang"
        ],
        "abstract": "Single image super-resolution (SR) aims to estimate a high-resolution (HR) image from a lowresolution (LR) input. Image priors are commonly learned to regularize the otherwise seriously ill-posed SR problem, either using external LR-HR pairs or internal similar patterns. We propose joint SR to adaptively combine the advantages of both external and internal SR methods. We define two loss functions using sparse coding based external examples, and epitomic matching based on internal examples, as well as a corresponding adaptive weight to automatically balance their contributions according to their reconstruction errors. Extensive SR results demonstrate the effectiveness of the proposed method over the existing state-of-the-art methods, and is also verified by our subjective evaluation study.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01224",
        "title": "Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition",
        "authors": [
            "Peng Wang",
            "Yuanzhouhan Cao",
            "Chunhua Shen",
            "Lingqiao Liu",
            "Heng Tao Shen"
        ],
        "abstract": "Encouraged by the success of Convolutional Neural Networks (CNNs) in image classification, recently much effort is spent on applying CNNs to video based action recognition problems. One challenge is that video contains a varying number of frames which is incompatible to the standard input format of CNNs. Existing methods handle this issue either by directly sampling a fixed number of frames or bypassing this issue by introducing a 3D convolutional layer which conducts convolution in spatial-temporal domain.\n",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01313",
        "title": "A Novel Performance Evaluation Methodology for Single-Target Trackers",
        "authors": [
            "Matej Kristan",
            "Jiri Matas",
            "Ales Leonardis",
            "Tomas Vojir",
            "Roman Pflugfelder",
            "Gustavo Fernandez",
            "Georg Nebehay",
            "Fatih Porikli",
            "Luka Cehovin"
        ],
        "abstract": "This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01393",
        "title": "A Hierarchical Approach for Joint Multi-view Object Pose Estimation and Categorization",
        "authors": [
            "Mete Ozay",
            "Krzysztof Walas",
            "Ales Leonardis"
        ],
        "abstract": "We propose a joint object pose estimation and categorization approach which extracts information about object poses and categories from the object parts and compositions constructed at different layers of a hierarchical object representation algorithm, namely Learned Hierarchy of Parts (LHOP). In the proposed approach, we first employ the LHOP to learn hierarchical part libraries which represent entity parts and compositions across different object categories and views. Then, we extract statistical and geometric features from the part realizations of the objects in the images in order to represent the information about object pose and category at each different layer of the hierarchy. Unlike the traditional approaches which consider specific layers of the hierarchies in order to extract information to perform specific tasks, we combine the information extracted at different layers to solve a joint object pose estimation and categorization problem using distributed optimization algorithms. We examine the proposed generative-discriminative learning approach and the algorithms on two benchmark 2-D multi-view image datasets. The proposed approach and the algorithms outperform state-of-the-art classification, regression and feature extraction algorithms. In addition, the experimental results shed light on the relationship between object categorization, pose estimation and the part realizations observed at different layers of the hierarchy.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01444",
        "title": "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications",
        "authors": [
            "Tae-Hyun Oh",
            "Yu-Wing Tai",
            "Jean-Charles Bazin",
            "Hyeongwoo Kim",
            "In So Kweon"
        ],
        "abstract": "Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01508",
        "title": "Do We Need More Training Data?",
        "authors": [
            "Xiangxin Zhu",
            "Carl Vondrick",
            "Charless Fowlkes",
            "Deva Ramanan"
        ],
        "abstract": "Datasets for training object recognition systems are steadily increasing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or saturate in performance due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of discriminatively trained templates defined on oriented gradient features. We investigate the performance of mixtures of templates as the number of mixture components and the amount of training data grows. Surprisingly, even with proper treatment of regularization and \"outliers\", the performance of classic mixture models appears to saturate quickly ($\\sim$10 templates and $\\sim$100 positive training examples per template). This is not a limitation of the feature space as compositional mixtures that share template parameters via parts and that can synthesize new templates not encountered during training yield significantly better performance. Based on our analysis, we conjecture that the greatest gains in detection performance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01531",
        "title": "Spectral Clustering by Ellipsoid and Its Connection to Separable Nonnegative Matrix Factorization",
        "authors": [
            "Tomohiko Mizutani"
        ],
        "abstract": "This paper proposes a variant of the normalized cut algorithm for spectral clustering. Although the normalized cut algorithm applies the K-means algorithm to the eigenvectors of a normalized graph Laplacian for finding clusters, our algorithm instead uses a minimum volume enclosing ellipsoid for them. We show that the algorithm shares similarity with the ellipsoidal rounding algorithm for separable nonnegative matrix factorization. Our theoretical insight implies that the algorithm can serve as a bridge between spectral clustering and separable NMF. The K-means algorithm has the issues in that the choice of initial points affects the construction of clusters and certain choices result in poor clustering performance. The normalized cut algorithm inherits these issues since K-means is incorporated in it, whereas the algorithm proposed here does not. An empirical study is presented to examine the performance of the algorithm.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01532",
        "title": "Deep Temporal Appearance-Geometry Network for Facial Expression Recognition",
        "authors": [
            "Heechul Jung",
            "Sihaeng Lee",
            "Sunjeong Park",
            "Injae Lee",
            "Chunghyun Ahn",
            "Junmo Kim"
        ],
        "abstract": "Temporal information can provide useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal geometry features from temporal facial landmark points, while the other deep network extracts temporal appearance features from image sequences . These two models are combined in order to boost the performance of the facial expression recognition. Through several experiments, we showed that the two models cooperate with each other. As a result, we achieved superior performance to other state-of-the-art methods in CK+ and Oulu-CASIA databases. Furthermore, one of the main contributions of this paper is that our deep network catches the facial action points automatically.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01543",
        "title": "Learning to rank in person re-identification with metric ensembles",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "We propose an effective structured learning based approach to the problem of person re-identification which outperforms the current state-of-the-art on most benchmark data sets evaluated. Our framework is built on the basis of multiple low-level hand-crafted and high-level visual features. We then formulate two optimization algorithms, which directly optimize evaluation measures commonly used in person re-identification, also known as the Cumulative Matching Characteristic (CMC) curve. Our new approach is practical to many real-world surveillance applications as the re-identification performance can be concentrated in the range of most practical importance. The combination of these factors leads to a person re-identification system which outperforms most existing algorithms. More importantly, we advance state-of-the-art results on person re-identification by improving the rank-$1$ recognition rates from $40\\%$ to $50\\%$ on the iLIDS benchmark, $16\\%$ to $18\\%$ on the PRID2011 benchmark, $43\\%$ to $46\\%$ on the VIPeR benchmark, $34\\%$ to $53\\%$ on the CUHK01 benchmark and $21\\%$ to $62\\%$ on the CUHK03 benchmark.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01557",
        "title": "Supervised Discrete Hashing",
        "authors": [
            "Fumin Shen",
            "Chunhua Shen",
            "Wei Liu",
            "Heng Tao Shen"
        ],
        "abstract": "This paper has been withdrawn by the authour.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01563",
        "title": "Convex Optimization for Parallel Energy Minimization",
        "authors": [
            "K. S. Sesh Kumar",
            "Alvaro Barbero",
            "Stefanie Jegelka",
            "Suvrit Sra",
            "Francis Bach"
        ],
        "abstract": "Energy minimization has been an intensely studied core problem in computer vision. With growing image sizes (2D and 3D), it is now highly desirable to run energy minimization algorithms in parallel. But many existing algorithms, in particular, some efficient combinatorial algorithms, are difficult to par-allelize. By exploiting results from convex and submodular theory, we reformulate the quadratic energy minimization problem as a total variation denoising problem, which, when viewed geometrically, enables the use of projection and reflection based convex methods. The resulting min-cut algorithm (and code) is conceptually very simple, and solves a sequence of TV denoising problems. We perform an extensive empirical evaluation comparing state-of-the-art combinatorial algorithms and convex optimization techniques. On small problems the iterative convex methods match the combinatorial max-flow algorithms, while on larger problems they offer other flexibility and important gains: (a) their memory footprint is small; (b) their straightforward parallelizability fits multi-core platforms; (c) they can easily be warm-started; and (d) they quickly reach approximately good solutions, thereby enabling faster \"inexact\" solutions. A key consequence of our approach based on submodularity and convexity is that it is allows to combine any arbitrary combinatorial or convex methods as subroutines, which allows one to obtain hybrid combinatorial and convex optimization algorithms that benefit from the strengths of both.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01640",
        "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called BoxSup, produces competitive results supervised by boxes only, on par with strong baselines fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01646",
        "title": "Video-Based Facial Expression Recognition Using Local Directional Binary Pattern",
        "authors": [
            "Sahar Hooshmand",
            "Ali Jamali Avilaq",
            "Amir Hossein Rezaie"
        ],
        "abstract": "Automatic facial expression analysis is a challenging issue and influenced so many areas such as human computer interaction. Due to the uncertainties of the light intensity and light direction, the face gray shades are uneven and the expression recognition rate under simple Local Binary Pattern is not ideal and promising. In this paper we propose two state-of-the-art descriptors for person-independent facial expression recognition. First the face regions of the whole images in a video sequence are modeled with Volume Local Directional Binary pattern (VLDBP), which is an extended version of the LDBP operator, incorporating movement and appearance together. To make the survey computationally simple and easy to expand, only the co-occurrences of the Local Directional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated. After extracting the feature vectors the K-Nearest Neighbor classifier was used to recognize the expressions. The proposed methods are applied to the videos of the Extended Cohn-Kanade database (CK+) and the experimental outcomes demonstrate that the offered techniques achieve more accuracy in comparison with the classic and traditional algorithms.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01657",
        "title": "Color Image Classification via Quaternion Principal Component Analysis Network",
        "authors": [
            "Rui Zeng",
            "Jiasong Wu",
            "Zhuhong Shao",
            "Yang Chen",
            "Lotfi Senhadji",
            "Huazhong Shu"
        ],
        "abstract": "The Principal Component Analysis Network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various databases. However, the performance of PCANet may be degraded when dealing with color images. In this paper, a Quaternion Principal Component Analysis Network (QPCANet), which is an extension of PCANet, is proposed for color images classification. Compared to PCANet, the proposed QPCANet takes into account the spatial distribution information of color images and ensures larger amount of intra-class invariance of color images. Experiments conducted on different color image datasets such as Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealed that the proposed QPCANet achieves higher classification accuracy than PCANet.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01804",
        "title": "Frequency Domain TOF: Encoding Object Depth in Modulation Frequency",
        "authors": [
            "Achuta Kadambi",
            "Vage Taamazyan",
            "Suren Jayasuriya",
            "Ramesh Raskar"
        ],
        "abstract": "Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of flight sensors use phase-based sampling, where the phase delay between emitted and received, high-frequency signals encodes distance. In this paper, we present a new time of flight architecture that relies only on frequency---we refer to this technique as frequency-domain time of flight (FD-TOF). Inspired by optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth is high. With the increasing frequency of TOF sensors, new challenges to time of flight sensing continue to emerge. At high frequencies, FD-TOF offers several potential benefits over phase-based time of flight methods.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01832",
        "title": "Linear Global Translation Estimation with Feature Tracks",
        "authors": [
            "Zhaopeng Cui",
            "Nianjuan Jiang",
            "Chengzhou Tang",
            "Ping Tan"
        ],
        "abstract": "This paper derives a novel linear position constraint for cameras seeing a common scene point, which leads to a direct linear method for global camera translation estimation. Unlike previous solutions, this method deals with collinear camera motion and weak image association at the same time. The final linear formulation does not involve the coordinates of scene points, which makes it efficient even for large scale data. We solve the linear equation based on $L_1$ norm, which makes our system more robust to outliers in essential matrices and feature correspondences. We experiment this method on both sequentially captured images and unordered Internet images. The experiments demonstrate its strength in robustness, accuracy, and efficiency.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01868",
        "title": "Total Variation Regularized Tensor RPCA for Background Subtraction from Compressive Measurements",
        "authors": [
            "Wenfei Cao",
            "Yao Wang",
            "Jian Sun",
            "Deyu Meng",
            "Can Yang",
            "Andrzej Cichocki",
            "Zongben Xu"
        ],
        "abstract": "Background subtraction has been a fundamental and widely studied task in video analysis, with a wide range of applications in video surveillance, teleconferencing and 3D modeling. Recently, motivated by compressive imaging, background subtraction from compressive measurements (BSCM) is becoming an active research task in video surveillance. In this paper, we propose a novel tensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video frames into backgrounds with spatial-temporal correlations and foregrounds with spatio-temporal continuity in a tensor framework. In this approach, we use 3D total variation (TV) to enhance the spatio-temporal continuity of foregrounds, and Tucker decomposition to model the spatio-temporal correlations of video background. Based on this idea, we design a basic tensor RPCA model over the video frames, dubbed as the holistic TenRPCA model (H-TenRPCA). To characterize the correlations among the groups of similar 3D patches of video background, we further design a patch-group-based tensor RPCA model (PG-TenRPCA) by joint tensor Tucker decompositions of 3D patch groups for modeling the video background. Efficient algorithms using alternating direction method of multipliers (ADMM) are developed to solve the proposed models. Extensive experiments on simulated and real-world videos demonstrate the superiority of the proposed approaches over the existing state-of-the-art approaches.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2016-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01903",
        "title": "Partial light field tomographic reconstruction from a fixed-camera focal stack",
        "authors": [
            "A. Mousnier",
            "E. Vural",
            "C. Guillemot"
        ],
        "abstract": "This paper describes a novel approach to partially reconstruct high-resolution 4D light fields from a stack of differently focused photographs taken with a fixed camera. First, a focus map is calculated from this stack using a simple approach combining gradient detection and region expansion with graph-cut. Then, this focus map is converted into a depth map thanks to the calibration of the camera. We proceed after this with the tomographic reconstruction of the epipolar images by back-projecting the focused regions of the scene only. We call it masked back-projection. The angles of back-projection are calculated from the depth map. Thanks to the high angular resolution we achieve by suitably exploiting the image content captured over a large interval of focus distances, we are able to render puzzling perspective shifts although the original photographs were taken from a single fixed camera at a fixed position.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01918",
        "title": "Fast image-based obstacle detection from unmanned surface vehicles",
        "authors": [
            "Matej Kristan",
            "Vildana Sulic",
            "Stanislav Kovacic",
            "Janez Pers"
        ],
        "abstract": "Obstacle detection plays an important role in unmanned surface vehicles (USV). The USVs operate in highly diverse environments in which an obstacle may be a floating piece of wood, a scuba diver, a pier, or a part of a shoreline, which presents a significant challenge to continuous detection from images taken onboard. This paper addresses the problem of online detection by constrained unsupervised segmentation. To this end, a new graphical model is proposed that affords a fast and continuous obstacle image-map estimation from a single video stream captured onboard a USV. The model accounts for the semantic structure of marine environment as observed from USV by imposing weak structural constraints. A Markov random field framework is adopted and a highly efficient algorithm for simultaneous optimization of model parameters and segmentation mask estimation is derived. Our approach does not require computationally intensive extraction of texture features and comfortably runs in real-time. The algorithm is tested on a new, challenging, dataset for segmentation and obstacle detection in marine environments, which is the largest annotated dataset of its kind. Results on this dataset show that our model outperforms the related approaches, while requiring a fraction of computational effort.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01986",
        "title": "Convex Color Image Segmentation with Optimal Transport Distances",
        "authors": [
            "Julien Rabin",
            "Nicolas Papadakis"
        ],
        "abstract": "This work is about the use of regularized optimal-transport distances for convex, histogram-based image segmentation. In the considered framework, fixed exemplar histograms define a prior on the statistical features of the two regions in competition. In this paper, we investigate the use of various transport-based cost functions as discrepancy measures and rely on a primal-dual algorithm to solve the obtained convex optimization problem.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01993",
        "title": "Tomographic Image Reconstruction using Training images",
        "authors": [
            "Sara Soltani",
            "Martin S. Andersen",
            "Per Christian Hansen"
        ],
        "abstract": "We describe and examine an algorithm for tomographic image reconstruction where prior knowledge about the solution is available in the form of training images. We first construct a nonnegative dictionary based on prototype elements from the training images; this problem is formulated as a regularized non-negative matrix factorization. Incorporating the dictionary as a prior in a convex reconstruction problem, we then find an approximate solution with a sparse representation in the dictionary. The dictionary is applied to non-overlapping patches of the image, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and interplay of the model parameters and the regularization parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total variation regularization and tends to include more texture and more correct edges.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02041",
        "title": "On the Invariance of Dictionary Learning and Sparse Representation to Projecting Data to a Discriminative Space",
        "authors": [
            "Mehrdad J. Gangeh",
            "Ali Ghodsi"
        ],
        "abstract": "In this paper, it is proved that dictionary learning and sparse representation is invariant to a linear transformation. It subsumes the special case of transforming/projecting the data into a discriminative space. This is important because recently, supervised dictionary learning algorithms have been proposed, which suggest to include the category information into the learning of dictionary to improve its discriminative power. Among them, there are some approaches that propose to learn the dictionary in a discriminative projected space. To this end, two approaches have been proposed: first, assigning the discriminative basis as the dictionary and second, perform dictionary learning in the projected space. Based on the invariance of dictionary learning to any transformation in general, and to a discriminative space in particular, we advocate the first approach.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02090",
        "title": "Band selection in RKHS for fast nonlinear unmixing of hyperspectral images",
        "authors": [
            "T. Imbiriba",
            "J. C. M. Bermudez",
            "C. Richard",
            "J.-Y. Tourneret"
        ],
        "abstract": "The profusion of spectral bands generated by the acquisition process of hyperspectral images generally leads to high computational costs. Such difficulties arise in particular with nonlinear unmixing methods, which are naturally more complex than linear ones. This complexity, associated with the high redundancy of information within the complete set of bands, make the search of band selection algorithms relevant. With this work, we propose a band selection strategy in reproducing kernel Hilbert spaces that allows to drastically reduce the processing time required by nonlinear unmixing techniques. Simulation results show a complexity reduction of two orders of magnitude without compromising unmixing performance.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02136",
        "title": "An Improved Image Mosaicing Algorithm for Damaged Documents",
        "authors": [
            "Waheeda Dhokley",
            "Khan Munifa",
            "Shaikh Nazia",
            "Shaikh Saiqua"
        ],
        "abstract": "It is a common phenomenon in day to day life; where in some of the document gets damaged. Out of several reasons, the main reason for documents getting damaged is shredding by hands. Recovery of such documents is essential. Manual recovery of such damaged document is tedious and time consuming task. In this paper, we are describing an algorithm which recovers the original document from such shredded pieces of the same. In order to implement this, we are using a simple technique called Image Mosaicing. In this technique a complete new image is developed using two or more torn fragments. For simplicity of implementation, we are considering only two torn pieces of a document that will be mosaiced together. The successful implementation of this algorithm would lead to recovery of important information which in turn would be beneficial in various fields such as forensic sciences, archival study, etc\n    ",
        "submission_date": "2015-03-07T00:00:00",
        "last_modified_date": "2015-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02291",
        "title": "TED: A Tolerant Edit Distance for Segmentation Evaluation",
        "authors": [
            "Jan Funke",
            "Francesc Moreno-Noguer",
            "Albert Cardona",
            "Matthew Cook"
        ],
        "abstract": "In this paper, we present a novel error measure to compare a segmentation against ground truth. This measure, which we call Tolerant Edit Distance (TED), is motivated by two observations: (1) Some errors, like small boundary shifts, are tolerable in practice. Which errors are tolerable is application dependent and should be a parameter of the measure. (2) Non-tolerable errors have to be corrected manually. The time needed to do so should be reflected by the error measure. Using integer linear programming, the TED finds the minimal weighted sum of split and merge errors exceeding a given tolerance criterion, and thus provides a time-to-fix estimate. In contrast to commonly used measures like Rand index or variation of information, the TED (1) does not count small, but tolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fix estimate, and (4) can localize and classify the type of errors. By supporting both isotropic and anisotropic volumes and having a flexible tolerance criterion, the TED can be adapted to different requirements. On example segmentations for 3D neuron segmentation, we demonstrate that the TED is capable of counting topological errors, while ignoring small boundary shifts.\n    ",
        "submission_date": "2015-03-08T00:00:00",
        "last_modified_date": "2016-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02330",
        "title": "Fitting 3D Morphable Models using Local Features",
        "authors": [
            "Patrik Huber",
            "Zhen-Hua Feng",
            "William Christmas",
            "Josef Kittler",
            "Matthias R\u00e4tsch"
        ],
        "abstract": "In this paper, we propose a novel fitting method that uses local image features to fit a 3D Morphable Model to 2D images. To overcome the obstacle of optimising a cost function that contains a non-differentiable feature extraction operator, we use a learning-based cascaded regression method that learns the gradient direction from data. The method allows to simultaneously solve for shape and pose parameters. Our method is thoroughly evaluated on Morphable Model generated data and first results on real data are presented. Compared to traditional fitting methods, which use simple raw features like pixel colour or edge maps, local features have been shown to be much more robust against variations in imaging conditions. Our approach is unique in that we are the first to use local features to fit a Morphable Model.\n",
        "submission_date": "2015-03-08T00:00:00",
        "last_modified_date": "2015-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02351",
        "title": "Fully Connected Deep Structured Networks",
        "authors": [
            "Alexander G. Schwing",
            "Raquel Urtasun"
        ],
        "abstract": "Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02391",
        "title": "Deep Human Parsing with Active Template Regression",
        "authors": [
            "Xiaodan Liang",
            "Si Liu",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Luoqi Liu",
            "Jian Dong",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an Active Template Regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches $64.38\\%$ by our ATR framework, significantly higher than $44.76\\%$ based on the state-of-the-art algorithm.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02445",
        "title": "Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification",
        "authors": [
            "Muhammad Uzair",
            "Faisal Shafait",
            "Bernard Ghanem",
            "Ajmal Mian"
        ],
        "abstract": "Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-04-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02466",
        "title": "Brain Tumor Segmentation: A Comparative Analysis",
        "authors": [
            "Muhammad Ali Qadar",
            "Yan Zhaowen"
        ],
        "abstract": "Five different threshold segmentation based approaches have been reviewed and compared over here to extract the tumor from set of brain images. This research focuses on the analysis of image segmentation methods, a comparison of five semi-automated methods have been undertaken for evaluating their relative performance in the segmentation of tumor. Consequently, results are compared on the basis of quantitative and qualitative analysis of respective methods. The purpose of this study was to analytically identify the methods, most suitable for application for a particular genre of problems. The results show that of the region growing segmentation performed better than rest in most cases.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02619",
        "title": "MODS: Fast and Robust Method for Two-View Matching",
        "authors": [
            "Dmytro Mishkin",
            "Jiri Matas",
            "Michal Perdoch"
        ],
        "abstract": "A novel algorithm for wide-baseline matching called MODS - Matching On Demand with view Synthesis - is presented. The MODS algorithm is experimentally shown to solve a broader range of wide-baseline problems than the state of the art while being nearly as fast as standard matchers on simple problems. The apparent robustness vs. speed trade-off is finessed by the use of progressively more time-consuming feature detectors and by on-demand generation of synthesized images that is performed until a reliable estimate of geometry is obtained.\n",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02675",
        "title": "Global 6DOF Pose Estimation from Untextured 2D City Models",
        "authors": [
            "Clemens Arth",
            "Christian Pirchheim",
            "Jonathan Ventura",
            "Vincent Lepetit"
        ],
        "abstract": "We propose a method for estimating the 3D pose for the camera of a mobile device in outdoor conditions, using only an untextured 2D model. Previous methods compute only a relative pose using a SLAM algorithm, or require many registered images, which are cumbersome to acquire. By contrast, our method returns an accurate, absolute camera pose in an absolute referential using simple 2D+height maps, which are broadly available, to refine a first estimate of the pose provided by the device's sensors. We show how to first estimate the camera absolute orientation from straight line segments, and then how to estimate the translation by aligning the 2D map with a semantic segmentation of the input image. We demonstrate the robustness and accuracy of our approach on a challenging dataset.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02725",
        "title": "Deep Hierarchical Parsing for Semantic Segmentation",
        "authors": [
            "Abhishek Sharma",
            "Oncel Tuzel",
            "David W. Jacobs"
        ],
        "abstract": "This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02727",
        "title": "Video Compressive Sensing for Spatial Multiplexing Cameras using Motion-Flow Models",
        "authors": [
            "Aswin C. Sankaranarayanan",
            "Lina Xu",
            "Christoph Studer",
            "Yun Li",
            "Kevin Kelly",
            "Richard G. Baraniuk"
        ],
        "abstract": "Spatial multiplexing cameras (SMCs) acquire a (typically static) scene through a series of coded projections using a spatial light modulator (e.g., a digital micro-mirror device) and a few optical sensors. This approach finds use in imaging applications where full-frame sensors are either too expensive (e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC systems reconstruct static scenes using techniques from compressive sensing (CS). For videos, however, existing acquisition and recovery methods deliver poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI) sensing and recovery framework for high-quality video acquisition and recovery using SMCs. Our framework features novel sensing matrices that enable the efficient computation of a low-resolution video preview, while enabling high-resolution video recovery using convex optimization. To further improve the quality of the reconstructed videos, we extract optical-flow estimates from the low-resolution previews and impose them as constraints in the recovery procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of synthetic and real measured SMC video data, and we show that high-quality videos can be recovered at roughly $60\\times$ compression.\n    ",
        "submission_date": "2015-03-09T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02945",
        "title": "Fast Multi-class Dictionaries Learning with Geometrical Directions in MRI Reconstruction",
        "authors": [
            "Zhifang Zhan",
            "Jian-Feng Cai",
            "Di Guo",
            "Yunsong Liu",
            "Zhong Chen",
            "Xiaobo Qu"
        ],
        "abstract": "Objective: Improve the reconstructed image with fast and multi-class dictionaries learning when magnetic resonance imaging is accelerated by undersampling the k-space data. Methods: A fast orthogonal dictionary learning method is introduced into magnetic resonance image reconstruction to providing adaptive sparse representation of images. To enhance the sparsity, image is divided into classified patches according to the same geometrical direction and dictionary is trained within each class. A new sparse reconstruction model with the multi-class dictionaries is proposed and solved using a fast alternating direction method of multipliers. Results: Experiments on phantom and brain imaging data with acceleration factor up to 10 and various undersampling patterns are conducted. The proposed method is compared with state-of-the-art magnetic resonance image reconstruction methods. Conclusion: Artifacts are better suppressed and image edges are better preserved than the compared methods. Besides, the computation of the proposed approach is much faster than the typical K-SVD dictionary learning method in magnetic resonance image reconstruction. Significance: The proposed method can be exploited in undersapmled magnetic resonance imaging to reduce data acquisition time and reconstruct images with better image quality.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03004",
        "title": "Fast and Robust Fixed-Rank Matrix Recovery",
        "authors": [
            "German Ros",
            "Julio Guerrero"
        ],
        "abstract": "We address the problem of efficient sparse fixed-rank (S-FR) matrix decomposition, i.e., splitting a corrupted matrix $M$ into an uncorrupted matrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rank constraints are usually imposed by the physical restrictions of the system under study. Here we propose a method to perform accurate and very efficient S-FR decomposition that is more suitable for large-scale problems than existing approaches. Our method is a grateful combination of geometrical and algebraical techniques, which avoids the bottleneck caused by the Truncated SVD (TSVD). Instead, a polar factorization is used to exploit the manifold structure of fixed-rank problems as the product of two Stiefel and an SPD manifold, leading to a better convergence and stability. Then, closed-form projectors help to speed up each iteration of the method. We introduce a novel and fast projector for the $\\text{SPD}$ manifold and a proof of its validity. Further acceleration is achieved using a Nystrom scheme. Extensive experiments with synthetic and real data in the context of robust photometric stereo and spectral clustering show that our proposals outperform the state of the art.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03163",
        "title": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder",
        "authors": [
            "Xi Zhang",
            "Yanwei Fu",
            "Andi Zang",
            "Leonid Sigal",
            "Gady Agam"
        ],
        "abstract": "We propose a method for using synthetic data to help learning classifiers. Synthetic data, even is generated based on real data, normally results in a shift from the distribution of real data in feature space. To bridge the gap between the real and synthetic data, and jointly learn from synthetic and real data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by suing MCAE, it is possible to learn a better feature representation for classification. To evaluate the proposed approach, we conduct experiments on two types of datasets. Experimental results on two datasets validate the efficiency of our MCAE model and our methodology of generating synthetic data.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03167",
        "title": "Deep Convolutional Inverse Graphics Network",
        "authors": [
            "Tejas D. Kulkarni",
            "Will Whitney",
            "Pushmeet Kohli",
            "Joshua B. Tenenbaum"
        ],
        "abstract": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03187",
        "title": "Simple, Accurate, and Robust Nonparametric Blind Super-Resolution",
        "authors": [
            "Wen-Ze Shao",
            "Michael Elad"
        ],
        "abstract": "This paper proposes a simple, accurate, and robust approach to single image nonparametric blind Super-Resolution (SR). This task is formulated as a functional to be minimized with respect to both an intermediate super-resolved image and a nonparametric blur-kernel. The proposed approach includes a convolution consistency constraint which uses a non-blind learning-based SR result to better guide the estimation process. Another key component is the unnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharp image and the blur-kernel, which is shown to be quite beneficial for estimating the blur-kernel accurately. The numerical optimization is implemented by coupling the splitting augmented Lagrangian and the conjugate gradient (CG). Using the pre-estimated blur-kernel, we finally reconstruct the SR image by a very simple non-blind SR method that uses a natural image prior. The proposed approach is demonstrated to achieve better performance than the recent method by Michaeli and Irani [2] in both terms of the kernel estimation accuracy and image SR quality.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03191",
        "title": "A model-based approach to recovering the structure of a plant from images",
        "authors": [
            "Ben Ward",
            "John Bastian",
            "Anton van den Hengel",
            "Daniel Pooley",
            "Rajendra Bari",
            "Bettina Berger",
            "Mark Tester"
        ],
        "abstract": "We present a method for recovering the structure of a plant directly from a small set of widely-spaced images. Structure recovery is more complex than shape estimation, but the resulting structure estimate is more closely related to phenotype than is a 3D geometric model. The method we propose is applicable to a wide variety of plants, but is demonstrated on wheat. Wheat is made up of thin elements with few identifiable features, making it difficult to analyse using standard feature matching techniques. Our method instead analyses the structure of plants using only their silhouettes. We employ a generate-and-test method, using a database of manually modelled leaves and a model for their composition to synthesise plausible plant structures which are evaluated against the images. The method is capable of efficiently recovering accurate estimates of plant structure in a wide variety of imaging scenarios, with no manual intervention.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03270",
        "title": "A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine",
        "authors": [
            "Vandna Bhalla",
            "Santanu Chaudhury",
            "Arihant Jain"
        ],
        "abstract": "Machine learning methods are used today for most recognition problems. Convolutional Neural Networks (CNN) have time and again proved successful for many image processing tasks primarily for their architecture. In this paper we propose to apply CNN to small data sets like for example, personal albums or other similar environs where the size of training dataset is a limitation, within the framework of a proposed hybrid CNN-AIS model. We use Artificial Immune System Principles to enhance small size of training data set. A layer of Clonal Selection is added to the local filtering and max pooling of CNN Architecture. The proposed Architecture is evaluated using the standard MNIST dataset by limiting the data size and also with a small personal data sample belonging to two different classes. Experimental results show that the proposed hybrid CNN-AIS based recognition engine works well when the size of training data is limited in size\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03278",
        "title": "Stochastic Texture Difference for Scale-Dependent Data Analysis",
        "authors": [
            "Nicolas Brodu",
            "Hussein Yahia"
        ],
        "abstract": "This article introduces the Stochastic Texture Difference method for analyzing data at prescribed spatial and value scales. This method relies on constrained random walks around each pixel, describing how nearby image values typically evolve on each side of this pixel. Textures are represented as probability distributions of such random walks, so a texture difference operator is statistically defined as a distance between these distributions in a suitable reproducing kernel Hilbert space. The method is thus not limited to scalar pixel values: any data type for which a kernel is available may be considered, from color triplets and multispectral vector data to strings, graphs, and more. By adjusting the size of the neighborhoods that are compared, the method is implicitly scale-dependent. It is also able to focus on either small changes or large gradients. We demonstrate how it can be used to infer spatial and data value characteristic scales in measured signals and natural images.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03429",
        "title": "Dense image registration and deformable surface reconstruction in presence of occlusions and minimal texture",
        "authors": [
            "Dat Tien Ngo",
            "Sanghuyk Park",
            "Anne Jorstad",
            "Alberto Crivellaro",
            "Chang Yoo",
            "Pascal Fua"
        ],
        "abstract": "Deformable surface tracking from monocular images is well-known to be under-constrained. Occlusions often make the task even more challenging, and can result in failure if the surface is not sufficiently textured. In this work, we explicitly address the problem of 3D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a relevancy score. Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03491",
        "title": "Properties of simple sets in digital spaces. Contractions of simple sets preserving the homotopy type of a digital space",
        "authors": [
            "Alexander V. Evako"
        ],
        "abstract": "A point of a digital space is called simple if it can be deleted from the space without altering topology. This paper introduces the notion simple set of points of a digital space. The definition is based on contractible spaces and contractible transformations. A set of points in a digital space is called simple if it can be contracted to a point without changing topology of the space. It is shown that contracting a simple set of points does not change the homotopy type of a digital space, and the number of points in a digital space without simple points can be reduces by contracting simple sets. Using the process of contracting, we can substantially compress a digital space while preserving the topology. The paper proposes a method for thinning a digital space which shows that this approach can contribute to computer science such as medical imaging, computer graphics and pattern analysis.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03514",
        "title": "Appearance-based indoor localization: A comparison of patch descriptor performance",
        "authors": [
            "Jose Rivera-Rubio",
            "Ioannis Alexiou",
            "Anil A. Bharath"
        ],
        "abstract": "Vision is one of the most important of the senses, and humans use it extensively during navigation. We evaluated different types of image and video frame descriptors that could be used to determine distinctive visual landmarks for localizing a person based on what is seen by a camera that they carry. To do this, we created a database containing over 3 km of video-sequences with ground-truth in the form of distance travelled along different corridors. Using this database, the accuracy of localization - both in terms of knowing which route a user is on - and in terms of position along a certain route, can be evaluated. For each type of descriptor, we also tested different techniques to encode visual structure and to search between journeys to estimate a user's position. The techniques include single-frame descriptors, those using sequences of frames, and both colour and achromatic descriptors. We found that single-frame indexing worked better within this particular dataset. This might be because the motion of the person holding the camera makes the video too dependent on individual steps and motions of one particular journey. Our results suggest that appearance-based information could be an additional source of navigational data indoors, augmenting that provided by, say, radio signal strength indicators (RSSIs). Such visual information could be collected by crowdsourcing low-resolution video feeds, allowing journeys made by different users to be associated with each other, and location to be inferred without requiring explicit mapping. This offers a complementary approach to methods based on simultaneous localization and mapping (SLAM) algorithms.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03606",
        "title": "Low-Level Features for Image Retrieval Based on Extraction of Directional Binary Patterns and Its Oriented Gradients Histogram",
        "authors": [
            "Nagaraja S.",
            "Prabhakar C.J."
        ],
        "abstract": "In this paper, we present a novel approach for image retrieval based on extraction of low level features using techniques such as Directional Binary Code, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC texture descriptor captures the spatial relationship between any pair of neighbourhood pixels in a local region along a given direction, while Local Binary Patterns descriptor considers the relationship between a given pixel and its surrounding neighbours. Therefore, DBC captures more spatial information than LBP and its variants, also it can extract more edge information than LBP. Hence, we employ DBC technique in order to extract grey level texture feature from each RGB channels individually and computed texture maps are further combined which represents colour texture features of an image. Then, we decomposed the extracted colour texture map and original image using Haar wavelet transform. Finally, we encode the shape and local features of wavelet transformed images using Histogram of Oriented Gradients for content based image retrieval. The performance of proposed method is compared with existing methods on two databases such as Wang's corel image and Caltech 256. The evaluation results show that our approach outperforms the existing methods for image retrieval.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03621",
        "title": "Designing A Composite Dictionary Adaptively From Joint Examples",
        "authors": [
            "Zhangyang Wang",
            "Yingzhen Yang",
            "Jianchao Yang",
            "Thomas S. Huang"
        ],
        "abstract": "We study the complementary behaviors of external and internal examples in image restoration, and are motivated to formulate a composite dictionary design framework. The composite dictionary consists of the global part learned from external examples, and the sample-specific part learned from internal examples. The dictionary atoms in both parts are further adaptively weighted to emphasize their model statistics. Experiments demonstrate that the joint utilization of external and internal examples leads to substantial improvements, with successful applications in image denoising and super resolution.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03630",
        "title": "Single image super-resolution by approximated Heaviside functions",
        "authors": [
            "Liang-Jian Deng",
            "Weihong Guo",
            "Ting-Zhu Huang"
        ],
        "abstract": "Image super-resolution is a process to enhance image resolution. It is widely used in medical imaging, satellite imaging, target recognition, etc. In this paper, we conduct continuous modeling and assume that the unknown image intensity function is defined on a continuous domain and belongs to a space with a redundant basis. We propose a new iterative model for single image super-resolution based on an observation: an image is consisted of smooth components and non-smooth components, and we use two classes of approximated Heaviside functions (AHFs) to represent them respectively. Due to sparsity of the non-smooth components, a $L_{1}$ model is employed. In addition, we apply the proposed iterative model to image patches to reduce computation and storage. Comparisons with some existing competitive methods show the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03637",
        "title": "On Computing the Translations Norm in the Epipolar Graph",
        "authors": [
            "Federica Arrigoni",
            "Beatrice Rossi",
            "Andrea Fusiello"
        ],
        "abstract": "This paper deals with the problem of recovering the unknown norm of relative translations between cameras based on the knowledge of relative rotations and translation directions. We provide theoretical conditions for the solvability of such a problem, and we propose a two-stage method to solve it. First, a cycle basis for the epipolar graph is computed, then all the scaling factors are recovered simultaneously by solving a homogeneous linear system. We demonstrate the accuracy of our solution by means of synthetic and real experiments.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03741",
        "title": "2D Face Recognition System Based on Selected Gabor Filters and Linear Discriminant Analysis LDA",
        "authors": [
            "Samir F. Hafez",
            "Mazen M. Selim",
            "Hala H. Zayed"
        ],
        "abstract": "We present a new approach for face recognition system. The method is based on 2D face image features using subset of non-correlated and Orthogonal Gabor Filters instead of using the whole Gabor Filter Bank, then compressing the output feature vector using Linear Discriminant Analysis (LDA). The face image has been enhanced using multi stage image processing technique to normalize it and compensate for illumination variation. Experimental results show that the proposed system is effective for both dimension reduction and good recognition performance when compared to the complete Gabor filter bank. The system has been tested using CASIA, ORL and Cropped YaleB 2D face images Databases and achieved average recognition rate of 98.9 %.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03771",
        "title": "Learning to Detect Vehicles by Clustering Appearance Patterns",
        "authors": [
            "Eshed Ohn-Bar",
            "Mohan M. Trivedi"
        ],
        "abstract": "This paper studies efficient means for dealing with intra-category diversity in object detection. Strategies for occlusion and orientation handling are explored by learning an ensemble of detection models from visual and geometrical clusters of object instances. An AdaBoost detection scheme is employed with pixel lookup features for fast detection. The analysis provides insight into the design of a robust vehicle detection system, showing promise in terms of detection performance and orientation estimation accuracy.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03832",
        "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
        "authors": [
            "Florian Schroff",
            "Dmitry Kalenichenko",
            "James Philbin"
        ],
        "abstract": "Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.\n",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03913",
        "title": "Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in Wavelet and MFDFA domain",
        "authors": [
            "Sabyasachi Mukhopadhyay",
            "Soham Mandal",
            "Nandan K Das",
            "Subhadip Dey",
            "Asish Mitra",
            "Nirmalya Ghosh",
            "Prasanta K Panigrahi"
        ],
        "abstract": "CT scan images of human brain of a particular patient in different cross sections are taken, on which wavelet transform and multi-fractal analysis are applied. The vertical and horizontal unfolding of images are done before analyzing these images. A systematic investigation of de-noised CT scan images of human brain in different cross-sections are carried out through wavelet normalized energy and wavelet semi-log plots, which clearly points out the mismatch between results of vertical and horizontal unfolding. The mismatch of results confirms the heterogeneity in spatial domain. Using the multi-fractal de-trended fluctuation analysis (MFDFA), the mismatch between the values of Hurst exponent and width of singularity spectrum by vertical and horizontal unfolding confirms the same.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04036",
        "title": "Characterizing driving behavior using automatic visual analysis",
        "authors": [
            "Mrinal Haloi",
            "Dinesh Babu Jayagopi"
        ],
        "abstract": "In this work, we present the problem of rash driving detection algorithm using a single wide angle camera sensor, particularly useful in the Indian context. To our knowledge this rash driving problem has not been addressed using Image processing techniques (existing works use other sensors such as accelerometer). Car Image processing literature, though rich and mature, does not address the rash driving problem. In this work-in-progress paper, we present the need to address this problem, our approach and our future plans to build a rash driving detector.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04065",
        "title": "Hybrid multi-layer Deep CNN/Aggregator feature for image classification",
        "authors": [
            "Praveen Kulkarni",
            "Joaquin Zepeda",
            "Frederic Jurie",
            "Patrick Perez",
            "Louis Chevallier"
        ],
        "abstract": "Deep Convolutional Neural Networks (DCNN) have established a remarkable performance benchmark in the field of image classification, displacing classical approaches based on hand-tailored aggregations of local descriptors. Yet DCNNs impose high computational burdens both at training and at testing time, and training them requires collecting and annotating large amounts of training data. Supervised adaptation methods have been proposed in the literature that partially re-learn a transferred DCNN structure from a new target dataset. Yet these require expensive bounding-box annotations and are still computationally expensive to learn. In this paper, we address these shortcomings of DCNN adaptation schemes by proposing a hybrid approach that combines conventional, unsupervised aggregators such as Bag-of-Words (BoW), with the DCNN pipeline by treating the output of intermediate layers as densely extracted local descriptors.\n",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04115",
        "title": "Sparse Code Formation with Linear Inhibition",
        "authors": [
            "Nam Do-Hoang Le"
        ],
        "abstract": "Sparse code formation in the primary visual cortex (V1) has been inspiration for many state-of-the-art visual recognition systems. To stimulate this behavior, networks are trained networks under mathematical constraint of sparsity or selectivity. In this paper, the authors exploit another approach which uses lateral interconnections in feature learning networks. However, instead of adding direct lateral interconnections among neurons, we introduce an inhibitory layer placed right after normal encoding layer. This idea overcomes the challenge of computational cost and complexity on lateral networks while preserving crucial objective of sparse code formation. To demonstrate this idea, we use sparse autoencoder as normal encoding layer and apply inhibitory layer. Early experiments in visual recognition show relative improvements over traditional approach on CIFAR-10 dataset. Moreover, simple installment and training process using Hebbian rule allow inhibitory layer to be integrated into existing networks, which enables further analysis in the future.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04144",
        "title": "Exploiting Image-trained CNN Architectures for Unconstrained Video Classification",
        "authors": [
            "Shengxin Zha",
            "Florian Luisier",
            "Walter Andrews",
            "Nitish Srivastava",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED'14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF-101 dataset.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04265",
        "title": "A Dictionary-based Approach for Estimating Shape and Spatially-Varying Reflectance",
        "authors": [
            "Zhuo Hui",
            "Aswin C. Sankaranarayanan"
        ],
        "abstract": "We present a technique for estimating the shape and reflectance of an object in terms of its surface normals and spatially-varying BRDF. We assume that multiple images of the object are obtained under fixed view-point and varying illumination, i.e, the setting of photometric stereo. Assuming that the BRDF at each pixel lies in the non-negative span of a known BRDF dictionary, we derive a per-pixel surface normal and BRDF estimation framework that requires neither iterative optimization techniques nor careful initialization, both of which are endemic to most state-of-the-art techniques. We showcase the performance of our technique on a wide range of simulated and real scenes where we outperform competing methods.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04267",
        "title": "LiSens --- A Scalable Architecture for Video Compressive Sensing",
        "authors": [
            "Jian Wang",
            "Mohit Gupta",
            "Aswin C. Sankaranarayanan"
        ],
        "abstract": "The measurement rate of cameras that take spatially multiplexed measurements by using spatial light modulators (SLM) is often limited by the switching speed of the SLMs. This is especially true for single-pixel cameras where the photodetector operates at a rate that is many orders-of-magnitude greater than the SLM. We study the factors that determine the measurement rate for such spatial multiplexing cameras (SMC) and show that increasing the number of pixels in the device improves the measurement rate, but there is an optimum number of pixels (typically, few thousands) beyond which the measurement rate does not increase. This motivates the design of LiSens, a novel imaging architecture, that replaces the photodetector in the single-pixel camera with a 1D linear array or a line-sensor. We illustrate the optical architecture underlying LiSens, build a prototype, and demonstrate results of a range of indoor and outdoor scenes. LiSens delivers on the promise of SMCs: imaging at a megapixel resolution, at video rate, using an inexpensive low-resolution sensor.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04444",
        "title": "Pattern Recognition of Bearing Faults using Smoother Statistical Features",
        "authors": [
            "Muhammad Masood Tahir",
            "Ayyaz Hussain"
        ],
        "abstract": "A pattern recognition (PR) based diagnostic scheme is presented to identify bearing faults, using time domain features. Vibration data is acquired from faulty bearings using a test rig. The features are extracted from the data, and processed prior to utilize in the PR process. The processing involves smoothing of feature distributions. This reduces the undesired impact of vibration randomness on the PR process, and thus enhances the diagnostic accuracy of the model.\n    ",
        "submission_date": "2015-03-15T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04598",
        "title": "PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and Multi-Illumination Images",
        "authors": [
            "Reza Sabzevari",
            "Vittori Murino",
            "Alessio Del Bue"
        ],
        "abstract": "In this paper, we address the problem of dense 3D reconstruction from multiple view images subject to strong lighting variations. In this regard, a new piecewise framework is proposed to explicitly take into account the change of illumination across several wide-baseline images. Unlike multi-view stereo and multi-view photometric stereo methods, this pipeline deals with wide-baseline images that are uncalibrated, in terms of both camera parameters and lighting conditions. Such a scenario is meant to avoid use of any specific imaging setup and provide a tool for normal users without any expertise. To the best of our knowledge, this paper presents the first work that deals with such unconstrained setting. We propose a coarse-to-fine approach, in which a coarse mesh is first created using a set of geometric constraints and, then, fine details are recovered by exploiting photometric properties of the scene. Augmenting the fine details on the coarse mesh is done via a final optimization step. Note that the method does not provide a generic solution for multi-view photometric stereo problem but it relaxes several common assumptions of this problem. The approach scales very well in size given its piecewise nature, dealing with large scale optimization and with severe missing data. Experiments on a benchmark dataset Robot data-set show the method performance against 3D ground truth.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04643",
        "title": "Template-based Monocular 3D Shape Recovery using Laplacian Meshes",
        "authors": [
            "Dat Tien Ngo",
            "Jonas Ostlund",
            "Pascal Fua"
        ],
        "abstract": "We show that by extending the Laplacian formalism, which was first introduced in the Graphics community to regularize 3D meshes, we can turn the monocular 3D shape reconstruction of a deformable surface given correspondences with a reference image into a much better-posed problem. This allows us to quickly and reliably eliminate outliers by simply solving a linear least squares problem. This yields an initial 3D shape estimate, which is not necessarily accurate, but whose 2D projections are. The initial shape is then refined by a constrained optimization problem to output the final surface reconstruction.\n",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04729",
        "title": "Skilled Impostor Attacks Against Fingerprint Verification Systems And Its Remedy",
        "authors": [
            "Carsten Gottschlich"
        ],
        "abstract": "Fingerprint verification systems are becoming ubiquitous in everyday life. This trend is propelled especially by the proliferation of mobile devices with fingerprint sensors such as smartphones and tablet computers, and fingerprint verification is increasingly applied for authenticating financial transactions. In this study we describe a novel attack vector against fingerprint verification systems which we coin skilled impostor attack. We show that existing protocols for performance evaluation of fingerprint verification systems are flawed and as a consequence of this, the system's real vulnerability is systematically underestimated. We examine a scenario in which a fingerprint verification system is tuned to operate at false acceptance rate of 0.1% using the traditional verification protocols with random impostors (zero-effort attacks). We demonstrate that an active and intelligent attacker can achieve a chance of success in the area of 89% or more against this system by performing skilled impostor attacks. We describe a new protocol for evaluating fingerprint verification performance in order to improve the assessment of potential and limitations of fingerprint recognition systems. This new evaluation protocol enables a more informed decision concerning the operating threshold in practical applications and the respective trade-off between security (low false acceptance rates) and usability (low false rejection rates). The skilled impostor attack is a general attack concept which is independent of specific databases or comparison algorithms. The proposed protocol relying on skilled impostor attacks can directly be applied for evaluating the verification performance of other biometric modalities such as e.g. iris, face, ear, finger vein, gait or speaker recognition.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04949",
        "title": "Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks",
        "authors": [
            "Varun Jampani",
            "Martin Kiefel",
            "Peter V. Gehler"
        ],
        "abstract": "Bilateral filters have wide spread use due to their edge-preserving properties. The common use case is to manually choose a parametric filter type, usually a Gaussian filter. In this paper, we will generalize the parametrization and in particular derive a gradient descent algorithm so the filter parameters can be learned from data. This derivation allows to learn high dimensional linear filters that operate in sparsely populated feature spaces. We build on the permutohedral lattice construction for efficient filtering. The ability to learn more general forms of high-dimensional filters can be used in several diverse applications. First, we demonstrate the use in applications where single filter applications are desired for runtime reasons. Further, we show how this algorithm can be used to learn the pairwise potentials in densely connected conditional random fields and apply these to different image segmentation tasks. Finally, we introduce layers of bilateral filters in CNNs and propose bilateral neural networks for the use of high-dimensional sparse data. This view provides new ways to encode model structure into network architectures. A diverse set of experiments empirically validates the usage of general forms of filters.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05038",
        "title": "3D Object Class Detection in the Wild",
        "authors": [
            "Bojan Pepik",
            "Michael Stark",
            "Peter Gehler",
            "Tobias Ritschel",
            "Bernt Schiele"
        ],
        "abstract": "Object class detection has been a synonym for 2D bounding box localization for the longest time, fueled by the success of powerful statistical learning techniques, combined with robust image representations. Only recently, there has been a growing interest in revisiting the promise of computer vision from the early days: to precisely delineate the contents of a visual scene, object by object, in 3D. In this paper, we draw from recent advances in object detection and 2D-3D object lifting in order to design an object class detector that is particularly tailored towards 3D object class detection. Our 3D object class detection method consists of several stages gradually enriching the object detection output with object viewpoint, keypoints and 3D shape estimates. Following careful design, in each stage it constantly improves the performance and achieves state-ofthe-art performance in simultaneous 2D bounding box and viewpoint estimation on the challenging Pascal3D+ dataset.\n    ",
        "submission_date": "2015-03-17T00:00:00",
        "last_modified_date": "2015-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05430",
        "title": "What Properties are Desirable from an Electron Microscopy Segmentation Algorithm",
        "authors": [
            "Toufiq Parag"
        ],
        "abstract": "The prospect of neural reconstruction from Electron Microscopy (EM) images has been elucidated by the automatic segmentation algorithms. Although segmentation algorithms eliminate the necessity of tracing the neurons by hand, significant manual effort is still essential for correcting the mistakes they make. A considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework. It is critically important to diminish the dependence on human interaction in the overall reconstruction system. This study proposes a novel classifier training algorithm for EM segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks. Instead of using an exhaustive pixel level groundtruth, an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels. Because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors, our algorithm is designed to prioritize minimization of false-merges over false-split mistakes. Our experiments on both 2D and 3D data suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05521",
        "title": "Nonparametric Detection of Nonlinearly Mixed Pixels and Endmember Estimation in Hyperspectral Images",
        "authors": [
            "Tales Imbiriba",
            "Jos\u00e9 Carlos Moreira Bermudez",
            "C\u00e9dric Richard",
            "Jean-Yves Tourneret"
        ],
        "abstract": "Mixing phenomena in hyperspectral images depend on a variety of factors such as the resolution of observation devices, the properties of materials, and how these materials interact with incident light in the scene. Different parametric and nonparametric models have been considered to address hyperspectral unmixing problems. The simplest one is the linear mixing model. Nevertheless, it has been recognized that mixing phenomena can also be nonlinear. The corresponding nonlinear analysis techniques are necessarily more challenging and complex than those employed for linear unmixing. Within this context, it makes sense to detect the nonlinearly mixed pixels in an image prior to its analysis, and then employ the simplest possible unmixing technique to analyze each pixel. In this paper, we propose a technique for detecting nonlinearly mixed pixels. The detection approach is based on the comparison of the reconstruction errors using both a Gaussian process regression model and a linear regression model. The two errors are combined into a detection statistics for which a probability density function can be reasonably approximated. We also propose an iterative endmember extraction algorithm to be employed in combination with the detection algorithm. The proposed Detect-then-Unmix strategy, which consists of extracting endmembers, detecting nonlinearly mixed pixels and unmixing, is tested with synthetic and real images.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05528",
        "title": "Video Inpainting of Complex Scenes",
        "authors": [
            "Alasdair Newson",
            "Andr\u00e9s Almansa",
            "Matthieu Fradet",
            "Yann Gousseau",
            "Patrick P\u00e9rez"
        ],
        "abstract": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction. Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video \" inpainting \". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are available, such as Photoshop's \" Content Aware fill \" [1]. However, while some impressive results have been obtained in the case of videos, the subject has been studied far less extensively than image inpainting. This relative lack of research can largely be attributed to high time complexity due to the added temporal dimension. Indeed, it has only very recently become possible to produce good quality inpainting results on high definition videos, and this only in a semi-automatic manner. Nevertheless, high-quality video inpainting has many important and useful applications such as film restoration, professional post-production in cinema and video editing for personal use. For this reason, we believe that an automatic, generic video inpainting algorithm would be extremely useful for both academic and professional communities.\n    ",
        "submission_date": "2015-03-18T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05689",
        "title": "Edge Detection: A Collection of Pixel based Approach for Colored Images",
        "authors": [
            "B.O Sadiq",
            "S.M Sani",
            "S Garba"
        ],
        "abstract": "The existing traditional edge detection algorithms process a single pixel on an image at a time, thereby calculating a value which shows the edge magnitude of the pixel and the edge orientation. Most of these existing algorithms convert the coloured images into gray scale before detection of edges. However, this process leads to inaccurate precision of recognized edges, thus producing false and broken edges in the image. This paper presents a profile modelling scheme for collection of pixels based on the step and ramp edges, with a view to reducing the false and broken edges present in the image. The collection of pixel scheme generated is used with the Vector Order Statistics to reduce the imprecision of recognized edges when converting from coloured to gray scale images. The Pratt Figure of Merit (PFOM) is used as a quantitative comparison between the existing traditional edge detection algorithm and the developed algorithm as a means of validation. The PFOM value obtained for the developed algorithm is 0.8480, which showed an improvement over the existing traditional edge detection algorithms.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05692",
        "title": "An approach to improving edge detection for facial and remotely sensed images using vector order statistics",
        "authors": [
            "B O. Sadiq",
            "S.M. Sani",
            "S. Garba"
        ],
        "abstract": "This paper presents an improved edge detection algorithm for facial and remotely sensed images using vector order statistics. The developed algorithm processes colored images directly without been converted to gray scale. A number of the existing algorithms converts the colored images into gray scale before detection of edges. But this process leads to inaccurate precision of recognized edges, thus producing false and broken edges in the output edge map. Facial and remotely sensed images consist of curved edge lines which have to be detected continuously to prevent broken edges. In order to deal with this, a collection of pixel approach is introduced with a view to minimizing the false and broken edges that exists in the generated output edge map of facial and remotely sensed images.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05767",
        "title": "Automatic Pollen Grain and Exine Segmentation from Microscope Images",
        "authors": [
            "Fran\u00e7ois Chung",
            "Tom\u00e1s Rodr\u00edguez"
        ],
        "abstract": "In this article, we propose an automatic method for the segmentation of pollen grains from microscope images, followed by the automatic segmentation of their exine. The objective of exine segmentation is to separate the pollen grain in two regions of interest: exine and inner part. A coarse-to-fine approach ensures a smooth and accurate segmentation of both structures. As a rough stage, grain segmentation is performed by a procedure involving clustering and morphological operations, while the exine is approximated by an iterative procedure consisting in consecutive cropping steps of the pollen grain. A snake-based segmentation is performed to refine the segmentation of both structures. Results have shown that our segmentation method is able to deal with different pollen types, as well as with different types of exine and inner part appearance. The proposed segmentation method aims to be generic and has been designed as one of the core steps of an automatic pollen classification framework.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05768",
        "title": "On learning optimized reaction diffusion processes for effective image restoration",
        "authors": [
            "Yunjin Chen",
            "Wei Yu",
            "Thomas Pock"
        ],
        "abstract": "For several decades, image restoration remains an active research topic in low-level computer vision and hence new approaches are constantly emerging. However, many recently proposed algorithms achieve state-of-the-art performance only at the expense of very high computation time, which clearly limits their practical relevance. In this work, we propose a simple but effective approach with both high computational efficiency and high restoration quality. We extend conventional nonlinear reaction diffusion models by several parametrized linear filters as well as several parametrized influence functions. We propose to train the parameters of the filters and the influence functions through a loss based approach. Experiments show that our trained nonlinear reaction diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for image restoration. Due to their structural simplicity, our trained models are highly efficient and are also well-suited for parallel computation on GPUs.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05782",
        "title": "Learning Hypergraph-regularized Attribute Predictors",
        "authors": [
            "Sheng Huang",
            "Mohamed Elhoseiny",
            "Ahmed Elgammal",
            "Dan Yang"
        ],
        "abstract": "We present a novel attribute learning framework named Hypergraph-based Attribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the attribute relations in the data. Then the attribute prediction problem is casted as a regularized hypergraph cut problem in which HAP jointly learns a collection of attribute projections from the feature space to a hypergraph embedding space aligned with the attribute space. The learned projections directly act as attribute classifiers (linear and kernelized). This formulation leads to a very efficient approach. By considering our model as a multi-graph cut task, our framework can flexibly incorporate other available information, in particular class label. We apply our approach to attribute prediction, Zero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUB databases demonstrate the value of our methods in comparison with the state-of-the-art approaches.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05786",
        "title": "A General Framework for Multi-focal Image Classification and Authentication: Application to Microscope Pollen Images",
        "authors": [
            "Fran\u00e7ois Chung",
            "Tom\u00e1s Rodr\u00edguez"
        ],
        "abstract": "In this article, we propose a general framework for multi-focal image classification and authentication, the methodology being demonstrated on microscope pollen images. The framework is meant to be generic and based on a brute force-like approach aimed to be efficient not only on any kind, and any number, of pollen images (regardless of the pollen type), but also on any kind of multi-focal images. All stages of the framework's pipeline are designed to be used in an automatic fashion. First, the optimal focus is selected using the absolute gradient method. Then, pollen grains are extracted using a coarse-to-fine approach involving both clustering and morphological techniques (coarse stage), and a snake-based segmentation (fine stage). Finally, features are extracted and selected using a generalized approach, and their classification is tested with four classifiers: Weighted Neighbor Distance, Neural Network, Decision Tree and Random Forest. The latter method, which has shown the best and more robust classification accuracy results (above 97\\% for any number of pollen types), is finally used for the authentication stage.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05830",
        "title": "Sign Language Fingerspelling Classification from Depth and Color Images using a Deep Belief Network",
        "authors": [
            "Lucas Rioux-Maldague",
            "Philippe Gigu\u00e8re"
        ],
        "abstract": "Automatic sign language recognition is an open problem that has received a lot of attention recently, not only because of its usefulness to signers, but also due to the numerous applications a sign classifier can have. In this article, we present a new feature extraction technique for hand pose recognition using depth and intensity images captured from a Microsoft Kinect sensor. We applied our technique to American Sign Language fingerspelling classification using a Deep Belief Network, for which our feature extraction technique is tailored. We evaluated our results on a multi-user data set with two scenarios: one with all known users and one with an unseen user. We achieved 99% recall and precision on the first, and 77% recall and 79% precision on the second. Our method is also capable of real-time sign classification and is adaptive to any environment or lightning intensity.\n    ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05860",
        "title": "Building Statistical Shape Spaces for 3D Human Modeling",
        "authors": [
            "Leonid Pishchulin",
            "Stefanie Wuhrer",
            "Thomas Helten",
            "Christian Theobalt",
            "Bernt Schiele"
        ],
        "abstract": "Statistical models of 3D human shape and pose learned from scan databases have developed into valuable tools to solve a variety of vision and graphics problems. Unfortunately, most publicly available models are of limited expressiveness as they were learned on very small databases that hardly reflect the true variety in human body shapes. In this paper, we contribute by rebuilding a widely used statistical body representation from the largest commercially available scan database, and making the resulting model available to the community (visit ",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2017-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06275",
        "title": "Skin Detection of Animation Characters",
        "authors": [
            "Kazi Tanvir Ahmed Siddiqui",
            "Abu Wasif"
        ],
        "abstract": "The increasing popularity of animes makes it vulnerable to unwanted usages like copyright violations and pornography. That is why, we need to develop a method to detect and recognize animation characters. Skin detection is one of the most important steps in this way. Though there are some methods to detect human skin color, but those methods do not work properly for anime characters. Anime skin varies greatly from human skin in color, texture, tone and in different kinds of lighting. They also vary greatly among themselves. Moreover, many other things (for example leather, shirt, hair etc.), which are not skin, can have color similar to skin. In this paper, we have proposed three methods that can identify an anime character skin more successfully as compared with Kovac, Swift, Saleh and Osman methods, which are primarily designed for human skin detection. Our methods are based on RGB values and their comparative relations.\n    ",
        "submission_date": "2015-03-21T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06323",
        "title": "Wavelet based approach for tissue fractal parameter measurement: Pre cancer detection",
        "authors": [
            "Sabyasachi Mukhopadhyay",
            "Nandan K. Das",
            "Soham Mandal",
            "Sawon Pratiher",
            "Asish Mitra",
            "Asima Pradhan",
            "Nirmalya Ghosh",
            "Prasanta K. Panigrahi"
        ],
        "abstract": "In this paper, we have carried out the detail studies of pre-cancer by wavelet coherency and multifractal based detrended fluctuation analysis (MFDFA) on differential interference contrast (DIC) images of stromal region among different grades of pre-cancer tissues. Discrete wavelet transform (DWT) through Daubechies basis has been performed for identifying fluctuations over polynomial trends for clear characterization and differentiation of tissues. Wavelet coherence plots are performed for identifying the level of correlation in time scale plane between normal and various grades of DIC samples. Applying MFDFA on refractive index variations of cervical tissues, we have observed that the values of Hurst exponent (correlation) decreases from healthy (normal) to pre-cancer tissues. The width of singularity spectrum has a sudden degradation at grade-I in comparison of healthy (normal) tissue but later on it increases as cancer progresses from grade-II to grade-III.\n    ",
        "submission_date": "2015-03-21T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06350",
        "title": "Boosting Convolutional Features for Robust Object Proposals",
        "authors": [
            "Nikolaos Karianakis",
            "Thomas J. Fuchs",
            "Stefano Soatto"
        ],
        "abstract": "Deep Convolutional Neural Networks (CNNs) have demonstrated excellent performance in image classification, but still show room for improvement in object-detection tasks with many categories, in particular for cluttered scenes and occlusion. Modern detection algorithms like Regions with CNNs (Girshick et al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions which with high probability represent objects, where in turn CNNs are deployed for classification. Selective Search represents a family of sophisticated algorithms that are engineered with multiple segmentation, appearance and saliency cues, typically coming with a significant run-time overhead. Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low reproducibility due to unstable superpixels, even for slight image perturbations. Although CNNs are subsequently used for classification in top-performing object-detection pipelines, current proposal methods are agnostic to how these models parse objects and their rich learned representations. As a result they may propose regions which may not resemble high-level objects or totally miss some of them. To overcome these drawbacks we propose a boosting approach which directly takes advantage of hierarchical CNN features for detecting regions of interest fast. We demonstrate its performance on ImageNet 2013 detection benchmark and compare it with state-of-the-art methods.\n    ",
        "submission_date": "2015-03-21T00:00:00",
        "last_modified_date": "2015-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06383",
        "title": "Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder",
        "authors": [
            "Angshul Majumdar"
        ],
        "abstract": "In this work we address the problem of real-time dynamic MRI reconstruction. There are a handful of studies on this topic; these techniques are either based on compressed sensing or employ Kalman Filtering. These techniques cannot achieve the reconstruction speed necessary for real-time reconstruction. In this work, we propose a new approach to MRI reconstruction. We learn a non-linear mapping from the unstructured aliased images to the corresponding clean images using a stacked denoising autoencoder (SDAE). The training for SDAE is slow, but the reconstruction is very fast - only requiring a few matrix vector multiplications. In this work, we have shown that using SDAE one can reconstruct the MRI frame faster than the data acquisition rate, thereby achieving real-time reconstruction. The quality of reconstruction is of the same order as a previous compressed sensing based online reconstruction technique.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2015-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06465",
        "title": "Lifting Object Detection Datasets into 3D",
        "authors": [
            "Joao Carreira",
            "Sara Vicente",
            "Lourdes Agapito",
            "Jorge Batista"
        ],
        "abstract": "While data has certainly taken the center stage in computer vision in recent years, it can still be difficult to obtain in certain scenarios. In particular, acquiring ground truth 3D shapes of objects pictured in 2D images remains a challenging feat and this has hampered progress in recognition-based object reconstruction from a single image. Here we propose to bypass previous solutions such as 3D scanning or manual design, that scale poorly, and instead populate object category detection datasets semi-automatically with dense, per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground truth figure-ground segmentations and (iii) a small set of keypoint annotations. Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion and then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions. The visual hull sampling process attempts to intersect an object's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points. We show that our method is able to produce convincing per-object 3D reconstructions and to accurately estimate cameras viewpoints on one of the most challenging existing object-category detection datasets, PASCAL VOC. We hope that our results will re-stimulate interest on joint object recognition and 3D reconstruction from a single image.\n    ",
        "submission_date": "2015-03-22T00:00:00",
        "last_modified_date": "2016-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06642",
        "title": "Superpixelizing Binary MRF for Image Labeling Problems",
        "authors": [
            "Junyan Wang",
            "Sai-Kit Yeung"
        ],
        "abstract": "Superpixels have become prevalent in computer vision. They have been used to achieve satisfactory performance at a significantly smaller computational cost for various tasks. People have also combined superpixels with Markov random field (MRF) models. However, it often takes additional effort to formulate MRF on superpixel-level, and to the best of our knowledge there exists no principled approach to obtain this formulation. In this paper, we show how generic pixel-level binary MRF model can be solved in the superpixel space. As the main contribution of this paper, we show that a superpixel-level MRF can be derived from the pixel-level MRF by substituting the superpixel representation of the pixelwise label into the original pixel-level MRF energy. The resultant superpixel-level MRF energy also remains submodular for a submodular pixel-level MRF. The derived formula hence gives us a handy way to formulate MRF energy in superpixel-level. In the experiments, we demonstrate the efficacy of our approach on several computer vision problems.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06643",
        "title": "A novel pLSA based Traffic Signs Classification System",
        "authors": [
            "Mrinal Haloi"
        ],
        "abstract": "In this work we developed a novel and fast traffic sign recognition system, a very important part for advanced driver assistance system and for autonomous driving. Traffic signs play a very vital role in safe driving and avoiding accident. We have used image processing and topic discovery model pLSA to tackle this challenging multiclass classification problem. Our algorithm is consist of two parts, shape classification and sign classification for improved accuracy. For processing and representation of image we have used bag of features model with SIFT local descriptor. Where a visual vocabulary of size 300 words are formed using k-means codebook formation algorithm. We exploited the concept that every image is a collection of visual topics and images having same topics will belong to same category. Our algorithm is tested on German traffic sign recognition benchmark (GTSRB) and gives very promising result near to existing state of the art techniques.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06648",
        "title": "Vehicle Local Position Estimation System",
        "authors": [
            "Mrinal Haloi",
            "Dinesh Babu Jayagopi"
        ],
        "abstract": "In this paper, a robust vehicle local position estimation with the help of single camera sensor and GPS is presented. A modified Inverse Perspective Mapping, illuminant Invariant techniques and object detection based approach is used to localize the vehicle in the road. Vehicles current lane, its position from road boundary and other cars are used to define its local position. For this purpose Lane markings are detected using a Laplacian edge feature, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space and illuminant invariant techniques. Lanes are assumed to be as parabolic model and fitted using robust RANSAC. This method can reliably detect all lanes of the road, estimate lane departure angle and local position of vehicle relative to lanes, road boundary and other cars. Different type of obstacle like pedestrians, vehicles are detected using HOG feature based deformable part model.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06680",
        "title": "Structural Similarity Index SSIMplified: Is there really a simpler concept at the heart of image quality measurement?",
        "authors": [
            "Kieran Gerard Larkin"
        ],
        "abstract": "The Structural Similarity Index (SSIM) is generally considered to be a milestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM's accepted development from the product of three heuristic factors continues to obscure it's real underlying simplicity. Starting instead from a symmetric-antisymmetric reformulation we first show SSIM to be a contrast or visibility function in the classic sense. Furthermore, the previously enigmatic structural covariance is revealed to be the difference of variances. The second step, eliminating the intrinsic quadratic nature of SSIM, allows a near linear correlation with human observer scores, and without invoking the usual, but arbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpreted in terms of perceptual masking: it is essentially equivalent to a normalised error or noise visibility function (NVF), and, furthermore, the NVF alone explains it success in modelling perceptual image quality. We use the term Dissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derived NVF. It seems that IQA researchers may now have two choices: 1) Continue to use the complex SSIM formula, but noting that SSIM only works coincidentally since the covariance term is actually the mean square error (MSE) in disguise. 2) Use the simplest of all perceptually-masked image quality metrics, namely NVF or DQ. On this choice Occam is clear: in the absence of differences in predictive ability, the fewer assumptions that are made, the better.\n    ",
        "submission_date": "2015-01-29T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06699",
        "title": "Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories",
        "authors": [
            "Zhengwu Zhang",
            "Jingyong Su",
            "Eric Klassen",
            "Huiling Le",
            "Anuj Srivastava"
        ],
        "abstract": "Statistical classification of actions in videos is mostly performed by extracting relevant features, particularly covariance features, from image frames and studying time series associated with temporal evolutions of these features. A natural mathematical representation of activity videos is in form of parameterized trajectories on the covariance manifold, i.e. the set of symmetric, positive-definite matrices (SPDMs). The variable execution-rates of actions implies variable parameterizations of the resulting trajectories, and complicates their classification. Since action classes are invariant to execution rates, one requires rate-invariant metrics for comparing trajectories. A recent paper represented trajectories using their transported square-root vector fields (TSRVFs), defined by parallel translating scaled-velocity vectors of trajectories to a reference tangent space on the manifold. To avoid arbitrariness of selecting the reference and to reduce distortion introduced during this mapping, we develop a purely intrinsic approach where SPDM trajectories are represented by redefining their TSRVFs at the starting points of the trajectories, and analyzed as elements of a vector bundle on the manifold. Using a natural Riemannain metric on vector bundles of SPDMs, we compute geodesic paths and geodesic distances between trajectories in the quotient space of this vector bundle, with respect to the re-parameterization group. This makes the resulting comparison of trajectories invariant to their re-parameterization. We demonstrate this framework on two applications involving video classification: visual speech recognition or lip-reading and hand-gesture recognition. In both cases we achieve results either comparable to or better than the current literature.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06813",
        "title": "Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation",
        "authors": [
            "Haopeng Zhang",
            "Tarek El-Gaaly",
            "Ahmed Elgammal",
            "Zhiguo Jiang"
        ],
        "abstract": "Due to large variations in shape, appearance, and viewing conditions, object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parameterized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we develop a novel framework to jointly solve the three challenging recognition sub-problems, by explicitly modeling the deformations of object manifolds and factorizing it in a view-invariant space for recognition. We perform extensive experiments on several challenging datasets and achieve state-of-the-art results.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06917",
        "title": "Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector",
        "authors": [
            "Qiang Zhang",
            "Yilin Wang",
            "Baoxin Li"
        ],
        "abstract": "Visual saliency, which predicts regions in the field of view that draw the most visual attention, has attracted a lot of interest from researchers. It has already been used in several vision tasks, e.g., image classification, object detection, foreground segmentation. Recently, the spectrum analysis based visual saliency approach has attracted a lot of interest due to its simplicity and good performance, where the phase information of the image is used to construct the saliency map. In this paper, we propose a new approach for detecting spatiotemporal visual saliency based on the phase spectrum of the videos, which is easy to implement and computationally efficient. With the proposed algorithm, we also study how the spatiotemporal saliency can be used in two important vision task, abnormality detection and spatiotemporal interest point detection. The proposed algorithm is evaluated on several commonly used datasets with comparison to the state-of-art methods from the literature. The experiments demonstrate the effectiveness of the proposed approach to spatiotemporal visual saliency detection and its application to the above vision tasks\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06959",
        "title": "Fast keypoint detection in video sequences",
        "authors": [
            "Luca Baroffio",
            "Matteo Cesana",
            "Alessandro Redondi",
            "Marco Tagliasacchi"
        ],
        "abstract": "A number of computer vision tasks exploit a succinct representation of the visual content in the form of sets of local features. Given an input image, feature extraction algorithms identify a set of keypoints and assign to each of them a description vector, based on the characteristics of the visual content surrounding the interest point. Several tasks might require local features to be extracted from a video sequence, on a frame-by-frame basis. Although temporal downsampling has been proven to be an effective solution for mobile augmented reality and visual search, high temporal resolution is a key requirement for time-critical applications such as object tracking, event recognition, pedestrian detection, surveillance. In recent years, more and more computationally efficient visual feature detectors and decriptors have been proposed. Nonetheless, such approaches are tailored to still images. In this paper we propose a fast keypoint detection algorithm for video sequences, that exploits the temporal coherence of the sequence of keypoints. According to the proposed method, each frame is preprocessed so as to identify the parts of the input frame for which keypoint detection and description need to be performed. Our experiments show that it is possible to achieve a reduction in computational time of up to 40%, without significantly affecting the task accuracy.\n    ",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07274",
        "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks",
        "authors": [
            "Elman Mansimov",
            "Nitish Srivastava",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "We propose a new way of incorporating temporal information present in videos into Spatial Convolutional Neural Networks (ConvNets) trained on images, that avoids training Spatio-Temporal ConvNets from scratch. We describe several initializations of weights in 3D Convolutional Layers of Spatio-Temporal ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it is important to initialize 3D Convolutional Weights judiciously in order to learn temporal representations of videos. We evaluate our methods on the UCF-101 dataset and demonstrate improvement over Spatial ConvNets.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07297",
        "title": "A Brief Survey of Recent Edge-Preserving Smoothing Algorithms on Digital Images",
        "authors": [
            "Chandrajit Pal",
            "Amlan Chakrabarti",
            "Ranjan Ghosh"
        ],
        "abstract": "Edge preserving filters preserve the edges and its information while blurring an image. In other words they are used to smooth an image, while reducing the edge blurring effects across the edge like halos, phantom etc. They are nonlinear in nature. Examples are bilateral filter, anisotropic diffusion filter, guided filter, trilateral filter etc. Hence these family of filters are very useful in reducing the noise in an image making it very demanding in computer vision and computational photography applications like denoising, video abstraction, demosaicing, optical-flow estimation, stereo matching, tone mapping, style transfer, relighting etc. This paper provides a concrete introduction to edge preserving filters starting from the heat diffusion equation in olden to recent eras, an overview of its numerous applications, as well as mathematical analysis, various efficient and optimized ways of implementation and their interrelationships, keeping focus on preserving the boundaries, spikes and canyons in presence of noise. Furthermore it provides a realistic notion for efficient implementation with a research scope for hardware realization for further acceleration.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07384",
        "title": "Compressed sensing MRI using masked DCT and DFT measurements",
        "authors": [
            "Elma Hot",
            "Petar Sekuli\u0107"
        ],
        "abstract": "This paper presents modification of the TwIST algorithm for Compressive Sensing MRI images reconstruction. Compressive Sensing is new approach in signal processing whose basic idea is recovering signal form small set of available samples. The application of the Compressive Sensing in biomedical imaging has found great importance. It allows significant lowering of the acquisition time, and therefore, save the patient from the negative impact of the MR apparatus. TwIST is commonly used algorithm for 2D signals reconstruction using Compressive Sensing principle. It is based on the Total Variation minimization. Standard version of the TwIST uses masked 2D Discrete Fourier Transform coefficients as Compressive Sensing measurements. In this paper, different masks and different transformation domains for coefficients selection are tested. Certain percent of the measurements is used from the mask, as well as small number of coefficients outside the mask. Comparative analysis using 2D DFT and 2D DCT coefficients, with different mask shapes is performed. The theory is proved with experimental results.\n    ",
        "submission_date": "2015-03-25T00:00:00",
        "last_modified_date": "2015-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07460",
        "title": "RANSAC based three points algorithm for ellipse fitting of spherical object's projection",
        "authors": [
            "Shenghui Xu"
        ],
        "abstract": "As the spherical object can be seen everywhere, we should extract the ellipse image accurately and fit it by implicit algebraic curve in order to finish the 3D reconstruction. In this paper, we propose a new ellipse fitting algorithm which only needs three points to fit the projection of spherical object and is different from the traditional algorithms that need at least five point. The fitting procedure is just similar as the estimation of Fundamental Matrix estimation by seven points, and the RANSAC algorithm has also been used to exclude the interference of noise and scattered points.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07697",
        "title": "Robust Eye Centers Localization with Zero--Crossing Encoded Image Projections",
        "authors": [
            "Laura Florea",
            "Corneliu Florea",
            "Constantin Vertan"
        ],
        "abstract": "This paper proposes a new framework for the eye centers localization by the joint use of encoding of normalized image projections and a Multi Layer Perceptron (MLP) classifier. The encoding is novel and it consists in identifying the zero-crossings and extracting the relevant parameters from the resulting modes. The compressed normalized projections produce feature descriptors that are inputs to a properly-trained MLP, for discriminating among various categories of image regions. The proposed framework forms a fast and reliable system for the eye centers localization, especially in the context of face expression analysis in unconstrained environments. We successfully test the proposed method on a wide variety of databases including BioID, Cohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07706",
        "title": "Pain Intensity Estimation by a Self--Taught Selection of Histograms of Topographical Features",
        "authors": [
            "Corneliu Florea",
            "Laura Florea",
            "Raluca Boia",
            "Alessandra Bandrabur",
            "Constantin Vertan"
        ],
        "abstract": "Pain assessment through observational pain scales is necessary for special categories of patients such as neonates, patients with dementia, critically ill patients, etc. The recently introduced Prkachin-Solomon score allows pain assessment directly from facial images opening the path for multiple assistive applications. In this paper, we introduce the Histograms of Topographical (HoT) features, which are a generalization of the topographical primal sketch, for the description of the face parts contributing to the mentioned score. We propose a semi-supervised, clustering oriented self--taught learning procedure developed on the emotion oriented Cohn-Kanade database. We use this procedure to improve the discrimination between different pain intensity levels and the generalization with respect to the monitored persons, while testing on the UNBC McMaster Shoulder Pain database.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07783",
        "title": "Towards Learning free Naive Bayes Nearest Neighbor-based Domain Adaptation",
        "authors": [
            "Faraz Saeedan",
            "Barbara Caputo"
        ],
        "abstract": "As of today, object categorization algorithms are not able to achieve the level of robustness and generality necessary to work reliably in the real world. Even the most powerful convolutional neural network we can train fails to perform satisfactorily when trained and tested on data from different databases. This issue, known as domain adaptation and/or dataset bias in the literature, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. Recent work showed that by casting the problem into the image-to-class recognition framework, the domain adaptation problem is significantly alleviated \\cite{danbnn}. Here we follow this approach, and show how a very simple, learning free Naive Bayes Nearest Neighbor (NBNN)-based domain adaptation algorithm can significantly alleviate the distribution mismatch among source and target data, especially when the number of classes and the number of sources grow. Experiments on standard benchmarks used in the literature show that our approach (a) is competitive with the current state of the art on small scale problems, and (b) achieves the current state of the art as the number of classes and sources grows, with minimal computational requirements.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07816",
        "title": "Content-Based Bird Retrieval using Shape context, Color moments and Bag of Features",
        "authors": [
            "Bahri Abdelkhalak",
            "Hamid Zouaki"
        ],
        "abstract": "In this paper we propose a new descriptor for birds search. First, our work was carried on the choice of a descriptor. This choice is usually driven by the application requirements such as robustness to noise, stability with respect to bias, the invariance to geometrical transformations or tolerance to occlusions. In this context, we introduce a descriptor which combines the shape and color descriptors to have an effectiveness description of birds. The proposed descriptor is an adaptation of a descriptor based on the contours defined in article Belongie et al. [5] combined with color moments [19]. Specifically, points of interest are extracted from each image and information's in the region in the vicinity of these points are represented by descriptors of shape context concatenated with color moments. Thus, the approach bag of visual words is applied to the latter. The experimental results show the effectiveness of our descriptor for the bird search by content.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-03-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07989",
        "title": "Discriminative Bayesian Dictionary Learning for Classification",
        "authors": [
            "Naveed Akhtar",
            "Faisal Shafait",
            "Ajmal Mian"
        ],
        "abstract": "We propose a Bayesian approach to learn discriminative dictionaries for sparse representation of data. The proposed approach infers probability distributions over the atoms of a discriminative dictionary using a Beta Process. It also computes sets of Bernoulli distributions that associate class labels to the learned dictionary atoms. This association signifies the selection probabilities of the dictionary atoms in the expansion of class-specific data. Furthermore, the non-parametric character of the proposed approach allows it to infer the correct size of the dictionary. We exploit the aforementioned Bernoulli distributions in separately learning a linear classifier. The classifier uses the same hierarchical Bayesian model as the dictionary, which we present along the analytical inference solution for Gibbs sampling. For classification, a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier. We performed experiments for face and action recognition; and object and scene-category classification using five public datasets and compared the results with state-of-the-art discriminative sparse representation approaches. Experiments show that the proposed Bayesian approach consistently outperforms the existing approaches.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08223",
        "title": "A System View of the Recognition and Interpretation of Observed Human Shape, Pose and Action",
        "authors": [
            "David W. Arathorn"
        ],
        "abstract": "There is physiological evidence that our ability to interpret human pose and action from 2D visual imagery (binocular or monocular) engages the circuitry of the motor cortices as well as the visual areas of the brain. This implies that the capability of the motor cortices to solve inverse kinematics is flexible enough to apply to both motion planning as well as serving as a generative model for the visual processing of human figures, despite the differing functional requirements of the two tasks. This paper provides a computational model of the cooperation between visual and motor areas: in other words, a system view of an important class of brain computations. The model unifies the solution of the separate inverse problems involved in the task, visual transformation discovery, inverse kinematics, and adaptation to morphology variations, using several instances of the Map-seeking Circuit algorithm. While the paper is weighted toward the exposition of a neurobiological hypothesis, from mathematical formalization of the problem to neuronal circuitry, the algorithmic expression of the solution is also a functional machine vision system for human figure recognition, and 3D pose and body morphology reconstruction from monocular, perspective-less input imagery. With an inverse kinematic generative model capable of imposing a variety of endogenous and exogenous constraints the machine vision implementation acquires characteristics currently unique among such systems.\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08263",
        "title": "CRF Learning with CNN Features for Image Segmentation",
        "authors": [
            "Fayao Liu",
            "Guosheng Lin",
            "Chunhua Shen"
        ],
        "abstract": "Conditional Random Rields (CRF) have been widely applied in image segmentations. While most studies rely on hand-crafted features, we here propose to exploit a pre-trained large convolutional neural network (CNN) to generate deep features for CRF learning. The deep CNN is trained on the ImageNet dataset and transferred to image segmentations here for constructing potentials of superpixels. Then the CRF parameters are learnt using a structured support vector machine (SSVM). To fully exploit context information in inference, we construct spatially related co-occurrence pairwise potentials and incorporate them into the energy function. This prefers labelling of object pairs that frequently co-occur in a certain spatial layout and at the same time avoids implausible labellings during the inference. Extensive experiments on binary and multi-class segmentation benchmarks demonstrate the promise of the proposed method. We thus provide new baselines for the segmentation performance on the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC 2011 datasets.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2015-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08596",
        "title": "Fast Optimal Transport Averaging of Neuroimaging Data",
        "authors": [
            "Alexandre Gramfort",
            "Gabriel Peyr\u00e9",
            "Marco Cuturi"
        ],
        "abstract": "Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before group linear averaging. In this work we build on ideas originally introduced by Kantorovich to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of an entropic smoothing approach. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08663",
        "title": "Visual Saliency Based on Multiscale Deep Features",
        "authors": [
            "Guanbin Li",
            "Yizhou Yu"
        ],
        "abstract": "Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this CVPR 2015 paper, we discover that a high-quality visual saliency model can be trained with multiscale features extracted using a popular deep learning architecture, convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotation. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08677",
        "title": "Label-Embedding for Image Classification",
        "authors": [
            "Zeynep Akata",
            "Florent Perronnin",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08843",
        "title": "Globally Tuned Cascade Pose Regression via Back Propagation with Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images",
        "authors": [
            "Peng Sun",
            "James K. Min",
            "Guanglei Xiong"
        ],
        "abstract": "Recently, a successful pose estimation algorithm, called Cascade Pose Regression (CPR), was proposed in the literature. Trained over Pose Index Feature, CPR is a regressor ensemble that is similar to Boosting. In this paper we show how CPR can be represented as a Neural Network. Specifically, we adopt a Graph Transformer Network (GTN) representation and accordingly train CPR with Back Propagation (BP) that permits globally tuning. In contrast, previous CPR literature only took a layer wise training without any post fine tuning. We empirically show that global training with BP outperforms layer-wise (pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor, which utilized sparse connection to learn local image feature representation. We tested the proposed CPR-GTN on 2D face pose estimation problem as in previous CPR literature. Besides, we also investigated the possibility of extending CPR-GTN to 3D pose estimation by doing experiments using 3D Computed Tomography dataset for heart segmentation.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08853",
        "title": "Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations",
        "authors": [
            "Ali Borji",
            "James Tanner"
        ],
        "abstract": "Predicting where people look in natural scenes has attracted a lot of interest in computer vision and computational neuroscience over the past two decades. Two seemingly contrasting categories of cues have been proposed to influence where people look: \\textit{low-level image saliency} and \\textit{high-level semantic information}. Our first contribution is to take a detailed look at these cues to confirm the hypothesis proposed by Henderson~\\cite{henderson1993eye} and Nuthmann \\& Henderson~\\cite{nuthmann2010object} that observers tend to look at the center of objects. We analyzed fixation data for scene free-viewing over 17 observers on 60 fully annotated images with various types of objects. Images contained different types of scenes, such as natural scenes, line drawings, and 3D rendered scenes. Our second contribution is to propose a simple combined model of low-level saliency and object center-bias that outperforms each individual component significantly over our data, as well as on the OSIE dataset by Xu et al.~\\cite{xu2014predicting}. The results reconcile saliency with object center-bias hypotheses and highlight that both types of cues are important in guiding fixations. Our work opens new directions to understand strategies that humans use in observing scenes and objects, and demonstrates the construction of combined models of low-level saliency and high-level object-based information.\n    ",
        "submission_date": "2015-03-30T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08909",
        "title": "Beyond Short Snippets: Deep Networks for Video Classification",
        "authors": [
            "Joe Yue-Hei Ng",
            "Matthew Hausknecht",
            "Sudheendra Vijayanarasimhan",
            "Oriol Vinyals",
            "Rajat Monga",
            "George Toderici"
        ],
        "abstract": "Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00028",
        "title": "Real-World Font Recognition Using Deep Network and Domain Adaptation",
        "authors": [
            "Zhangyang Wang",
            "Jianchao Yang",
            "Hailin Jin",
            "Eli Shechtman",
            "Aseem Agarwala",
            "Jonathan Brandt",
            "Thomas S. Huang"
        ],
        "abstract": "We address a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Networks, and use an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a real-world dataset.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00045",
        "title": "Weakly Supervised Learning of Objects, Attributes and their Associations",
        "authors": [
            "Zhiyuan Shi",
            "Yongxin Yang",
            "Timothy M. Hospedales",
            "Tao Xiang"
        ],
        "abstract": "When humans describe images they tend to use combinations of nouns and adjectives, corresponding to objects and their associated attributes respectively. To generate such a description automatically, one needs to model objects, attributes and their associations. Conventional methods require strong annotation of object and attribute locations, making them less scalable. In this paper, we model object-attribute associations from weakly labelled images, such as those widely available on media sharing sites (e.g. Flickr), where only image-level labels (either object or attributes) are given, without their locations and associations. This is achieved by introducing a novel weakly supervised non-parametric Bayesian model. Once learned, given a new image, our model can describe the image, including objects, attributes and their associations, as well as their locations and segmentation. Extensive experiments on benchmark datasets demonstrate that our weakly supervised model performs at par with strongly supervised models on tasks such as image description and retrieval based on object-attribute associations.\n    ",
        "submission_date": "2015-03-31T00:00:00",
        "last_modified_date": "2015-03-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00325",
        "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
        "authors": [
            "Xinlei Chen",
            "Hao Fang",
            "Tsung-Yi Lin",
            "Ramakrishna Vedantam",
            "Saurabh Gupta",
            "Piotr Dollar",
            "C. Lawrence Zitnick"
        ],
        "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.\n    ",
        "submission_date": "2015-04-01T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00976",
        "title": "Convex Denoising using Non-Convex Tight Frame Regularization",
        "authors": [
            "Ankit Parekh",
            "Ivan W. Selesnick"
        ],
        "abstract": "This paper considers the problem of signal denoising using a sparse tight-frame analysis prior. The L1 norm has been extensively used as a regularizer to promote sparsity; however, it tends to under-estimate non-zero values of the underlying signal. To more accurately estimate non-zero values, we propose the use of a non-convex regularizer, chosen so as to ensure convexity of the objective function. The convexity of the objective function is ensured by constraining the parameter of the non-convex penalty. We use ADMM to obtain a solution and show how to guarantee that ADMM converges to the global optimum of the objective function. We illustrate the proposed method for 1D and 2D signal denoising.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00983",
        "title": "Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images",
        "authors": [
            "Chen Sun",
            "Sanketh Shetty",
            "Rahul Sukthankar",
            "Ram Nevatia"
        ],
        "abstract": "We address the problem of fine-grained action localization from temporally untrimmed web videos. We assume that only weak video-level annotations are available for training. The goal is to use these weak labels to identify temporal segments corresponding to the actions, and learn models that generalize to unconstrained web videos. We find that web images queried by action names serve as well-localized highlights for many actions, but are noisily labeled. To solve this problem, we propose a simple yet effective method that takes weak video labels and noisy image labels as input, and generates localized action frames as output. This is achieved by cross-domain transfer between video frames and web images, using pre-trained deep convolutional neural networks. We then use the localized action frames to train action recognition models with long short-term memory networks. We collect a fine-grained sports action data set FGA-240 of more than 130,000 YouTube videos. It has 240 fine-grained actions under 85 sports activities. Convincing results are shown on the FGA-240 data set, as well as the THUMOS 2014 localization data set with untrimmed training videos.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01013",
        "title": "Efficient piecewise training of deep structured models for semantic segmentation",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Anton van dan Hengel",
            "Ian Reid"
        ],
        "abstract": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore `patch-patch' context between image regions, and `patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01052",
        "title": "Fast algorithms for morphological operations using run-length encoded binary images",
        "authors": [
            "Gregor Ehrensperger",
            "Alexander Ostermann",
            "Felix Schwitzer"
        ],
        "abstract": "This paper presents innovative algorithms to efficiently compute erosions and dilations of run-length encoded (RLE) binary images with arbitrary shaped structuring elements. An RLE image is given by a set of runs, where a run is a horizontal concatenation of foreground pixels. The proposed algorithms extract the skeleton of the structuring element and build distance tables of the input image, which are storing the distance to the next background pixel on the left and right hand sides. This information is then used to speed up the calculations of the erosion and dilation operator by enabling the use of techniques which allow to skip the analysis of certain pixels whenever a hit or miss occurs. Additionally the input image gets trimmed during the preprocessing steps on the base of two primitive criteria. Experimental results show the advantages over other algorithms. The source code of our algorithms is available in C++.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2015-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01124",
        "title": "Discriminative and Efficient Label Propagation on Complementary Graphs for Multi-Object Tracking",
        "authors": [
            "Amit Kumar K.C.",
            "Laurent Jacques",
            "Christophe De Vleeschouwer"
        ],
        "abstract": "Given a set of detections, detected at each time instant independently, we investigate how to associate them across time. This is done by propagating labels on a set of graphs, each graph capturing how either the spatio-temporal or the appearance cues promote the assignment of identical or distinct labels to a pair of detections. The graph construction is motivated by a locally linear embedding of the detection features. Interestingly, the neighborhood of a node in appearance graph is defined to include all the nodes for which the appearance feature is available (even if they are temporally distant). This gives our framework the uncommon ability to exploit the appearance features that are available only sporadically. Once the graphs have been defined, multi-object tracking is formulated as the problem of finding a label assignment that is consistent with the constraints captured each graph, which results into a difference of convex (DC) program. We propose to decompose the global objective function into node-wise sub-problems. This not only allows a computationally efficient solution, but also supports an incremental and scalable construction of the graph, thereby making the framework applicable to large graphs and practical tracking scenarios. Moreover, it opens the possibility of parallel implementation.\n    ",
        "submission_date": "2015-04-05T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01220",
        "title": "Matching-CNN Meets KNN: Quasi-Parametric Human Parsing",
        "authors": [
            "Si Liu",
            "Xiaodan Liang",
            "Luoqi Liu",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Changsheng Xu",
            "Liang Lin",
            "Xiaochun Cao",
            "Shuicheng Yan"
        ],
        "abstract": "Both parametric and non-parametric approaches have demonstrated encouraging performances in the human parsing task, namely segmenting a human image into several semantic regions (e.g., hat, bag, left arm, face). In this work, we aim to develop a new solution with the advantages of both methodologies, namely supervision from annotated data and the flexibility to use newly annotated (possibly uncommon) images, and present a quasi-parametric human parsing model. Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict the matching confidence and displacements of the best matched region in the testing image for a particular semantic region in one KNN image. Given a testing image, we first retrieve its KNN images from the annotated/manually-parsed human image corpus. Then each semantic region in each KNN image is matched with confidence to the testing image using M-CNN, and the matched regions from all KNN images are further fused, followed by a superpixel smoothing procedure to obtain the ultimate human parsing result. The M-CNN differs from the classic CNN in that the tailored cross image matching filters are introduced to characterize the matching between the testing image and the semantic region of a KNN image. The cross image matching filters are defined at different convolutional layers, each aiming to capture a particular range of displacements. Comprehensive evaluations over a large dataset with 7,700 annotated human images well demonstrate the significant performance gain from the quasi-parametric model over the state-of-the-arts, for the human parsing task.\n    ",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01420",
        "title": "Knowledge driven Offline to Online Script Conversion",
        "authors": [
            "Sunil Kopparapu",
            "Devanuj",
            "Akhilesh Srivastava",
            "P.V.S. Rao"
        ],
        "abstract": "The problem of offline to online script conversion is a challenging and an ill-posed problem. The interest in offline to online conversion exists because there are a plethora of robust algorithms in online script literature which can not be used on offline scripts. In this paper, we propose a method, based on heuristics, to extract online script information from offline bitmap image. We show the performance of the proposed method on a real sample signature offline image, whose online information is known.\n    ",
        "submission_date": "2015-04-06T00:00:00",
        "last_modified_date": "2015-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01441",
        "title": "Locally Non-rigid Registration for Mobile HDR Photography",
        "authors": [
            "Orazio Gallo",
            "Alejandro Troccoli",
            "Jun Hu",
            "Kari Pulli",
            "Jan Kautz"
        ],
        "abstract": "Image registration for stack-based HDR photography is challenging. If not properly accounted for, camera motion and scene changes result in artifacts in the composite image. Unfortunately, existing methods to address this problem are either accurate, but too slow for mobile devices, or fast, but prone to failing. We propose a method that fills this void: our approach is extremely fast---under 700ms on a commercial tablet for a pair of 5MP images---and prevents the artifacts that arise from insufficient registration quality.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01476",
        "title": "Mobile Phone Based Vehicle License Plate Recognition for Road Policing",
        "authors": [
            "Lajish V. L.",
            "Sunil Kumar Kopparapu"
        ],
        "abstract": "Identity of a vehicle is done through the vehicle license plate by traffic police in general. Au- tomatic vehicle license plate recognition has several applications in intelligent traffic management systems. The security situation across the globe and particularly in India demands a need to equip the traffic police with a system that enables them to get instant details of a vehicle. The system should be easy to use, should be mobile, and work 24 x 7. In this paper, we describe a mobile phone based, client-server architected, license plate recognition system. While we use the state of the art image processing and pattern recognition algorithms tuned for Indian conditions to automatically recognize non-uniform license plates, the main contribution is in creating an end to end usable solution. The client application runs on a mobile device and a server application, with access to vehicle information database, is hosted centrally. The solution enables capture of license plate image captured by the phone camera and passes to the server; on the server the license plate number is recognized; the data associated with the number plate is then sent back to the mobile device, instantaneously. We describe the end to end system architecture in detail. A working prototype of the proposed system has been implemented in the lab environment.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01488",
        "title": "On-line Handwritten Devanagari Character Recognition using Fuzzy Directional Features",
        "authors": [
            "Sunil Kumar Kopparapu",
            "Lajish VL"
        ],
        "abstract": "This paper describes a new feature set for use in the recognition of on-line handwritten Devanagari script based on Fuzzy Directional Features. Experiments are conducted for the automatic recognition of isolated handwritten character primitives (sub-character units). Initially we describe the proposed feature set, called the Fuzzy Directional Features (FDF) and then show how these features can be effectively utilized for writer independent character recognition. Experimental results show that FDF set perform well for writer independent data set at stroke level recognition. The main contribution of this paper is the introduction of a novel feature set and establish experimentally its ability in recognition of handwritten Devanagari script.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01492",
        "title": "Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition",
        "authors": [
            "Peng Wang",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Conditional Random Fields (CRF) have been widely used in a variety of computer vision tasks. Conventional CRFs typically define edges on neighboring image pixels, resulting in a sparse graph such that efficient inference can be performed. However, these CRFs fail to model long-range contextual relationships. Fully-connected CRFs have thus been proposed. While there are efficient approximate inference methods for such CRFs, usually they are sensitive to initialization and make strong assumptions. In this work, we develop an efficient, yet general algorithm for inference on fully-connected CRFs. The algorithm is based on a scalable SDP algorithm and the low- rank approximation of the similarity/kernel matrix. The core of the proposed algorithm is a tailored quasi-Newton method that takes advantage of the low-rank matrix approximation when solving the specialized SDP dual problem. Experiments demonstrate that our method can be applied on fully-connected CRFs that cannot be solved previously, such as pixel-level image co-segmentation.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01502",
        "title": "Separable time-causal and time-recursive spatio-temporal receptive fields",
        "authors": [
            "Tony Lindeberg"
        ],
        "abstract": "We present an improved model and theory for time-causal and time-recursive spatio-temporal receptive fields, obtained by a combination of Gaussian receptive fields over the spatial domain and first-order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain. Compared to previous spatio-temporal scale-space formulations in terms of non-enhancement of local extrema or scale invariance, these receptive fields are based on different scale-space axiomatics over time by ensuring non-creation of new local extrema or zero-crossings with increasing temporal scale. Specifically, extensions are presented about parameterizing the intermediate temporal scale levels, analysing the resulting temporal dynamics and transferring the theory to a discrete implementation in terms of recursive filters over time.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01515",
        "title": "Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing",
        "authors": [
            "Paris Giampouras",
            "Konstantinos Themelis",
            "Athanasios Rontogiannis",
            "Konstantinos Koutroumbas"
        ],
        "abstract": "In a plethora of applications dealing with inverse problems, e.g. in image processing, social networks, compressive sensing, biological data processing etc., the signal of interest is known to be structured in several ways at the same time. This premise has recently guided the research to the innovative and meaningful idea of imposing multiple constraints on the parameters involved in the problem under study. For instance, when dealing with problems whose parameters form sparse and low-rank matrices, the adoption of suitably combined constraints imposing sparsity and low-rankness, is expected to yield substantially enhanced estimation results. In this paper, we address the spectral unmixing problem in hyperspectral images. Specifically, two novel unmixing algorithms are introduced, in an attempt to exploit both spatial correlation and sparse representation of pixels lying in homogeneous regions of hyperspectral images. To this end, a novel convex mixed penalty term is first defined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window. This penalty term is then used to regularize a conventional quadratic cost function and impose simultaneously sparsity and row-rankness on the abundance matrix. The resulting regularized cost function is minimized by a) an incremental proximal sparse and low-rank unmixing algorithm and b) an algorithm based on the alternating minimization method of multipliers (ADMM). The effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01561",
        "title": "Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification",
        "authors": [
            "Zuxuan Wu",
            "Xi Wang",
            "Yu-Gang Jiang",
            "Hao Ye",
            "Xiangyang Xue"
        ],
        "abstract": "Classifying videos according to content semantics is an important problem with a wide range of applications. In this paper, we propose a hybrid deep learning framework for video classification, which is able to model static spatial information, short-term motion, as well as long-term temporal clues in the videos. Specifically, the spatial and the short-term motion features are extracted separately by two Convolutional Neural Networks (CNN). These two types of CNN-based features are then combined in a regularized feature fusion network for classification, which is able to learn and utilize feature relationships for improved performance. In addition, Long Short Term Memory (LSTM) networks are applied on top of the two features to further model longer-term temporal clues. The main contribution of this work is the hybrid learning framework that can model several important aspects of the video data. We also show that (1) combining the spatial and the short-term motion features in the regularized fusion network is better than direct classification and fusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is highly complementary to the traditional classification strategy without considering the temporal frame orders. Extensive experiments are conducted on two popular and challenging benchmarks, the UCF-101 Human Actions and the Columbia Consumer Videos (CCV). On both benchmarks, our framework achieves to-date the best reported performance: $91.3\\%$ on the UCF-101 and $83.5\\%$ on the CCV.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01639",
        "title": "Ego-Object Discovery",
        "authors": [
            "Marc Bola\u00f1os",
            "Petia Radeva"
        ],
        "abstract": "Lifelogging devices are spreading faster everyday. This growth can represent great benefits to develop methods for extraction of meaningful information about the user wearing the device and his/her environment. In this paper, we propose a semi-supervised strategy for easily discovering objects relevant to the person wearing a first-person camera. Given an egocentric video/images sequence acquired by the camera, our algorithm uses both the appearance extracted by means of a convolutional neural network and an object refill methodology that allows to discover objects even in case of small amount of object appearance in the collection of images. An SVM filtering strategy is applied to deal with the great part of the False Positive object candidates found by most of the state of the art object detectors. We validate our method on a new egocentric dataset of 4912 daily images acquired by 4 persons as well as on both PASCAL 2012 and MSRC datasets. We obtain for all of them results that largely outperform the state of the art approach. We make public both the EDUB dataset and the algorithm code.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01753",
        "title": "Design and Implementation of a 3D Undersea Camera System",
        "authors": [
            "Xida Chen",
            "Steve Sutphen",
            "Paul Macoun",
            "Yee-Hong Yang"
        ],
        "abstract": "In this paper, we present the design and development of an undersea camera system. The goal of our system is to provide a 3D model of the undersea habitat in a long-term continuous manner. The most important feature of our system is the use of multiple cameras and multiple projectors, which is able to provide accurate 3D models with an accuracy of a millimeter. By introducing projectors in our system, we can use many different structured light methods for different tasks. There are two main advantages comparing our system with using ROVs or AUVs. First, our system can provide continuous monitoring of the undersea habitat. Second, our system has a low hardware cost. Comparing to existing deployed camera systems, the advantage of our system is that it can provide accurate 3D models and provides opportunities for future development of innovative algorithms for undersea research.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01777",
        "title": "Heterogeneous Tensor Decomposition for Clustering via Manifold Optimization",
        "authors": [
            "Yanfeng Sun",
            "Junbin Gao",
            "Xia Hong",
            "Bamdev Mishra",
            "Baocai Yin"
        ],
        "abstract": "Tensors or multiarray data are generalizations of matrices. Tensor clustering has become a very important research topic due to the intrinsically rich structures in real-world multiarray datasets. Subspace clustering based on vectorizing multiarray data has been extensively researched. However, vectorization of tensorial data does not exploit complete structure information. In this paper, we propose a subspace clustering algorithm without adopting any vectorization process. Our approach is based on a novel heterogeneous Tucker decomposition model. In contrast to existing techniques, we propose a new clustering algorithm that alternates between different modes of the proposed heterogeneous tensor model. All but the last mode have closed-form updates. Updating the last mode reduces to optimizing over the so-called multinomial manifold, for which we investigate second order Riemannian geometry and propose a trust-region algorithm. Numerical experiments show that our proposed algorithm compete effectively with state-of-the-art clustering algorithms that are based on tensor factorization.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01800",
        "title": "A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor Images",
        "authors": [
            "Mohammed Khader",
            "A. Ben Hamza"
        ],
        "abstract": "We propose a nonrigid registration approach for diffusion tensor images using a multicomponent information-theoretic measure. Explicit orientation optimization is enabled by incorporating tensor reorientation, which is necessary for wrapping diffusion tensor images. Experimental results on diffusion tensor images indicate the feasibility of the proposed approach and a much better performance compared to the affine registration method based on mutual information in terms of registration accuracy in the presence of geometric distortion.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01806",
        "title": "Kernelized Low Rank Representation on Grassmann Manifolds",
        "authors": [
            "Boyue Wang",
            "Yongli Hu",
            "Junbin Gao",
            "Yanfeng Sun",
            "Baocai Yin"
        ],
        "abstract": "Low rank representation (LRR) has recently attracted great interest due to its pleasing efficacy in exploring low-dimensional subspace structures embedded in data. One of its successful applications is subspace clustering which means data are clustered according to the subspaces they belong to. In this paper, at a higher level, we intend to cluster subspaces into classes of subspaces. This is naturally described as a clustering problem on Grassmann manifold. The novelty of this paper is to generalize LRR on Euclidean space onto an LRR model on Grassmann manifold in a uniform kernelized framework. The new methods have many applications in computer vision tasks. Several clustering experiments are conducted on handwritten digit images, dynamic textures, human face clips and traffic scene sequences. The experimental results show that the proposed methods outperform a number of state-of-the-art subspace clustering methods.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01807",
        "title": "Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective",
        "authors": [
            "Boyue Wang",
            "Yongli Hu",
            "Junbin Gao",
            "Yanfeng Sun",
            "Baocai Yin"
        ],
        "abstract": "Many computer vision algorithms employ subspace models to represent data. The Low-rank representation (LRR) has been successfully applied in subspace clustering for which data are clustered according to their subspace structures. The possibility of extending LRR on Grassmann manifold is explored in this paper. Rather than directly embedding Grassmann manifold into a symmetric matrix space, an extrinsic view is taken by building the self-representation of LRR over the tangent space of each Grassmannian point. A new algorithm for solving the proposed Grassmannian LRR model is designed and implemented. Several clustering experiments are conducted on handwritten digits dataset, dynamic texture video clips and YouTube celebrity face video data. The experimental results show our method outperforms a number of existing methods.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01883",
        "title": "Robust real time face recognition and tracking on gpu using fusion of rgb and depth image",
        "authors": [
            "Narmada Naik",
            "G.N Rathna"
        ],
        "abstract": "This paper presents a real-time face recognition system using kinect sensor. The algorithm is implemented on GPU using opencl and significant speed improvements are observed. We use kinect depth image to increase the robustness and reduce computational cost of conventional LBP based face recognition. The main objective of this paper was to perform robust, high speed fusion based face recognition and tracking. The algorithm is mainly composed of three steps. First step is to detect all faces in the video using viola jones algorithm. The second step is online database generation using a tracking window on the face. A modified LBP feature vector is calculated using fusion information from depth and greyscale image on GPU. This feature vector is used to train a svm classifier. Third step involves recognition of multiple faces based on our modified feature vector.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01920",
        "title": "Evaluating Two-Stream CNN for Video Classification",
        "authors": [
            "Hao Ye",
            "Zuxuan Wu",
            "Rui-Wei Zhao",
            "Xi Wang",
            "Yu-Gang Jiang",
            "Xiangyang Xue"
        ],
        "abstract": "Videos contain very rich semantic information. Traditional hand-crafted features are known to be inadequate in analyzing complex video semantics. Inspired by the huge success of the deep learning methods in analyzing image, audio and text data, significant efforts are recently being devoted to the design of deep nets for video analytics. Among the many practical needs, classifying videos (or video clips) based on their major semantic categories (e.g., \"skiing\") is useful in many applications. In this paper, we conduct an in-depth study to investigate important implementation options that may affect the performance of deep nets on video classification. Our evaluations are conducted on top of a recent two-stream convolutional neural network (CNN) pipeline, which uses both static frames and motion optical flows, and has demonstrated competitive performance against the state-of-the-art methods. In order to gain insights and to arrive at a practical guideline, many important options are studied, including network architectures, model fusion, learning parameters and the final prediction methods. Based on the evaluations, very competitive results are attained on two popular video classification benchmarks. We hope that the discussions and conclusions from this work can help researchers in related fields to quickly set up a good basis for further investigations along this very promising direction.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01942",
        "title": "MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking",
        "authors": [
            "Laura Leal-Taix\u00e9",
            "Anton Milan",
            "Ian Reid",
            "Stefan Roth",
            "Konrad Schindler"
        ],
        "abstract": "In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01954",
        "title": "Image Subset Selection Using Gabor Filters and Neural Networks",
        "authors": [
            "Heider K. Ali",
            "Anthony Whitehead"
        ],
        "abstract": "An automatic method for the selection of subsets of images, both modern and historic, out of a set of landmark large images collected from the Internet is presented in this paper. This selection depends on the extraction of dominant features using Gabor filtering. Features are selected carefully from a preliminary image set and fed into a neural network as a training data. The method collects a large set of raw landmark images containing modern and historic landmark images and non-landmark images. The method then processes these images to classify them as landmark and non-landmark images. The classification performance highly depends on the number of candidate features of the landmark.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01989",
        "title": "Pixel-wise Deep Learning for Contour Detection",
        "authors": [
            "Jyh-Jing Hwang",
            "Tyng-Luh Liu"
        ],
        "abstract": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500.\n    ",
        "submission_date": "2015-04-08T00:00:00",
        "last_modified_date": "2015-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02174",
        "title": "Connectivity Preserving Multivalued Functions in Digital Topology",
        "authors": [
            "Laurence Boxer",
            "P. Christopher Staecker"
        ],
        "abstract": "We study connectivity preserving multivalued functions between digital images. This notion generalizes that of continuous multivalued functions studied mostly in the setting of the digital plane $Z^2$. We show that connectivity preserving multivalued functions, like continuous multivalued functions, are appropriate models for digital morpholological operations. Connectivity preservation, unlike continuity, is preserved by compositions, and generalizes easily to higher dimensions and arbitrary adjacency relations.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02235",
        "title": "Extraction of Protein Sequence Motif Information using PSO K-Means",
        "authors": [
            "R. Gowri",
            "R. Rathipriya"
        ],
        "abstract": "The main objective of the paper is to find the motif ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02340",
        "title": "Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor",
        "authors": [
            "Wongun Choi"
        ],
        "abstract": "In this paper, we focus on the two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-arts.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02351",
        "title": "When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition",
        "authors": [
            "Guosheng Hu",
            "Yongxin Yang",
            "Dong Yi",
            "Josef Kittler",
            "William Christmas",
            "Stan Z. Li",
            "Timothy Hospedales"
        ],
        "abstract": "Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02437",
        "title": "Predicting Complete 3D Models of Indoor Scenes",
        "authors": [
            "Ruiqi Guo",
            "Chuhang Zou",
            "Derek Hoiem"
        ],
        "abstract": "One major goal of vision is to infer physical models of objects, surfaces, and their layout from sensors. In this paper, we aim to interpret indoor scenes from one RGBD image. Our representation encodes the layout of walls, which must conform to a Manhattan structure but is otherwise flexible, and the layout and extent of objects, modeled with CAD-like 3D shapes. We represent both the visible and occluded portions of the scene, producing a complete 3D parse. Such a scene interpretation is useful for robotics and visual reasoning, but difficult to produce due to the well-known challenge of segmentation, the high degree of occlusion, and the diversity of objects in indoor scene. We take a data-driven approach, generating sets of potential object regions, matching to regions in training images, and transferring and aligning associated 3D models while encouraging fit to observations and overall consistency. We demonstrate encouraging results on the NYU v2 dataset and highlight a variety of interesting directions for future work.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02485",
        "title": "What Do Deep CNNs Learn About Objects?",
        "authors": [
            "Xingchao Peng",
            "Baochen Sun",
            "Karim Ali",
            "Kate Saenko"
        ],
        "abstract": "Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, finding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism?\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02518",
        "title": "Unsupervised Feature Learning from Temporal Data",
        "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "David Eigen",
            "Yann LeCun"
        ],
        "abstract": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02531",
        "title": "HEp-2 Cell Image Classification with Deep Convolutional Neural Networks",
        "authors": [
            "Zhimin Gao",
            "Lei Wang",
            "Luping Zhou",
            "Jianjia Zhang"
        ],
        "abstract": "Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitate the diagnosis of many autoimmune diseases. This paper presents an automatic framework for this classification task, by utilizing the deep convolutional neural networks (CNNs) which have recently attracted intensive attention in visual recognition. This paper elaborates the important components of this framework, discusses multiple key factors that impact the efficiency of training a deep CNN, and systematically compares this framework with the well-established image classification models in the literature. Experiments on benchmark datasets show that i) the proposed framework can effectively outperform existing models by properly applying data augmentation; ii) our CNN-based framework demonstrates excellent adaptability across different datasets, which is highly desirable for classification under varying laboratory settings. Our system is ranked high in the cell image classification competition hosted by ICPR 2014.\n    ",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02648",
        "title": "Time-causal and time-recursive spatio-temporal receptive fields",
        "authors": [
            "Tony Lindeberg"
        ],
        "abstract": "We present an improved model and theory for time-causal and time-recursive spatio-temporal receptive fields, based on a combination of Gaussian receptive fields over the spatial domain and first-order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain.\n",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02763",
        "title": "Performance measures for classification systems with rejection",
        "authors": [
            "Filipe Condessa",
            "Jelena Kovacevic",
            "Jose Bioucas-Dias"
        ],
        "abstract": "Classifiers with rejection are essential in real-world applications where misclassifications and their effects are critical. However, if no problem specific cost function is defined, there are no established measures to assess the performance of such classifiers. We introduce a set of desired properties for performance measures for classifiers with rejection, based on which we propose a set of three performance measures for the evaluation of the performance of classifiers with rejection that satisfy the desired properties. The nonrejected accuracy measures the ability of the classifier to accurately classify nonrejected samples; the classification quality measures the correct decision making of the classifier with rejector; and the rejection quality measures the ability to concentrate all misclassified samples onto the set of rejected samples. From the measures, we derive the concept of relative optimality that allows us to connect the measures to a family of cost functions that take into account the trade-off between rejection and misclassification. We illustrate the use of the proposed performance measures on classifiers with rejection applied to synthetic and real-world data.\n    ",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02764",
        "title": "A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition",
        "authors": [
            "Roozbeh Mottaghi",
            "Yu Xiang",
            "Silvio Savarese"
        ],
        "abstract": "Despite the fact that object detection, 3D pose estimation, and sub-category recognition are highly correlated tasks, they are usually addressed independently from each other because of the huge space of parameters. To jointly model all of these tasks, we propose a coarse-to-fine hierarchical representation, where each level of the hierarchy represents objects at a different level of granularity. The hierarchical representation prevents performance loss, which is often caused by the increase in the number of parameters (as we consider more tasks to model), and the joint modelling enables resolving ambiguities that exist in independent modelling of these tasks. We augment PASCAL3D+ dataset with annotations for these tasks and show that our hierarchical model is effective in joint modelling of object detection, 3D pose estimation, and sub-category recognition.\n    ",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02789",
        "title": "Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
        "authors": [
            "Ashesh Jain",
            "Hema S. Koppula",
            "Bharad Raghavan",
            "Shane Soh",
            "Ashutosh Saxena"
        ],
        "abstract": "Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger.\n",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02840",
        "title": "siftservice.com - Turning a Computer Vision algorithm into a World Wide Web Service",
        "authors": [
            "Ahmad Pahlavan Tafti",
            "Hamid Hassannia",
            "Zeyun Yu"
        ],
        "abstract": "Image features detection and description is a longstanding topic in computer vision and pattern recognition areas. The Scale Invariant Feature Transform (SIFT) is probably the most popular and widely demanded feature descriptor which facilitates a variety of computer vision applications such as image registration, object tracking, image forgery detection, and 3D surface reconstruction. This work introduces a Software as a Service (SaaS) based implementation of the SIFT algorithm which is freely available at ",
        "submission_date": "2015-04-11T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02856",
        "title": "High Density Noise Removal by Cascading Algorithms",
        "authors": [
            "Arabinda Dash",
            "Sujaya Kumar Sathua"
        ],
        "abstract": "An advanced non-linear cascading filter algorithm for the removal of high density salt and pepper noise from the digital images is proposed. The proposed method consists of two stages. The first stage Decision base Median Filter (DMF) acts as the preliminary noise removal algorithm. The second stage is either Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) or Modified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which is used to remove the remaining noise and enhance the image quality. The DMF algorithm performs well at low noise density but it fails to remove the noise at medium and high level. The MDBPTGMF and MDUTMF have excellent performance at low, medium and high noise density but these reduce the image quality and blur the image at high noise level. So the basic idea behind this paper is to combine the advantages of the filters used in both the stages to remove the Salt and Pepper noise and enhance the image quality at all the noise density level. The proposed method is tested against different gray scale images and it gives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) and Image Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), Decision Base Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision Base Unsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial Trimmed Global Mean Filter (DBPTGMF).\n    ",
        "submission_date": "2015-04-11T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02863",
        "title": "Appearance-Based Gaze Estimation in the Wild",
        "authors": [
            "Xucong Zhang",
            "Yusuke Sugano",
            "Mario Fritz",
            "Andreas Bulling"
        ],
        "abstract": "Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild.\n    ",
        "submission_date": "2015-04-11T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03083",
        "title": "Joint Learning of Distributed Representations for Images and Texts",
        "authors": [
            "Xiaodong He",
            "Rupesh Srivastava",
            "Jianfeng Gao",
            "Li Deng"
        ],
        "abstract": "This technical report provides extra details of the deep multimodal similarity model (DMSM) which was proposed in (Fang et al. 2015, ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03285",
        "title": "Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors - Extended Version",
        "authors": [
            "Filip Radenovic",
            "Herve Jegou",
            "Ondrej Chum"
        ],
        "abstract": "This paper addresses the construction of a short-vector (128D) image representation for large-scale image and particular object retrieval. In particular, the method of joint dimensionality reduction of multiple vocabularies is considered. We study a variety of vocabulary generation techniques: different k-means initializations, different descriptor transformations, different measurement regions for descriptor extraction. Our extensive evaluation shows that different combinations of vocabularies, each partitioning the descriptor space in a different yet complementary manner, results in a significant performance improvement, which exceeds the state-of-the-art.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03293",
        "title": "Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction",
        "authors": [
            "Yuting Zhang",
            "Kihyuk Sohn",
            "Ruben Villegas",
            "Gang Pan",
            "Honglak Lee"
        ],
        "abstract": "Object detection systems based on the deep convolutional neural network (CNN) have recently made ground- breaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy. In experiments, we demonstrated that each of the proposed methods improves the detection performance over the baseline method on PASCAL VOC 2007 and 2012 datasets. Furthermore, two methods are complementary and significantly outperform the previous state-of-the-art when combined.\n    ",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2016-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03315",
        "title": "A Novel Approach to Develop a New Hybrid Technique for Trademark Image Retrieval",
        "authors": [
            "Saurabh Agarwal",
            "Punit Kumar Johari"
        ],
        "abstract": "Trademark Image Retrieval is playing a vital role as a part of CBIR System. Trademark is of great significance because it carries the status value of any company. To retrieve such a fake or copied trademark we design a retrieval system which is based on hybrid techniques. It contains a mixture of two different feature vector which combined together to give a suitable retrieval system. In the proposed system we extract the corner feature which is applied on an edge pixel image. This feature is used to extract the relevant image and to more purify the result we apply other feature which is the invariant moment feature. From the experimental result we conclude that the system is 85 percent efficient.\n    ",
        "submission_date": "2014-11-08T00:00:00",
        "last_modified_date": "2014-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03409",
        "title": "Clustering Assisted Fundamental Matrix Estimation",
        "authors": [
            "Hao Wu",
            "Yi Wan"
        ],
        "abstract": "In computer vision, the estimation of the fundamental matrix is a basic problem that has been extensively studied. The accuracy of the estimation imposes a significant influence on subsequent tasks such as the camera trajectory determination and 3D reconstruction. In this paper we propose a new method for fundamental matrix estimation that makes use of clustering a group of 4D vectors. The key insight is the observation that among the 4D vectors constructed from matching pairs of points obtained from the SIFT algorithm, well-defined cluster points tend to be reliable inliers suitable for fundamental matrix estimation. Based on this, we utilizes a recently proposed efficient clustering method through density peaks seeking and propose a new clustering assisted method. Experimental results show that the proposed algorithm is faster and more accurate than currently commonly used methods.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03410",
        "title": "Simultaneous Feature Learning and Hash Coding with Deep Neural Networks",
        "authors": [
            "Hanjiang Lai",
            "Yan Pan",
            "Ye Liu",
            "Shuicheng Yan"
        ],
        "abstract": "Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03439",
        "title": "Image Denoising Using Low Rank Minimization With Modified Noise Estimation",
        "authors": [
            "Zahid Hussain Shamsi",
            "Hyun Sook Oh",
            "Dai-Gyoung Kim"
        ],
        "abstract": "Recently, the application of low rank minimization to image denoising has shown remarkable denoising results which are equivalent or better than those of the existing state-of-the-art algorithms. However, due to iterative nature of low rank optimization, estimation of residual noise is an essential requirement after each iteration. Currently, this noise is estimated by using the filtered noise in the previous iteration without considering the geometric structure of the given image. This estimate may be affected in the presence of moderate and severe levels of noise. To obtain a more reliable estimate of residual noise, we propose a modified algorithm (GWNNM) which includes the contribution of the geometric structure of an image to the existing noise estimation. Furthermore, the proposed algorithm exploits the difference of large and small singular values to enhance the edges and textures during the denoising process. Consequently, the proposed modifications achieve significant improvements in the denoising results of the existing low rank optimization algorithms.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03504",
        "title": "Sketch-based 3D Shape Retrieval using Convolutional Neural Networks",
        "authors": [
            "Fang Wang",
            "Le Kang",
            "Yi Li"
        ],
        "abstract": "Retrieving 3D models from 2D human sketches has received considerable attention in the areas of graphics, image retrieval, and computer vision. Almost always in state of the art approaches a large amount of \"best views\" are computed for 3D models, with the hope that the query sketch matches one of these 2D projections of 3D models using predefined features.\n",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03522",
        "title": "Efficient Scene Text Localization and Recognition with Local Character Refinement",
        "authors": [
            "Luk\u00e1\u0161 Neumann",
            "Ji\u0159\u00ed Matas"
        ],
        "abstract": "An unconstrained end-to-end text localization and recognition method is presented. The method detects initial text hypothesis in a single pass by an efficient region-based method and subsequently refines the text hypothesis using a more robust local text model, which deviates from the common assumption of region-based methods that all characters are detected as connected components.\n",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03573",
        "title": "Building Proteins in a Day: Efficient 3D Molecular Reconstruction",
        "authors": [
            "Marcus A. Brubaker",
            "Ali Punjani",
            "David J. Fleet"
        ],
        "abstract": "Discovering the 3D atomic structure of molecules such as proteins and viruses is a fundamental research problem in biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising vision-based technique for structure estimation which attempts to reconstruct 3D structures from 2D images. This paper addresses the challenging problem of 3D reconstruction from 2D Cryo-EM images. A new framework for estimation is introduced which relies on modern stochastic optimization techniques to scale to large datasets. We also introduce a novel technique which reduces the cost of evaluating the objective function during optimization by over five orders or magnitude. The net result is an approach capable of estimating 3D molecular structure from large scale datasets in about a day on a single workstation.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03641",
        "title": "Learning to Compare Image Patches via Convolutional Neural Networks",
        "authors": [
            "Sergey Zagoruyko",
            "Nikos Komodakis"
        ],
        "abstract": "In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03707",
        "title": "Background Subtraction via Generalized Fused Lasso Foreground Modeling",
        "authors": [
            "Bo Xin",
            "Yuan Tian",
            "Yizhou Wang",
            "Wen Gao"
        ],
        "abstract": "Background Subtraction (BS) is one of the key steps in video analysis. Many background models have been proposed and achieved promising performance on public data sets. However, due to challenges such as illumination change, dynamic background etc. the resulted foreground segmentation often consists of holes as well as background noise. In this regard, we consider generalized fused lasso regularization to quest for intact structured foregrounds. Together with certain assumptions about the background, such as the low-rank assumption or the sparse-composition assumption (depending on whether pure background frames are provided), we formulate BS as a matrix decomposition problem using regularization terms for both the foreground and background matrices. Moreover, under the proposed formulation, the two generally distinctive background assumptions can be solved in a unified manner. The optimization was carried out via applying the augmented Lagrange multiplier (ALM) method in such a way that a fast parametric-flow algorithm is used for updating the foreground matrix. Experimental results on several popular BS data sets demonstrate the advantage of the proposed model compared to state-of-the-arts.\n    ",
        "submission_date": "2015-04-14T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03810",
        "title": "Text Localization in Video Using Multiscale Weber's Local Descriptor",
        "authors": [
            "B.H. Shekar",
            "Smitha M.L."
        ],
        "abstract": "In this paper, we propose a novel approach for detecting the text present in videos and scene images based on the Multiscale Weber's Local Descriptor (MWLD). Given an input video, the shots are identified and the key frames are extracted based on their spatio-temporal relationship. From each key frame, we detect the local region information using WLD with different radius and neighborhood relationship of pixel values and hence obtained intensity enhanced key frames at multiple scales. These multiscale WLD key frames are merged together and then the horizontal gradients are computed using morphological operations. The obtained results are then binarized and the false positives are eliminated based on geometrical properties. Finally, we employ connected component analysis and morphological dilation operation to determine the text regions that aids in text localization. The experimental results obtained on publicly available standard Hua, Horizontal-1 and Horizontal-2 video dataset illustrate that the proposed method can accurately detect and localize texts of various sizes, fonts and colors in videos.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03811",
        "title": "Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos",
        "authors": [
            "Meng-Che Chuang",
            "Jenq-Neng Hwang",
            "Kresimir Williams",
            "Richard Towler"
        ],
        "abstract": "Non-extractive fish abundance estimation with the aid of visual analysis has drawn increasing attention. Unstable illumination, ubiquitous noise and low frame rate video capturing in the underwater environment, however, make conventional tracking methods unreliable. In this paper, we present a multiple fish tracking system for low-contrast and low-frame-rate stereo videos with the use of a trawl-based underwater camera system. An automatic fish segmentation algorithm overcomes the low-contrast issues by adopting a histogram backprojection approach on double local-thresholded images to ensure an accurate segmentation on the fish shape boundaries. Built upon a reliable feature-based object matching method, a multiple-target tracking algorithm via a modified Viterbi data association is proposed to overcome the poor motion continuity and frequent entrance/exit of fish targets under low-frame-rate scenarios. In addition, a computationally efficient block-matching approach performs successful stereo matching, which enables an automatic fish-body tail compensation to greatly reduce segmentation error and allows for an accurate fish length measurement. Experimental results show that an effective and reliable tracking performance for multiple live fish with underwater stereo cameras is achieved.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03834",
        "title": "Comparisons of wavelet functions in QRS signal to noise ratio enhancement and detection accuracy",
        "authors": [
            "Pornchai Phukpattaranont"
        ],
        "abstract": "We compare the capability of wavelet functions used for noise removal in preprocessing step of a QRS detection algorithm in the electrocardiogram (ECG) signal. The QRS signal to noise ratio enhancement and the detection accuracy of each wavelet function are evaluated using three measures: (1) the ratio of the maximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of absolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet functions from previous well-known publications are explored, i.e., Bior1.3, Db10, and Mexican hat wavelet functions. Results evaluated with the ECG signal from MIT-BIH arrhythmia database show that the Mexican hat wavelet function is better than the others. While the scale 8 of Mexican hat wavelet function can provide the best enhancement in QRS signal to noise ratio, the scale 4 of Mexican hat wavelet function can provide the best detection accuracy. These results may be combined and may enable the use of a single fixed threshold for all ECG records leading to the reduction in computational complexity of the QRS detection algorithm.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03871",
        "title": "Bio-inspired Unsupervised Learning of Visual Features Leads to Robust Invariant Object Recognition",
        "authors": [
            "Saeed Reza Kheradpisheh",
            "Mohammad Ganjtabesh",
            "Timoth\u00e9e Masquelier"
        ],
        "abstract": "Retinal image of surrounding objects varies tremendously due to the changes in position, size, pose, illumination condition, background context, occlusion, noise, and nonrigid deformations. But despite these huge variations, our visual system is able to invariantly recognize any object in just a fraction of a second. To date, various computational models have been proposed to mimic the hierarchical processing of the ventral visual pathway, with limited success. Here, we show that the association of both biologically inspired network architecture and learning rule significantly improves the models' performance when facing challenging invariant object recognition problems. Our model is an asynchronous feedforward spiking neural network. When the network is presented with natural images, the neurons in the entry layers detect edges, and the most activated ones fire first, while neurons in higher layers are equipped with spike timing-dependent plasticity. These neurons progressively become selective to intermediate complexity visual features appropriate for object categorization. The model is evaluated on 3D-Object and ETH-80 datasets which are two benchmarks for invariant object recognition, and is shown to outperform state-of-the-art models, including DeepConvNet and HMAX. This demonstrates its ability to accurately recognize different instances of multiple object classes even under various appearance conditions (different views, scales, tilts, and backgrounds). Several statistical analysis techniques are used to show that our model extracts class specific and highly informative features.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03967",
        "title": "Deep convolutional networks for pancreas segmentation in CT imaging",
        "authors": [
            "Holger R. Roth",
            "Amal Farag",
            "Le Lu",
            "Evrim B. Turkbey",
            "Ronald M. Summers"
        ],
        "abstract": "Automatic organ segmentation is an important prerequisite for many computer-aided diagnosis systems. The high anatomical variability of organs in the abdomen, such as the pancreas, prevents many segmentation methods from achieving high accuracies when compared to other segmentation of organs like the liver, heart or kidneys. Recently, the availability of large annotated training sets and the accessibility of affordable parallel computing resources via GPUs have made it feasible for \"deep learning\" methods such as convolutional networks (ConvNets) to succeed in image classification tasks. These methods have the advantage that used classification features are trained directly from the imaging data. We present a fully-automated bottom-up method for pancreas segmentation in computed tomography (CT) images of the abdomen. The method is based on hierarchical coarse-to-fine classification of local image regions (superpixels). Superpixels are extracted from the abdominal region using Simple Linear Iterative Clustering (SLIC). An initial probability response map is generated, using patch-level confidences and a two-level cascade of random forest classifiers, from which superpixel regions with probabilities larger 0.5 are retained. These retained superpixels serve as a highly sensitive initial input of the pancreas and its surroundings to a ConvNet that samples a bounding box around each superpixel at different scales (and random non-rigid deformations at training time) in order to assign a more distinct probability of each superpixel region being pancreas or not. We evaluate our method on CT images of 82 patients (60 for training, 2 for validation, and 20 for testing). Using ConvNets we achieve average Dice scores of 68%+-10% (range, 43-80%) in testing. This shows promise for accurate pancreas segmentation, using a deep learning approach and compares favorably to state-of-the-art methods.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04003",
        "title": "Anatomy-specific classification of medical images using deep convolutional nets",
        "authors": [
            "Holger R. Roth",
            "Christopher T. Lee",
            "Hoo-Chang Shin",
            "Ari Seff",
            "Lauren Kim",
            "Jianhua Yao",
            "Le Lu",
            "Ronald M. Summers"
        ],
        "abstract": "Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. \"Deep learning\" methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04085",
        "title": "FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave Infrared",
        "authors": [
            "Huaijin Chen",
            "M. Salman Asif",
            "Aswin C. Sankaranarayanan",
            "Ashok Veeraraghavan"
        ],
        "abstract": "Cameras for imaging in short and mid-wave infrared spectra are significantly more expensive than their counterparts in visible imaging. As a result, high-resolution imaging in those spectrum remains beyond the reach of most consumers. Over the last decade, compressive sensing (CS) has emerged as a potential means to realize inexpensive short-wave infrared cameras. One approach for doing this is the single-pixel camera (SPC) where a single detector acquires coded measurements of a high-resolution image. A computational reconstruction algorithm is then used to recover the image from these coded measurements. Unfortunately, the measurement rate of a SPC is insufficient to enable imaging at high spatial and temporal resolutions.\n",
        "submission_date": "2015-04-16T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04090",
        "title": "Segmentation of Subspaces in Sequential Data",
        "authors": [
            "Stephen Tierney",
            "Yi Guo",
            "Junbin Gao"
        ],
        "abstract": "We propose Ordered Subspace Clustering (OSC) to segment data drawn from a sequentially ordered union of subspaces. Similar to Sparse Subspace Clustering (SSC) we formulate the problem as one of finding a sparse representation but include an additional penalty term to take care of sequential data. We test our method on data drawn from infrared hyper spectral, video and motion capture data. Experiments show that our method, OSC, outperforms the state of the art methods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR) and SSC.\n    ",
        "submission_date": "2015-04-16T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04531",
        "title": "Hyperspectral pansharpening: a review",
        "authors": [
            "Laetitia Loncan",
            "Luis B. Almeida",
            "Jos\u00e9 M. Bioucas-Dias",
            "Xavier Briottet",
            "Jocelyn Chanussot",
            "Nicolas Dobigeon",
            "Sophie Fabre",
            "Wenzhi Liao",
            "Giorgio A. Licciardi",
            "Miguel Sim\u00f5es",
            "Jean-Yves Tourneret",
            "Miguel A. Veganzones",
            "Gemine Vivone",
            "Qi Wei",
            "Naoto Yokoya"
        ],
        "abstract": "Pansharpening aims at fusing a panchromatic image with a multispectral one, to generate an image with the high spatial resolution of the former and the high spectral resolution of the latter. In the last decade, many algorithms have been presented in the literature for pansharpening using multispectral data. With the increasing availability of hyperspectral systems, these methods are now being adapted to hyperspectral images. In this work, we compare new pansharpening techniques designed for hyperspectral data with some of the state of the art methods for multispectral pansharpening, which have been adapted for hyperspectral data. Eleven methods from different classes (component substitution, multiresolution analysis, hybrid, Bayesian and matrix factorization) are analyzed. These methods are applied to three datasets and their effectiveness and robustness are evaluated with widely used performance indicators. In addition, all the pansharpening techniques considered in this paper have been implemented in a MATLAB toolbox that is made available to the community.\n    ",
        "submission_date": "2015-04-17T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04548",
        "title": "Color Constancy Using CNNs",
        "authors": [
            "Simone Bianco",
            "Claudio Cusano",
            "Raimondo Schettini"
        ],
        "abstract": "In this work we describe a Convolutional Neural Network (CNN) to accurately predict the scene illumination. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max pooling, one fully connected layer and three output nodes. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating scene illumination. This approach achieves state-of-the-art performance on a standard dataset of RAW images. Preliminary experiments on images with spatially varying illumination demonstrate the stability of the local illuminant estimation ability of our CNN.\n    ",
        "submission_date": "2015-04-17T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04651",
        "title": "Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint Recognition for Infants and Toddlers",
        "authors": [
            "Anil K. Jain",
            "Sunpreet S. Arora",
            "Lacey Best-Rowden",
            "Kai Cao",
            "Prem Sewak Sudhish",
            "Anjoo Bhatnagar"
        ],
        "abstract": "With a number of emerging applications requiring biometric recognition of children (e.g., tracking child vaccination schedules, identifying missing children and preventing newborn baby swaps in hospitals), investigating the temporal stability of biometric recognition accuracy for children is important. The persistence of recognition accuracy of three of the most commonly used biometric traits (fingerprints, face and iris) has been investigated for adults. However, persistence of biometric recognition accuracy has not been studied systematically for children in the age group of 0-4 years. Given that very young children are often uncooperative and do not comprehend or follow instructions, in our opinion, among all biometric modalities, fingerprints are the most viable for recognizing children. This is primarily because it is easier to capture fingerprints of young children compared to other biometric traits, e.g., iris, where a child needs to stare directly towards the camera to initiate iris capture. In this report, we detail our initiative to investigate the persistence of fingerprint recognition for children in the age group of 0-4 years. Based on preliminary results obtained for the data collected in the first phase of our study, use of fingerprints for recognition of 0-4 year-old children appears promising.\n    ",
        "submission_date": "2015-04-17T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04660",
        "title": "A spectral optical flow method for determining velocities from digital imagery",
        "authors": [
            "Neal Hurlburt",
            "Steve Jaffey"
        ],
        "abstract": "We present a method for determining surface flows from solar images based upon optical flow techniques. We apply the method to sets of images obtained by a variety of solar imagers to assess its performance. The {\\tt opflow3d} procedure is shown to extract accurate velocity estimates when provided perfect test data and quickly generates results consistent with completely distinct methods when applied on global scales. We also validate it in detail by comparing it to an established method when applied to high-resolution datasets and find that it provides comparable results without the need to tune, filter or otherwise preprocess the images before its application.\n    ",
        "submission_date": "2015-04-17T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04763",
        "title": "Understanding the Fisher Vector: a multimodal part model",
        "authors": [
            "David Novotn\u00fd",
            "Diane Larlus",
            "Florent Perronnin",
            "Andrea Vedaldi"
        ],
        "abstract": "Fisher Vectors and related orderless visual statistics have demonstrated excellent performance in object detection, sometimes superior to established approaches such as the Deformable Part Models. However, it remains unclear how these models can capture complex appearance variations using visual codebooks of limited sizes and coarse geometric information. In this work, we propose to interpret Fisher-Vector-based object detectors as part-based models. Through the use of several visualizations and experiments, we show that this is a useful insight to explain the good performance of the model. Furthermore, we reveal for the first time several interesting properties of the FV, including its ability to work well using only a small subset of input patches and visual words. Finally, we discuss the relation of the FV and DPM detectors, pointing out differences and commonalities between them.\n    ",
        "submission_date": "2015-04-18T00:00:00",
        "last_modified_date": "2015-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04792",
        "title": "Visual Recognition Using Directional Distribution Distance",
        "authors": [
            "Jianxin Wu",
            "Bin-Bin Gao",
            "Guoqing Liu"
        ],
        "abstract": "In computer vision, an entity such as an image or video is often represented as a set of instance vectors, which can be SIFT, motion, or deep learning feature vectors extracted from different parts of that entity. Thus, it is essential to design efficient and effective methods to compare two sets of instance vectors. Existing methods such as FV, VLAD or Super Vectors have achieved excellent results. However, this paper shows that these methods are designed based on a generative perspective, and a discriminative method can be more effective in categorizing images or videos. The proposed D3 (discriminative distribution distance) method effectively compares two sets as two distributions, and proposes a directional total variation distance (DTVD) to measure how separated are they. Furthermore, a robust classifier-based method is proposed to estimate DTVD robustly. The D3 method is evaluated in action and image recognition tasks and has achieved excellent accuracy and speed. D3 also has a synergy with FV. The combination of D3 and FV has advantages over D3, FV, and VLAD.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04871",
        "title": "DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets",
        "authors": [
            "Sukrit Shankar",
            "Vikas K. Garg",
            "Roberto Cipolla"
        ],
        "abstract": "Most of the approaches for discovering visual attributes in images demand significant supervision, which is cumbersome to obtain. In this paper, we aim to discover visual attributes in a weakly supervised setting that is commonly encountered with contemporary image search engines. Deep Convolutional Neural Networks (CNNs) have enjoyed remarkable success in vision applications recently. However, in a weakly supervised scenario, widely used CNN training procedures do not learn a robust model for predicting multiple attribute labels simultaneously. The primary reason is that the attributes highly co-occur within the training data. To ameliorate this limitation, we propose Deep-Carving, a novel training procedure with CNNs, that helps the net efficiently carve itself for the task of multiple attribute prediction. During training, the responses of the feature maps are exploited in an ingenious way to provide the net with multiple pseudo-labels (for training images) for subsequent iterations. The process is repeated periodically after a fixed number of iterations, and enables the net carve itself iteratively for efficiently disentangling features. Additionally, we contribute a noun-adjective pairing inspired Natural Scenes Attributes Dataset to the research community, CAMIT - NSAD, containing a number of co-occurring attributes within a noun category. We describe, in detail, salient aspects of this dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset, with weak supervision, clearly demonstrate that the Deep-Carved CNNs consistently achieve considerable improvement in the precision of attribute prediction over popular baseline methods.\n    ",
        "submission_date": "2015-04-19T00:00:00",
        "last_modified_date": "2015-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04923",
        "title": "Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition",
        "authors": [
            "Ruizhi Qiao",
            "Lingqiao Liu",
            "Chunhua Shen",
            "Anton von den Hengel"
        ],
        "abstract": "The introduction of low-cost RGB-D sensors has promoted the research in skeleton-based human action recognition. Devising a representation suitable for characterising actions on the basis of noisy skeleton sequences remains a challenge, however. We here provide two insights into this challenge. First, we show that the discriminative information of a skeleton sequence usually resides in a short temporal interval and we propose a simple-but-effective local descriptor called trajectorylet to capture the static and kinematic information within this interval. Second, we further propose to encode each trajectorylet with a discriminative trajectorylet detector set which is selected from a large number of candidate detectors trained through exemplar-SVMs. The action-level representation is obtained by pooling trajectorylet encodings. Evaluating on standard datasets acquired from the Kinect sensor, it is demonstrated that our method obtains superior results over existing approaches under various experimental setups.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04943",
        "title": "Weakly Supervised Fine-Grained Image Categorization",
        "authors": [
            "Yu Zhang",
            "Xiu-shen Wei",
            "Jianxin Wu",
            "Jianfei Cai",
            "Jiangbo Lu",
            "Viet-Anh Nguyen",
            "Minh N. Do"
        ],
        "abstract": "In this paper, we categorize fine-grained images without using any object / part annotation neither in the training nor in the testing stage, a step towards making it suitable for deployments. Fine-grained image categorization aims to classify objects with subtle distinctions. Most existing works heavily rely on object / part detectors to build the correspondence between object parts by using object or object part annotations inside training images. The need for expensive object annotations prevents the wide usage of these methods. Instead, we propose to select useful parts from multi-scale part proposals in objects, and use them to compute a global image representation for categorization. This is specially designed for the annotation-free fine-grained categorization task, because useful parts have shown to play an important role in existing annotation-dependent works but accurate part detectors can be hardly acquired. With the proposed image representation, we can further detect and visualize the key (most discriminative) parts in objects of different classes. In the experiment, the proposed annotation-free method achieves better accuracy than that of state-of-the-art annotation-free and most existing annotation-dependent methods on two challenging datasets, which shows that it is not always necessary to use accurate object / part annotations in fine-grained image categorization.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05133",
        "title": "Exploiting Local Features from Deep Networks for Image Retrieval",
        "authors": [
            "Joe Yue-Hei Ng",
            "Fan Yang",
            "Larry S. Davis"
        ],
        "abstract": "Deep convolutional neural networks have been successfully applied to image classification tasks. When these same networks have been applied to image retrieval, the assumption has been made that the last layers would give the best performance, as they do in classification. We show that for instance-level image retrieval, lower layers often perform better than the last layers in convolutional neural networks. We present an approach for extracting convolutional features from different layers of the networks, and adopt VLAD encoding to encode features into a single vector for each image. We investigate the effect of different layers and scales of input images on the performance of convolutional features using the recent deep networks OxfordNet and GoogLeNet. Experiments demonstrate that intermediate layers or higher layers with finer scales produce better results for image retrieval, compared to the last layer. When using compressed 128-D VLAD descriptors, our method obtains state-of-the-art results and outperforms other VLAD and CNN based approaches on two out of three test datasets. Our work provides guidance for transferring deep networks trained on image classification to image retrieval tasks.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05137",
        "title": "Application of Enhanced-2D-CWT in Topographic Images for Mapping Landslide Risk Areas",
        "authors": [
            "V.V. Vermehren Valenzuela",
            "R.D. Lins",
            "H.M. de Oliveira"
        ],
        "abstract": "There has been lately a number of catastrophic events of landslides and mudslides in the mountainous region of Rio de Janeiro, Brazil. Those were caused by intense rain in localities where there was unplanned occupation of slopes of hills and mountains. Thus, it became imperative creating an inventory of landslide risk areas in densely populated cities. This work presents a way of demarcating risk areas by using the bidimensional Continuous Wavelet Transform (2D-CWT) applied to high resolution topographic images of the mountainous region of Rio de Janeiro.\n    ",
        "submission_date": "2015-04-15T00:00:00",
        "last_modified_date": "2015-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05277",
        "title": "Deep Spatial Pyramid: The Devil is Once Again in the Details",
        "authors": [
            "Bin-Bin Gao",
            "Xiu-Shen Wei",
            "Jianxin Wu",
            "Weiyao Lin"
        ],
        "abstract": "In this paper we show that by carefully making good choices for various detailed but important factors in a visual recognition framework using deep learning features, one can achieve a simple, efficient, yet highly accurate image classification system. We first list 5 important factors, based on both existing researches and ideas proposed in this paper. These important detailed factors include: 1) $\\ell_2$ matrix normalization is more effective than unnormalized or $\\ell_2$ vector normalization, 2) the proposed natural deep spatial pyramid is very effective, and 3) a very small $K$ in Fisher Vectors surprisingly achieves higher accuracy than normally used large $K$ values. Along with other choices (convolutional activations and multiple scales), the proposed DSP framework is not only intuitive and efficient, but also achieves excellent classification accuracy on many benchmark datasets. For example, DSP's accuracy on SUN397 is 59.78%, significantly higher than previous state-of-the-art (53.86%).\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05298",
        "title": "Viewpoint distortion compensation in practical surveillance systems",
        "authors": [
            "Ognjen Arandjelovic",
            "Duc-Son Pham",
            "Svetha Venkatesh"
        ],
        "abstract": "Our aim is to estimate the perspective-effected geometric distortion of a scene from a video feed. In contrast to all previous work we wish to achieve this using from low-level, spatio-temporally local motion features used in commercial semi-automatic surveillance systems. We: (i) describe a dense algorithm which uses motion features to estimate the perspective distortion at each image locus and then polls all such local estimates to arrive at the globally best estimate, (ii) present an alternative coarse algorithm which subdivides the image frame into blocks, and uses motion features to derive block-specific motion characteristics and constrain the relationships between these characteristics, with the perspective estimate emerging as a result of a global optimization scheme, and (iii) report the results of an evaluation using nine large sets acquired using existing close-circuit television (CCTV) cameras. Our findings demonstrate that both of the proposed methods are successful, their accuracy matching that of human labelling using complete visual data.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05299",
        "title": "Groupwise registration of aerial images",
        "authors": [
            "Ognjen Arandjelovic",
            "Duc-Son Pham",
            "Svetha Venkatesh"
        ],
        "abstract": "This paper addresses the task of time separated aerial image registration. The ability to solve this problem accurately and reliably is important for a variety of subsequent image understanding applications. The principal challenge lies in the extent and nature of transient appearance variation that a land area can undergo, such as that caused by the change in illumination conditions, seasonal variations, or the occlusion by non-persistent objects (people, cars). Our work introduces several novelties: (i) unlike all previous work on aerial image registration, we approach the problem using a set-based paradigm; (ii) we show how local, pair-wise constraints can be used to enforce a globally good registration using a constraints graph structure; (iii) we show how a simple holistic representation derived from raw aerial images can be used as a basic building block of the constraints graph in a manner which achieves both high registration accuracy and speed. We demonstrate: (i) that the proposed method outperforms the state-of-the-art for pair-wise registration already, achieving greater accuracy and reliability, while at the same time reducing the computational cost of the task; and (ii) that the increase in the number of available images in a set consistently reduces the average registration error.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05302",
        "title": "The adaptable buffer algorithm for high quantile estimation in non-stationary data streams",
        "authors": [
            "Ognjen Arandjelovic",
            "Duc-Son Pham",
            "Svetha Venkatesh"
        ],
        "abstract": "The need to estimate a particular quantile of a distribution is an important problem which frequently arises in many computer vision and signal processing applications. For example, our work was motivated by the requirements of many semi-automatic surveillance analytics systems which detect abnormalities in close-circuit television (CCTV) footage using statistical models of low-level motion features. In this paper we specifically address the problem of estimating the running quantile of a data stream with non-stationary stochasticity when the memory for storing observations is limited. We make several major contributions: (i) we derive an important theoretical result which shows that the change in the quantile of a stream is constrained regardless of the stochastic properties of data, (ii) we describe a set of high-level design goals for an effective estimation algorithm that emerge as a consequence of our theoretical findings, (iii) we introduce a novel algorithm which implements the aforementioned design goals by retaining a sample of data values in a manner adaptive to changes in the distribution of data and progressively narrowing down its focus in the periods of quasi-stationary stochasticity, and (iv) we present a comprehensive evaluation of the proposed algorithm and compare it with the existing methods in the literature on both synthetic data sets and three large `real-world' streams acquired in the course of operation of an existing commercial surveillance system. Our findings convincingly demonstrate that the proposed method is highly successful and vastly outperforms the existing alternatives, especially when the target quantile is high valued and the available buffer capacity severely limited.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05308",
        "title": "Automatic Face Recognition from Video",
        "authors": [
            "Ognjen Arandjelovic"
        ],
        "abstract": "The objective of this work is to automatically recognize faces from video sequences in a realistic, unconstrained setup in which illumination conditions are extreme and greatly changing, viewpoint and user motion pattern have a wide variability, and video input is of low quality. At the centre of focus are face appearance manifolds: this thesis presents a significant advance of their understanding and application in the sphere of face recognition. The two main contributions are the Generic Shape-Illumination Manifold recognition algorithm and the Anisotropic Manifold Space clustering. The Generic Shape-Illumination Manifold is evaluated on a large data corpus acquired in real-world conditions and its performance is shown to greatly exceed that of state-of-the-art methods in the literature and the best performing commercial software. Empirical evaluation of the Anisotropic Manifold Space clustering on a popular situation comedy is also described with excellent preliminary results.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05369",
        "title": "Key-Pose Prediction in Cyclic Human Motion",
        "authors": [
            "Dan Zecha",
            "Rainer Lienhart"
        ],
        "abstract": "In this paper we study the problem of estimating innercyclic time intervals within repetitive motion sequences of top-class swimmers in a swimming channel. Interval limits are given by temporal occurrences of key-poses, i.e. distinctive postures of the body. A key-pose is defined by means of only one or two specific features of the complete posture. It is often difficult to detect such subtle features directly. We therefore propose the following method: Given that we observe the swimmer from the side, we build a pictorial structure of poselets to robustly identify random support poses within the regular motion of a swimmer. We formulate a maximum likelihood model which predicts a key-pose given the occurrences of multiple support poses within one stroke. The maximum likelihood can be extended with prior knowledge about the temporal location of a key-pose in order to improve the prediction recall. We experimentally show that our models reliably and robustly detect key-poses with a high precision and that their performance can be improved by extending the framework with additional camera views.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05451",
        "title": "Adaptive Compressive Tracking via Online Vector Boosting Feature Selection",
        "authors": [
            "Qingshan Liu",
            "Jing Yang",
            "Kaihua Zhang",
            "Yi Wu"
        ],
        "abstract": "Recently, the compressive tracking (CT) method has attracted much attention due to its high efficiency, but it cannot well deal with the large scale target appearance variations due to its data-independent random projection matrix that results in less discriminative features. To address this issue, in this paper we propose an adaptive CT approach, which selects the most discriminative features to design an effective appearance model. Our method significantly improves CT in three aspects: Firstly, the most discriminative features are selected via an online vector boosting method. Secondly, the object representation is updated in an effective online manner, which preserves the stable features while filtering out the noisy ones. Finally, a simple and effective trajectory rectification approach is adopted that can make the estimated location more accurate. Extensive experiments on the CVPR2013 tracking benchmark demonstrate the superior performance of our algorithm compared over state-of-the-art tracking algorithms.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05524",
        "title": "A robust and efficient video representation for action recognition",
        "authors": [
            "Heng Wang",
            "Dan Oneata",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "This paper introduces a state-of-the-art video representation and applies it to efficient action recognition and detection. We first propose to improve the popular dense trajectory features by explicit camera motion estimation. More specifically, we extract feature point matches between frames using SURF descriptors and dense optical flow. The matches are used to estimate a homography with RANSAC. To improve the robustness of homography estimation, a human detector is employed to remove outlier matches from the human body as human motion is not constrained by the camera. Trajectories consistent with the homography are considered as due to camera motion, and thus removed. We also use the homography to cancel out camera motion from the optical flow. This results in significant improvement on motion-based HOF and MBH descriptors. We further explore the recent Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram, and consider different ways to include spatial layout information in these encodings. We present a large and varied set of evaluations, considering (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that our improved trajectory features significantly outperform previous dense trajectories, and that Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, we show substantial improvements over the state-of-the-art results.\n    ",
        "submission_date": "2015-04-21T00:00:00",
        "last_modified_date": "2015-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05623",
        "title": "Median and Mode Ellipse Parameterization for Robust Contour Fitting",
        "authors": [
            "Michael A. Greminger"
        ],
        "abstract": "Problems that require the parameterization of closed contours arise frequently in computer vision applications. This article introduces a new curve parameterization algorithm that is able to fit a closed curve to a set of points while being robust to the presence of outliers and occlusions in the data. This robustness property makes this algorithm applicable to computer vision applications where misclassification of features may lead to outliers. The algorithm starts by fitting ellipses to numerous five point subsets from the source data. The closed curve is parameterized by determining the median perimeter of the set of ellipses. The resulting curve is not an ellipse, allowing arbitrary closed contours to be parameterized. The use of the modal perimeter rather than the median perimeter is also explored. A detailed comparison is made between the proposed curve fitting algorithm and existing robust ellipse fitting algorithms. Finally, the utility of the algorithm for computer vision applications is demonstrated through the parameterization of the boundary of fuel droplets during combustion. The performance of the proposed algorithm and the performance of existing algorithms are compared to a ground truth segmentation of the fuel droplet images, which demonstrates improved performance for both area quantification and edge deviation.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05776",
        "title": "Combining local regularity estimation and total variation optimization for scale-free texture segmentation",
        "authors": [
            "Nelly Pustelnik",
            "Herwig Wendt",
            "Patrice Abry",
            "Nicolas Dobigeon"
        ],
        "abstract": "Texture segmentation constitutes a standard image processing task, crucial to many applications. The present contribution focuses on the particular subset of scale-free textures and its originality resides in the combination of three key ingredients: First, texture characterization relies on the concept of local regularity ; Second, estimation of local regularity is based on new multiscale quantities referred to as wavelet leaders ; Third, segmentation from local regularity faces a fundamental bias variance trade-off: In nature, local regularity estimation shows high variability that impairs the detection of changes, while a posteriori smoothing of regularity estimates precludes from locating correctly changes. Instead, the present contribution proposes several variational problem formulations based on total variation and proximal resolutions that effectively circumvent this trade-off. Estimation and segmentation performance for the proposed procedures are quantified and compared on synthetic as well as on real-world textures.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2016-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05809",
        "title": "LOAD: Local Orientation Adaptive Descriptor for Texture and Material Classification",
        "authors": [
            "Xianbiao Qi",
            "Guoying Zhao",
            "Linlin Shen",
            "Qingquan Li",
            "Matti Pietikainen"
        ],
        "abstract": "In this paper, we propose a novel local feature, called Local Orientation Adaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD, we proposed to define point description on an Adaptive Coordinate System (ACS), adopt a binary sequence descriptor to capture relationships between one point and its neighbors and use multi-scale strategy to enhance the discriminative power of the descriptor. The proposed LOAD enjoys not only discriminative power to capture the texture information, but also has strong robustness to illumination variation and image rotation. Extensive experiments on benchmark data sets of texture classification and real-world material recognition show that the proposed LOAD yields the state-of-the-art performance. It is worth to mention that we achieve a 65.4\\% classification accuracy-- which is, to the best of our knowledge, the highest record by far --on Flickr Material Database by using a single feature. Moreover, by combining LOAD with the feature extracted by Convolutional Neural Networks (CNN), we obtain significantly better performance than both the LOAD and CNN. This result confirms that the LOAD is complementary to the learning-based features.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05843",
        "title": "Exploit Bounding Box Annotations for Multi-label Object Recognition",
        "authors": [
            "Hao Yang",
            "Joey Tianyi Zhou",
            "Yu Zhang",
            "Bin-Bin Gao",
            "Jianxin Wu",
            "Jianfei Cai"
        ],
        "abstract": "Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multi-instance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art hand-crafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2016-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06036",
        "title": "Edge Detection Based on Global and Local Parameters of the Image",
        "authors": [
            "Andrew F. C. Brustolin"
        ],
        "abstract": "This paper presents an edge detection method based on global and local parameters of the image, which produces satisfactory results on the edge detection of complex images and has a simple structure for execution. The local and global parameters of the image are arithmetic means and standard deviations, the former acquired from a three sized window representing five pixels, the latter acquired from the entire row or column. We obtain the differences of grayscale intensities between two adjacent pixels and the sum of the modulus of these differences from the horizontal and vertical scans of the image. Using these obtained values, we calculate the local and global parameters. After the gathering of the local and global parameters, we compare each sum of the modulus of differences with its own local and global parameter. In the case of the comparison is true, the consecutive pixel to the modulus sum of differences index is marked as an edge. We present the results of the tests with grayscale images using different parameters and discuss the advantages and disadvantages of each parameter value and algorithm structure chosen on the edge processing. There is a comparison of results between this papers detector and Canny, where we evaluate the quality of the presented detector.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06055",
        "title": "Understanding and Diagnosing Visual Tracking Systems",
        "authors": [
            "Naiyan Wang",
            "Jianping Shi",
            "Dit-Yan Yeung",
            "Jiaya Jia"
        ],
        "abstract": "Several benchmark datasets for visual tracking research have been proposed in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06063",
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li"
        ],
        "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06066",
        "title": "Object Detection Networks on Convolutional Feature Maps",
        "authors": [
            "Shaoqing Ren",
            "Kaiming He",
            "Ross Girshick",
            "Xiangyu Zhang",
            "Jian Sun"
        ],
        "abstract": "Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them \"Networks on Convolutional feature maps\" (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06103",
        "title": "Online Adaptive Hidden Markov Model for Multi-Tracker Fusion",
        "authors": [
            "Tomas Vojir",
            "Jiri Matas",
            "Jana Noskova"
        ],
        "abstract": "In this paper, we propose a novel method for visual object tracking called HMMTxD. The method fuses observations from complementary out-of-the box trackers and a detector by utilizing a hidden Markov model whose latent states correspond to a binary vector expressing the failure of individual trackers. The Markov model is trained in an unsupervised way, relying on an online learned detector to provide a source of tracker-independent information for a modified Baum- Welch algorithm that updates the model w.r.t. the partially annotated data.\n",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06133",
        "title": "Sparse Radial Sampling LBP for Writer Identification",
        "authors": [
            "Anguelos Nicolaou",
            "Andrew D. Bagdanov",
            "Marcus Liwicki",
            "Dimosthenis Karatzas"
        ],
        "abstract": "In this paper we present the use of Sparse Radial Sampling Local Binary Patterns, a variant of Local Binary Patterns (LBP) for text-as-texture classification. By adapting and extending the standard LBP operator to the particularities of text we get a generic text-as-texture classification scheme and apply it to writer identification. In experiments on CVL and ICDAR 2013 datasets, the proposed feature-set demonstrates State-Of-the-Art (SOA) performance. Among the SOA, the proposed method is the only one that is based on dense extraction of a single local feature descriptor. This makes it fast and applicable at the earliest stages in a DIA pipeline without the need for segmentation, binarization, or extraction of multiple features.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06151",
        "title": "Robust Principal Component Analysis on Graphs",
        "authors": [
            "Nauman Shahid",
            "Vassilis Kalofolias",
            "Xavier Bresson",
            "Michael Bronstein",
            "Pierre Vandergheynst"
        ],
        "abstract": "Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called \"Robust PCA on Graphs\" which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06201",
        "title": "High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision",
        "authors": [
            "Gedas Bertasius",
            "Jianbo Shi",
            "Lorenzo Torresani"
        ],
        "abstract": "Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a \"High-for-Low\" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run.\n",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06206",
        "title": "An Elastic Image Registration Approach for Wireless Capsule Endoscope Localization",
        "authors": [
            "Isabel N. Figueiredo",
            "Carlos Leal",
            "Lu\u00eds Pinto",
            "Pedro N. Figueiredo",
            "Richard Tsai"
        ],
        "abstract": "Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits physicians to examine all the areas of the Gastrointestinal (GI) tract. It is especially important for the small intestine, where traditional invasive endoscopies cannot reach. Although WCE represents an extremely important advance in medical imaging, a major drawback that remains unsolved is the WCE precise location in the human body during its operating time. This is mainly due to the complex physiological environment and the inherent capsule effects during its movement. When an abnormality is detected, in the WCE images, medical doctors do not know precisely where this abnormality is located relative to the intestine and therefore they can not proceed efficiently with the appropriate therapy. The primary objective of the present paper is to give a contribution to WCE localization, using image-based methods. The main focus of this work is on the description of a multiscale elastic image registration approach, its experimental application on WCE videos, and comparison with a multiscale affine registration. The proposed approach includes registrations that capture both rigid-like and non-rigid deformations, due respectively to the rigid-like WCE movement and the elastic deformation of the small intestine originated by the GI peristaltic movement. Under this approach a qualitative information about the WCE speed can be obtained, as well as the WCE location and orientation via projective geometry. The results of the experimental tests with real WCE video frames show the good performance of the proposed approach, when elastic deformations of the small intestine are involved in successive frames, and its superiority with respect to a multiscale affine image registration, which accounts for rigid-like deformations only and discards elastic deformations.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06243",
        "title": "Person Re-identification with Correspondence Structure Learning",
        "authors": [
            "Yang Shen",
            "Weiyao Lin",
            "Junchi Yan",
            "Mingliang Xu",
            "Jianxin Wu",
            "Jingdong Wang"
        ],
        "abstract": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06266",
        "title": "Evolving Fuzzy Image Segmentation with Self-Configuration",
        "authors": [
            "Ahmed Othman",
            "Hamid R. Tizhoosh",
            "Farzad Khalvati"
        ],
        "abstract": "Current image segmentation techniques usually require that the user tune several parameters in order to obtain maximum segmentation accuracy, a computationally inefficient approach, especially when a large number of images must be processed sequentially in daily practice. The use of evolving fuzzy systems for designing a method that automatically adjusts parameters to segment medical images according to the quality expectation of expert users has been proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS suffers from a few limitations when used in practice mainly due to some fixed parameters. For instance, EFIS depends on auto-detection of the object of interest for feature calculation, a task that is highly application-dependent. This shortcoming limits the applicability of EFIS, which was proposed with the ultimate goal of offering a generic but adjustable segmentation scheme. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to self-estimate the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same segmentation algorithms and the same dataset as for EFIS. The results show that SC-EFIS can provide the same results as EFIS but with a higher level of automation.\n    ",
        "submission_date": "2015-04-23T00:00:00",
        "last_modified_date": "2015-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06375",
        "title": "Holistically-Nested Edge Detection",
        "authors": [
            "Saining Xie",
            "Zhuowen Tu"
        ],
        "abstract": "We develop a new edge detection algorithm that tackles two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to approach the human ability resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06378",
        "title": "Depth-based hand pose estimation: methods, data, and challenges",
        "authors": [
            "James Steven Supancic III",
            "Gregory Rogez",
            "Yi Yang",
            "Jamie Shotton",
            "Deva Ramanan"
        ],
        "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06434",
        "title": "Situational Object Boundary Detection",
        "authors": [
            "Jasper Uijlings",
            "Vittorio Ferrari"
        ],
        "abstract": "Intuitively, the appearance of true object boundaries varies from image to image. Hence the usual monolithic approach of training a single boundary predictor and applying it to all images regardless of their content is bound to be suboptimal. In this paper we therefore propose situational object boundary detection: We first define a variety of situations and train a specialized object boundary detector for each of them using [Dollar and Zitnick 2013]. Then given a test image, we classify it into these situations using its context, which we model by global image appearance. We apply the corresponding situational object boundary detectors, and fuse them based on the classification probabilities. In experiments on ImageNet, Microsoft COCO, and Pascal VOC 2012 segmentation we show that our situational object boundary detection gives significant improvements over a monolithic approach. Additionally, our method substantially outperforms [Hariharan et al. 2011] on semantic contour detection on their SBD dataset.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06507",
        "title": "Local Variation as a Statistical Hypothesis Test",
        "authors": [
            "Michael Baltaxe",
            "Peter Meer",
            "Michael Lindenbaum"
        ],
        "abstract": "The goal of image oversegmentation is to divide an image into several pieces, each of which should ideally be part of an object. One of the simplest and yet most effective oversegmentation algorithms is known as local variation (LV) (Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm and show that algorithms similar to LV can be devised by applying different statistical models and decisions, thus providing further theoretical justification and a well-founded explanation for the unexpected high performance of the LV approach. Some of these algorithms are based on statistics of natural images and on a hypothesis testing decision; we denote these algorithms probabilistic local variation (pLV). The best pLV algorithm, which relies on censored estimation, presents state-of-the-art results while keeping the same computational complexity of the LV algorithm.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06567",
        "title": "Cultural Event Recognition with Visual ConvNets and Temporal Models",
        "authors": [
            "Amaia Salvador",
            "Matthias Zeppelzauer",
            "Daniel Manchon-Vizuete",
            "Andrea Calafell",
            "Xavier Giro-i-Nieto"
        ],
        "abstract": "This paper presents our contribution to the ChaLearn Challenge 2015 on Cultural Event Classification. The challenge in this task is to automatically classify images from 50 different cultural events. Our solution is based on the combination of visual features extracted from convolutional neural networks with temporal information using a hierarchical classifier scheme. We extract visual features from the last three fully connected layers of both CaffeNet (pretrained with ImageNet) and our fine tuned version for the ChaLearn challenge. We propose a late fusion strategy that trains a separate low-level SVM on each of the extracted neural codes. The class predictions of the low-level SVMs form the input to a higher level SVM, which gives the final event scores. We achieve our best result by adding a temporal refinement step into our classification scheme, which is applied directly to the output of each low-level SVM. Our approach penalizes high classification scores based on visual features when their time stamp does not match well an event-specific temporal distribution learned from the training and validation data. Our system achieved the second best result in the ChaLearn Challenge 2015 on Cultural Event Classification with a mean average precision of 0.767 on the test set.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06587",
        "title": "Semantic Motion Segmentation Using Dense CRF Formulation",
        "authors": [
            "N. Dinesh Reddy",
            "Prateek Singhal",
            "K. Madhava Krishna"
        ],
        "abstract": "While the literature has been fairly dense in the areas of scene understanding and semantic labeling there have been few works that make use of motion cues to embellish semantic performance and vice versa. In this paper, we address the problem of semantic motion segmentation, and show how semantic and motion priors augments performance. We pro- pose an algorithm that jointly infers the semantic class and motion labels of an object. Integrating semantic, geometric and optical ow based constraints into a dense CRF-model we infer both the object class as well as motion class, for each pixel. We found improvement in performance using a fully connected CRF as compared to a standard clique-based CRFs. For inference, we use a Mean Field approximation based algorithm. Our method outperforms recently pro- posed motion detection algorithms and also improves the semantic labeling compared to the state-of-the-art Automatic Labeling Environment algorithm on the challenging KITTI dataset especially for object classes such as pedestrians and cars that are critical to an outdoor robotic navigation scenario.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06591",
        "title": "Object Level Deep Feature Pooling for Compact Image Representation",
        "authors": [
            "Konda Reddy Mopuri",
            "R. Venkatesh Babu"
        ],
        "abstract": "Convolutional Neural Network (CNN) features have been successfully employed in recent works as an image descriptor for various vision tasks. But the inability of the deep CNN features to exhibit invariance to geometric transformations and object compositions poses a great challenge for image search. In this work, we demonstrate the effectiveness of the objectness prior over the deep CNN features of image regions for obtaining an invariant image representation. The proposed approach represents the image as a vector of pooled CNN features describing the underlying objects. This representation provides robustness to spatial layout of the objects in the scene and achieves invariance to general geometric transformations, such as translation, rotation and scaling. The proposed approach also leads to a compact representation of the scene, making each image occupy a smaller memory footprint. Experiments show that the proposed representation achieves state of the art retrieval results on a set of challenging benchmark image datasets, while maintaining a compact representation.\n    ",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06603",
        "title": "WxBS: Wide Baseline Stereo Generalizations",
        "authors": [
            "Dmytro Mishkin",
            "Jiri Matas",
            "Michal Perdoch",
            "Karel Lenc"
        ],
        "abstract": "We have presented a new problem -- the wide multiple baseline stereo (WxBS) -- which considers matching of images that simultaneously differ in more than one image acquisition factor such as viewpoint, illumination, sensor type or where object appearance changes significantly, e.g. over time. A new dataset with the ground truth for evaluation of matching algorithms has been introduced and will be made public.\n",
        "submission_date": "2015-04-24T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06678",
        "title": "Differential Recurrent Neural Networks for Action Recognition",
        "authors": [
            "Vivek Veeriah",
            "Naifan Zhuang",
            "Guo-Jun Qi"
        ],
        "abstract": "The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any sequential time-series data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.\n    ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06692",
        "title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images",
        "authors": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Zhiheng Huang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06719",
        "title": "Adaptive Locally Affine-Invariant Shape Matching",
        "authors": [
            "Smit Marvaniya",
            "Raj Gupta",
            "Anurag Mittal"
        ],
        "abstract": "Matching deformable objects using their shapes is an important problem in computer vision since shape is perhaps the most distinguishable characteristic of an object. The problem is difficult due to many factors such as intra-class variations, local deformations, articulations, viewpoint changes and missed and extraneous contour portions due to errors in shape extraction. While small local deformations has been handled in the literature by allowing some leeway in the matching of individual contour points via methods such as Chamfer distance and Hausdorff distance, handling more severe deformations and articulations has been done by applying local geometric corrections such as similarity or affine. However, determining which portions of the shape should be used for the geometric corrections is very hard, although some methods have been tried. In this paper, we address this problem by an efficient search for the group of contour segments to be clustered together for a geometric correction using Dynamic Programming by essentially searching for the segmentations of two shapes that lead to the best matching between them. At the same time, we allow portions of the contours to remain unmatched to handle missing and extraneous contour portions. Experiments indicate that our method outperforms other algorithms, especially when the shapes to be matched are more complex.\n    ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06740",
        "title": "SIFT Vs SURF: Quantifying the Variation in Transformations",
        "authors": [
            "Siddharth Srivastava"
        ],
        "abstract": "This paper studies the robustness of SIFT and SURF against different image transforms (rigid body, similarity, affine and projective) by quantitatively analyzing the variations in the extent of transformations. Previous studies have been comparing the two techniques on absolute transformations rather than the specific amount of deformation caused by the transformation. The paper establishes an exhaustive empirical analysis of such deformations and matching capability of SIFT and SURF with variations in matching parameters and the amount of tolerance. This is helpful in choosing the specific use case for applying these techniques.\n    ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06755",
        "title": "TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking",
        "authors": [
            "Pingmei Xu",
            "Krista A Ehinger",
            "Yinda Zhang",
            "Adam Finkelstein",
            "Sanjeev R. Kulkarni",
            "Jianxiong Xiao"
        ],
        "abstract": "Traditional eye tracking requires specialized hardware, which means collecting gaze data from many observers is expensive, tedious and slow. Therefore, existing saliency prediction datasets are order-of-magnitudes smaller than typical datasets for other vision recognition tasks. The small size of these datasets limits the potential for training data intensive algorithms, and causes overfitting in benchmark evaluation. To address this deficiency, this paper introduces a webcam-based gaze tracking system that supports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical Turk (AMTurk). By a combination of careful algorithm and gaming protocol design, our system obtains eye tracking data for saliency prediction comparable to data gathered in a traditional lab setting, with relatively lower cost and less effort on the part of the researchers. Using this tool, we build a saliency dataset for a large number of natural images. We will open-source our tool and provide a web server where researchers can upload their images to get eye tracking results from AMTurk.\n    ",
        "submission_date": "2015-04-25T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06779",
        "title": "Computational Cost Reduction in Learned Transform Classifications",
        "authors": [
            "Emerson Lopes Machado",
            "Cristiano Jacques Miosso",
            "Ricardo von Borries",
            "Murilo Coutinho",
            "Pedro de Azevedo Berger",
            "Thiago Marques",
            "Ricardo Pezzuol Jacobi"
        ],
        "abstract": "We present a theoretical analysis and empirical evaluations of a novel set of techniques for computational cost reduction of classifiers that are based on learned transform and soft-threshold. By modifying optimization procedures for dictionary and classifier training, as well as the resulting dictionary entries, our techniques allow to reduce the bit precision and to replace each floating-point multiplication by a single integer bit shift. We also show how the optimization algorithms in some dictionary training methods can be modified to penalize higher-energy dictionaries. We applied our techniques with the classifier Learning Algorithm for Soft-Thresholding, testing on the datasets used in its original paper. Our results indicate it is feasible to use solely sums and bit shifts of integers to classify at test time with a limited reduction of the classification accuracy. These low power operations are a valuable trade off in FPGA implementations as they increase the classification throughput while decrease both energy consumption and manufacturing cost.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06852",
        "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
        "authors": [
            "Philipp Fischer",
            "Alexey Dosovitskiy",
            "Eddy Ilg",
            "Philip H\u00e4usser",
            "Caner Haz\u0131rba\u015f",
            "Vladimir Golkov",
            "Patrick van der Smagt",
            "Daniel Cremers",
            "Thomas Brox"
        ],
        "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.\n",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06864",
        "title": "Fast Dictionary Matching for Content-based Image Retrieval",
        "authors": [
            "Patryk Najgebauer",
            "Janusz Rygal",
            "Tomasz Nowak",
            "Jakub Romanowski",
            "Leszek Rutkowski",
            "Sviatoslav Voloshynovskiy",
            "Rafal Scherer"
        ],
        "abstract": "This paper describes a method for searching for common sets of descriptors between collections of images. The presented method operates on local interest keypoints, which are generated using the SURF algorithm. The use of a dictionary of descriptors allowed achieving good performance of the content-based image retrieval. The method can be used to initially determine a set of similar pairs of keypoints between images. For this purpose, we use a certain level of tolerance between values of descriptors, as values of feature descriptors are almost never equal but similar between different images. After that, the method compares the structure of rotation and location of interest points in one image with the point structure in other images. Thus, we were able to find similar areas in images and determine the level of similarity between them, even when images contain different scenes.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06897",
        "title": "Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse Coding for Image Classification",
        "authors": [
            "Chengqiang Bao",
            "Liangtian He",
            "Yilun Wang"
        ],
        "abstract": "Recently sparse coding have been highly successful in image classification mainly due to its capability of incorporating the sparsity of image representation. In this paper, we propose an improved sparse coding model based on linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform (SIFT ) descriptors. The novelty is the simultaneous non-convex and non-negative characters added to the sparse coding model. Our numerical experiments show that the improved approach using non-convex and non-negative sparse coding is superior than the original ScSPM[1] on several typical databases.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06921",
        "title": "Detection and Recognition of Malaysian Special License Plate Based On SIFT Features",
        "authors": [
            "Hooi Sin Ng",
            "Yong Haur Tay",
            "Kim Meng Liang",
            "Hamam Mokayed",
            "Hock Woon Hon"
        ],
        "abstract": "Automated car license plate recognition systems are developed and applied for purpose of facilitating the surveillance, law enforcement, access control and intelligent transportation monitoring with least human intervention. In this paper, an algorithm based on SIFT feature points clustering and matching is proposed to address the issue of recognizing Malaysian special plates. These special plates do not follow the format of standard car plates as they may contain italic, cursive, connected and small letters. The algorithm is tested with 150 Malaysian special plate images under different environment and the promising experimental results demonstrate that the proposed algorithm is relatively robust.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06993",
        "title": "Compression Artifacts Reduction by a Deep Convolutional Network",
        "authors": [
            "Chao Dong",
            "Yubin Deng",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar \"easy to hard\" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use case (i.e. Twitter). In addition, we show that our method can be applied as pre-processing to facilitate other low-level vision routines when they take compressed images as input.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07021",
        "title": "On-Board Vision Processing For Small UAVs: Time to Rethink Strategy",
        "authors": [
            "Shoaib Ehsan",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "The ultimate research goal for unmanned aerial vehicles (UAVs) is to facilitate autonomy of operation. Research in the last decade has highlighted the potential of vision sensing in this regard. Although vital for accomplishment of missions assigned to any type of unmanned aerial vehicles, vision sensing is more critical for small aerial vehicles due to lack of high precision inertial sensors. In addition, uncertainty of GPS signal in indoor and urban environments calls for more reliance on vision sensing for such small vehicles. With off-line processing does not offer an attractive option in terms of autonomy, these vehicles have been challenging platforms to implement vision processing onboard due to their strict payload capacity and power budget. The strict constraints drive the need for new vision processing architectures for small unmanned aerial vehicles. Recent research has shown encouraging results with FPGA based hardware architectures. This paper reviews the bottle necks involved in implementing vision processing on-board, advocates the potential of hardware based solutions to tackle strict constraints of small unmanned aerial vehicles and finally analyzes feasibility of ASICs, Structured ASICs and FPGAs for use on future systems.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07028",
        "title": "SegSALSA-STR: A convex formulation to supervised hyperspectral image segmentation using hidden fields and structure tensor regularization",
        "authors": [
            "Filipe Condessa",
            "Jose Bioucas-Dias",
            "Jelena Kovacevic"
        ],
        "abstract": "We present a supervised hyperspectral image segmentation algorithm based on a convex formulation of a marginal maximum a posteriori segmentation with hidden fields and structure tensor regularization: Segmentation via the Constraint Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization (SegSALSA-STR). This formulation avoids the generally discrete nature of segmentation problems and the inherent NP-hardness of the integer optimization associated.\n",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07029",
        "title": "Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection",
        "authors": [
            "David Novotny",
            "Jiri Matas"
        ],
        "abstract": "A novel efficient method for extraction of object proposals is introduced. Its \"objectness\" function exploits deep spatial pyramid features, a novel fast-to-compute HoG-based edge statistic and the EdgeBoxes score. The efficiency is achieved by the use of spatial bins in a novel combination with sparsity-inducing group normalized SVM. State-of-the-art recall performance is achieved on Pascal VOC07, significantly outperforming methods with comparable speed. Interestingly, when only 100 proposals per image are considered the method attains 78% recall on VOC07. The method improves mAP of the RCNN state-of-the-art class-specific detector, increasing it by 10 points when only 50 proposals are used in each image. The system trained on twenty classes performs well on the two hundred class ILSVRC2013 set confirming generalization capability.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07082",
        "title": "Shape Representation and Classification through Pattern Spectrum and Local Binary Pattern - A Decision Level Fusion Approach",
        "authors": [
            "B.H.Shekar",
            "Bharathi Pilar"
        ],
        "abstract": "In this paper, we present a decision level fused local Morphological Pattern Spectrum(PS) and Local Binary Pattern (LBP) approach for an efficient shape representation and classification. This method makes use of Earth Movers Distance(EMD) as the measure in feature matching and shape retrieval process. The proposed approach has three major phases : Feature Extraction, Construction of hybrid spectrum knowledge base and Classification. In the first phase, feature extraction of the shape is done using pattern spectrum and local binary pattern method. In the second phase, the histograms of both pattern spectrum and local binary pattern are fused and stored in the knowledge base. In the third phase, the comparison and matching of the features, which are represented in the form of histograms, is done using Earth Movers Distance(EMD) as metric. The top-n shapes are retrieved for each query shape. The accuracy is tested by means of standard Bulls eye score method. The experiments are conducted on publicly available shape datasets like Kimia-99, Kimia-216 and MPEG-7. The comparative study is also provided with the well known approaches to exhibit the retrieval accuracy of the proposed approach.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07159",
        "title": "Combining Local Appearance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation",
        "authors": [
            "Xiaochuan Fan",
            "Kang Zheng",
            "Yuewei Lin",
            "Song Wang"
        ],
        "abstract": "We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation. Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body. Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch. Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose. The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07259",
        "title": "Image Segmentation and Restoration Using Parametric Contours With Free Endpoints",
        "authors": [
            "Heike Benninghoff",
            "Harald Garcke"
        ],
        "abstract": "In this paper, we introduce a novel approach for active contours with free endpoints. A scheme is presented for image segmentation and restoration based on a discrete version of the Mumford-Shah functional where the contours can be both closed and open curves. Additional to a flow of the curves in normal direction, evolution laws for the tangential flow of the endpoints are derived. Using a parametric approach to describe the evolving contours together with an edge-preserving denoising, we obtain a fast method for image segmentation and restoration. The analytical and numerical schemes are presented followed by numerical experiments with artificial test images and with a real medical image.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07269",
        "title": "Dynamic Body VSLAM with Semantic Constraints",
        "authors": [
            "N. Dinesh Reddy",
            "Prateek Singhal",
            "Visesh Chari",
            "K. Madhava Krishna"
        ],
        "abstract": "Image based reconstruction of urban environments is a challenging problem that deals with   optimization of large number of variables, and has several sources of errors like the presence of   dynamic objects. Since most large scale approaches  make the assumption of observing static scenes, dynamic objects are relegated to the noise   modeling section of such systems. This is an approach of convenience since the RANSAC based framework used to compute most   multiview geometric quantities for static scenes naturally confine dynamic objects to the class of outlier   measurements. However, reconstructing dynamic objects along with the static environment helps us   get a complete picture of an urban environment. Such understanding can then be used for important   robotic tasks like path planning for autonomous navigation, obstacle tracking and avoidance,  and other areas.  In this paper, we propose a system for robust SLAM that works in both static and dynamic   environments. To overcome the challenge of dynamic objects in the scene, we propose a new model   to incorporate semantic constraints into the reconstruction algorithm. While some of these  constraints are based on multi-layered dense CRFs trained over appearance as well as motion   cues, other proposed constraints can be expressed as additional terms in the bundle adjustment   optimization process that does iterative refinement of 3D structure and camera / object motion   trajectories. We show results on the challenging KITTI urban dataset for accuracy of motion   segmentation and reconstruction of the trajectory and shape of moving objects relative to ground   truth. We are able to show average relative error reduction by a significant amount for moving   object trajectory reconstruction relative to state-of-the-art methods like VISO 2,  as well as standard bundle adjustment algorithms.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07284",
        "title": "Mid-level Elements for Object Detection",
        "authors": [
            "Aayush Bansal",
            "Abhinav Shrivastava",
            "Carl Doersch",
            "Abhinav Gupta"
        ],
        "abstract": "Building on the success of recent discriminative mid-level elements, we propose a surprisingly simple approach for object detection which performs comparable to the current state-of-the-art approaches on PASCAL VOC comp-3 detection challenge (no external data). Through extensive experiments and ablation analysis, we show how our approach effectively improves upon the HOG-based pipelines by adding an intermediate mid-level representation for the task of object detection. This representation is easily interpretable and allows us to visualize what our object detector \"sees\". We also discuss the insights our approach shares with CNN-based methods, such as sharing representation between categories helps.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07339",
        "title": "Convolutional Channel Features",
        "authors": [
            "Bin Yang",
            "Junjie Yan",
            "Zhen Lei",
            "Stan Z. Li"
        ],
        "abstract": "Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07442",
        "title": "Embedded Platforms for Computer Vision-based Advanced Driver Assistance Systems: a Survey",
        "authors": [
            "Gorka Velez",
            "Oihana Otaegui"
        ],
        "abstract": "Computer Vision, either alone or combined with other technologies such as radar or Lidar, is one of the key technologies used in Advanced Driver Assistance Systems (ADAS). Its role understanding and analysing the driving scene is of great importance as it can be noted by the number of ADAS applications that use this technology. However, porting a vision algorithm to an embedded automotive system is still very challenging, as there must be a trade-off between several design requisites. Furthermore, there is not a standard implementation platform, so different alternatives have been proposed by both the scientific community and the industry. This paper aims to review the requisites and the different embedded implementation platforms that can be used for Computer Vision-based ADAS, with a critical analysis and an outlook to future trends.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07460",
        "title": "Identifying Reliable Annotations for Large Scale Image Segmentation",
        "authors": [
            "Alexander Kolesnikov",
            "Christoph H. Lampert"
        ],
        "abstract": "Challenging computer vision tasks, in particular semantic image segmentation, require large training sets of annotated images. While obtaining the actual images is often unproblematic, creating the necessary annotation is a tedious and costly process. Therefore, one often has to work with unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this work, we present a Gaussian process (GP) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed. Alternatively, the model can also just be used to identify the most reliably annotated images from the training set, which can then be used for training any other segmentation method. By relying on \"deep features\" in combination with a linear covariance function, our GP can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization. This makes our method scalable even to large datasets with several million training instances.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07469",
        "title": "Compact CNN for Indexing Egocentric Videos",
        "authors": [
            "Yair Poleg",
            "Ariel Ephrat",
            "Shmuel Peleg",
            "Chetan Arora"
        ],
        "abstract": "While egocentric video is becoming increasingly popular, browsing it is very difficult. In this paper we present a compact 3D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow.\n",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07488",
        "title": "Speeding Up Neural Networks for Large Scale Classification using WTA Hashing",
        "authors": [
            "Amir H. Bakhtiary",
            "Agata Lapedriza",
            "David Masip"
        ],
        "abstract": "In this paper we propose to use the Winner Takes All hashing technique to speed up forward propagation and backward propagation in fully connected layers in convolutional neural networks. The proposed technique reduces significantly the computational complexity, which in turn, allows us to train layers with a large number of kernels with out the associated time penalty.\n",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07575",
        "title": "Becoming the Expert - Interactive Multi-Class Machine Teaching",
        "authors": [
            "Edward Johns",
            "Oisin Mac Aodha",
            "Gabriel J. Brostow"
        ],
        "abstract": "Compared to machines, humans are extremely good at classifying images into categories, especially when they possess prior knowledge of the categories at hand. If this prior information is not available, supervision in the form of teaching images is required. To learn categories more quickly, people should see important and representative images first, followed by less important images later - or not at all. However, image-importance is individual-specific, i.e. a teaching image is important to a student if it changes their overall ability to discriminate between classes. Further, students keep learning, so while image-importance depends on their current knowledge, it also varies with time.\n",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07590",
        "title": "A Robust Lane Detection and Departure Warning System",
        "authors": [
            "Mrinal Haloi",
            "Dinesh Babu Jayagopi"
        ],
        "abstract": "In this work, we have developed a robust lane detection and departure warning technique. Our system is based on single camera sensor. For lane detection a modified Inverse Perspective Mapping using only a few extrinsic camera parameters and illuminant Invariant techniques is used. Lane markings are represented using a combination of 2nd and 4th order steerable filters, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space, and illuminant invariant representation. Lanes are assumed to be cubic curves and fitted using robust RANSAC. This method can reliably detect lanes of the road and its boundary. This method has been experimented in Indian road conditions under different challenging situations and the result obtained were very good. For lane departure angle an optical flow based method were used.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07858",
        "title": "Intelligent Health Recommendation System for Computer Users",
        "authors": [
            "Qi Guo",
            "Zixuan Wang",
            "Ming Li",
            "Hamid Aghajan"
        ],
        "abstract": "The time people spend in front of computers has been increasing steadily due to the role computers play in modern society. Individuals who sit in front of computers for an extended period of time, specifically with improper postures may incur various health issues. In this work, individuals' behaviors in front of computers are studied using web cameras. By means of non-rigid face tracking system, data are analyzed to determine the 3D head pose, blink rate and yawn frequency of computer users. When combining these visual cues, a system of intelligent personal assistants for computer users is proposed.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07889",
        "title": "Bilinear CNNs for Fine-grained Visual Recognition",
        "authors": [
            "Tsung-Yu Lin",
            "Aruni RoyChowdhury",
            "Subhransu Maji"
        ],
        "abstract": "We present a simple and effective architecture for fine-grained visual recognition called Bilinear Convolutional Neural Networks (B-CNNs). These networks represent an image as a pooled outer product of features derived from two CNNs and capture localized feature interactions in a translationally invariant manner. B-CNNs belong to the class of orderless texture representations but unlike prior work they can be trained in an end-to-end manner. Our most accurate model obtains 84.1%, 79.4%, 86.9% and 91.3% per-image accuracy on the Caltech-UCSD birds [67], NABirds [64], FGVC aircraft [42], and Stanford cars [33] dataset respectively and runs at 30 frames-per-second on a NVIDIA Titan X GPU. We then present a systematic analysis of these networks and show that (1) the bilinear features are highly redundant and can be reduced by an order of magnitude in size without significant loss in accuracy, (2) are also effective for other image classification tasks such as texture and scene recognition, and (3) can be trained from scratch on the ImageNet dataset offering consistent improvements over the baseline architecture. Finally, we present visualizations of these models on various datasets using top activations of neural units and gradient-based inversion techniques. The source code for the complete system is available at ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2017-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07901",
        "title": "Comparative study of image registration techniques for bladder video-endoscopy",
        "authors": [
            "Achraf Ben-Hamadou",
            "Charles Soussen",
            "Walter Blondel",
            "Christian Daul",
            "Didier Wolf"
        ],
        "abstract": "Bladder cancer is widely spread in the world. Many adequate diagnosis techniques exist. Video-endoscopy remains the standard clinical procedure for visual exploration of the bladder internal surface. However, video-endoscopy presents the limit that the imaged area for each image is about nearly 1cm2. And, lesions are, typically, spread over several images. The aim of this contribution is to assess the performance of two mosaicing algorithms leading to the construction of panoramic maps (one unique image) of bladder walls. The quantitative comparison study is performed on a set of real endoscopic exam data and on simulated data relative to bladder phantom.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07907",
        "title": "A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching",
        "authors": [
            "Quynh Nguyen",
            "Antoine Gautier",
            "Matthias Hein"
        ],
        "abstract": "The estimation of correspondences between two images resp. point sets is a core problem in computer vision. One way to formulate the problem is graph matching leading to the quadratic assignment problem which is NP-hard. Several so called second order methods have been proposed to solve this problem. In recent years hypergraph matching leading to a third order problem became popular as it allows for better integration of geometric information. For most of these third order algorithms no theoretical guarantees are known. In this paper we propose a general framework for tensor block coordinate ascent methods for hypergraph matching. We propose two algorithms which both come along with the guarantee of monotonic ascent in the matching score on the set of discrete assignment matrices. In the experiments we show that our new algorithms outperform previous work both in terms of achieving better matching scores and matching accuracy. This holds in particular for very challenging settings where one has a high number of outliers and other forms of noise.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07918",
        "title": "Robust hyperspectral image classification with rejection fields",
        "authors": [
            "Filipe Condessa",
            "Jose Bioucas-Dias",
            "Jelena Kovacevic"
        ],
        "abstract": "In this paper we present a novel method for robust hyperspectral image classification using context and rejection. Hyperspectral image classification is generally an ill-posed image problem where pixels may belong to unknown classes, and obtaining representative and complete training sets is costly. Furthermore, the need for high classification accuracies is frequently greater than the need to classify the entire image.\n",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07947",
        "title": "Patch-based Convolutional Neural Network for Whole Slide Tissue Image Classification",
        "authors": [
            "Le Hou",
            "Dimitris Samaras",
            "Tahsin M. Kurc",
            "Yi Gao",
            "James E. Davis",
            "Joel H. Saltz"
        ],
        "abstract": "Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07958",
        "title": "Exploring Integral Image Word Length Reduction Techniques for SURF Detector",
        "authors": [
            "Shoaib Ehsan",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "Speeded Up Robust Features (SURF) is a state of the art computer vision algorithm that relies on integral image representation for performing fast detection and description of image features that are scale and rotation invariant. Integral image representation, however, has major draw back of large binary word length that leads to substantial increase in memory size. When designing a dedicated hardware to achieve real-time performance for the SURF algorithm, it is imperative to consider the adverse effects of integral image on memory size, bus width and computational resources. With the objective of minimizing hardware resources, this paper presents a novel implementation concept of a reduced word length integral image based SURF detector. It evaluates two existing word length reduction techniques for the particular case of SURF detector and extends one of these to achieve more reduction in word length. This paper also introduces a novel method to achieve integral image word length reduction for SURF detector.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07962",
        "title": "Hardware based Scale- and Rotation-Invariant Feature Extraction: A Retrospective Analysis and Future Directions",
        "authors": [
            "Shoaib Ehsan",
            "Adrian F. Clark",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "Computer Vision techniques represent a class of algorithms that are highly computation and data intensive in nature. Generally, performance of these algorithms in terms of execution speed on desktop computers is far from real-time. Since real-time performance is desirable in many applications, special-purpose hardware is required in most cases to achieve this goal. Scale- and rotation-invariant local feature extraction is a low level computer vision task with very high computational complexity. The state-of-the-art algorithms that currently exist in this domain, like SIFT and SURF, suffer from slow execution speeds and at best can only achieve rates of 2-3 Hz on modern desktop computers. Hardware-based scale- and rotation-invariant local feature extraction is an emerging trend enabling real-time performance for these computationally complex algorithms. This paper takes a retrospective look at the advances made so far in this field, discusses the hardware design strategies employed and results achieved, identifies current research gaps and suggests future research directions.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07967",
        "title": "Improved repeatability measures for evaluating performance of feature detectors",
        "authors": [
            "Shoaib Ehsan",
            "Nadia Kanwal",
            "Adrian F. Clark",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "The most frequently employed measure for performance characterisation of local feature detectors is repeatability, but it has been observed that this does not necessarily mirror actual performance. Presented are improved repeatability formulations which correlate much better with the true performance of feature detectors. Comparative results for several state-of-the-art feature detectors are presented using these measures; it is found that Hessian-based detectors are generally superior at identifying features when images are subject to various geometric and photometric transformations.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08023",
        "title": "Anticipating Visual Representations from Unlabeled Video",
        "authors": [
            "Carl Vondrick",
            "Hamed Pirsiavash",
            "Antonio Torralba"
        ],
        "abstract": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2016-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08083",
        "title": "Fast R-CNN",
        "authors": [
            "Ross Girshick"
        ],
        "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08200",
        "title": "Predicting People's 3D Poses from Short Sequences",
        "authors": [
            "Bugra Tekin",
            "Xiaolu Sun",
            "Xinchao Wang",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people. Instead of computing candidate poses in individual frames and then linking them, as is often done, we regress directly from a spatio-temporal block of frames to a 3D pose in the central one. We will demonstrate that this approach allows us to effectively overcome ambiguities and to improve upon the state-of-the-art on challenging sequences.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08219",
        "title": "Hierarchical Subquery Evaluation for Active Learning on a Graph",
        "authors": [
            "Oisin Mac Aodha",
            "Neill D.F. Campbell",
            "Jan Kautz",
            "Gabriel J. Brostow"
        ],
        "abstract": "To train good supervised and semi-supervised object classifiers, it is critical that we not waste the time of the human experts who are providing the training labels. Existing active learning strategies can have uneven performance, being efficient on some datasets but wasteful on others, or inconsistent just between runs on the same dataset. We propose perplexity based graph construction and a new hierarchical subquery evaluation algorithm to combat this variability, and to release the potential of Expected Error Reduction.\n",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08289",
        "title": "Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks",
        "authors": [
            "Marcel Simon",
            "Erik Rodner"
        ],
        "abstract": "Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, NA birds, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios. The source code of our method is available online at ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08308",
        "title": "Efficient Image-Space Extraction and Representation of 3D Surface Topography",
        "authors": [
            "Matthias Zeppelzauer",
            "Markus Seidl"
        ],
        "abstract": "Surface topography refers to the geometric micro-structure of a surface and defines its tactile characteristics (typically in the sub-millimeter range). High-resolution 3D scanning techniques developed recently enable the 3D reconstruction of surfaces including their surface topography. In his paper, we present an efficient image-space technique for the extraction of surface topography from high-resolution 3D reconstructions. Additionally, we filter noise and enhance topographic attributes to obtain an improved representation for subsequent topography classification. Comprehensive experiments show that the our representation captures well topographic attributes and significantly improves classification performance compared to alternative 2D and 3D representations.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08362",
        "title": "PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions",
        "authors": [
            "Michael Figurnov",
            "Aijan Ibraimova",
            "Dmitry Vetrov",
            "Pushmeet Kohli"
        ],
        "abstract": "We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we show that perforation is complementary to the recently proposed acceleration method of Zhang et al.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2016-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00040",
        "title": "Overlapping and Non-overlapping Camera Layouts for Robot Pose Estimation",
        "authors": [
            "Mohammad Ehab Ragab"
        ],
        "abstract": "We study the use of overlapping and non-overlapping camera layouts in estimating the ego-motion of a moving robot. To estimate the location and orientation of the robot, we investigate using four cameras as non-overlapping individuals, and as two stereo pairs. The pros and cons of the two approaches are elucidated. The cameras work independently and can have larger field of view in the non-overlapping layout. However, a scale factor ambiguity should be dealt with. On the other hand, stereo systems provide more accuracy but require establishing feature correspondence with more computational demand. For both approaches, the extended Kalman filter is used as a real-time recursive estimator. The approaches studied are verified with synthetic and real experiments alike.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00066",
        "title": "Pose Induction for Novel Object Categories",
        "authors": [
            "Shubham Tulsiani",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "We address the task of predicting pose for objects of unannotated object categories from a small seed set of annotated object classes. We present a generalized classifier that can reliably induce pose given a single instance of a novel category. In case of availability of a large collection of novel instances, our approach then jointly reasons over all instances to improve the initial estimates. We empirically validate the various components of our algorithm and quantitatively show that our method produces reliable pose estimates. We also show qualitative results on a diverse set of classes and further demonstrate the applicability of our system for learning shape models of novel object classes.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00074",
        "title": "Image Denoising using Optimally Weighted Bilateral Filters: A Sure and Fast Approach",
        "authors": [
            "Kunal N. Chaudhury",
            "Kollipara Rithwik"
        ],
        "abstract": "The bilateral filter is known to be quite effective in denoising images corrupted with small dosages of additive Gaussian noise. The denoising performance of the filter, however, is known to degrade quickly with the increase in noise level. Several adaptations of the filter have been proposed in the literature to address this shortcoming, but often at a substantial computational overhead. In this paper, we report a simple pre-processing step that can substantially improve the denoising performance of the bilateral filter, at almost no additional cost. The modified filter is designed to be robust at large noise levels, and often tends to perform poorly below a certain noise threshold. To get the best of the original and the modified filter, we propose to combine them in a weighted fashion, where the weights are chosen to minimize (a surrogate of) the oracle mean-squared-error (MSE). The optimally-weighted filter is thus guaranteed to perform better than either of the component filters in terms of the MSE, at all noise levels. We also provide a fast algorithm for the weighted filtering. Visual and quantitative denoising results on standard test images are reported which demonstrate that the improvement over the original filter is significant both visually and in terms of PSNR. Moreover, the denoising performance of the optimally-weighted bilateral filter is competitive with the computation-intensive non-local means filter.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00077",
        "title": "Fast and Accurate Bilateral Filtering using Gauss-Polynomial Decomposition",
        "authors": [
            "Kunal N. Chaudhury"
        ],
        "abstract": "The bilateral filter is a versatile non-linear filter that has found diverse applications in image processing, computer vision, computer graphics, and computational photography. A widely-used form of the filter is the Gaussian bilateral filter in which both the spatial and range kernels are Gaussian. A direct implementation of this filter requires $O(\\sigma^2)$ operations per pixel, where $\\sigma$ is the standard deviation of the spatial Gaussian. In this paper, we propose an accurate approximation algorithm that can cut down the computational complexity to $O(1)$ per pixel for any arbitrary $\\sigma$ (constant-time implementation). This is based on the observation that the range kernel operates via the translations of a fixed Gaussian over the range space, and that these translated Gaussians can be accurately approximated using the so-called Gauss-polynomials. The overall algorithm emerging from this approximation involves a series of spatial Gaussian filtering, which can be implemented in constant-time using separability and recursion. We present some preliminary results to demonstrate that the proposed algorithm compares favorably with some of the existing fast algorithms in terms of speed and accuracy.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00110",
        "title": "The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs",
        "authors": [
            "Hongping Cai",
            "Qi Wu",
            "Tadeo Corradi",
            "Peter Hall"
        ],
        "abstract": "The cross-depiction problem is that of recognising visual objects regardless of whether they are photographed, painted, drawn, etc. It is a potentially significant yet under-researched problem. Emulating the remarkable human ability to recognise objects in an astonishingly wide variety of depictive forms is likely to advance both the foundations and the applications of Computer Vision.\n",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00145",
        "title": "Quality Control in Crowdsourced Object Segmentation",
        "authors": [
            "Ferran Cabezas",
            "Axel Carlier",
            "Amaia Salvador",
            "Xavier Gir\u00f3-i-Nieto",
            "Vincent Charvillat"
        ],
        "abstract": "This paper explores processing techniques to deal with noisy data in crowdsourced object segmentation tasks. We use the data collected with \"Click'n'Cut\", an online interactive segmentation tool, and we perform several experiments towards improving the segmentation results. First, we introduce different superpixel-based techniques to filter users' traces, and assess their impact on the segmentation result. Second, we present different criteria to detect and discard the traces from potential bad users, resulting in a remarkable increase in performance. Finally, we show a novel superpixel-based segmentation algorithm which does not require any prior filtering and is based on weighting each user's contribution according to his/her level of expertise.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00171",
        "title": "SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes",
        "authors": [
            "Ankur Handa",
            "Viorica Patraucean",
            "Vijay Badrinarayanan",
            "Simon Stent",
            "Roberto Cipolla"
        ],
        "abstract": "We are interested in automatic scene understanding from geometric cues. To this end, we aim to bring semantic segmentation in the loop of real-time reconstruction. Our semantic segmentation is built on a deep autoencoder stack trained exclusively on synthetic depth data generated from our novel 3D scene library, SynthCam3D. Importantly, our network is able to segment real world scenes without any noise modelling. We present encouraging preliminary results.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00192",
        "title": "Application of S-Transform on Hyper kurtosis based Modified Duo Histogram Equalized DIC images for Pre-cancer Detection",
        "authors": [
            "Sabyasachi Mukhopadhyay",
            "Soham Mandal",
            "Sawon Pratiher",
            "Ritwik Barman",
            "M.Venkatesh",
            "Nirmalya Ghosh",
            "Prasanta K. Panigrahi"
        ],
        "abstract": "Our proposed hyper kurtosis based histogram equalized DIC images enhances the contrast by preserving the brightness. The evolution and development of precancerous activity among tissues are studied through S-transform (ST). The significant variations of amplitude spectra can be observed due to increased medium roughness from normal tissue were observed in time-frequency domain. The randomness and inhomogeneity of the tissue structures among human normal and different grades of DIC tissues is recognized by ST based timefrequency analysis. This study offers a simpler and better way to recognize the substantial changes among different stages of DIC tissues, which are reflected by spatial information containing within the inhomogeneity structures of different types of tissue.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00193",
        "title": "Segmentation and Restoration of Images on Surfaces by Parametric Active Contours with Topology Changes",
        "authors": [
            "Heike Benninghoff",
            "Harald Garcke"
        ],
        "abstract": "In this article, a new method for segmentation and restoration of images on two-dimensional surfaces is given. Active contour models for image segmentation are extended to images on surfaces. The evolving curves on the surfaces are mathematically described using a parametric approach. For image restoration, a diffusion equation with Neumann boundary conditions is solved in a postprocessing step in the individual regions. Numerical schemes are presented which allow to efficiently compute segmentations and denoised versions of images on surfaces. Also topology changes of the evolving curves are detected and performed using a fast sub-routine. Finally, several experiments are presented where the developed methods are applied on different artificial and real images defined on different surfaces.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00218",
        "title": "Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions",
        "authors": [
            "Yuri Boykov",
            "Hossam Isack",
            "Carl Olsson",
            "Ismail Ben Ayed"
        ],
        "abstract": "Many standard optimization methods for segmentation and reconstruction compute ML model estimates for appearance or geometry of segments, e.g. Zhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012. We observe that the standard likelihood term in these formulations corresponds to a generalized probabilistic K-means energy. In learning it is well known that this energy has a strong bias to clusters of equal size, which can be expressed as a penalty for KL divergence from a uniform distribution of cardinalities. However, this volumetric bias has been mostly ignored in computer vision. We demonstrate significant artifacts in standard segmentation and reconstruction methods due to this bias. Moreover, we propose binary and multi-label optimization techniques that either (a) remove this bias or (b) replace it by a KL divergence term for any given target volume distribution. Our general ideas apply to many continuous or discrete energy formulations in segmentation, stereo, and other reconstruction problems.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00249",
        "title": "Image Segmentation by Size-Dependent Single Linkage Clustering of a Watershed Basin Graph",
        "authors": [
            "Aleksandar Zlateski",
            "H.Sebastian Seung"
        ],
        "abstract": "We present a method for hierarchical image segmentation that defines a disaffinity graph on the image, over-segments it into watershed basins, defines a new graph on the basins, and then merges basins with a modified, size-dependent version of single linkage clustering. The quasilinear runtime of the method makes it suitable for segmenting large images. We illustrate the method on the challenging problem of segmenting 3D electron microscopic brain images.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00256",
        "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving",
        "authors": [
            "Chenyi Chen",
            "Ari Seff",
            "Alain Kornhauser",
            "Jianxiong Xiao"
        ],
        "abstract": "Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00276",
        "title": "Joint Object and Part Segmentation using Deep Learned Potentials",
        "authors": [
            "Peng Wang",
            "Xiaohui Shen",
            "Zhe Lin",
            "Scott Cohen",
            "Brian Price",
            "Alan Yuille"
        ],
        "abstract": "Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-channel fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.\n    ",
        "submission_date": "2015-05-01T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00295",
        "title": "Dense Optical Flow Prediction from a Static Image",
        "authors": [
            "Jacob Walker",
            "Abhinav Gupta",
            "Martial Hebert"
        ],
        "abstract": "Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.\n    ",
        "submission_date": "2015-05-02T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00296",
        "title": "Object-Scene Convolutional Neural Networks for Event Recognition in Images",
        "authors": [
            "Limin Wang",
            "Zhe Wang",
            "Wenbin Du",
            "Yu Qiao"
        ],
        "abstract": "Event recognition from still images is of great importance for image understanding. However, compared with event recognition in videos, there are much fewer research works on event recognition in images. This paper addresses the issue of event recognition from images and proposes an effective method with deep neural networks. Specifically, we design a new architecture, called Object-Scene Convolutional Neural Network (OS-CNN). This architecture is decomposed into object net and scene net, which extract useful information for event understanding from the perspective of objects and scene context, respectively. Meanwhile, we investigate different network architectures for OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks to the task of event recognition. Furthermore, we find that the deep and very-deep networks are complementary to each other. Finally, based on the proposed OS-CNN and comparative study of different network architectures, we come up with a solution of five-stream CNN for the track of cultural event recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method obtains the performance of 85.5% and ranks the $1^{st}$ place in this challenge.\n    ",
        "submission_date": "2015-05-02T00:00:00",
        "last_modified_date": "2015-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00308",
        "title": "Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models",
        "authors": [
            "Tejaswi Nimmagadda",
            "Anima Anandkumar"
        ],
        "abstract": "Deep learning has shown state-of-art classification performance on datasets such as ImageNet, which contain a single object in each image. However, multi-object classification is far more challenging. We present a unified framework which leverages the strengths of multiple machine learning methods, viz deep learning, probabilistic models and kernel methods to obtain state-of-art performance on Microsoft COCO, consisting of non-iconic images. We incorporate contextual information in natural images through a conditional latent tree probabilistic model (CLTM), where the object co-occurrences are conditioned on the extracted fc7 features from pre-trained Imagenet CNN as input. We learn the CLTM tree structure using conditional pairwise probabilities for object co-occurrences, estimated through kernel methods, and we learn its node and edge potentials by training a new 3-layer neural network, which takes fc7 features as input. Object classification is carried out via inference on the learnt conditional tree model, and we obtain significant gain in precision-recall and F-measures on MS-COCO, especially for difficult object categories. Moreover, the latent variables in the CLTM capture scene information: the images with top activations for a latent node have common themes such as being a grasslands or a food scene, and on on. In addition, we show that a simple k-means clustering of the inferred latent nodes alone significantly improves scene classification performance on the MIT-Indoor dataset, without the need for any retraining, and without using scene labels during training. Thus, we present a unified framework for multi-object classification and unsupervised scene understanding.\n    ",
        "submission_date": "2015-05-02T00:00:00",
        "last_modified_date": "2015-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00315",
        "title": "Learning Temporal Embeddings for Complex Video Analysis",
        "authors": [
            "Vignesh Ramanathan",
            "Kevin Tang",
            "Greg Mori",
            "Li Fei-Fei"
        ],
        "abstract": "In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video.\n    ",
        "submission_date": "2015-05-02T00:00:00",
        "last_modified_date": "2015-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00353",
        "title": "Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos",
        "authors": [
            "Xi Yin",
            "Xiaoming Liu",
            "Jin Chen",
            "David M. Kramer"
        ],
        "abstract": "This paper proposes a novel framework for fluorescence plant video processing. The plant research community is interested in the leaf-level photosynthetic analysis within a plant. A prerequisite for such analysis is to segment all leaves, estimate their structures, and track them over time. We identify this as a joint multi-leaf segmentation, alignment, and tracking problem. First, leaf segmentation and alignment are applied on the last frame of a plant video to find a number of well-aligned leaf candidates. Second, leaf tracking is applied on the remaining frames with leaf candidate transformation from the previous frame. We form two optimization problems with shared terms in their objective functions for leaf alignment and tracking respectively. A quantitative evaluation framework is formulated to evaluate the performance of our algorithm with four metrics. Two models are learned to predict the alignment accuracy and detect tracking failure respectively in order to provide guidance for subsequent plant biology analysis. The limitation of our algorithm is also studied. Experimental results show the effectiveness, efficiency, and robustness of the proposed method.\n    ",
        "submission_date": "2015-05-02T00:00:00",
        "last_modified_date": "2017-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00389",
        "title": "Detail-preserving and Content-aware Variational Multi-view Stereo Reconstruction",
        "authors": [
            "Zhaoxin Li",
            "Kuanquan Wang",
            "Wangmeng Zuo",
            "Deyu Meng",
            "Lei Zhang"
        ],
        "abstract": "Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view images is a fundamental yet active research area in computer vision. Despite the steady progress in multi-view stereo reconstruction, most existing methods are still limited in recovering fine-scale details and sharp features while suppressing noises, and may fail in reconstructing regions with few textures. To address these limitations, this paper presents a Detail-preserving and Content-aware Variational (DCV) multi-view stereo method, which reconstructs the 3D surface by alternating between reprojection error minimization and mesh denoising. In reprojection error minimization, we propose a novel inter-image similarity measure, which is effective to preserve fine-scale details of the reconstructed surface and builds a connection between guided image filtering and image registration. In mesh denoising, we propose a content-aware $\\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value and regularization parameters based on the current input. It is much more promising in suppressing noise while preserving sharp features than conventional isotropic mesh smoothing. Experimental results on benchmark datasets demonstrate that our DCV method is capable of recovering more surface details, and obtains cleaner and more accurate reconstructions than state-of-the-art methods. In particular, our method achieves the best results among all published methods on the Middlebury dino ring and dino sparse ring datasets in terms of both completeness and accuracy.\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00393",
        "title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks",
        "authors": [
            "Francesco Visin",
            "Kyle Kastner",
            "Kyunghyun Cho",
            "Matteo Matteucci",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00412",
        "title": "On a fast bilateral filtering formulation using functional rearrangements",
        "authors": [
            "Gonzalo Galiano",
            "Juli\u00e1n Velasco"
        ],
        "abstract": "We introduce an exact reformulation of a broad class of neighborhood filters, among which the bilateral filters, in terms of two functional rearrangements: the decreasing and the relative rearrangements.\n",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00424",
        "title": "Electron Neutrino Classification in Liquid Argon Time Projection Chamber Detector",
        "authors": [
            "Piotr P\u0142o\u0144ski",
            "Dorota Stefan",
            "Robert Sulej",
            "Krzysztof Zaremba"
        ],
        "abstract": "Neutrinos are one of the least known elementary particles. The detection of neutrinos is an extremely difficult task since they are affected only by weak sub-atomic force or gravity. Therefore large detectors are constructed to reveal neutrino's properties. Among them the Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. The computerized methods for automatic reconstruction and identification of particles are needed to fully exploit the potential of the LAr-TPC technique. Herein, the novel method for electron neutrino classification is presented. The method constructs a feature descriptor from images of observed event. It characterizes the signal distribution propagated from vertex of interest, where the particle interacts with the detector medium. The classifier is learned with a constructed feature descriptor to decide whether the images represent the electron neutrino or cascade produced by photons. The proposed approach assumes that the position of primary interaction vertex is known. The method's performance in dependency to the noise in a primary vertex position and deposited energy of particles is studied.\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00432",
        "title": "Object Class Detection and Classification using Multi Scale Gradient and Corner Point based Shape Descriptors",
        "authors": [
            "Basura Fernando",
            "Sezer Karaoglu",
            "Sajib Kumar Saha"
        ],
        "abstract": "This paper presents a novel multi scale gradient and a corner point based shape descriptors. The novel multi scale gradient based shape descriptor is combined with generic Fourier descriptors to extract contour and region based shape information. Shape information based object class detection and classification technique with a random forest classifier has been optimized. Proposed integrated descriptor in this paper is robust to rotation, scale, translation, affine deformations, noisy contours and noisy shapes. The new corner point based interpolated shape descriptor has been exploited for fast object detection and classification with higher accuracy.\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-05-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00487",
        "title": "Sequence to Sequence -- Video to Text",
        "authors": [
            "Subhashini Venugopalan",
            "Marcus Rohrbach",
            "Jeff Donahue",
            "Raymond Mooney",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Real-world videos often have complex dynamics; and methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).\n    ",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00523",
        "title": "Modeling Representation of Videos for Anomaly Detection using Deep Learning: A Review",
        "authors": [
            "Yong Shean Chong",
            "Yong Haur Tay"
        ],
        "abstract": "This review article surveys the current progresses made toward video-based anomaly detection. We address the most fundamental aspect for video anomaly detection, that is, video feature representation. Much research works have been done in finding the right representation to perform anomaly detection in video streams accurately with an acceptable false alarm rate. However, this is very challenging due to large variations in environment and human movement, and high space-time complexity due to huge dimensionality of video data. The weakly supervised nature of deep learning algorithms can help in learning representations from the video data itself instead of manually designing the right feature for specific scenes. In this paper, we would like to review the existing methods of modeling video representations using deep learning techniques for the task of anomaly detection and action recognition.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00529",
        "title": "Learning Document Image Binarization from Data",
        "authors": [
            "Yue Wu",
            "Stephen Rawls",
            "Wael AbdAlmageed",
            "Premkumar Natarajan"
        ],
        "abstract": "In this paper we present a fully trainable binarization solution for degraded document images. Unlike previous attempts that often used simple features with a series of pre- and post-processing, our solution encodes all heuristics about whether or not a pixel is foreground text into a high-dimensional feature vector and learns a more complicated decision function. In particular, we prepare features of three types: 1) existing features for binarization such as intensity [1], contrast [2], [3], and Laplacian [4], [5]; 2) reformulated features from existing binarization decision functions such those in [6] and [7]; and 3) our newly developed features, namely the Logarithm Intensity Percentile (LIP) and the Relative Darkness Index (RDI). Our initial experimental results show that using only selected samples (about 1.5% of all available training data), we can achieve a binarization performance comparable to those fine-tuned (typically by hand), state-of-the-art methods. Additionally, the trained document binarization classifier shows good generalization capabilities on out-of-domain data.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00571",
        "title": "Higher Order Maximum Persistency and Comparison Theorems",
        "authors": [
            "Alexander Shekhovtsov"
        ],
        "abstract": "We address combinatorial problems that can be formulated as minimization of a partially separable function of discrete variables (energy minimization in graphical models, weighted constraint satisfaction, pseudo-Boolean optimization, 0-1 polynomial programming). For polyhedral relaxations of such problems it is generally not true that variables integer in the relaxed solution will retain the same values in the optimal discrete solution. Those which do are called persistent. Such persistent variables define a part of a globally optimal solution. Once identified, they can be excluded from the problem, reducing its size.\n",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00581",
        "title": "Activity recognition from videos with parallel hypergraph matching on GPUs",
        "authors": [
            "Eric Lombardi",
            "Christian Wolf",
            "Oya Celiktutan",
            "B\u00fclent Sankur"
        ],
        "abstract": "In this paper, we propose a method for activity recognition from videos based on sparse local features and hypergraph matching. We benefit from special properties of the temporal domain in the data to derive a sequential and fast graph matching algorithm for GPUs.\n",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00663",
        "title": "See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG",
        "authors": [
            "Wei-Chen Chiu",
            "Mario Fritz"
        ],
        "abstract": "The Histogram of Oriented Gradient (HOG) descriptor has led to many advances in computer vision over the last decade and is still part of many state of the art approaches. We realize that the associated feature computation is piecewise differentiable and therefore many pipelines which build on HOG can be made differentiable. This lends to advanced introspection as well as opportunities for end-to-end optimization. We present our implementation of $\\nabla$HOG based on the auto-differentiation toolbox Chumpy and show applications to pre-image visualization and pose estimation which extends the existing differentiable renderer OpenDR pipeline. Both applications improve on the respective state-of-the-art HOG approaches.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00670",
        "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation",
        "authors": [
            "Hoo-Chang Shin",
            "Le Lu",
            "Lauren Kim",
            "Ari Seff",
            "Jianhua Yao",
            "Ronald M. Summers"
        ],
        "abstract": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of representative ~216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on document- and sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of large-scale learning and prediction in electronic patient records available in most modern clinical institutions.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00687",
        "title": "Unsupervised Learning of Visual Representations using Videos",
        "authors": [
            "Xiaolong Wang",
            "Abhinav Gupta"
        ],
        "abstract": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00737",
        "title": "A Gaussian Scale Space Approach For Exudates Detection, Classification And Severity Prediction",
        "authors": [
            "Mrinal Haloi",
            "Samarendra Dandapat",
            "Rohit Sinha"
        ],
        "abstract": "In the context of Computer Aided Diagnosis system for diabetic retinopathy, we present a novel method for detection of exudates and their classification for disease severity prediction. The method is based on Gaussian scale space based interest map and mathematical morphology. It makes use of support vector machine for classification and location information of the optic disc and the macula region for severity prediction. It can efficiently handle luminance variation and it is suitable for varied sized exudates. The method has been probed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate detection the proposed method achieved a sensitivity of 96.54% and prediction of 98.35% in DIARETDB1V2 database.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00855",
        "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature",
        "authors": [
            "Babak Saleh",
            "Ahmed Elgammal"
        ],
        "abstract": "In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Measuring the visual similarity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these features. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting's style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00866",
        "title": "Adaptive diffusion constrained total variation scheme with application to `cartoon + texture + edge' image decomposition",
        "authors": [
            "Juan C. Moreno",
            "V. B. Surya Prasath",
            "D. Vorotnikov",
            "H. Proenca",
            "K. Palaniappan"
        ],
        "abstract": "We consider an image decomposition model involving a variational (minimization) problem and an evolutionary partial differential equation (PDE). We utilize a linear inhomogenuous diffusion constrained and weighted total variation (TV) scheme for image adaptive decomposition. An adaptive weight along with TV regularization splits a given image into three components representing the geometrical (cartoon), textural (small scale - microtextures), and edges (big scale - macrotextures). We study the wellposedness of the coupled variational-PDE scheme along with an efficient numerical scheme based on Chambolle's dual minimization method. We provide extensive experimental results in cartoon-texture-edges decomposition, and denoising as well compare with other related variational, coupled anisotropic diffusion PDE based methods.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00880",
        "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition",
        "authors": [
            "Hang Su",
            "Subhransu Maji",
            "Evangelos Kalogerakis",
            "Erik Learned-Miller"
        ],
        "abstract": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00996",
        "title": "Fast Guided Filter",
        "authors": [
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "The guided filter is a technique for edge-aware image filtering. Because of its nice visual quality, fast speed, and ease of implementation, the guided filter has witnessed various applications in real products, such as image editing apps in phones and stereo reconstruction, and has been included in official MATLAB and OpenCV. In this note, we remind that the guided filter can be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In a variety of applications, this leads to a speedup of >10x with almost no visible degradation. We hope this acceleration will improve performance of current applications and further popularize this filter. Code is released.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01065",
        "title": "Proceedings of The 39th Annual Workshop of the Austrian Association for Pattern Recognition (OAGM), 2015",
        "authors": [
            "Sebastian Hegenbart",
            "Roland Kwitt",
            "Andreas Uhl"
        ],
        "abstract": "The 39th annual workshop of the Austrian Association for Pattern Recognition (OAGM/AAPR) provides a platform for presentation and discussion of research progress as well as research projects within the OAGM/AAPR community.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01085",
        "title": "In Defense of the Direct Perception of Affordances",
        "authors": [
            "David F. Fouhey",
            "Xiaolong Wang",
            "Abhinav Gupta"
        ],
        "abstract": "The field of functional recognition or affordance estimation from images has seen a revival in recent years. As originally proposed by Gibson, the affordances of a scene were directly perceived from the ambient light: in other words, functional properties like sittable were estimated directly from incoming pixels. Recent work, however, has taken a mediated approach in which affordances are derived by first estimating semantics or geometry and then reasoning about the affordances. In a tribute to Gibson, this paper explores his theory of affordances as originally proposed. We propose two approaches for direct perception of affordances and show that they obtain good results and can out-perform mediated approaches. We hope this paper can rekindle discussion around direct perception and its implications in the long term.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01121",
        "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01130",
        "title": "Visual Summary of Egocentric Photostreams by Representative Keyframes",
        "authors": [
            "Marc Bola\u00f1os",
            "Ricard Mestre",
            "Estefan\u00eda Talavera",
            "Xavier Gir\u00f3-i-Nieto",
            "Petia Radeva"
        ],
        "abstract": "Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01173",
        "title": "Deep Learning for Object Saliency Detection and Image Segmentation",
        "authors": [
            "Hengyue Pan",
            "Bo Wang",
            "Hui Jiang"
        ],
        "abstract": "In this paper, we propose several novel deep learning methods for object saliency detection based on the powerful convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify an input image based on the pixel-wise gradients to reduce a cost function measuring the class-specific objectness of the image. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. The discrepancy between the modified image and the original one may be used as a saliency map for the image. Moreover, we have further proposed several new training methods to learn saliency-specific convolutional nets for object saliency detection, in order to leverage the available pixel-wise segmentation information. Our methods are extremely computationally efficient (processing 20-40 images per second in one GPU). In this work, we use the computed saliency maps for image segmentation. Experimental results on two benchmark tasks, namely Microsoft COCO and Pascal VOC 2012, have shown that our proposed methods can generate high-quality salience maps, clearly outperforming many existing methods. In particular, our approaches excel in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01197",
        "title": "Contextual Action Recognition with R*CNN",
        "authors": [
            "Georgia Gkioxari",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2016-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01257",
        "title": "A Deeper Look at Dataset Bias",
        "authors": [
            "Tatiana Tommasi",
            "Novi Patricia",
            "Barbara Caputo",
            "Tinne Tuytelaars"
        ],
        "abstract": "The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this paper we propose to verify the potential of the DeCAF features when facing the dataset bias problem. We conduct a series of analyses looking at how existing datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01350",
        "title": "Classification of Occluded Objects using Fast Recurrent Processing",
        "authors": [
            "Ozgur Yilmaz"
        ],
        "abstract": "Recurrent neural networks are powerful tools for handling incomplete data problems in computer vision, thanks to their significant generative capabilities. However, the computational demand for these algorithms is too high to work in real time, without specialized hardware or software solutions. In this paper, we propose a framework for augmenting recurrent processing capabilities into a feedforward network without sacrificing much from computational efficiency. We assume a mixture model and generate samples of the last hidden layer according to the class decisions of the output layer, modify the hidden layer activity using the samples, and propagate to lower layers. For visual occlusion problem, the iterative procedure emulates feedforward-feedback loop, filling-in the missing hidden layer activity with meaningful representations. The proposed algorithm is tested on a widely used dataset, and shown to achieve 2$\\times$ improvement in classification accuracy for occluded objects. When compared to Restricted Boltzmann Machines, our algorithm shows superior performance for occluded object classification.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01429",
        "title": "Geometry-Aware Neighborhood Search for Learning Local Models for Image Reconstruction",
        "authors": [
            "Julio Cesar Ferreira",
            "Elif Vural",
            "Christine Guillemot"
        ],
        "abstract": "Local learning of sparse image models has proven to be very effective to solve inverse problems in many computer vision applications. To learn such models, the data samples are often clustered using the K-means algorithm with the Euclidean distance as a dissimilarity metric. However, the Euclidean distance may not always be a good dissimilarity measure for comparing data samples lying on a manifold. In this paper, we propose two algorithms for determining a local subset of training samples from which a good local model can be computed for reconstructing a given input test sample, where we take into account the underlying geometry of the data. The first algorithm, called Adaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme which can be seen as an out-of-sample extension of the replicator graph clustering method for local model learning. The second method, called Geometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive alternative for training subset selection. The proposed AGNN and GOC methods are evaluated in image super-resolution, deblurring and denoising applications and shown to outperform spectral clustering, soft clustering, and geodesic distance based subset selection in most settings.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01554",
        "title": "Webly Supervised Learning of Convolutional Networks",
        "authors": [
            "Xinlei Chen",
            "Abhinav Gupta"
        ],
        "abstract": "We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01560",
        "title": "Adaptive Nonparametric Image Parsing",
        "authors": [
            "Tam V. Nguyen",
            "Canyi Lu",
            "Jose Sepulveda",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we present an adaptive nonparametric solution to the image parsing task, namely annotating each image pixel with its corresponding category label. For a given test image, first, a locality-aware retrieval set is extracted from the training data based on super-pixel matching similarities, which are augmented with feature extraction for better differentiation of local super-pixels. Then, the category of each super-pixel is initialized by the majority vote of the $k$-nearest-neighbor super-pixels in the retrieval set. Instead of fixing $k$ as in traditional non-parametric approaches, here we propose a novel adaptive nonparametric approach which determines the sample-specific k for each test image. In particular, $k$ is adaptively set to be the number of the fewest nearest super-pixels which the images in the retrieval set can use to get the best category prediction. Finally, the initial super-pixel labels are further refined by contextual smoothing. Extensive experiments on challenging datasets demonstrate the superiority of the new solution over other state-of-the-art nonparametric solutions.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01589",
        "title": "Shadow Optimization from Structured Deep Edge Detection",
        "authors": [
            "Li Shen",
            "Teck Wee Chua",
            "Karianto Leman"
        ],
        "abstract": "Local structures of shadow boundaries as well as complex interactions of image regions remain largely unexploited by previous shadow detection approaches. In this paper, we present a novel learning-based framework for shadow region recovery from a single image. We exploit the local structures of shadow edges by using a structured CNN learning framework. We show that using the structured label information in the classification can improve the local consistency of the results and avoid spurious labelling. We further propose and formulate a shadow/bright measure to model the complex interactions among image regions. The shadow and bright measures of each patch are computed from the shadow edges detected in the image. Using the global interaction constraints on patches, we formulate a least-square optimization problem for shadow recovery that can be solved efficiently. Our shadow recovery method achieves state-of-the-art results on the major shadow benchmark databases collected under various conditions.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01596",
        "title": "Learning to See by Moving",
        "authors": [
            "Pulkit Agrawal",
            "Joao Carreira",
            "Jitendra Malik"
        ],
        "abstract": "The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01599",
        "title": "Filter characteristics in image decomposition with singular spectrum analysis",
        "authors": [
            "Kenji Kume",
            "Naoko Nose-Togawa"
        ],
        "abstract": "Singular spectrum analysis is developed as a nonparametric spectral decomposition of a time series. It can be easily extended to the decomposition of multidimensional lattice-like data through the filtering interpretation. In this viewpoint, the singular spectrum analysis can be understood as the adaptive and optimal generation of the filters and their two-step point-symmetric operation to the original data. In this paper, we point out that, when applied to the multidimensional data, the adaptively generated filters exhibit symmetry properties resulting from the bisymmetric nature of the lag-covariance matrices. The eigenvectors of the lag-covariance matrix are either symmetric or antisymmetric, and for the 2D image data, these lead to the differential-type filters with even- or odd-order derivatives. The dominant filter is a smoothing filter, reflecting the dominance of low-frequency components of the photo images. The others are the edge-enhancement or the noise filters corresponding to the band-pass or the high-pass filters. The implication of the decomposition to the image denoising is briefly discussed.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01631",
        "title": "Data Fusion of Objects Using Techniques Such as Laser Scanning, Structured Light and Photogrammetry for Cultural Heritage Applications",
        "authors": [
            "Citlalli Gamez Serna",
            "Ruven Pillay",
            "Alain Tremeau"
        ],
        "abstract": "In this paper we present a semi-automatic 2D-3D local registration pipeline capable of coloring 3D models obtained from 3D scanners by using uncalibrated images. The proposed pipeline exploits the Structure from Motion (SfM) technique in order to reconstruct a sparse representation of the 3D object and obtain the camera parameters from image feature matches. We then coarsely register the reconstructed 3D model to the scanned one through the Scale Iterative Closest Point (SICP) algorithm. SICP provides the global scale, rotation and translation parameters, using minimal manual user intervention. In the final processing stage, a local registration refinement algorithm optimizes the color projection of the aligned photos on the 3D object removing the blurring/ghosting artefacts introduced due to small inaccuracies during the registration. The proposed pipeline is capable of handling real world cases with a range of characteristics from objects with low level geometric features to complex ones.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01728",
        "title": "Integrating K-means with Quadratic Programming Feature Selection",
        "authors": [
            "Yamuna Prasad",
            "K. K. Biswas"
        ],
        "abstract": "Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01740",
        "title": "Fast Spectral Unmixing based on Dykstra's Alternating Projection",
        "authors": [
            "Qi Wei",
            "Jose Bioucas-Dias",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "abstract": "This paper presents a fast spectral unmixing algorithm based on Dykstra's alternating projection. The proposed algorithm formulates the fully constrained least squares optimization problem associated with the spectral unmixing task as an unconstrained regression problem followed by a projection onto the intersection of several closed convex sets. This projection is achieved by iteratively projecting onto each of the convex sets individually, following Dyktra's scheme. The sequence thus obtained is guaranteed to converge to the sought projection. Thanks to the preliminary matrix decomposition and variable substitution, the projection is implemented intrinsically in a subspace, whose dimension is very often much lower than the number of bands. A benefit of this strategy is that the order of the computational complexity for each projection is decreased from quadratic to linear time. Numerical experiments considering diverse spectral unmixing scenarios provide evidence that the proposed algorithm competes with the state-of-the-art, namely when the number of endmembers is relatively small, a circumstance often observed in real hyperspectral applications.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01749",
        "title": "Object detection via a multi-region & semantic segmentation-aware CNN model",
        "authors": [
            "Spyros Gidaris",
            "Nikos Komodakis"
        ],
        "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01861",
        "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language",
        "authors": [
            "Yingwei Pan",
            "Tao Mei",
            "Ting Yao",
            "Houqiang Li",
            "Yong Rui"
        ],
        "abstract": "Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true.\n",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01936",
        "title": "Noise in Structured-Light Stereo Depth Cameras: Modeling and its Applications",
        "authors": [
            "Avishek Chatterjee",
            "Venu Madhav Govindu"
        ],
        "abstract": "Depth maps obtained from commercially available structured-light stereo based depth cameras, such as the Kinect, are easy to use but are affected by significant amounts of noise. This paper is devoted to a study of the intrinsic noise characteristics of such depth maps, i.e. the standard deviation of noise in estimated depth varies quadratically with the distance of the object from the depth camera. We validate this theoretical model against empirical observations and demonstrate the utility of this noise model in three popular applications: depth map denoising, volumetric scan merging for 3D modeling, and identification of 3D planes in depth maps.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02108",
        "title": "MegaFace: A Million Faces for Recognition at Scale",
        "authors": [
            "D. Miller",
            "E. Brossard",
            "S. Seitz",
            "I. Kemelmacher-Shlizerman"
        ],
        "abstract": "Recent face recognition experiments on the LFW benchmark show that face recognition is performing stunningly well, surpassing human recognition rates. In this paper, we study face recognition at scale. Specifically, we have collected from Flickr a \\textbf{Million} faces and evaluated state of the art face recognition algorithms on this dataset. We found that the performance of algorithms varies--while all perform great on LFW, once evaluated at scale recognition rates drop drastically for most algorithms. Interestingly, deep learning based approach by \\cite{schroff2015facenet} performs much better, but still gets less robust at scale. We consider both verification and identification problems, and evaluate how pose affects recognition at scale. Moreover, we ran an extensive human study on Mechanical Turk to evaluate human recognition at scale, and report results. All the photos are creative commons photos and is released at \\small{\\url{",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02146",
        "title": "DeepBox: Learning Objectness with Convolutional Networks",
        "authors": [
            "Weicheng Kuo",
            "Bharath Hariharan",
            "Jitendra Malik"
        ],
        "abstract": "Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that objectness is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02206",
        "title": "Learning image representations tied to ego-motion",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02247",
        "title": "Performance Evaluation of Vision-Based Algorithms for MAVs",
        "authors": [
            "T. Holzmann",
            "R. Prettenthaler",
            "J. Pestana",
            "D. Muschick",
            "G. Graber",
            "C. Mostegel",
            "F. Fraundorfer",
            "H. Bischof"
        ],
        "abstract": "An important focus of current research in the field of Micro Aerial Vehicles (MAVs) is to increase the safety of their operation in general unstructured environments. Especially indoors, where GPS cannot be used for localization, reliable algorithms for localization and mapping of the environment are necessary in order to keep an MAV airborne safely. In this paper, we compare vision-based real-time capable methods for localization and mapping and point out their strengths and weaknesses. Additionally, we describe algorithms for state estimation, control and navigation, which use the localization and mapping results of our vision-based algorithms as input.\n    ",
        "submission_date": "2015-05-09T00:00:00",
        "last_modified_date": "2015-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02269",
        "title": "Subset Feature Learning for Fine-Grained Category Classification",
        "authors": [
            "Zongyuan Ge",
            "Christopher Mccool",
            "Conrad Sanderson",
            "Peter Corke"
        ],
        "abstract": "Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy.\n    ",
        "submission_date": "2015-05-09T00:00:00",
        "last_modified_date": "2015-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02438",
        "title": "Deep Learning for Semantic Part Segmentation with High-Level Guidance",
        "authors": [
            "S. Tsogkas",
            "I. Kokkinos",
            "G. Papandreou",
            "A. Vedaldi"
        ],
        "abstract": "In this work we address the task of segmenting an object into its parts, or semantic part segmentation. We start by adapting a state-of-the-art semantic segmentation system to this task, and show that a combination of a fully-convolutional Deep CNN system coupled with Dense CRF labelling provides excellent results for a broad range of object categories. Still, this approach remains agnostic to high-level constraints between object parts. We introduce such prior information by means of the Restricted Boltzmann Machine, adapted to our task and train our model in an discriminative fashion, as a hidden CRF, demonstrating that prior information can yield additional improvements. We also investigate the performance of our approach ``in the wild'', without information concerning the objects' bounding boxes, using an object detector to guide a multi-scale segmentation scheme. We evaluate the performance of our approach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsing and face labelling respectively. We show superior performance with respect to competitive methods that have been extensively engineered on these benchmarks, as well as realistic qualitative results on part segmentation, even for occluded or deformable objects. We also provide quantitative and extensive qualitative results on three classes from the PASCAL Parts dataset. Finally, we show that our multi-scale segmentation scheme can boost accuracy, recovering segmentations for finer parts.\n    ",
        "submission_date": "2015-05-10T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02496",
        "title": "Training Deeper Convolutional Networks with Deep Supervision",
        "authors": [
            "Liwei Wang",
            "Chen-Yu Lee",
            "Zhuowen Tu",
            "Svetlana Lazebnik"
        ],
        "abstract": "One of the most promising ways of improving the performance of deep convolutional neural networks is by increasing the number of convolutional layers. However, adding layers makes training more difficult and computationally expensive. In order to train deeper networks, we propose to add auxiliary supervision branches after certain intermediate layers during training. We formulate a simple rule of thumb to determine where these branches should be added. The resulting deeply supervised structure makes the training much easier and also produces better classification results on ImageNet and the recently released, larger MIT Places dataset\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02505",
        "title": "A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained Visual Categorization",
        "authors": [
            "Guo Lihua",
            "Guo Chenggan"
        ],
        "abstract": "Fine-grained categories are more difficulty distinguished than generic categories due to the similarity of inter-class and the diversity of intra-class. Therefore, the fine-grained visual categorization (FGVC) is considered as one of challenge problems in computer vision recently. A new feature learning framework, which is based on a two-layer local constrained sparse coding architecture, is proposed in this paper. The two-layer architecture is introduced for learning intermediate-level features, and the local constrained term is applied to guarantee the local smooth of coding coefficients. For extracting more discriminative information, local orientation histograms are the input of sparse coding instead of raw pixels. Moreover, a quick dictionary updating process is derived to further improve the training speed. Two experimental results show that our method achieves 85.29% accuracy on the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird dataset, and the performance of our framework is highly competitive with existing literatures.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02731",
        "title": "Removing Camera Shake via Weighted Fourier Burst Accumulation",
        "authors": [
            "Mauricio Delbracio",
            "Guillermo Sapiro"
        ],
        "abstract": "Numerous recent approaches attempt to remove image blur due to camera shake, either with one or multiple input images, by explicitly solving an inverse and inherently ill-posed deconvolution problem. If the photographer takes a burst of images, a modality available in virtually all modern digital cameras, we show that it is possible to combine them to get a clean sharp version. This is done without explicitly solving any blur estimation and subsequent inverse problem. The proposed algorithm is strikingly simple: it performs a weighted average in the Fourier domain, with weights depending on the Fourier spectrum magnitude. The method can be seen as a generalization of the align and average procedure, with a weighted average, motivated by hand-shake physiology and theoretically supported, taking place in the Fourier domain. The method's rationale is that camera shake has a random nature and therefore each image in the burst is generally blurred differently. Experiments with real camera data, and extensive comparisons, show that the proposed Fourier Burst Accumulation (FBA) algorithm achieves state-of-the-art results an order of magnitude faster, with simplicity for on-board implementation on camera phones. Finally, we also present experiments in real high dynamic range (HDR) scenes, showing how the method can be straightforwardly extended to HDR photography.\n    ",
        "submission_date": "2015-05-11T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02890",
        "title": "Sparse 3D convolutional neural networks",
        "authors": [
            "Ben Graham"
        ],
        "abstract": "We have implemented a convolutional neural network designed for processing sparse three-dimensional input data. The world we live in is three dimensional so there are a large number of potential applications including 3D object recognition and analysis of space-time objects. In the quest for efficiency, we experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02921",
        "title": "How Far Can You Get By Combining Change Detection Algorithms?",
        "authors": [
            "Simone Bianco",
            "Gianluigi Ciocca",
            "Raimondo Schettini"
        ],
        "abstract": "Given the existence of many change detection algorithms, each with its own peculiarities and strengths, we propose a combination strategy, that we termed IUTIS (In Unity There Is Strength), based on a genetic Programming framework. This combination strategy is aimed at leveraging the strengths of the algorithms and compensate for their weakness. In this paper we show our findings in applying the proposed strategy in two different scenarios. The first scenario is purely performance-based. The second scenario performance and efficiency must be balanced. Results demonstrate that starting from simple algorithms we can achieve comparable results with respect to more complex state-of-the-art change detection algorithms, while keeping the computational complexity affordable for real-time applications.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2018-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02982",
        "title": "Automatic Script Identification in the Wild",
        "authors": [
            "Baoguang Shi",
            "Cong Yao",
            "Chengquan Zhang",
            "Xiaowei Guo",
            "Feiyue Huang",
            "Xiang Bai"
        ],
        "abstract": "With the rapid increase of transnational communication and cooperation, people frequently encounter multilingual scenarios in various situations. In this paper, we are concerned with a relatively new problem: script identification at word or line levels in natural scenes. A large-scale dataset with a great quantity of natural images and 10 types of widely used languages is constructed and released. In allusion to the challenges in script identification in real-world scenarios, a deep learning based algorithm is proposed. The experiments on the proposed dataset demonstrate that our algorithm achieves superior performance, compared with conventional image classification methods, such as the original CNN architecture and LLC.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03046",
        "title": "Improving Computer-aided Detection using Convolutional Neural Networks and Random View Aggregation",
        "authors": [
            "Holger R. Roth",
            "Le Lu",
            "Jiamin Liu",
            "Jianhua Yao",
            "Ari Seff",
            "Kevin Cherry",
            "Lauren Kim",
            "Ronald M. Summers"
        ],
        "abstract": "Automated computer-aided detection (CADe) in medical imaging has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities but at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities of $\\sim$100% but at high FP levels. By leveraging existing CAD systems, coordinates of regions or volumes of interest (ROI or VOI) for lesion candidates are generated in this step and function as input for a second tier, which is our focus in this study. In this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations with respect to each ROI's centroid coordinates. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the trained ConvNets are employed to assign class (e.g., lesion, pathology) probabilities for a new set of $N$ random views that are then averaged at each ROI to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three different data sets with different numbers of patients: 59 patients for sclerotic metastases detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve CADe performance markedly in all cases. CADe sensitivities improved from 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03159",
        "title": "Monocular Object Instance Segmentation and Depth Ordering with CNNs",
        "authors": [
            "Ziyu Zhang",
            "Alexander G. Schwing",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance ID encodes the depth ordering within image patches. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm. It aims to predict accurate instance-level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show good performance on both tasks.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03205",
        "title": "Leveraging Image based Prior for Visual Place Recognition",
        "authors": [
            "Tsukamoto Taisho",
            "Tanaka Kanji"
        ],
        "abstract": "In this study, we propose a novel scene descriptor for visual place recognition. Unlike popular bag-of-words scene descriptors which rely on a library of vector quantized visual features, our proposed descriptor is based on a library of raw image data, such as publicly available photo collections from Google StreetView and Flickr. The library images need not to be associated with spatial information regarding the viewpoint and orientation of the scene. As a result, these images are cheaper than the database images; in addition, they are readily available. Our proposed descriptor directly mines the image library to discover landmarks (i.e., image patches) that suitably match an input query/database image. The discovered landmarks are then compactly described by their pose and shape (i.e., library image ID, bounding boxes) and used as a compact discriminative scene descriptor for the input image. We evaluate the effectiveness of our scene description framework by comparing its performance to that of previous approaches.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03227",
        "title": "PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence",
        "authors": [
            "Keze Wang",
            "Liang Lin",
            "Jiangbo Lu",
            "Chenglong Li",
            "Keyang Shi"
        ],
        "abstract": "Driven by recent vision and graphics applications such as image segmentation and object recognition, computing pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly important. In this paper, we propose a unified framework called PISA, which stands for Pixelwise Image Saliency Aggregating various bottom-up cues and priors. It generates spatially coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and overcomes the limitations of previous methods which use homogeneous superpixel-based and color only treatment. PISA aggregates multiple saliency cues in a global context such as complementary color and structure contrast measures with their spatial priors in the image domain. The saliency confidence is further jointly modeled with a neighborhood consistence constraint into an energy minimization formulation, in which each pixel will be evaluated with multiple hypothetical saliency levels. Instead of using global discrete optimization methods, we employ the cost-volume filtering technique to solve our formulation, assigning the saliency levels smoothly while preserving the edge-aware structure details. In addition, a faster version of PISA is developed using a gradient-driven image sub-sampling strategy to greatly improve the runtime efficiency while keeping comparable detection accuracy. Extensive experiments on a number of public datasets suggest that PISA convincingly outperforms other state-of-the-art approaches. In addition, with this work we also create a new dataset containing $800$ commodity images for evaluating saliency detection. The dataset and source code of PISA can be downloaded at ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03229",
        "title": "APAC: Augmented PAttern Classification with Neural Networks",
        "authors": [
            "Ikuro Sato",
            "Hiroki Nishimura",
            "Kensuke Yokoi"
        ],
        "abstract": "Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classification problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classifiers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classification, which is a way of classification using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classifiers. Even our multilayer perceptron model beats some of the convolutional models with recently invented stochastic regularization techniques on the CIFAR-10 dataset.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03344",
        "title": "A Framework for Fast Face and Eye Detection",
        "authors": [
            "Anjith George",
            "Anirban Dasgupta",
            "Aurobinda Routray"
        ],
        "abstract": "Face detection is an essential step in many computer vision applications like surveillance, tracking, medical analysis, facial expression analysis etc. Several approaches have been made in the direction of face detection. Among them, Haar-like features based method is a robust method. In spite of the robustness, Haar - like features work with some limitations. However, with some simple modifications in the algorithm, its performance can be made faster and more robust. The present work refers to the increase in speed of operation of the original algorithm by down sampling the frames and its analysis with different scale factors. It also discusses the detection of tilted faces using an affine transformation of the input image.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03352",
        "title": "A Vision Based System for Monitoring the Loss of Attention in Automotive Drivers",
        "authors": [
            "Anirban Dasgupta",
            "Anjith George",
            "S. L. Happy",
            "Aurobinda Routray"
        ],
        "abstract": "On board monitoring of the alertness level of an automotive driver has been a challenging research in transportation safety and management. In this paper, we propose a robust real time embedded platform to monitor the loss of attention of the driver during day as well as night driving conditions. The PERcentage of eye CLOSure (PERCLOS) has been used as the indicator of the alertness level. In this approach, the face is detected using Haar like features and tracked using a Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA) during day time and the block Local Binary Pattern (LBP) features during night. Finally the eye state is classified as open or closed using Support Vector Machines(SVM). In plane and off plane rotations of the drivers face have been compensated using Affine and Perspective Transformation respectively. Compensation in illumination variation is carried out using Bi Histogram Equalization (BHE). The algorithm has been cross validated using brain signals and finally been implemented on a Single Board Computer (SBC) having Intel Atom processor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XP operating system. The system is found to be robust under actual driving conditions.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03365",
        "title": "MRF Optimization by Graph Approximation",
        "authors": [
            "Wonsik Kim",
            "Kyoung Mu Lee"
        ],
        "abstract": "Graph cuts-based algorithms have achieved great success in energy minimization for many computer vision applications. These algorithms provide approximated solutions for multi-label energy functions via move-making approach. This approach fuses the current solution with a proposal to generate a lower-energy solution. Thus, generating the appropriate proposals is necessary for the success of the move-making approach. However, not much research efforts has been done on the generation of \"good\" proposals, especially for non-metric energy functions. In this paper, we propose an application-independent and energy-based approach to generate \"good\" proposals. With these proposals, we present a graph cuts-based move-making algorithm called GA-fusion (fusion with graph approximation-based proposals). Extensive experiments support that our proposal generation is effective across different classes of energy functions. The proposed algorithm outperforms others both on real and synthetic problems.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03489",
        "title": "A Review Paper: Noise Models in Digital Image Processing",
        "authors": [
            "Ajay Kumar Boyat",
            "Brijendra Kumar Joshi"
        ],
        "abstract": "Noise is always presents in digital images during image acquisition, coding, transmission, and processing steps. Noise is very difficult to remove it from the digital images without the prior knowledge of noise model. That is why, review of noise models are essential in the study of image denoising techniques. In this paper, we express a brief overview of various noise models. These noise models can be selected by analysis of their origin. In this way, we present a complete and quantitative analysis of noise models available in digital images.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03493",
        "title": "Modified Hausdorff Fractal Dimension (MHFD)",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "abstract": "The Hausdorff fractal dimension has been a fast-to-calculate method to estimate complexity of fractal shapes. In this work, a modified version of this fractal dimension is presented in order to make it more robust when applied in estimating complexity of non-fractal images. The modified Hausdorff fractal dimension stands on two features that weaken the requirement of presence of a shape and also reduce the impact of the noise possibly presented in the input image. The new algorithm has been evaluated on a set of images of different character with promising performance.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03505",
        "title": "On a spatial-temporal decomposition of the optical flow",
        "authors": [
            "Aniello Raffale Patrone",
            "Otmar Scherzer"
        ],
        "abstract": "In this paper we present a decomposition algorithm for computation of the spatial-temporal optical flow of a dynamic image sequence. We consider several applications, such as the extraction of temporal motion features and motion detection in dynamic sequences under varying illumination conditions, such as they appear for instance in psychological flickering experiments. For the numerical implementation we are solving an integro-differential equation by a fixed point iteration. For comparison purposes we use a standard time dependent optical flow algorithm, which in contrast to our method, constitutes in solving a spatial-temporal differential equation.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2017-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03540",
        "title": "Brain Tumor Segmentation with Deep Neural Networks",
        "authors": [
            "Mohammad Havaei",
            "Axel Davy",
            "David Warde-Farley",
            "Antoine Biard",
            "Aaron Courville",
            "Yoshua Bengio",
            "Chris Pal",
            "Pierre-Marc Jodoin",
            "Hugo Larochelle"
        ],
        "abstract": "In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.\n",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03566",
        "title": "COROLA: A Sequential Solution to Moving Object Detection Using Low-rank Approximation",
        "authors": [
            "Moein Shakeri",
            "Hong Zhang"
        ],
        "abstract": "Extracting moving objects from a video sequence and estimating the background of each individual image are fundamental issues in many practical applications such as visual surveillance, intelligent vehicle navigation, and traffic monitoring. Recently, some methods have been proposed to detect moving objects in a video via low-rank approximation and sparse outliers where the background is modeled with the computed low-rank component of the video and the foreground objects are detected as the sparse outliers in the low-rank approximation. All of these existing methods work in a batch manner, preventing them from being applied in real time and long duration tasks. In this paper, we present an online sequential framework, namely contiguous outliers representation via online low-rank approximation (COROLA), to detect moving objects and learn the background model at the same time. We also show that our model can detect moving objects with a moving camera. Our experimental evaluation uses simulated data and real public datasets and demonstrates the superior performance of COROLA in terms of both accuracy and execution time.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03578",
        "title": "Vanishing Point Attracts Eye Movements in Scene Free-viewing",
        "authors": [
            "Ali Borji",
            "Mengyang Feng",
            "Huchuan Lu"
        ],
        "abstract": "Eye movements are crucial in understanding complex scenes. By predicting where humans look in natural scenes, we can understand how they percieve scenes and priotriaze information for further high-level processing. Here, we study the effect of a particular type of scene structural information known as vanishing point and show that human gaze is attracted to vanishing point regions. We then build a combined model of traditional saliency and vanishing point channel that outperforms state of the art saliency models.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03581",
        "title": "CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research",
        "authors": [
            "Ali Borji",
            "Laurent Itti"
        ],
        "abstract": "Saliency modeling has been an active research area in computer vision for about two decades. Existing state of the art models perform very well in predicting where people look in natural scenes. There is, however, the risk that these models may have been overfitting themselves to available small scale biased datasets, thus trapping the progress in a local minimum. To gain a deeper insight regarding current issues in saliency modeling and to better gauge progress, we recorded eye movements of 120 observers while they freely viewed a large number of naturalistic and artificial images. Our stimuli includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings. We analyze some basic properties of this dataset and compare some successful models. We believe that our dataset opens new challenges for the next generation of saliency models and helps conduct behavioral studies on bottom-up visual attention.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03597",
        "title": "Multi-scale Volumes for Deep Object Detection and Localization",
        "authors": [
            "Eshed Ohn-Bar",
            "M. M. Trivedi"
        ],
        "abstract": "This study aims to analyze the benefits of improved multi-scale reasoning for object detection and localization with deep convolutional neural networks. To that end, an efficient and general object detection framework which operates on scale volumes of a deep feature pyramid is proposed. In contrast to the proposed approach, most current state-of-the-art object detectors operate on a single-scale in training, while testing involves independent evaluation across scales. One benefit of the proposed approach is in better capturing of multi-scale contextual information, resulting in significant gains in both detection performance and localization quality of objects on the PASCAL VOC dataset and a multi-view highway vehicles dataset. The joint detection and localization scale-specific models are shown to especially benefit detection of challenging object categories which exhibit large scale variation as well as detection of small objects.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03795",
        "title": "Fast and numerically stable circle fit",
        "authors": [
            "Houssam Abdul-Rahman",
            "Nikolai Chernov"
        ],
        "abstract": "We develop a new algorithm for fitting circles that does not have drawbacks commonly found in existing circle fits. Our fit achieves ultimate accuracy (to machine precision), avoids divergence, and is numerically stable even when fitting circles get arbitrary large. Lastly, our algorithm takes less than 10 iterations to converge, on average.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03825",
        "title": "Unsupervised Object Discovery and Tracking in Video Collections",
        "authors": [
            "Suha Kwak",
            "Minsu Cho",
            "Ivan Laptev",
            "Jean Ponce",
            "Cordelia Schmid"
        ],
        "abstract": "This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates successive similar object regions within the same video. Interestingly, our algorithm also discovers the implicit topology of frames associated with instances of the same object class across different videos, a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle video collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03832",
        "title": "Parametric Regression on the Grassmannian",
        "authors": [
            "Yi Hong",
            "Nikhil Singh",
            "Roland Kwitt",
            "Nuno Vasconcelos",
            "Marc Niethammer"
        ],
        "abstract": "We address the problem of fitting parametric curves on the Grassmann manifold for the purpose of intrinsic parametric regression. As customary in the literature, we start from the energy minimization formulation of linear least-squares in Euclidean spaces and generalize this concept to general nonflat Riemannian manifolds, following an optimal-control point of view. We then specialize this idea to the Grassmann manifold and demonstrate that it yields a simple, extensible and easy-to-implement solution to the parametric regression problem. In fact, it allows us to extend the basic geodesic model to (1) a time-warped variant and (2) cubic splines. We demonstrate the utility of the proposed solution on different vision problems, such as shape regression as a function of age, traffic-speed estimation and crowd-counting from surveillance video clips. Most notably, these problems can be conveniently solved within the same framework without any specifically-tailored steps along the processing pipeline.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03840",
        "title": "Non-unique games over compact groups and orientation estimation in cryo-EM",
        "authors": [
            "Afonso S. Bandeira",
            "Yutong Chen",
            "Amit Singer"
        ],
        "abstract": "Let $\\mathcal{G}$ be a compact group and let $f_{ij} \\in L^2(\\mathcal{G})$. We define the Non-Unique Games (NUG) problem as finding $g_1,\\dots,g_n \\in \\mathcal{G}$ to minimize $\\sum_{i,j=1}^n f_{ij} \\left( g_i g_j^{-1}\\right)$. We devise a relaxation of the NUG problem to a semidefinite program (SDP) by taking the Fourier transform of $f_{ij}$ over $\\mathcal{G}$, which can then be solved efficiently. The NUG framework can be seen as a generalization of the little Grothendieck problem over the orthogonal group and the Unique Games problem and includes many practically relevant problems, such as the maximum likelihood estimator} to registering bandlimited functions over the unit sphere in $d$-dimensions and orientation estimation in cryo-Electron Microscopy.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03873",
        "title": "Improving Image Classification with Location Context",
        "authors": [
            "Kevin Tang",
            "Manohar Paluri",
            "Li Fei-Fei",
            "Rob Fergus",
            "Lubomir Bourdev"
        ],
        "abstract": "With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a 7% gain in mean average precision.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03932",
        "title": "Using Ensemble Models in the Histological Examination of Tissue Abnormalities",
        "authors": [
            "Giancarlo Crocetti",
            "Michael Coakley",
            "Phil Dressner",
            "Wanda Kellum",
            "Tamba Lamin"
        ],
        "abstract": "Classification models for the automatic detection of abnormalities on histological samples do exists, with an active debate on the cost associated with false negative diagnosis (underdiagnosis) and false positive diagnosis (overdiagnosis). Current models tend to underdiagnose, failing to recognize a potentially fatal disease.\n",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04026",
        "title": "Automatic Facial Expression Recognition Using Features of Salient Facial Patches",
        "authors": [
            "S L Happy",
            "Aurobinda Routray"
        ],
        "abstract": "Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04028",
        "title": "Biometric Matching and Fusion System for Fingerprints from Non-Distal Phalanges",
        "authors": [
            "Mehmet Kayaoglu",
            "Berkay Topcu",
            "Umut Uludag"
        ],
        "abstract": "Market research indicates that fingerprints are still the most popular biometric modality for personal authentication. Even with the onset of new modalities (e.g. vein matching), many applications within different domains (e-ID, banking, border control...) and geographies rely on fingerprints obtained from the distal phalanges (a.k.a. sections, digits) of the human hand structure. Motivated by the problem of poor quality distal fingerprint images affecting a non-trivial portion of the population (which decreases associated authentication accuracy), we designed and tested a multifinger, multiphalanx fusion scheme, that combines minutiae matching scores originating from non-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion, (ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion. Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinct images) database collected in our laboratory with a commercial optical fingerprint sensor, and a commercial minutiae extractor & matcher (without any modification), allowed us to simulate a real-world fingerprint authentication setting. Detailed analyses including ROC curves with statistical confidence intervals show that the proposed system can be a viable alternative for cases where (i) distal phalanx images are not usable (e.g. due to missing digits, or low quality finger surface due to manual labor), and (ii) switching to a new biometric modality (e.g. iris) is not possible due to economical or infrastructure limits. Further, we show that when distal phalanx images are in fact usable, combining them with images from other phalanges increases accuracy as well.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04030",
        "title": "Robust Facial Expression Classification Using Shape and Appearance Features",
        "authors": [
            "S. L. Happy",
            "Aurobinda Routray"
        ],
        "abstract": "Facial expression recognition has many potential applications which has attracted the attention of researchers in the last decade. Feature extraction is one important step in expression analysis which contributes toward fast and accurate expression recognition. This paper represents an approach of combining the shape and appearance features to form a hybrid feature vector. We have extracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors and Local Binary Patterns (LBP) as appearance features. The proposed framework involves a novel approach of extracting hybrid features from active facial patches. The active facial patches are located on the face regions which undergo a major change during different expressions. After detection of facial landmarks, the active patches are localized and hybrid features are calculated from these patches. The use of small parts of face instead of the whole face for extracting features reduces the computational cost and prevents the over-fitting of the features for classification. By using linear discriminant analysis, the dimensionality of the feature is reduced which is further classified by using the support vector machine (SVM). The experimental results on two publicly available databases show promising accuracy in recognizing all expression classes.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04055",
        "title": "A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications",
        "authors": [
            "S L Happy",
            "Anirban Dasgupta",
            "Anjith George",
            "Aurobinda Routray"
        ],
        "abstract": "Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04058",
        "title": "A Real Time Facial Expression Classification System Using Local Binary Patterns",
        "authors": [
            "S. L. Happy",
            "Anjith George",
            "Aurobinda Routray"
        ],
        "abstract": "Facial expression analysis is one of the popular fields of research in human computer interaction (HCI). It has several applications in next generation user interfaces, human emotion analysis, behavior and cognitive modeling. In this paper, a facial expression classification algorithm is proposed which uses Haar classifier for face detection purpose, Local Binary Patterns (LBP) histogram of different block sizes of a face image as feature vectors and classifies various facial expressions using Principal Component Analysis (PCA). The algorithm is implemented in real time for expression classification since the computational complexity of the algorithm is small. A customizable approach is proposed for facial expression analysis, since the various expressions and intensity of expressions vary from person to person. The system uses grayscale frontal face images of a person to classify six basic emotions namely happiness, sadness, disgust, fear, surprise and anger.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04117",
        "title": "Discovering Attribute Shades of Meaning with the Crowd",
        "authors": [
            "Adriana Kovashka",
            "Kristen Grauman"
        ],
        "abstract": "To learn semantic attributes, existing methods typically train one discriminative model for each word in a vocabulary of nameable properties. However, this \"one model per word\" assumption is problematic: while a word might have a precise linguistic definition, it need not have a precise visual definition. We propose to discover shades of attribute meaning. Given an attribute name, we use crowdsourced image labels to discover the latent factors underlying how different annotators perceive the named concept. We show that structure in those latent factors helps reveal shades, that is, interpretations for the attribute shared by some group of annotators. Using these shades, we train classifiers to capture the primary (often subtle) variants of the attribute. The resulting models are both semantic and visually precise. By catering to users' interpretations, they improve attribute prediction accuracy on novel images. Shades also enable more successful attribute-based image search, by providing robust personalized models for retrieving multi-attribute query results. They are widely applicable to tasks that involve describing visual content, such as zero-shot category learning and organization of photo collections.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04141",
        "title": "WhittleSearch: Interactive Image Search with Relative Attribute Feedback",
        "authors": [
            "Adriana Kovashka",
            "Devi Parikh",
            "Kristen Grauman"
        ],
        "abstract": "We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query \"black shoes\", the user might state, \"Show me shoe images like these, but sportier.\" Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently \"whittle away\" irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient -- both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.\n    ",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04143",
        "title": "Dense Semantic Correspondence where Every Pixel is a Classifier",
        "authors": [
            "Hilton Bristow",
            "Jack Valmadre",
            "Simon Lucey"
        ],
        "abstract": "Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ.\n",
        "submission_date": "2015-05-15T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04260",
        "title": "The color of smiling: computational synaesthesia of facial expressions",
        "authors": [
            "Vittorio Cuculo",
            "Raffaella Lanzarotti",
            "Giuseppe Boccignone"
        ],
        "abstract": "This note gives a preliminary account of the transcoding or rechanneling problem between different stimuli as it is of interest for the natural interaction or affective computing fields. By the consideration of a simple example, namely the color response of an affective lamp to a sensed facial expression, we frame the problem within an information- theoretic perspective. A full justification in terms of the Information Bottleneck principle promotes a latent affective space, hitherto surmised as an appealing and intuitive solution, as a suitable mediator between the different stimuli.\n    ",
        "submission_date": "2015-05-16T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04286",
        "title": "Robust Real-time Extraction of Fiducial Facial Feature Points using Haar-like Features",
        "authors": [
            "Harry Commin"
        ],
        "abstract": "In this paper, we explore methods of robustly extracting fiducial facial feature points - an important process for numerous facial image processing tasks. We consider various methods to first detect face, then facial features and finally salient facial feature points. Colour-based models are analysed and their overall unsuitability for this task is summarised. The bulk of the report is then dedicated to proposing a learning-based method centred on the Viola-Jones algorithm. The specific difficulties and considerations relating to feature point detection are laid out in this context and a novel approach is established to address these issues. On a sequence of clear and unobstructed face images, our proposed system achieves average detection rates of over 90%. Then, using a more varied sample dataset, we identify some possible areas for future development of our system.\n    ",
        "submission_date": "2015-05-16T00:00:00",
        "last_modified_date": "2015-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04364",
        "title": "Salient Structure Detection by Context-Guided Visual Search",
        "authors": [
            "Kai-Fu Yang",
            "Hui Li",
            "Chao-Yi Li",
            "Yong-Jie Li"
        ],
        "abstract": "We define the task of salient structure (SS) detection to unify the saliency-related tasks like fixation prediction, salient object detection, and other detection of structures of interest. In this study, we propose a unified framework for SS detection by modeling the two-pathway-based guided search strategy of biological vision. Firstly, context-based spatial prior (CBSP) is extracted based on the layout of edges in the given scene along a fast visual pathway, called non-selective pathway. This is a rough and non-selective estimation of the locations where the potential SSs present. Secondly, another flow of local feature extraction is executed in parallel along the selective pathway. Finally, Bayesian inference is used to integrate local cues guided by CBSP, and to predict the exact locations of SSs in the input scene. The proposed model is invariant to size and features of objects. Experimental results on four datasets (two fixation prediction datasets and two salient object datasets) demonstrate that our system achieves competitive performance for SS detection (i.e., both the tasks of fixation prediction and salient object detection) comparing to the state-of-the-art methods.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04366",
        "title": "Learning Deconvolution Network for Semantic Segmentation",
        "authors": [
            "Hyeonwoo Noh",
            "Seunghoon Hong",
            "Bohyung Han"
        ],
        "abstract": "We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04373",
        "title": "Evolutionary Cost-sensitive Extreme Learning Machine",
        "authors": [
            "Lei Zhang",
            "David Zhang"
        ],
        "abstract": "Conventional extreme learning machines solve a Moore-Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition based access control system, where misclassifying a stranger as a family member may result in more serious disaster than misclassifying a family member as a stranger. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another, in many realistic cases the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive extreme learning machine (ECSELM), with the following merits: 1) to our best knowledge, it is the first proposal of ELM in evolutionary cost-sensitive classification scenario; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Experiments in a variety of cost-sensitive tasks well demonstrate the effectiveness of the proposed approaches, with about 5%~10% improvements.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04382",
        "title": "Robust Visual Knowledge Transfer via EDA",
        "authors": [
            "Lei Zhang",
            "David Zhang"
        ],
        "abstract": "We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine based cross-domain network learning framework, that is called Extreme Learning Machine (ELM) based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the l_(2,1)-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as base classifiers. The network output weights cannot only be analytically determined, but also transferrable. Additionally, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semi-supervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition, demonstrate that our EDA methods outperform existing cross-domain learning methods.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04424",
        "title": "Improved Microaneurysm Detection using Deep Neural Networks",
        "authors": [
            "Mrinal Haloi"
        ],
        "abstract": "In this work, we propose a novel microaneurysm (MA) detection for early diabetic retinopathy screening using color fundus images. Since MA usually the first lesions to appear as an indicator of diabetic retinopathy, accurate detection of MA is necessary for treatment. Each pixel of the image is classified as either MA or non-MA using a deep neural network with dropout training procedure using maxout activation function. No preprocessing step or manual feature extraction is required. Substantial improvements over standard MA detection method based on the pipeline of preprocessing, feature extraction, classification followed by post processing is achieved. The presented method is evaluated in publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 database and achieved state-of-the-art accuracy.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2016-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04427",
        "title": "The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition",
        "authors": [
            "Zhenzhong Lan",
            "Dezhong Yao",
            "Ming Lin",
            "Shoou-I Yu",
            "Alexander Hauptmann"
        ],
        "abstract": "Motivated by the success of data-driven convolutional neural networks (CNNs) in object recognition on static images, researchers are working hard towards developing CNN equivalents for learning video features. However, learning video features globally has proven to be quite a challenge due to its high dimensionality, the lack of labelled data and the difficulty in processing large-scale video data. Therefore, we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system.\n",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04467",
        "title": "Exploring Nearest Neighbor Approaches for Image Captioning",
        "authors": [
            "Jacob Devlin",
            "Saurabh Gupta",
            "Ross Girshick",
            "Margaret Mitchell",
            "C. Lawrence Zitnick"
        ],
        "abstract": "We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the \"consensus\" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04474",
        "title": "Visual Semantic Role Labeling",
        "authors": [
            "Saurabh Gupta",
            "Jitendra Malik"
        ],
        "abstract": "In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction. Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action. We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action. To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action. Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work.\n    ",
        "submission_date": "2015-05-17T00:00:00",
        "last_modified_date": "2015-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04502",
        "title": "Reproducible Evaluation of Pan-Tilt-Zoom Tracking",
        "authors": [
            "Gengjie Chen",
            "Pierre-Luc St-Charles",
            "Wassim Bouachir",
            "Thomas Joeisseint",
            "Guillaume-Alexandre Bilodeau",
            "Robert Bergevin"
        ],
        "abstract": "Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in computer vision for many years. However, it is very difficult to assess the progress that has been made on this topic because there is no standard evaluation methodology. The difficulty in evaluating PTZ tracking algorithms arises from their dynamic nature. In contrast to other forms of tracking, PTZ tracking involves both locating the target in the image and controlling the motors of the camera to aim it so that the target stays in its field of view. This type of tracking can only be performed online. In this paper, we propose a new evaluation framework based on a virtual PTZ camera. With this framework, tracking scenarios do not change for each experiment and we are able to replicate online PTZ camera control and behavior including camera positioning delays, tracker processing delays, and numerical zoom. We tested our evaluation framework with the Camshift tracker to show its viability and to establish baseline results.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04585",
        "title": "Global Variational Method for Fingerprint Segmentation by Three-part Decomposition",
        "authors": [
            "Duy Hoang Thai",
            "Carsten Gottschlich"
        ],
        "abstract": "Verifying an identity claim by fingerprint recognition is a commonplace experience for millions of people in their daily life, e.g. for unlocking a tablet computer or smartphone. The first processing step after fingerprint image acquisition is segmentation, i.e. dividing a fingerprint image into a foreground region which contains the relevant features for the comparison algorithm, and a background region. We propose a novel segmentation method by global three-part decomposition (G3PD). Based on global variational analysis, the G3PD method decomposes a fingerprint image into cartoon, texture and noise parts. After decomposition, the foreground region is obtained from the non-zero coefficients in the texture image using morphological processing. The segmentation performance of the G3PD method is compared to five state-of-the-art methods on a benchmark which comprises manually marked ground truth segmentation for 10560 images. Performance evaluations show that the G3PD method consistently outperforms existing methods in terms of segmentation accuracy.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04597",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04617",
        "title": "Joint Representation Classification for Collective Face Recognition",
        "authors": [
            "Liping Wang",
            "Songcan Chen"
        ],
        "abstract": "Sparse representation based classification (SRC) is popularly used in many applications such as face recognition, and implemented in two steps: representation coding and classification. For a given set of testing images, SRC codes every image over the base images as a sparse representation then classifies it to the class with the least representation error. This scheme utilizes an individual representation rather than the collective one to classify such a set of images, doing so obviously ignores the correlation among the given images. In this paper, a joint representation classification (JRC) for collective face recognition is proposed. JRC takes the correlation of multiple images as well as a single representation into account. Under the assumption that the given face images are generally related to each other, JRC codes all the testing images over the base images simultaneously to facilitate recognition. To this end, the testing inputs are aligned into a matrix and the joint representation coding is formulated to a generalized $l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the induced optimization problems for any $q\\in[1,2]$ and $p\\in (0,2]$, an iterative quadratic method (IQM) is developed. IQM is proved to be a strict descent algorithm with convergence to the optimal solution. Moreover, a more practical IQM is proposed for large-scale case. Experimental results on three public databases show that the JRC with practical IQM no only saves much computational cost but also achieves better performance in collective face recognition than the state-of-the-arts.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04803",
        "title": "Predicting Important Objects for Egocentric Video Summarization",
        "authors": [
            "Yong Jae Lee",
            "Kristen Grauman"
        ],
        "abstract": "We present a video summarization approach for egocentric or \"wearable\" camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer's day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video---such as the nearness to hands, gaze, and frequency of occurrence---and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compactness of the final summary given either an importance selection criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets show the method's promise relative to existing techniques for saliency and summarization.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04845",
        "title": "Multi-Image Matching via Fast Alternating Minimization",
        "authors": [
            "Xiaowei Zhou",
            "Menglong Zhu",
            "Kostas Daniilidis"
        ],
        "abstract": "In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04868",
        "title": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors",
        "authors": [
            "Limin Wang",
            "Yu Qiao",
            "Xiaoou Tang"
        ],
        "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets (HMDB51 65.9%, UCF101 91.5%).\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04870",
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
        "authors": [
            "Bryan A. Plummer",
            "Liwei Wang",
            "Chris M. Cervantes",
            "Juan C. Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2016-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04873",
        "title": "Have a Look at What I See",
        "authors": [
            "Lior Talker",
            "Yael Moses",
            "Ilan Shimshoni"
        ],
        "abstract": "We propose a method for guiding a photographer to rotate her/his smartphone camera to obtain an image that overlaps with another image of the same scene. The other image is taken by another photographer from a different viewpoint. Our method is applicable even when the images do not have overlapping fields of view. Straightforward applications of our method include sharing attention to regions of interest for social purposes, or adding missing images to improve structure for motion results. Our solution uses additional images of the scene, which are often available since many people use their smartphone cameras regularly. These images may be available online from other photographers who are present at the scene. Our method avoids 3D scene reconstruction; it relies instead on a new representation that consists of the spatial orders of the scene points on two axes, x and y. This representation allows a sequence of points to be chosen efficiently and projected onto the photographers images, using epipolar point transfer. Overlaying these epipolar lines on the live preview of the camera produces a convenient interface to guide the user. The method was tested on challenging datasets of images and succeeded in guiding a photographer from one view to a non-overlapping destination view.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04922",
        "title": "Character-level Chinese Writer Identification using Path Signature Feature, DropStroke and Deep CNN",
        "authors": [
            "Weixin Yang",
            "Lianwen Jin",
            "Manfei Liu"
        ],
        "abstract": "Most existing online writer-identification systems require that the text content is supplied in advance and rely on separately designed features and classifiers. The identifications are based on lines of text, entire paragraphs, or entire documents; however, these materials are not always available. In this paper, we introduce a path-signature feature to an end-to-end text-independent writer-identification system with a deep convolutional neural network (DCNN). Because deep models require a considerable amount of data to achieve good performance, we propose a data-augmentation method named DropStroke to enrich personal handwriting. Experiments were conducted on online handwritten Chinese characters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes from 420 writers. For each writer, we only used 200 samples for training and the remaining 3,666. The results reveal that the path-signature feature is useful for writer identification, and the proposed DropStroke technique enhances the generalization and significantly improves performance.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04925",
        "title": "High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps",
        "authors": [
            "Zhuoyao Zhong",
            "Lianwen Jin",
            "Zecheng Xie"
        ],
        "abstract": "Just like its great success in solving many computer vision problems, the convolutional neural networks (CNN) provided new end-to-end approach to handwritten Chinese character recognition (HCCR) with very promising results in recent years. However, previous CNNs so far proposed for HCCR were neither deep enough nor slim enough. We show in this paper that, a deeper architecture can benefit HCCR a lot to achieve higher performance, meanwhile can be designed with less parameters. We also show that the traditional feature extraction methods, such as Gabor or gradient feature maps, are still useful for enhancing the performance of CNN. We design a streamlined version of GoogLeNet [13], which was original proposed for image classification in recent years with very deep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we used is 19 layers deep but involves with only 7.26 million parameters. Experiments were conducted using the ICDAR 2013 offline HCCR competition dataset. It has been shown that with the proper incorporation with traditional directional feature maps, the proposed single and ensemble HCCR-GoogLeNet models achieve new state of the art recognition accuracy of 96.35% and 96.74%, respectively, outperforming previous best result with significant gap.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05190",
        "title": "Image Reconstruction from Bag-of-Visual-Words",
        "authors": [
            "Hiroharu Kato",
            "Tatsuya Harada"
        ],
        "abstract": "The objective of this work is to reconstruct an original image from Bag-of-Visual-Words (BoVW). Image reconstruction from features can be a means of identifying the characteristics of features. Additionally, it enables us to generate novel images via features. Although BoVW is the de facto standard feature for image recognition and retrieval, successful image reconstruction from BoVW has not been reported yet. What complicates this task is that BoVW lacks the spatial information for including visual words. As described in this paper, to estimate an original arrangement, we propose an evaluation function that incorporates the naturalness of local adjacency and the global position, with a method to obtain related parameters using an external image database. To evaluate the performance of our method, we reconstruct images of objects of 101 kinds. Additionally, we apply our method to analyze object classifiers and to generate novel images via BoVW.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05192",
        "title": "Unsupervised Visual Representation Learning by Context Prediction",
        "authors": [
            "Carl Doersch",
            "Abhinav Gupta",
            "Alexei A. Efros"
        ],
        "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2016-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05212",
        "title": "Barcode Annotations for Medical Image Retrieval: A Preliminary Investigation",
        "authors": [
            "Hamid R. Tizhoosh"
        ],
        "abstract": "This paper proposes to generate and to use barcodes to annotate medical images and/or their regions of interest such as organs, tumors and tissue types. A multitude of efficient feature-based image retrieval methods already exist that can assign a query image to a certain image class. Visual annotations may help to increase the retrieval accuracy if combined with existing feature-based classification paradigms. Whereas with annotations we usually mean textual descriptions, in this paper barcode annotations are proposed. In particular, Radon barcodes (RBC) are introduced. As well, local binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as barcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test images is used to verify how barcodes could facilitate image retrieval.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05225",
        "title": "Image aesthetic evaluation using paralleled deep convolution neural network",
        "authors": [
            "Guo Lihua",
            "Li Fudi"
        ],
        "abstract": "Image aesthetic evaluation has attracted much attention in recent years. Image aesthetic evaluation methods heavily depend on the effective aesthetic feature. Traditional meth-ods always extract hand-crafted features. However, these hand-crafted features are always designed to adapt particu-lar datasets, and extraction of them needs special design. Rather than extracting hand-crafted features, an automati-cally learn of aesthetic features based on deep convolutional neural network (DCNN) is first adopt in this paper. As we all know, when the training dataset is given, the DCNN architecture with high complexity may meet the over-fitting problem. On the other side, the DCNN architecture with low complexity would not efficiently extract effective features. For these reasons, we further propose a paralleled convolutional neural network (PDCNN) with multi-level structures to automatically adapt to the training dataset. Experimental results show that our proposed PDCNN architecture achieves better performance than other traditional methods.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05232",
        "title": "Multi-scale recognition with DAG-CNNs",
        "authors": [
            "Songfan Yang",
            "Deva Ramanan"
        ],
        "abstract": "We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multiscale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even \"off-the-self\" multiscale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9% and 9.5%, respectively.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05233",
        "title": "Visual Understanding via Multi-Feature Shared Learning with Global Consistency",
        "authors": [
            "Lei Zhang",
            "David Zhang"
        ],
        "abstract": "Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing the attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed for better preserving the manifold structure of each feature, such that the label prediction power is much improved through the semi-supervised learning with global label consistency. For convenience, we call the proposed approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed method include: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; 3) an efficient alternative optimization method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach compares favorably with the state-of-the-art algorithms. An extensive experiment on the deep convolutional activation features also show the effectiveness of the proposed approach. The code is available on ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05240",
        "title": "Benchmarking KAZE and MCM for Multiclass Classification",
        "authors": [
            "Siddharth Srivastava",
            "Prerana Mukherjee",
            "Brejesh Lall"
        ],
        "abstract": "In this paper, we propose a novel approach for feature generation by appropriately fusing KAZE and SIFT features. We then use this feature set along with Minimal Complexity Machine(MCM) for object classification. We show that KAZE and SIFT features are complementary. Experimental results indicate that an elementary integration of these techniques can outperform the state-of-the-art approaches.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05254",
        "title": "Live Video Synopsis for Multiple Cameras",
        "authors": [
            "Yedid Hoshen",
            "Shmuel Peleg"
        ],
        "abstract": "Video surveillance cameras generate most of recorded video, and there is far more recorded video than operators can watch. Much progress has recently been made using summarization of recorded video, but such techniques do not have much impact on live video surveillance.\n",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05286",
        "title": "Measuring Visibility using Atmospheric Transmission and Digital Surface Model",
        "authors": [
            "Jean-Philippe Andreu",
            "Stefan Mayer",
            "Karlheinz Gutjahr",
            "Harald Ganster"
        ],
        "abstract": "Reliable and exact assessment of visibility is essential for safe air traffic. In order to overcome the drawbacks of the currently subjective reports from human observers, we present an approach to automatically derive visibility measures by means of image processing. It first exploits image based estimation of the atmospheric transmission describing the portion of the light that is not scattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the camera. Once the atmospheric transmission is estimated, a 3D representation of the vicinity (digital surface model: DMS) is used to compute depth measurements for the haze-free pixels and then derive a global visibility estimation for the airport. Results on foggy images demonstrate the validity of the proposed method.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05338",
        "title": "Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination of an Imaging System",
        "authors": [
            "Poorna Banerjee Dasgupta"
        ],
        "abstract": "Edge detection is one of the most principal techniques for detecting discontinuities in the gray levels of image pixels. The Modulation Transfer Function (MTF) is one of the main criteria for assessing imaging quality and is a parameter frequently used for measuring the sharpness of an imaging system. In order to determine the MTF, it is essential to determine the best edge from the target image so that an edge profile can be developed and then the line spread function and hence the MTF, can be computed accordingly. For regular image sizes, the human visual system is adept enough to identify suitable edges from the image. But considering huge image datasets, such as those obtained from satellites, the image size may range in few gigabytes and in such a case, manual inspection of images for determination of the best suitable edge is not plausible and hence, edge profiling tasks have to be automated. This paper presents a novel, yet simple, algorithm for edge ranking and detection from image data-sets for MTF computation, which is ideal for automation on vectorised graphical processing units.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05354",
        "title": "DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition",
        "authors": [
            "Weixin Yang",
            "Lianwen Jin",
            "Dacheng Tao",
            "Zecheng Xie",
            "Ziyong Feng"
        ],
        "abstract": "Inspired by the theory of Leitners learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher probability of being selected as training data in the next iteration; in contrast, well-trained and well-recognized samples with very high confidence will have a lower probability of being involved in the next training iteration and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05459",
        "title": "Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect",
        "authors": [
            "Hamed Sarbolandi",
            "Damien Lefloch",
            "Andreas Kolb"
        ],
        "abstract": "Recently, the new Kinect One has been issued by Microsoft, providing the next generation of real-time range sensing devices based on the Time-of-Flight (ToF) principle. As the first Kinect version was using a structured light approach, one would expect various differences in the characteristics of the range data delivered by both devices. This paper presents a detailed and in-depth comparison between both devices. In order to conduct the comparison, we propose a framework of seven different experimental setups, which is a generic basis for evaluating range cameras such as Kinect. The experiments have been designed with the goal to capture individual effects of the Kinect devices as isolatedly as possible and in a way, that they can also be adopted, in order to apply them to any other range sensing device. The overall goal of this paper is to provide a solid insight into the pros and cons of either device. Thus, scientists that are interested in using Kinect range sensing cameras in their specific application scenario can directly assess the expected, specific benefits and potential problem of either device.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05601",
        "title": "Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm",
        "authors": [
            "S L Happy",
            "Swarnadip Chatterjee",
            "Debdoot Sheet"
        ],
        "abstract": "Overlapping of cervical cells and poor contrast of cell cytoplasm are the major issues in accurate detection and segmentation of cervical cells. An unsupervised cell segmentation approach is presented here. Cell clump segmentation was carried out using the extended depth of field (EDF) image created from the images of different focal planes. A modified Otsu method with prior class weights is proposed for accurate segmentation of nuclei from the cell clumps. The cell cytoplasm was further segmented from cell clump depending upon the number of nucleus detected in that cell clump. Level set model was used for cytoplasm segmentation.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05612",
        "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering",
        "authors": [
            "Haoyuan Gao",
            "Junhua Mao",
            "Jie Zhou",
            "Zhiheng Huang",
            "Lei Wang",
            "Wei Xu"
        ],
        "abstract": "In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05641",
        "title": "Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views",
        "authors": [
            "Hao Su",
            "Charles R. Qi",
            "Yangyan Li",
            "Leonidas Guibas"
        ],
        "abstract": "Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs. We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05643",
        "title": "Object Modelling with a Handheld RGB-D Camera",
        "authors": [
            "Aitor Aldoma",
            "Johann Prankl",
            "Alexander Svejda",
            "Markus Vincze"
        ],
        "abstract": "This work presents a flexible system to reconstruct 3D models of objects captured with an RGB-D sensor. A major advantage of the method is that our reconstruction pipeline allows the user to acquire a full 3D model of the object. This is achieved by acquiring several partial 3D models in different sessions that are automatically merged together to reconstruct a full model. In addition, the 3D models acquired by our system can be directly used by state-of-the-art object instance recognition and object tracking modules, providing object-perception capabilities for different applications, such as human-object interaction analysis or robot grasping. The system does not impose constraints in the appearance of objects (textured, untextured) nor in the modelling setup (moving camera with static object or a turn-table setup). The proposed reconstruction system has been used to model a large number of objects resulting in metrically accurate and visually appealing 3D models.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05753",
        "title": "GazeDPM: Early Integration of Gaze Information in Deformable Part Models",
        "authors": [
            "Iaroslav Shcherbatyi",
            "Andreas Bulling",
            "Mario Fritz"
        ],
        "abstract": "An increasing number of works explore collaborative human-computer systems in which human gaze is used to enhance computer vision systems. For object detection these efforts were so far restricted to late integration approaches that have inherent limitations, such as increased precision without increase in recall. We propose an early integration approach in a deformable part model, which constitutes a joint formulation over gaze and visual data. We show that our GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a recent method for gaze-supported object detection by 3% on the public POET dataset. Our approach additionally provides introspection of the learnt models, can reveal salient image structures, and allows us to investigate the interplay between gaze attracting and repelling areas, the importance of view-specific models, as well as viewers' personal biases in gaze patterns. We finally study important practical aspects of our approach, such as the impact of using saliency maps instead of real fixations, the impact of the number of fixations, as well as robustness to gaze estimation error.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05769",
        "title": "Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos",
        "authors": [
            "Ishan Misra",
            "Abhinav Shrivastava",
            "Martial Hebert"
        ],
        "abstract": "We present a semi-supervised approach that localizes multiple unknown object instances in long videos. We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances. We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift. Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data. Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. In contrast, existing approaches either do not consider multiple object instances per video, or rely heavily on the motion of the objects present. The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality, coverage (recall), diversity, and relevance to training an object detector.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05819",
        "title": "New HSL Distance Based Colour Clustering Algorithm",
        "authors": [
            "Vasile Patrascu"
        ],
        "abstract": "In this paper, we define a distance for the HSL colour system. Next, the proposed distance is used for a fuzzy colour clustering algorithm construction. The presented algorithm is related to the well-known fuzzy c-means algorithm. Finally, the clustering algorithm is used as colour reduction method. The obtained experimental results are presented to demonstrate the effectiveness of our approach.\n    ",
        "submission_date": "2015-02-24T00:00:00",
        "last_modified_date": "2015-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05836",
        "title": "Object-Proposal Evaluation Protocol is 'Gameable'",
        "authors": [
            "Neelima Chavali",
            "Harsh Agrawal",
            "Aroma Mahendru",
            "Dhruv Batra"
        ],
        "abstract": "Object proposals have quickly become the de-facto pre-processing step in a number of vision pipelines (for object detection, object discovery, and other tasks). Their performance is usually evaluated on partially annotated datasets. In this paper, we argue that the choice of using a partially annotated dataset for evaluation of object proposals is problematic -- as we demonstrate via a thought experiment, the evaluation protocol is 'gameable', in the sense that progress under this protocol does not necessarily correspond to a \"better\" category independent object proposal algorithm.\n",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05914",
        "title": "A Multi-scale Multiple Instance Video Description Network",
        "authors": [
            "Huijuan Xu",
            "Subhashini Venugopalan",
            "Vasili Ramanishka",
            "Marcus Rohrbach",
            "Kate Saenko"
        ],
        "abstract": "Generating natural language descriptions for in-the-wild videos is a challenging task. Most state-of-the-art methods for solving this problem borrow existing deep convolutional neural network (CNN) architectures (AlexNet, GoogLeNet) to extract a visual representation of the input video. However, these deep CNN architectures are designed for single-label centered-positioned object classification. While they generate strong semantic features, they have no inherent structure allowing them to detect multiple objects of different sizes and locations in the frame. Our paper tries to solve this problem by integrating the base CNN into several fully convolutional neural networks (FCNs) to form a multi-scale network that handles multiple receptive field sizes in the original image. FCNs, previously applied to image segmentation, can generate class heat-maps efficiently compared to sliding window mechanisms, and can easily handle multiple scales. To further handle the ambiguity over multiple objects and locations, we incorporate the Multiple Instance Learning mechanism (MIL) to consider objects in different positions and at different scales simultaneously. We integrate our multi-scale multi-instance architecture with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Ours is the first end-to-end trainable architecture that is capable of multi-scale region processing. Evaluation on a Youtube video dataset shows the advantage of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can potentially be extended to support other video processing tasks.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05916",
        "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
        "authors": [
            "Erroll Wood",
            "Tadas Baltrusaitis",
            "Xucong Zhang",
            "Yusuke Sugano",
            "Peter Robinson",
            "Andreas Bulling"
        ],
        "abstract": "Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05957",
        "title": "Joint Inference of Groups, Events and Human Roles in Aerial Videos",
        "authors": [
            "Tianmin Shu",
            "Dan Xie",
            "Brandon Rothrock",
            "Sinisa Todorovic",
            "Song-Chun Zhu"
        ],
        "abstract": "With the advent of drones, aerial video analysis becomes increasingly important; yet, it has received scant attention in the literature. This paper addresses a new problem of parsing low-resolution aerial videos of large spatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning roles to people engaged in events. We propose a novel framework aimed at conducting joint inference of the above tasks, as reasoning about each in isolation typically fails in our setting. Given noisy tracklets of people and detections of large objects and scene surfaces (e.g., building, grass), we use a spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain Monte Carlo and dynamic programming. We also introduce a new formalism of spatiotemporal templates characterizing latent sub-events. For evaluation, we have collected and released a new aerial videos dataset using a hex-rotor flying over picnic areas rich with group events. Our results demonstrate that we successfully address above inference tasks under challenging conditions.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06027",
        "title": "Weakly-Supervised Alignment of Video With Text",
        "authors": [
            "Piotr Bojanowski",
            "R\u00e9mi Lajugie",
            "Edouard Grave",
            "Francis Bach",
            "Ivan Laptev",
            "Jean Ponce",
            "Cordelia Schmid"
        ],
        "abstract": "Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence. Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [36], using both bag-of-words and continuous representations for text.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06079",
        "title": "Robust Rotation Synchronization via Low-rank and Sparse Matrix Decomposition",
        "authors": [
            "Federica Arrigoni",
            "Andrea Fusiello",
            "Beatrice Rossi",
            "Pasqualina Fragneto"
        ],
        "abstract": "This paper deals with the rotation synchronization problem, which arises in global registration of 3D point-sets and in structure from motion. The problem is formulated in an unprecedented way as a \"low-rank and sparse\" matrix decomposition that handles both outliers and missing data. A minimization strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against state-of-the-art algorithms on simulated and real data. The results show that R-GoDec is the fastest among the robust algorithms.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2017-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06162",
        "title": "Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers",
        "authors": [
            "Anjith George",
            "Aurobinda Routray"
        ],
        "abstract": "The alertness level of drivers can be estimated with the use of computer vision based methods. The level of fatigue can be found from the value of PERCLOS. It is the ratio of closed eye frames to the total frames processed. The main objective of the thesis is the design and implementation of real-time algorithms for measurement of PERCLOS. In this work we have developed a real-time system which is able to process the video onboard and to alarm the driver in case the driver is in alert. For accurate estimation of PERCLOS the frame rate should be greater than 4 and accuracy should be greater than 90%. For eye detection we have used mainly two approaches Haar classifier based method and Principal Component Analysis (PCA) based method for day time. During night time active Near Infra Red (NIR) illumination is used. Local Binary Pattern (LBP) histogram based method is used for the detection of eyes at night time. The accuracy rate of the algorithms was found to be more than 90% at frame rates more than 5 fps which was suitable for the application.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06163",
        "title": "Direct Variational Perspective Shape from Shading with Cartesian Depth Parametrisation",
        "authors": [
            "Yong Chul Ju",
            "Daniel Maurer",
            "Michael Breu\u00df",
            "Andr\u00e9s Bruhn"
        ],
        "abstract": "Most of today's state-of-the-art methods for perspective shape from shading are modelled in terms of partial differential equations (PDEs) of Hamilton-Jacobi type. To improve the robustness of such methods w.r.t. noise and missing data, first approaches have recently been proposed that seek to embed the underlying PDE into a variational framework with data and smoothness term. So far, however, such methods either make use of a radial depth parametrisation that makes the regularisation hard to interpret from a geometrical viewpoint or they consider indirect smoothness terms that require additional consistency constraints to provide valid solutions. Moreover the minimisation of such frameworks is an intricate task, since the underlying energy is typically non-convex. In our paper we address all three of the aforementioned issues. First, we propose a novel variational model that operates directly on the Cartesian depth. In this context, we also point out a common mistake in the derivation of the surface normal. Moreover, we employ a direct second-order regulariser with edge-preservation property. This direct regulariser yields by construction valid solutions without requiring additional consistency constraints. Finally, we also propose a novel coarse-to-fine minimisation framework based on an alternating explicit scheme. This framework allows us to avoid local minima during the minimisation and thus to improve the accuracy of the reconstruction. Experiments show the good quality of our model as well as the usefulness of the proposed numerical scheme.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06219",
        "title": "A comparative study between proposed Hyper Kurtosis based Modified Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human Brain CT scan images",
        "authors": [
            "Sabyasachi Mukhopadhyay",
            "Soham Mandal",
            "Sawon Pratiher",
            "Satyasaran Changdar",
            "Ritwik Burman",
            "Nirmalya Ghosh",
            "Prasanta K. Panigrahi"
        ],
        "abstract": "In this paper, a comparative study between proposed hyper kurtosis based modified duo-histogram equalization (HKMDHE) algorithm and contrast limited adaptive histogram enhancement (CLAHE) has been presented for the implementation of contrast enhancement and brightness preservation of low contrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement is done on the hyper-kurtosis based application. The results are very promising of proposed HKMDHE technique with improved PSNR values and lesser AMMBE values than CLAHE technique.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06236",
        "title": "A Bottom-up Approach for Pancreas Segmentation using Cascaded Superpixels and (Deep) Image Patch Labeling",
        "authors": [
            "Amal Farag",
            "Le Lu",
            "Holger R. Roth",
            "Jiamin Liu",
            "Evrim Turkbey",
            "Ronald M. Summers"
        ],
        "abstract": "Robust automated organ segmentation is a prerequisite for computer-aided diagnosis (CAD), quantitative imaging analysis and surgical assistance. For high-variability organs such as the pancreas, previous approaches report undesirably low accuracies. We present a bottom-up approach for pancreas segmentation in abdominal CT scans that is based on a hierarchy of information propagation by classifying image patches at different resolutions; and cascading superpixels. There are four stages: 1) decomposing CT slice images as a set of disjoint boundary-preserving superpixels; 2) computing pancreas class probability maps via dense patch labeling; 3) classifying superpixels by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. The dense image patch labeling are conducted by: efficient random forest classifier on image histogram, location and texture features; and more expensive (but with better specificity) deep convolutional neural network classification on larger image windows (with more spatial contexts). Evaluation of the approach is performed on a database of 80 manually segmented CT volumes in six-fold cross-validation (CV). Our achieved results are comparable, or better than the state-of-the-art methods (evaluated by \"leave-one-patient-out\"), with Dice 70.7% and Jaccard 57.9%. The computational efficiency has been drastically improved in the order of 6~8 minutes, comparing with others of ~10 hours per case. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same datasets. Under six-fold CV, our bottom-up segmentation method significantly outperforms its MALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNN patch labeling confidences offer more numerical stability, reflected by smaller standard deviations.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06237",
        "title": "Tunnel Surface 3D Reconstruction from Unoriented Image Sequences",
        "authors": [
            "Arnold Bauer",
            "Karlheinz Gutjahr",
            "Gerhard Paar",
            "Heiner Kontrus",
            "Robert Glatzl"
        ],
        "abstract": "The 3D documentation of the tunnel surface during construction requires fast and robust measurement systems. In the solution proposed in this paper, during tunnel advance a single camera is taking pictures of the tunnel surface from several positions. The recorded images are automatically processed to gain a 3D tunnel surface model. Image acquisition is realized by the tunneling/advance/driving personnel close to the tunnel face (= the front end of the advance). Based on the following fully automatic analysis/evaluation, a decision on the quality of the outbreak can be made within a few minutes. This paper describes the image recording system and conditions as well as the stereo-photogrammetry based workflow for the continuously merged dense 3D reconstruction of the entire advance region. Geo-reference is realized by means of signalized targets that are automatically detected in the images. We report on the results of recent testing under real construction conditions, and conclude with prospects for further development in terms of on-site performance.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06250",
        "title": "Efficient Large Scale Video Classification",
        "authors": [
            "Balakrishnan Varadarajan",
            "George Toderici",
            "Sudheendra Vijayanarasimhan",
            "Apostol Natsev"
        ],
        "abstract": "Video classification has advanced tremendously over the recent years. A large part of the improvements in video classification had to do with the work done by the image classification community and the use of deep convolutional networks (CNNs) which produce competitive results with hand- crafted motion features. These networks were adapted to use video frames in various ways and have yielded state of the art classification results. We present two methods that build on this work, and scale it up to work with millions of videos and hundreds of thousands of classes while maintaining a low computational cost. In the context of large scale video processing, training CNNs on video frames is extremely time consuming, due to the large number of frames involved. We propose to avoid this problem by training CNNs on either YouTube thumbnails or Flickr images, and then using these networks' outputs as features for other higher level classifiers. We discuss the challenges of achieving this and propose two models for frame-level and video-level classification. The first is a highly efficient mixture of experts while the latter is based on long short term memory neural networks. We present results on the Sports-1M video dataset (1 million videos, 487 classes) and on a new dataset which has 12 million videos and 150,000 labels.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2015-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06319",
        "title": "The Minimum Spanning Tree of Maximum Entropy",
        "authors": [
            "Samuel de Sousa",
            "Walter G. Kropatsch"
        ],
        "abstract": "In computer vision, we have the problem of creating graphs out of unstructured point-sets, i.e. the data graph. A common approach for this problem consists of building a triangulation which might not always lead to the best solution. Small changes in the location of the points might generate graphs with unstable configurations and the topology of the graph could change significantly. After building the data-graph, one could apply Graph Matching techniques to register the original point-sets. In this paper, we propose a data graph technique based on the Minimum Spanning Tree of Maximum Entropty (MSTME). We aim at a data graph construction which could be more stable than the Delaunay triangulation with respect to small variations in the neighborhood of points. Our technique aims at creating data graphs which could help the point-set registration process. We propose an algorithm with a single free parameter that weighs the importance between the total weight cost and the entropy of the current spanning tree. We compare our algorithm on a number of different databases with the Delaunay triangulation.\n    ",
        "submission_date": "2015-05-23T00:00:00",
        "last_modified_date": "2015-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06389",
        "title": "Image Segmentation Using Hierarchical Merge Tree",
        "authors": [
            "Ting Liu",
            "Mojtaba Seyedhosseini",
            "Tolga Tasdizen"
        ],
        "abstract": "This paper investigates one of the most fundamental computer vision problems: image segmentation. We propose a supervised hierarchical approach to object-independent image segmentation. Starting with over-segmenting superpixels, we use a tree structure to represent the hierarchy of region merging, by which we reduce the problem of segmenting image regions to finding a set of label assignment to tree nodes. We formulate the tree structure as a constrained conditional model to associate region merging with likelihoods predicted using an ensemble boundary classifier. Final segmentations can then be inferred by finding globally optimal solutions to the model efficiently. We also present an iterative training and testing algorithm that generates various tree structures and combines them to emphasize accurate boundaries by segmentation accumulation. Experiment results and comparisons with other very recent methods on six public data sets demonstrate that our approach achieves the state-of-the-art region accuracy and is very competitive in image segmentation without semantic priors.\n    ",
        "submission_date": "2015-05-24T00:00:00",
        "last_modified_date": "2016-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06531",
        "title": "Affine and Regional Dynamic Time Warpng",
        "authors": [
            "Tsu-Wei Chen",
            "Meena Abdelmaseeh",
            "Daniel Stashuk"
        ],
        "abstract": "Pointwise matches between two time series are of great importance in time series analysis, and dynamic time warping (DTW) is known to provide generally reasonable matches. There are situations where time series alignment should be invariant to scaling and offset in amplitude or where local regions of the considered time series should be strongly reflected in pointwise matches. Two different variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are proposed to handle scaling and offset in amplitude and provide regional emphasis respectively. Furthermore, ADTW and RDTW can be combined in two different ways to generate alignments that incorporate advantages from both methods, where the affine model can be applied either globally to the entire time series or locally to each region. The proposed alignment methods outperform DTW on specific simulated datasets, and one-nearest-neighbor classifiers using their associated difference measures are competitive with the difference measures associated with state-of-the-art alignment methods on real datasets.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06578",
        "title": "A Simple Yet Effective Improvement to the Bilateral Filter for Image Denoising",
        "authors": [
            "Kollipara Rithwik",
            "Kunal Narayan Chaudhury"
        ],
        "abstract": "The bilateral filter has diverse applications in image processing, computer vision, and computational photography. In particular, this non-linear filter is quite effective in denoising images corrupted with additive Gaussian noise. The filter, however, is known to perform poorly at large noise levels. Several adaptations of the filter have been proposed in the literature to address this shortcoming, but often at an added computational cost. In this paper, we report a simple yet effective modification that improves the denoising performance of the bilateral filter at almost no additional cost. We provide visual and quantitative results on standard test images which show that this improvement is significant both visually and in terms of PSNR and SSIM (often as large as 5 dB). We also demonstrate how the proposed filtering can be implemented at reduced complexity by adapting a recent idea for fast bilateral filtering.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06600",
        "title": "Fast Detection of Curved Edges at Low SNR",
        "authors": [
            "Nati Ofir",
            "Meirav Galun",
            "Boaz Nadler",
            "Ronen Basri"
        ],
        "abstract": "Detecting edges is a fundamental problem in computer vision with many applications, some involving very noisy images. While most edge detection methods are fast, they perform well only on relatively clean images. Indeed, edges in such images can be reliably detected using only local filters. Detecting faint edges under high levels of noise cannot be done locally at the individual pixel level, and requires more sophisticated global processing. Unfortunately, existing methods that achieve this goal are quite slow. In this paper we develop a novel multiscale method to detect curved edges in noisy images. While our algorithm searches for edges over a huge set of candidate curves, it does so in a practical runtime, nearly linear in the total number of image pixels. As we demonstrate experimentally, our algorithm is orders of magnitude faster than previous methods designed to deal with high noise levels. Nevertheless, it obtains comparable, if not better, edge detection quality on a variety of challenging noisy images.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06605",
        "title": "Expresso : A user-friendly GUI for Designing, Training and Exploring Convolutional Neural Networks",
        "authors": [
            "Ravi Kiran Sarvadevabhatla",
            "R. Venkatesh Babu"
        ],
        "abstract": "With a view to provide a user-friendly interface for designing, training and developing deep learning frameworks, we have developed Expresso, a GUI tool written in Python. Expresso is built atop Caffe, the open-source, prize-winning framework popularly used to develop Convolutional Neural Networks. Expresso provides a convenient wizard-like graphical interface which guides the user through various common scenarios -- data import, construction and training of deep networks, performing various experiments, analyzing and visualizing the results of these experiments. The multi-threaded nature of Expresso enables concurrent execution and notification of events related to the aforementioned scenarios. The GUI sub-components and inter-component interfaces in Expresso have been designed with extensibility in mind. We believe Expresso's flexibility and ease of use will come in handy to researchers, newcomers and seasoned alike, in their explorations related to deep learning.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06606",
        "title": "Robust Optimization for Deep Regression",
        "authors": [
            "Vasileios Belagiannis",
            "Christian Rupprecht",
            "Gustavo Carneiro",
            "Nassir Navab"
        ],
        "abstract": "Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06611",
        "title": "Smooth PARAFAC Decomposition for Tensor Completion",
        "authors": [
            "Tatsuya Yokota",
            "Qibin Zhao",
            "Andrzej Cichocki"
        ],
        "abstract": "In recent years, low-rank based tensor completion, which is a higher-order extension of matrix completion, has received considerable attention. However, the low-rank assumption is not sufficient for the recovery of visual data, such as color and 3D images, where the ratio of missing data is extremely high. In this paper, we consider \"smoothness\" constraints as well as low-rank approximations, and propose an efficient algorithm for performing tensor completion that is particularly powerful regarding visual data. The proposed method admits significant advantages, owing to the integration of smooth PARAFAC decomposition for incomplete tensors and the efficient selection of models in order to minimize the tensor rank. Thus, our proposed method is termed as \"smooth PARAFAC tensor completion (SPC).\" In order to impose the smoothness constraints, we employ two strategies, total variation (SPC-TV) and quadratic variation (SPC-QV), and invoke the corresponding algorithms for model learning. Extensive experimental evaluations on both synthetic and real-world visual data illustrate the significant improvements of our method, in terms of both prediction performance and efficiency, compared with many state-of-the-art tensor completion methods.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06623",
        "title": "Recognition Confidence Analysis of Handwritten Chinese Character with CNN",
        "authors": [
            "Meijun He",
            "Shuye Zhang",
            "Huiyun Mao",
            "Lianwen Jin"
        ],
        "abstract": "In this paper, we present an effective method to analyze the recognition confidence of handwritten Chinese character, based on the softmax regression score of a high performance convolutional neural networks (CNN). Through careful and thorough statistics of 827,685 testing samples that randomly selected from total 8836 different classes of Chinese characters, we find that the confidence measurement based on CNN is an useful metric to know how reliable the recognition results are. Furthermore, we find by experiments that the recognition confidence can be used to find out similar and confusable character-pairs, to check wrongly or cursively written samples, and even to discover and correct mis-labelled samples. Many interesting observations and statistics are given and analyzed in this study.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06702",
        "title": "Smooth and iteratively Restore: A simple and fast edge-preserving smoothing model",
        "authors": [
            "Philipp Kniefacz",
            "Walter Kropatsch"
        ],
        "abstract": "In image processing, it can be a useful pre-processing step to smooth away small structures, such as noise or unimportant details, while retaining the overall structure of the image by keeping edges, which separate objects, sharp. Typically this edge-preserving smoothing process is achieved using edge-aware filters. However such filters may preserve unwanted small structures as well if they contain edges. In this work we present a novel framework for edge-preserving smoothing which separates the process into two different steps: First the image is smoothed using a blurring filter and in the second step the important edges are restored using a guided edge-aware filter. The presented method proves to deliver very good results, compared to state-of-the-art edge-preserving smoothing filters, especially at removing unwanted small structures. Furthermore it is very versatile and can easily be adapted to different fields of applications while at the same time being very fast to compute and therefore well-suited for real time applications.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06769",
        "title": "VeinPLUS: A Transillumination and Reflection-based Hand Vein Database",
        "authors": [
            "Alexander Gruschina"
        ],
        "abstract": "This paper gives a short summary of work related to the creation of a department-hosted hand vein database. After the introducing section, special properties of the hand vein acquisition are explained, followed by a comparison table, which shows key differences to existing well-known hand vein databases. At the end, the ROI extraction process is described and sample images and ROIs are presented.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2015-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06795",
        "title": "An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability",
        "authors": [
            "Nikolaos Karianakis",
            "Jingming Dong",
            "Stefano Soatto"
        ],
        "abstract": "We conduct an empirical study to test the ability of Convolutional Neural Networks (CNNs) to reduce the effects of nuisance transformations of the input data, such as location, scale and aspect ratio. We isolate factors by adopting a common convolutional architecture either deployed globally on the image to compute class posterior distributions, or restricted locally to compute class conditional distributions given location, scale and aspect ratios of bounding boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior performance compared to proper marginalization. Yet empirical evidence suggests the converse, leading us to conclude that - at the current level of complexity of convolutional architectures and scale of the data sets used to train them - CNNs are not very effective at marginalizing nuisance variability. We also quantify the effects of context on the overall classification task and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using the Oxford and Fischer's datasets.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06798",
        "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection",
        "authors": [
            "Xiangyu Zhang",
            "Jianhua Zou",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., >=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06800",
        "title": "Boosting-like Deep Learning For Pedestrian Detection",
        "authors": [
            "Lei Wang",
            "Baochang Zhang"
        ],
        "abstract": "This paper proposes boosting-like deep learning (BDL) framework for pedestrian detection. Due to overtraining on the limited training samples, overfitting is a major problem of deep learning. We incorporate a boosting-like technique into deep learning to weigh the training samples, and thus prevent overtraining in the iterative process. We theoretically give the details of derivation of our algorithm, and report the experimental results on open data sets showing that BDL achieves a better stable performance than the state-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the average miss rate compared with ACF and JointDeep on the largest Caltech benchmark dataset, respectively.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06814",
        "title": "Discrete Independent Component Analysis (DICA) with Belief Propagation",
        "authors": [
            "Francesco A. N. Palmieri",
            "Amedeo Buonanno"
        ],
        "abstract": "We apply belief propagation to a Bayesian bipartite graph composed of discrete independent hidden variables and discrete visible variables. The network is the Discrete counterpart of Independent Component Analysis (DICA) and it is manipulated in a factor graph form for inference and learning. A full set of simulations is reported for character images from the MNIST dataset. The results show that the factorial code implemented by the sources contributes to build a good generative model for the data that can be used in various inference modes.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06821",
        "title": "Deep Ranking for Person Re-identification via Joint Representation Learning",
        "authors": [
            "Shi-Zhe Chen",
            "Chun-Chao Guo",
            "Jian-Huang Lai"
        ],
        "abstract": "This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better ability to generalize across datasets without fine-tuning.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06957",
        "title": "Sequential Dimensionality Reduction for Extracting Localized Features",
        "authors": [
            "Gabriella Casalino",
            "Nicolas Gillis"
        ],
        "abstract": "Linear dimensionality reduction techniques are powerful tools for image analysis as they allow the identification of important features in a data set. In particular, nonnegative matrix factorization (NMF) has become very popular as it is able to extract sparse, localized and easily interpretable features by imposing an additive combination of nonnegative basis elements. Nonnegative matrix underapproximation (NMU) is a closely related technique that has the advantage to identify features sequentially. In this paper, we propose a variant of NMU that is particularly well suited for image analysis as it incorporates the spatial information, that is, it takes into account the fact that neighboring pixels are more likely to be contained in the same features, and favors the extraction of localized features by looking for sparse basis elements. We show that our new approach competes favorably with comparable state-of-the-art techniques on synthetic, facial and hyperspectral image data sets.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06973",
        "title": "Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts",
        "authors": [
            "Margret Keuper",
            "Evgeny Levinkov",
            "Nicolas Bonneel",
            "Guillaume Lavou\u00e9",
            "Thomas Brox",
            "Bjoern Andres"
        ],
        "abstract": "Formulations of the Image Decomposition Problem as a Multicut Problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art, suggesting that the LMP is a competitive formulation of the Image Decomposition Problem. To demonstrate the generality of the LMP, we apply it also to the Mesh Decomposition Problem posed by the Princeton benchmark, obtaining state-of-the-art decompositions.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07192",
        "title": "Inner and Inter Label Propagation: Salient Object Detection in the Wild",
        "authors": [
            "Hongyang Li",
            "Huchuan Lu",
            "Zhe Lin",
            "Xiaohui Shen",
            "Brian Price"
        ],
        "abstract": "In this paper, we propose a novel label propagation based method for saliency detection. A key observation is that saliency in an image can be estimated by propagating the labels extracted from the most certain background and object regions. For most natural images, some boundary superpixels serve as the background labels and the saliency of other superpixels are determined by ranking their similarities to the boundary labels based on an inner propagation scheme. For images of complex scenes, we further deploy a 3-cue-center-biased objectness measure to pick out and propagate foreground labels. A co-transduction algorithm is devised to fuse both boundary and objectness labels based on an inter propagation scheme. The compactness criterion decides whether the incorporation of objectness labels is necessary, thus greatly enhancing computational efficiency. Results on five benchmark datasets with pixel-wise accurate annotations show that the proposed method achieves superior performance compared with the newest state-of-the-arts in terms of different evaluation metrics.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07203",
        "title": "New characterizations of minimum spanning trees and of saliency maps based on quasi-flat zones",
        "authors": [
            "Jean Cousty",
            "Laurent Najman",
            "Yukiko Kenmochi",
            "Silvio Guimar\u00c3\u00a3es"
        ],
        "abstract": "We study three representations of hierarchies of partitions: dendrograms (direct representations), saliency maps, and minimum spanning trees. We provide a new bijection between saliency maps and hierarchies based on quasi-flat zones as used in image processing and characterize saliency maps and minimum spanning trees as solutions to constrained minimization problems where the constraint is quasi-flat zones preservation. In practice, these results form a toolkit for new hierarchical methods where one can choose the most convenient representation. They also invite us to process non-image data with morphological hierarchies.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07293",
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling",
        "authors": [
            "Vijay Badrinarayanan",
            "Ankur Handa",
            "Roberto Cipolla"
        ],
        "abstract": "We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07376",
        "title": "Texture Synthesis Using Convolutional Neural Networks",
        "authors": [
            "Leon A. Gatys",
            "Alexander S. Ecker",
            "Matthias Bethge"
        ],
        "abstract": "Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07409",
        "title": "Improving Spatial Codification in Semantic Segmentation",
        "authors": [
            "Carles Ventura",
            "Xavier Gir\u00f3-i-Nieto",
            "Ver\u00f3nica Vilaplana",
            "Kevin McGuinness",
            "Ferran Marqu\u00e9s",
            "Noel E. O'Connor"
        ],
        "abstract": "This paper explores novel approaches for improving the spatial codification for the pooling of local descriptors to solve the semantic segmentation problem. We propose to partition the image into three regions for each object to be described: Figure, Border and Ground. This partition aims at minimizing the influence of the image context on the object description and vice versa by introducing an intermediate zone around the object contour. Furthermore, we also propose a richer visual descriptor of the object by applying a Spatial Pyramid over the Figure region. Two novel Spatial Pyramid configurations are explored: Cartesian-based and crown-based Spatial Pyramids. We test these approaches with state-of-the-art techniques and show that they improve the Figure-Ground based pooling in the Pascal VOC 2011 and 2012 semantic segmentation challenges.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07427",
        "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization",
        "authors": [
            "Alex Kendall",
            "Matthew Grimes",
            "Roberto Cipolla"
        ],
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07428",
        "title": "Training a Convolutional Neural Network for Appearance-Invariant Place Recognition",
        "authors": [
            "Ruben Gomez-Ojeda",
            "Manuel Lopez-Antequera",
            "Nicolai Petkov",
            "Javier Gonzalez-Jimenez"
        ],
        "abstract": "Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.\n    ",
        "submission_date": "2015-05-27T00:00:00",
        "last_modified_date": "2015-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07647",
        "title": "Visual Search at Pinterest",
        "authors": [
            "Yushi Jing",
            "David Liu",
            "Dmitry Kislyuk",
            "Andrew Zhai",
            "Jiajing Xu",
            "Jeff Donahue",
            "Sarah Tavel"
        ],
        "abstract": "We demonstrate that, with the availability of distributed computation platforms such as Amazon Web Services and open-source tools, it is possible for a small engineering team to build, launch and maintain a cost-effective, large-scale visual search system with widely available tools. We also demonstrate, through a comprehensive set of live experiments at Pinterest, that content recommendation powered by visual search improve user engagement. By sharing our implementation details and the experiences learned from launching a commercial visual search engines from scratch, we hope visual search are more widely incorporated into today's commercial applications.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07672",
        "title": "A Generative Model of Natural Texture Surrogates",
        "authors": [
            "Niklas Ludtke",
            "Debapriya Das",
            "Lucas Theis",
            "Matthias Bethge"
        ],
        "abstract": "Natural images can be viewed as patchworks of different textures, where the local image statistics is roughly stationary within a small neighborhood but otherwise varies from region to region. In order to model this variability, we first applied the parametric texture algorithm of Portilla and Simoncelli to image patches of 64X64 pixels in a large database of natural images such that each image patch is then described by 655 texture parameters which specify certain statistics, such as variances and covariances of wavelet coefficients or coefficient magnitudes within that patch.\n",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07675",
        "title": "Improved Deep Convolutional Neural Network For Online Handwritten Chinese Character Recognition using Domain-Specific Knowledge",
        "authors": [
            "Weixin Yang",
            "Lianwen Jin",
            "Zecheng Xie",
            "Ziyong Feng"
        ],
        "abstract": "Deep convolutional neural networks (DCNNs) have achieved great success in various computer vision and pattern recognition applications, including those for handwritten Chinese character recognition (HCCR). However, most current DCNN-based HCCR approaches treat the handwritten sample simply as an image bitmap, ignoring some vital domain-specific information that may be useful but that cannot be learnt by traditional networks. In this paper, we propose an enhancement of the DCNN approach to online HCCR by incorporating a variety of domain-specific knowledge, including deformation, non-linear normalization, imaginary strokes, path signature, and 8-directional features. Our contribution is twofold. First, these domain-specific technologies are investigated and integrated with a DCNN to form a composite network to achieve improved performance. Second, the resulting DCNNs with diversity in their domain knowledge are combined using a hybrid serial-parallel (HSP) strategy. Consequently, we achieve a promising accuracy of 97.20% and 96.87% on CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best results previously reported in the literature.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07778",
        "title": "Query by String word spotting based on character bi-gram indexing",
        "authors": [
            "Suman K. Ghosh",
            "Ernest Valveny"
        ],
        "abstract": "In this paper we propose a segmentation-free query by string word spotting method. Both the documents and query strings are encoded using a recently proposed word representa- tion that projects images and strings into a common atribute space based on a pyramidal histogram of characters(PHOC). These attribute models are learned using linear SVMs over the Fisher Vector representation of the images along with the PHOC labels of the corresponding strings. In order to search through the whole page, document regions are indexed per character bi- gram using a similar attribute representation. On top of that, we propose an integral image representation of the document using a simplified version of the attribute model for efficient computation. Finally we introduce a re-ranking step in order to boost retrieval performance. We show state-of-the-art results for segmentation-free query by string word spotting in single-writer and multi-writer standard datasets\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07922",
        "title": "Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network",
        "authors": [
            "Junshi Huang",
            "Rogerio S. Feris",
            "Qiang Chen",
            "Shuicheng Yan"
        ],
        "abstract": "We address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two sub-networks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07923",
        "title": "Fast Computation of PERCLOS and Saccadic Ratio",
        "authors": [
            "Anirban Dasgupta",
            "Aurobinda Routray"
        ],
        "abstract": "This thesis describes the development of fast algorithms for the computation of PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SR are two ocular parameters reported to be measures of alertness levels in human beings. PERCLOS is the percentage of time in which at least 80% of the eyelid remains closed over the pupil. Saccades are fast and simultaneous movement of both the eyes in the same direction. SR is the ratio of peak saccadic velocity to the saccadic duration. This thesis addresses the issues of image based estimation of PERCLOS and SR, prevailing in the literature such as illumination variation, poor illumination conditions, head rotations etc. In this work, algorithms for real-time PERCLOS computation has been developed and implemented on an embedded platform. The platform has been used as a case study for assessment of loss of attention in automotive drivers. The SR estimation has been carried out offline as real-time implementation requires high frame rates of processing which is difficult to achieve due to hardware limitations. The accuracy in estimation of the loss of attention using PERCLOS and SR has been validated using brain signals, which are reported to be an authentic cue for estimating the state of alertness in human beings. The major contributions of this thesis include database creation, design and implementation of fast algorithms for estimating PERCLOS and SR on embedded computing platforms.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07930",
        "title": "Salient Object Detection via Augmented Hypotheses",
        "authors": [
            "Tam V. Nguyen",
            "Jose Sepulveda"
        ],
        "abstract": "In this paper, we propose using \\textit{augmented hypotheses} which consider objectness, foreground and compactness for salient object detection. Our algorithm consists of four basic steps. First, our method generates the objectness map via objectness hypotheses. Based on the objectness map, we estimate the foreground margin and compute the corresponding foreground map which prefers the foreground objects. From the objectness map and the foreground map, the compactness map is formed to favor the compact objects. We then derive a saliency measure that produces a pixel-accurate saliency map which uniformly covers the objects of interest and consistently separates fore- and background. We finally evaluate the proposed framework on two challenging datasets, MSRA-1000 and iCoSeg. Our extensive experimental results show that our method outperforms state-of-the-art approaches.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07934",
        "title": "Symbolic Segmentation Using Algorithm Selection",
        "authors": [
            "Martin Lukac",
            "Kamila Abdiyeva",
            "Michitaka Kameyama"
        ],
        "abstract": "In this paper we present an alternative approach to symbolic segmentation; instead of implementing a new method we approach symbolic segmentation as an algorithm selection problem. That is, let there be $n$ available algorithms for symbolic segmentation, a selection mechanism forms a set of input features and image attributes and selects on a case by case basis the best algorithm. The selection mechanism is demonstrated from within an algorithm framework where the selection is done in a set of various algorithm networks. Two sets of experiments are performed and in both cases we demonstrate that the algorithm selection allows to increase the result of the symbolic segmentation by a considerable amount.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08070",
        "title": "General Deformations of Point Configurations Viewed By a Pinhole Model Camera",
        "authors": [
            "Yirmeyahu Kaminski",
            "Michael Werman"
        ],
        "abstract": "This paper is a theoretical study of the following Non-Rigid Structure from Motion problem. What can be computed from a monocular view of a parametrically deforming set of points? We treat various variations of this problem for affine and polynomial deformations with calibrated and uncalibrated cameras. We show that in general at least three images with quasi-identical two deformations are needed in order to have a finite set of solutions of the points' structure and calculate some simple examples.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2022-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08071",
        "title": "Geometry of Graph Edit Distance Spaces",
        "authors": [
            "Brijnesh J. Jain"
        ],
        "abstract": "In this paper we study the geometry of graph spaces endowed with a special class of graph edit distances. The focus is on geometrical results useful for statistical pattern recognition. The main result is the Graph Representation Theorem. It states that a graph is a point in some geometrical space, called orbit space. Orbit spaces are well investigated and easier to explore than the original graph space. We derive a number of geometrical results from the orbit space representation, translate them to the graph space, and indicate their significance and usefulness in statistical pattern recognition.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08082",
        "title": "Learning to count with deep object features",
        "authors": [
            "Santi Segu\u00ed",
            "Oriol Pujol",
            "Jordi Vitri\u00e0"
        ],
        "abstract": "Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08153",
        "title": "Feature Representation for Online Signature Verification",
        "authors": [
            "Mohsen Fayyaz",
            "Mohammad Hajizadeh_Saffar",
            "Mohammad Sabokrou",
            "Mahmood Fathy"
        ],
        "abstract": "Biometrics systems have been used in a wide range of applications and have improved people authentication. Signature verification is one of the most common biometric methods with techniques that employ various specifications of a signature. Recently, deep learning has achieved great success in many fields, such as image, sounds and text processing. In this paper, deep learning method has been used for feature extraction and feature selection.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00051",
        "title": "Bag of Genres for Video Retrieval",
        "authors": [
            "Leonardo A. Duarte",
            "Ot\u00e1vio A. B. Penatti",
            "Jurandy Almeida"
        ],
        "abstract": "Often, videos are composed of multiple concepts or even genres. For instance, news videos may contain sports, action, nature, etc. Therefore, encoding the distribution of such concepts/genres in a compact and effective representation is a challenging task. In this sense, we propose the Bag of Genres representation, which is based on a visual dictionary defined by a genre classifier. Each visual word corresponds to a region in the classification space. The Bag of Genres video vector contains a summary of the activations of each genre in the video content. We evaluate the proposed method for video genre retrieval using the dataset of MediaEval Tagging Task of 2012 and for video event retrieval using the EVVE dataset. Results show that the proposed method achieves results comparable or superior to state-of-the-art methods, with the advantage of providing a much more compact representation than existing features.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2020-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00060",
        "title": "A Three-stage Approach for Segmenting Degraded Color Images: Smoothing, Lifting and Thresholding (SLaT)",
        "authors": [
            "Xiaohao Cai",
            "Raymond Chan",
            "Mila Nikolova",
            "Tieyong Zeng"
        ],
        "abstract": "In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) method with three stages for multiphase segmentation of color images corrupted by different degradations: noise, information loss, and blur. At the first stage, a convex variant of the Mumford-Shah model is applied to each channel to obtain a smooth image. We show that the model has unique solution under the different degradations. In order to properly handle the color information, the second stage is dimension lifting where we consider a new vector-valued image composed of the restored image and its transform in the secondary color space with additional information. This ensures that even if the first color space has highly correlated channels, we can still have enough information to give good segmentation results. In the last stage, we apply multichannel thresholding to the combined vector-valued image to find the segmentation. The number of phases is only required in the last stage, so users can choose or change it all without the need of solving the previous stages again. Experiments demonstrate that our SLaT method gives excellent results in terms of segmentation quality and CPU time in comparison with other state-of-the-art segmentation methods.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00097",
        "title": "A Review of Feature and Data Fusion with Medical Images",
        "authors": [
            "Alex Pappachen James",
            "Belur Dasarathy"
        ],
        "abstract": "The fusion techniques that utilize multiple feature sets to form new features that are often more robust and contain useful information for future processing are referred to as feature fusion. The term data fusion is applied to the class of techniques used for combining decisions obtained from multiple feature sets to form global decisions. Feature and data fusion interchangeably represent two important classes of techniques that have proved to be of practical importance in a wide range of medical imaging problems\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00168",
        "title": "Using curvature to distinguish between surface reflections and vessel contents in computer vision based recognition of materials in transparent vessels",
        "authors": [
            "Sagi Eppel"
        ],
        "abstract": "The recognition of materials and objects inside transparent containers using computer vision has a wide range of applications, ranging from industrial bottles filling to the automation of chemistry laboratory. One of the main challenges in such recognition is the ability to distinguish between image features resulting from the vessels surface and image features resulting from the material inside the vessel. Reflections and the functional parts of a vessels surface can create strong edges that can be mistakenly identified as corresponding to the vessel contents, and cause recognition errors. The ability to evaluate whether a specific edge in an image stems from the vessels surface or from its contents can considerably improve the ability to identify materials inside transparent vessels. This work will suggest a method for such evaluation, based on the following two assumptions: 1) Areas of high curvature on the vessel surface are likely to cause strong edges due to changes in reflectivity, as is the appearance of functional parts (e.g. corks or valves). 2) Most transparent vessels (bottles, glasses) have high symmetry (cylindrical). As a result the curvature angle of the vessels surface at each point of the image is similar to the curvature angle of the contour line of the vessel in the same row in the image. These assumptions, allow the identification of image regions with strong edges corresponding to the vessel surface reflections. Combining this method with existing image analysis methods for detecting materials inside transparent containers allows considerable improvement in accuracy.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00278",
        "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering",
        "authors": [
            "Licheng Yu",
            "Eunbyung Park",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.\n    ",
        "submission_date": "2015-05-31T00:00:00",
        "last_modified_date": "2015-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00368",
        "title": "RBIR using Interest Regions and Binary Signatures",
        "authors": [
            "Thanh The Van",
            "Thanh Manh Le"
        ],
        "abstract": "In this paper, we introduce an approach to overcome the low accuracy of the Content-Based Image Retrieval (CBIR) (when using the global features). To increase the accuracy, we use Harris-Laplace detector to identify the interest regions of image. Then, we build the Region-Based Image Retrieval (RBIR). For the efficient image storage and retrieval, we encode images into binary signatures. The binary signature of a image is created from its interest regions. Furthermore, this paper also provides an algorithm for image retrieval on S-tree by comparing the images' signatures on a metric similarly to EMD (earth mover's distance). Finally, we evaluate the created models on COREL's images.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00395",
        "title": "Hierarchical structure-and-motion recovery from uncalibrated images",
        "authors": [
            "Roberto Toldo",
            "Riccardo Gherardi",
            "Michela Farenzena",
            "Andrea Fusiello"
        ],
        "abstract": "This paper addresses the structure-and-motion problem, that requires to find camera motion and 3D struc- ture from point matches. A new pipeline, dubbed Samantha, is presented, that departs from the prevailing sequential paradigm and embraces instead a hierarchical approach. This method has several advantages, like a provably lower computational complexity, which is necessary to achieve true scalability, and better error containment, leading to more stability and less drift. Moreover, a practical autocalibration procedure allows to process images without ancillary information. Experiments with real data assess the accuracy and the computational efficiency of the method.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00473",
        "title": "An Efficient Algorithm for Video Super-Resolution Based On a Sequential Model",
        "authors": [
            "Patrick H\u00e9as",
            "Ang\u00e9lique Dr\u00e9meau",
            "C\u00e9dric Herzet"
        ],
        "abstract": "In this work, we propose a novel procedure for video super-resolution, that is the recovery of a sequence of high-resolution images from its low-resolution counterpart. Our approach is based on a \"sequential\" model (i.e., each high-resolution frame is supposed to be a displaced version of the preceding one) and considers the use of sparsity-enforcing priors. Both the recovery of the high-resolution images and the motion fields relating them is tackled. This leads to a large-dimensional, non-convex and non-smooth problem. We propose an algorithmic framework to address the latter. Our approach relies on fast gradient evaluation methods and modern optimization techniques for non-differentiable/non-convex problems. Unlike some other previous works, we show that there exists a provably-convergent method with a complexity linear in the problem dimensions. We assess the proposed optimization method on {several video benchmarks and emphasize its good performance with respect to the state of the art.}\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2016-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00481",
        "title": "Robust Face Recognition with Structural Binary Gradient Patterns",
        "authors": [
            "Weilin Huang",
            "Hujun Yin"
        ],
        "abstract": "This paper presents a computationally efficient yet powerful binary framework for robust facial representation based on image gradients. It is termed as structural binary gradient patterns (SBGP). To discover underlying local structures in the gradient domain, we compute image gradients from multiple directions and simplify them into a set of binary strings. The SBGP is derived from certain types of these binary strings that have meaningful local structures and are capable of resembling fundamental textural information. They detect micro orientational edges and possess strong orientation and locality capabilities, thus enabling great discrimination. The SBGP also benefits from the advantages of the gradient domain and exhibits profound robustness against illumination variations. The binary strategy realized by pixel correlations in a small neighborhood substantially simplifies the computational complexity and achieves extremely efficient processing with only 0.0032s in Matlab for a typical face image. Furthermore, the discrimination power of the SBGP can be enhanced on a set of defined orientational image gradient magnitudes, further enforcing locality and orientation. Results of extensive experiments on various benchmark databases illustrate significant improvements of the SBGP based representations over the existing state-of-the-art local descriptors in the terms of discrimination, robustness and complexity. Codes for the SBGP methods will be available at ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00752",
        "title": "What Makes Kevin Spacey Look Like Kevin Spacey",
        "authors": [
            "Supasorn Suwajanakorn",
            "Ira Kemelmacher-Shlizerman",
            "Steve Seitz"
        ],
        "abstract": "We reconstruct a controllable model of a person from a large photo collection that captures his or her {\\em persona}, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or {\\em puppeteer} the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00761",
        "title": "Image Retrieval Based on Binary Signature ang S-kGraph",
        "authors": [
            "Thanh The Van",
            "Thanh Manh Le"
        ],
        "abstract": "In this paper, we introduce an optimum approach for querying similar images on large digital-image databases. Our work is based on RBIR (region-based image retrieval) method which uses multiple regions as the key to retrieval images. This method significantly improves the accuracy of queries. However, this also increases the cost of computing. To reduce this expensive computational cost, we implement binary signature encoder which maps an image to its identification in binary. In order to fasten the lookup, binary signatures of images are classified by the help of S-kGraph. Finally, our work is evaluated on COREL's images.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00815",
        "title": "Classify Images with Conceptor Network",
        "authors": [
            "Yuhuang Hu",
            "M.S. Ishwarya",
            "Chu Kiong Loo"
        ],
        "abstract": "This article demonstrates a new conceptor network based classifier in classifying images. Mathematical descriptions and analysis are presented. Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10 and CIFAR-100. The experiments displayed that conceptor network can offer superior results and flexible configurations than conventional classifiers such as Softmax Regression and Support Vector Machine (SVM).\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00925",
        "title": "Facial Expressions Tracking and Recognition: Database Protocols for Systems Validation and Evaluation",
        "authors": [
            "Catarina Runa Miranda",
            "Pedro Mendes",
            "Pedro Coelho",
            "Xenxo Alvarez",
            "Jo\u00e3o Freitas",
            "Miguel Sales Dias",
            "Ver\u00f3nica Costa Orvalho"
        ],
        "abstract": "Each human face is unique. It has its own shape, topology, and distinguishing features. As such, developing and testing facial tracking systems are challenging tasks. The existing face recognition and tracking algorithms in Computer Vision mainly specify concrete situations according to particular goals and applications, requiring validation methodologies with data that fits their purposes. However, a database that covers all possible variations of external and factors does not exist, increasing researchers' work in acquiring their own data or compiling groups of databases.\n",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01092",
        "title": "Bilinear Random Projections for Locality-Sensitive Binary Codes",
        "authors": [
            "Saehoon Kim",
            "Seungjin Choi"
        ],
        "abstract": "Locality-sensitive hashing (LSH) is a popular data-independent indexing method for approximate similarity search, where random projections followed by quantization hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. Most of high-dimensional visual descriptors for images exhibit a natural matrix structure. When visual descriptors are represented by high-dimensional feature vectors and long binary codes are assigned, a random projection matrix requires expensive complexities in both space and time. In this paper we analyze a bilinear random projection method where feature matrices are transformed to binary codes by two smaller random projection matrices. We base our theoretical analysis on extending Raginsky and Lazebnik's result where random Fourier features are composed with random binary quantizers to form locality sensitive binary codes. To this end, we answer the following two questions: (1) whether a bilinear random projection also yields similarity-preserving binary codes; (2) whether a bilinear random projection yields performance gain or loss, compared to a large linear projection. Regarding the first question, we present upper and lower bounds on the expected Hamming distance between binary codes produced by bilinear random projections. In regards to the second question, we analyze the upper and lower bounds on covariance between two bits of binary codes, showing that the correlation between two bits is small. Numerical experiments on MNIST and Flickr45K datasets confirm the validity of our method.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01115",
        "title": "Hyperspectral Image Classification and Clutter Detection via Multiple Structural Embeddings and Dimension Reductions",
        "authors": [
            "Alexandros-Stavros Iliopoulos",
            "Tiancheng Liu",
            "Xiaobai Sun"
        ],
        "abstract": "We present a new and effective approach for Hyperspectral Image (HSI) classification and clutter detection, overcoming a few long-standing challenges presented by HSI data characteristics. Residing in a high-dimensional spectral attribute space, HSI data samples are known to be strongly correlated in their spectral signatures, exhibit nonlinear structure due to several physical laws, and contain uncertainty and noise from multiple sources. In the presented approach, we generate an adaptive, structurally enriched representation environment, and employ the locally linear embedding (LLE) in it. There are two structure layers external to LLE. One is feature space embedding: the HSI data attributes are embedded into a discriminatory feature space where spatio-spectral coherence and distinctive structures are distilled and exploited to mitigate various difficulties encountered in the native hyperspectral attribute space. The other structure layer encloses the ranges of algorithmic parameters for LLE and feature embedding, and supports a multiplexing and integrating scheme for contending with multi-source uncertainty. Experiments on two commonly used HSI datasets with a small number of learning samples have rendered remarkably high-accuracy classification results, as well as distinctive maps of detected clutter regions.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01125",
        "title": "Unsupervised domain adaption dictionary learning for visual recognition",
        "authors": [
            "Zhun Zhong",
            "Zongmin Li",
            "Runlin Li",
            "Xiaoxia Sun"
        ],
        "abstract": "Over the last years, dictionary learning method has been extensively applied to deal with various computer vision recognition applications, and produced state-of-the-art results. However, when the data instances of a target domain have a different distribution than that of a source domain, the dictionary learning method may fail to perform well. In this paper, we address the cross-domain visual recognition problem and propose a simple but effective unsupervised domain adaption approach, where labeled data are only from source domain. In order to bring the original data in source and target domain into the same distribution, the proposed method forcing nearest coupled data between source and target domain to have identical sparse representations while jointly learning dictionaries for each domain, where the learned dictionaries can reconstruct original data in source and target domain respectively. So that sparse representations of original data can be used to perform visual recognition tasks. We demonstrate the effectiveness of our approach on standard datasets. Our method performs on par or better than competitive state-of-the-art methods.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01144",
        "title": "What value do explicit high level concepts have in vision to language problems?",
        "authors": [
            "Qi Wu",
            "Chunhua Shen",
            "Lingqiao Liu",
            "Anthony Dick",
            "Anton van den Hengel"
        ],
        "abstract": "Much of the recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. We propose here a method of incorporating high-level concepts into the very successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art performance in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. In doing so we provide an analysis of the value of high level semantic information in V2L problems.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01151",
        "title": "Understanding deep features with computer-generated imagery",
        "authors": [
            "Mathieu Aubry",
            "Bryan Russell"
        ],
        "abstract": "We introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses corresponding to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, and Oxford VGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01165",
        "title": "Image Retrieval System Base on EMD Similarity Measure and S-Tree",
        "authors": [
            "Thanh Manh Le",
            "Thanh The Van"
        ],
        "abstract": "The paper approaches the binary signature for each image based on the percentage of the pixels in each color images, at the same time the paper builds a similar measure between images based on EMD (Earth Mover's Distance). Besides, the paper proceeded to create the S-tree based on the similar measure EMD to store the image's binary signatures to quickly query image signature data. From there, the paper build an image retrieval algorithm and CBIR (Content-Based Image Retrieval) based on a similar measure EMD and S-tree. Based on this theory, the paper proceeded to build application and experimental assessment of the process of querying image on the database system which have over 10,000 images.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01166",
        "title": "Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree",
        "authors": [
            "Thanh The Van",
            "Thanh Manh Le"
        ],
        "abstract": "This chapter approaches the image retrieval system on the base of the colors of image. It creates fuzzy signature to describe the color of image on color space HSV and builds fuzzy Hamming distance (FHD) to evaluate the similarity between the images. In order to reduce the storage space and speed up the search of similar images, it aims to create S-tree to store fuzzy signature relies on FHD and builds image retrieval algorithm on S-tree. Then, it provides the content-based image retrieval (CBIR) and an image retrieval method on FHD and S-tree. Last but not least, based on this theory, it also presents an application and experimental assessment of the process of querying similar image on the database system over 10,000 images.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01186",
        "title": "Cyclical Learning Rates for Training Neural Networks",
        "authors": [
            "Leslie N. Smith"
        ],
        "abstract": "It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate \"reasonable bounds\" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2017-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01195",
        "title": "Implementation of Training Convolutional Neural Networks",
        "authors": [
            "Tianyi Liu",
            "Shuangsang Fang",
            "Yuehui Zhao",
            "Peng Wang",
            "Jun Zhang"
        ],
        "abstract": "Deep learning refers to the shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we gave a detailed analysis of the process of CNN algorithm both the forward process and back propagation. Then we applied the particular convolutional neural network to implement the typical face recognition problem by java. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01342",
        "title": "One-to-many face recognition with bilinear CNNs",
        "authors": [
            "Aruni RoyChowdhury",
            "Tsung-Yu Lin",
            "Subhransu Maji",
            "Erik Learned-Miller"
        ],
        "abstract": "The recent explosive growth in convolutional neural network (CNN) research has produced a variety of new architectures for deep learning. One intriguing new architecture is the bilinear CNN (B-CNN), which has shown dramatic performance gains on certain fine-grained recognition problems [15]. We apply this new CNN to the challenging new face recognition benchmark, the IARPA Janus Benchmark A (IJB-A) [12]. It features faces from a large number of identities in challenging real-world conditions. Because the face images were not identified automatically using a computerized face detection system, it does not have the bias inherent in such a database. We demonstrate the performance of the B-CNN model beginning from an AlexNet-style network pre-trained on ImageNet. We then show results for fine-tuning using a moderate-sized and public external database, FaceScrub [17]. We also present results with additional fine-tuning on the limited training data provided by the protocol. In each case, the fine-tuned bilinear model shows substantial improvements over the standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a large face database, the recently released VGG-Face model [20], can be converted into a B-CNN without any additional feature training. This B-CNN improves upon the CNN performance on the IJB-A benchmark, achieving 89.5% rank-1 recall.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01398",
        "title": "Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM",
        "authors": [
            "Jismy Alphonse",
            "Biju V. G."
        ],
        "abstract": "A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR (Synthetic aperture Radar) image change detection method is proposed in this paper. It involves three steps: Difference Image (DI) generation by using Gauss-log ratio operator, speckle noise reduction by SRAD (Speckle Reducing Anisotropic Diffusion), and the detection of changed regions by using MRFFCM. The proposed method is compared with existing methods such as FCM and MRFFCM using simulated and real SAR images. The measures used for evaluation includes Overall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient (KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). The results show that the proposed method is better compared to FCM and MRFFCM based change detection method.\n    ",
        "submission_date": "2015-06-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01437",
        "title": "ShapeFit: Exact location recovery from corrupted pairwise directions",
        "authors": [
            "Paul Hand",
            "Choongbum Lee",
            "Vladislav Voroninski"
        ],
        "abstract": "Let $t_1,\\ldots,t_n \\in \\mathbb{R}^d$ and consider the location recovery problem: given a subset of pairwise direction observations $\\{(t_i - t_j) / \\|t_i - t_j\\|_2\\}_{i<j \\in [n] \\times [n]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\\{t_i\\}_{i=1}^n$ up to a global translation and scale. We propose a novel algorithm for the location recovery problem, which consists of a simple convex program over $dn$ real variables. We prove that this program recovers a set of $n$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by an \\erdosrenyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. We also prove that the program exactly recovers Gaussian locations for $d=3$ if the fraction of corrupted observations at each location is, up to poly-logarithmic factors, at most a constant. Both of these recovery theorems are based on a set of deterministic conditions that we prove are sufficient for exact recovery.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01472",
        "title": "Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to Color Image Segmentation",
        "authors": [
            "Dibya Jyoti Bora",
            "Anil Kumar Gupta",
            "Fayaz Ahmad Khan"
        ],
        "abstract": "Color image segmentation is a very emerging topic for image processing research. Since it has the ability to present the result in a way that is much more close to the human yes perceive, so todays more research is going on this area. Choosing a proper color space is a very important issue for color image segmentation process. Generally LAB and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. For measuring their performance, we consider the parameters: mse and psnr . It is found that HSV color space is performing better than LAB.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01497",
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "authors": [
            "Shaoqing Ren",
            "Kaiming He",
            "Ross Girshick",
            "Jian Sun"
        ],
        "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01596",
        "title": "Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images",
        "authors": [
            "Roozbeh Rajabi",
            "Hassan Ghassemian"
        ],
        "abstract": "One of the challenges in hyperspectral data analysis is the presence of mixed pixels. Mixed pixels are the result of low spatial resolution of hyperspectral sensors. Spectral unmixing methods decompose a mixed pixel into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fraction values, NMF based methods are well suited to this problem. In this paper multilayer NMF has been used to improve the results of NMF methods for spectral unmixing of hyperspectral data under the linear mixing framework. Sparseness constraint on both spectral signatures and abundance fractions matrices are used in this paper. Evaluation of the proposed algorithm is done using synthetic and real datasets in terms of spectral angle and abundance angle distances. Results show that the proposed algorithm outperforms other previously proposed methods.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01698",
        "title": "The Long-Short Story of Movie Description",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Bernt Schiele"
        ],
        "abstract": "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII Movie Description allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01710",
        "title": "A Novel Approach Towards Clustering Based Image Segmentation",
        "authors": [
            "Dibya Jyoti Bora",
            "Anil Kumar Gupta"
        ],
        "abstract": "In computer vision, image segmentation is always selected as a major research topic by researchers. Due to its vital rule in image processing, there always arises the need of a better image segmentation method. Clustering is an unsupervised study with its application in almost every field of science and engineering. Many researchers used clustering in image segmentation process. But still there requires improvement of such approaches. In this paper, a novel approach for clustering based image segmentation is proposed. Here, we give importance on color space and choose lab for this task. The famous hard clustering algorithm K-means is used, but as its performance is dependent on choosing a proper distance measure, so, we go for cosine distance measure. Then the segmented image is filtered with sobel filter. The filtered image is analyzed with marker watershed algorithm to have the final segmented result of our original image. The MSE and PSNR values are evaluated to observe the performance.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01911",
        "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video",
        "authors": [
            "Lionel Pigou",
            "A\u00e4ron van den Oord",
            "Sander Dieleman",
            "Mieke Van Herreweghe",
            "Joni Dambre"
        ],
        "abstract": "Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01929",
        "title": "Learning to track for spatio-temporal action localization",
        "authors": [
            "Philippe Weinzaepfel",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01939",
        "title": "Facial Expressions recognition Based on Principal Component Analysis (PCA)",
        "authors": [
            "Abdelmajid Hassan Mansour",
            "Gafar Zen Alabdeen Salh",
            "Ali Shaif Alhalemi"
        ],
        "abstract": "The facial expression recognition is an ocular task that can be performed without human discomfort, is really a speedily growing on the computer research field. There are many applications and programs uses facial expression to evaluate human character, judgment, feelings, and viewpoint. The process of recognizing facial expression is a hard task due to the several circumstances such as facial occlusions, face shape, illumination, face colors, and etc. This paper present a PCA methodology to distinguish expressions of faces under different circumstances and identifying it. Relies on Eigen faces technique using standard Data base images. So as to overcome the problem of difficulty to computers to identify the features and expressions of persons.\n    ",
        "submission_date": "2014-12-23T00:00:00",
        "last_modified_date": "2014-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02025",
        "title": "Spatial Transformer Networks",
        "authors": [
            "Max Jaderberg",
            "Karen Simonyan",
            "Andrew Zisserman",
            "Koray Kavukcuoglu"
        ],
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02059",
        "title": "Sentence Directed Video Object Codetection",
        "authors": [
            "Haonan Yu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We tackle the problem of video object codetection by leveraging the weak semantic constraint implied by sentences that describe the video content. Unlike most existing work that focuses on codetecting large objects which are usually salient both in size and appearance, we can codetect objects that are small or medium sized. Our method assumes no human pose or depth information such as is required by the most recent state-of-the-art method. We employ weak semantic constraint on the codetection process by pairing the video with sentences. Although the semantic information is usually simple and weak, it can greatly boost the performance of our codetection framework by reducing the search space of the hypothesized object detections. Our experiment demonstrates an average IoU score of 0.423 on a new challenging dataset which contains 15 object classes and 150 videos with 12,509 frames in total, and an average IoU score of 0.373 on a subset of an existing dataset, originally intended for activity recognition, which contains 5 object classes and 75 videos with 8,854 frames in total.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02106",
        "title": "What's the Point: Semantic Segmentation with Point Supervision",
        "authors": [
            "Amy Bearman",
            "Olga Russakovsky",
            "Vittorio Ferrari",
            "Li Fei-Fei"
        ],
        "abstract": "The semantic image segmentation task presents a trade-off between test time accuracy and training-time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain, image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9% mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2016-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02108",
        "title": "Deeply Learning the Messages in Message Passing Inference",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Ian Reid",
            "Anton van den Hengel"
        ],
        "abstract": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02167",
        "title": "Color Constancy by Learning to Predict Chromaticity from Luminance",
        "authors": [
            "Ayan Chakrabarti"
        ],
        "abstract": "Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a \"classifier\" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier \"end-to-end\". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02178",
        "title": "Capturing Hands in Action using Discriminative Salient Points and Physics Simulation",
        "authors": [
            "Dimitrios Tzionas",
            "Luca Ballan",
            "Abhilash Srikantha",
            "Pablo Aponte",
            "Marc Pollefeys",
            "Juergen Gall"
        ],
        "abstract": "Hand motion capture is a popular research field, recently gaining more attention due to the ubiquity of RGB-D sensors. However, even most recent approaches focus on the case of a single isolated hand. In this work, we focus on hands that interact with other hands or objects and present a framework that successfully captures motion in such interaction scenarios for both rigid and articulated objects. Our framework combines a generative model with discriminatively trained salient points to achieve a low tracking error and with collision detection and physics simulation to achieve physically plausible estimates even in case of occlusions and missing visual data. Since all components are unified in a single objective function which is almost everywhere differentiable, it can be optimized with standard optimization techniques. Our approach works for monocular RGB-D sequences as well as setups with multiple synchronized RGB cameras. For a qualitative and quantitative evaluation, we captured 29 sequences with a large variety of interactions and up to 150 degrees of freedom.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02184",
        "title": "First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos",
        "authors": [
            "Jun Ye",
            "Hao Hu",
            "Kai Li",
            "Guo-Jun Qi",
            "Kien A. Hua"
        ],
        "abstract": "With the prevalence of the commodity depth cameras, the new paradigm of user interfaces based on 3D motion capturing and recognition have dramatically changed the way of interactions between human and computers. Human action recognition, as one of the key components in these devices, plays an important role to guarantee the quality of user experience. Although the model-driven methods have achieved huge success, they cannot provide a scalable solution for efficiently storing, retrieving and recognizing actions in the large-scale applications. These models are also vulnerable to the temporal translation and warping, as well as the variations in motion scales and execution rates. To address these challenges, we propose to treat the 3D human action recognition as a video-level hashing problem and propose a novel First-Take-All (FTA) Hashing algorithm capable of hashing the entire video into hash codes of fixed length. We demonstrate that this FTA algorithm produces a compact representation of the video invariant to the above mentioned variations, through which action recognition can be solved by an efficient nearest neighbor search by the Hamming distance between the FTA hash codes. Experiments on the public 3D human action datasets shows that the FTA algorithm can reach a recognition accuracy higher than 80%, with about 15 bits per frame considering there are 65 frames per video over the datasets.\n    ",
        "submission_date": "2015-06-06T00:00:00",
        "last_modified_date": "2015-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02203",
        "title": "Describing Common Human Visual Actions in Images",
        "authors": [
            "Matteo Ruggero Ronchi",
            "Pietro Perona"
        ],
        "abstract": "Which common human actions and interactions are recognizable in monocular still images? Which involve objects and/or other people? How many is a person performing at a time? We address these questions by exploring the actions and interactions that are detectable in the images of the MS COCO dataset. We make two main contributions. First, a list of 140 common `visual actions', obtained by analyzing the largest on-line verb lexicon currently available for English (VerbNet) and human sentences used to describe images in MS COCO. Second, a complete set of annotations for those `visual actions', composed of subject-object and associated verb, which we call COCO-a (a for `actions'). COCO-a is larger than existing action datasets in terms of number of actions and instances of these actions, and is unique because it is data-driven, rather than experimenter-biased. Other unique features are that it is exhaustive, and that all subjects and objects are localized. A statistical analysis of the accuracy of our annotations and of each action, interaction and subject-object combination is provided.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02211",
        "title": "Boosting Optical Character Recognition: A Super-Resolution Approach",
        "authors": [
            "Chao Dong",
            "Ximei Zhu",
            "Yubin Deng",
            "Chen Change Loy",
            "Yu Qiao"
        ],
        "abstract": "Text image super-resolution is a challenging yet open research problem in the computer vision community. In particular, low-resolution images hamper the performance of typical optical character recognition (OCR) systems. In this article, we summarize our entry to the ICDAR2015 Competition on Text Image Super-Resolution. Experiments are based on the provided ICDAR2015 TextSR dataset and the released Tesseract-OCR 3.02 system. We report that our winning entry of text image super-resolution framework has largely improved the OCR performance with low-resolution images used as input, reaching an OCR accuracy score of 77.19%, which is comparable with that of using the original high-resolution images 78.80%.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02247",
        "title": "Well-posedness of a nonlinear integro-differential problem and its rearranged formulation",
        "authors": [
            "Gonzalo Galiano",
            "Emanuele Schiavi",
            "Juli\u00e1n Velasco"
        ],
        "abstract": "We study the existence and uniqueness of solutions of a nonlinear integro-differential problem which we reformulate introducing the notion of the decreasing rearrangement of the solution. A dimensional reduction of the problem is obtained and a detailed analysis of the properties of the solutions of the model is provided. Finally, a fast numerical method is devised and implemented to show the performance of the model when typical image processing tasks such as filtering and segmentation are performed.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02265",
        "title": "Randomized Structural Sparsity based Support Identification with Applications to Locating Activated or Discriminative Brain Areas: A Multi-center Reproducibility Study",
        "authors": [
            "Yilun Wang",
            "Sheng Zhang",
            "Junjie Zheng",
            "Heng Chen",
            "Huafu Chen"
        ],
        "abstract": "In this paper, we focus on how to locate the relevant or discriminative brain regions related with external stimulus or certain mental decease, which is also called support identification, based on the neuroimaging data. The main difficulty lies in the extremely high dimensional voxel space and relatively few training samples, easily resulting in an unstable brain region discovery (or called feature selection in context of pattern recognition). When the training samples are from different centers and have betweencenter variations, it will be even harder to obtain a reliable and consistent result. Corresponding, we revisit our recently proposed algorithm based on stability selection and structural sparsity. It is applied to the multi-center MRI data analysis for the first time. A consistent and stable result is achieved across different centers despite the between-center data variation while many other state-of-the-art methods such as two sample t-test fail. Moreover, we have empirically showed that the performance of this algorithm is robust and insensitive to several of its key parameters. In addition, the support identification results on both functional MRI and structural MRI are interpretable and can be the potential biomarkers.\n    ",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02328",
        "title": "EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video",
        "authors": [
            "Guangnan Ye",
            "Yitong Li",
            "Hongliang Xu",
            "Dong Liu",
            "Shih-Fu Chang"
        ],
        "abstract": "Event-specific concepts are the semantic concepts designed for the events of interest, which can be used as a mid-level representation of complex events in videos. Existing methods only focus on defining event-specific concepts for a small number of predefined events, but cannot handle novel unseen events. This motivates us to build a large scale event-specific concept library that covers as many real-world events and their concepts as possible. Specifically, we choose WikiHow, an online forum containing a large number of how-to articles on human daily life events. We perform a coarse-to-fine event discovery process and discover 500 events from WikiHow articles. Then we use each event name as query to search YouTube and discover event-specific concepts from the tags of returned videos. After an automatic filter process, we end up with 95,321 videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model on the 95,321 videos over the 500 events, and use the model to extract deep learning feature from video content. With the learned deep learning feature, we train 4,490 binary SVM classifiers as the event-specific concept library. The concepts and events are further organized in a hierarchical structure defined by WikiHow, and the resultant concept library is called EventNet. Finally, the EventNet concept library is used to generate concept based representation of event videos. To the best of our knowledge, EventNet represents the first video event ontology that organizes events and their concepts into a semantic structure. It offers great potential for event retrieval and browsing. Extensive experiments over the zero-shot event retrieval task when no training samples are available show that the EventNet concept library consistently and significantly outperforms the state-of-the-art (such as the 20K ImageNet concepts trained with CNN) by a large margin up to 207%.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02345",
        "title": "Wavelets and continuous wavelet transform for autostereoscopic multiview images",
        "authors": [
            "Vladimir Saveljev"
        ],
        "abstract": "Recently, the reference functions for the synthesis and analysis of the autostereoscopic multiview and integral images in three-dimensional displays we introduced. In the current paper, we propose the wavelets to analyze such images. The wavelets are built on the reference functions as on the scaling functions of the wavelet analysis. The continuous wavelet transform was successfully applied to the testing wireframe binary objects. The restored locations correspond to the structure of the testing wireframe binary objects.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02432",
        "title": "Reflection Invariance: an important consideration of image orientation",
        "authors": [
            "Craig Henderson",
            "Ebroul Izquierdo"
        ],
        "abstract": "In this position paper, we consider the state of computer vision research with respect to invariance to the horizontal orientation of an image -- what we term reflection invariance. We describe why we consider reflection invariance to be an important property and provide evidence where the absence of this invariance produces surprising inconsistencies in state-of-the-art systems. We demonstrate inconsistencies in methods of object detection and scene classification when they are presented with images and the horizontal mirror of those images. Finally, we examine where some of the invariance is exhibited in feature detection and descriptors, and make a case for future consideration of reflection invariance as a measure of quality in computer vision algorithms.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02515",
        "title": "Fast ConvNets Using Group-wise Brain Damage",
        "authors": [
            "Vadim Lebedev",
            "Victor Lempitsky"
        ],
        "abstract": "We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on AlexNet, the method achieves very competitive performance.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02565",
        "title": "Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework",
        "authors": [
            "Yong-Deok Kim",
            "Taewoong Jang",
            "Bohyung Han",
            "Seungjin Choi"
        ],
        "abstract": "We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02588",
        "title": "Circulant temporal encoding for video retrieval and temporal alignment",
        "authors": [
            "Matthijs Douze",
            "J\u00e9r\u00f4me Revaud",
            "Jakob Verbeek",
            "Herv\u00e9 J\u00e9gou",
            "Cordelia Schmid"
        ],
        "abstract": "We address the problem of specific video event retrieval. Given a query video of a specific event, e.g., a concert of Madonna, the goal is to retrieve other videos of the same event that temporally overlap with the query. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to efficiently compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. The descriptors can be compressed in the frequency domain with a product quantizer adapted to complex numbers. In this case, video retrieval is performed without decompressing the descriptors. We also consider the temporal alignment of a set of videos. We exploit the matching confidence and an estimate of the temporal offset computed for all pairs of videos by our retrieval approach. Our robust algorithm aligns the videos on a global timeline by maximizing the set of temporally consistent matches. The global temporal alignment enables synchronous playback of the videos of a given scene.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02640",
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "authors": [
            "Joseph Redmon",
            "Santosh Divvala",
            "Ross Girshick",
            "Ali Farhadi"
        ],
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\n",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2016-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02776",
        "title": "Fast Geometric Fit Algorithm for Sphere Using Exact Solution",
        "authors": [
            "Sumith YD"
        ],
        "abstract": "Sphere fitting is a common problem in almost all science and engineering disciplines. Most of methods available are iterative in behavior. This involves fitting of the parameters in a least square sense or in a geometric sense. Here we extend the methods of Thomas Chan and Landau who fitted the 2D data using circle. This work closely resemble their work in redefining the error estimate and solving the sphere fitting problem exactly. The solutions for center and radius of the sphere can be found exactly and the equations can be hard coded for high performance. We have also shown some comparison with other popular methods and how this method behaves.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02897",
        "title": "Flowing ConvNets for Human Pose Estimation in Videos",
        "authors": [
            "Tomas Pfister",
            "James Charles",
            "Andrew Zisserman"
        ],
        "abstract": "The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.\n",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02923",
        "title": "Compact Shape Trees: A Contribution to the Forest of Shape Correspondences and Matching Methods",
        "authors": [
            "Abdulrahman Oladipupo Ibraheem"
        ],
        "abstract": "We propose a novel technique, termed compact shape trees, for computing correspondences of single-boundary 2-D shapes in O(n2) time. Together with zero or more features defined at each of n sample points on the shape's boundary, the compact shape tree of a shape comprises the O(n) collection of vectors emanating from any of the sample points on the shape's boundary to the rest of the sample points on the boundary. As it turns out, compact shape trees have a number of elegant properties both in the spatial and frequency domains. In particular, via a simple vector-algebraic argument, we show that the O(n) collection of vectors in a compact shape tree possesses at least the same discriminatory power as the O(n2) collection of lines emanating from each sample point to every other sample point on a shape's boundary. In addition, we describe neat approaches for achieving scale and rotation invariance with compact shape trees in the spatial domain; by viewing compact shape trees as aperiodic discrete signals, we also prove scale and rotation invariance properties for them in the Fourier domain. Towards these, along the way, using concepts from differential geometry and the Calculus, we propose a novel theory for sampling 2-D shape boundaries in a scale and rotation invariant manner. Finally, we propose a number of shape recognition experiments to test the efficacy of our concept.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03011",
        "title": "Learning to Linearize Under Uncertainty",
        "authors": [
            "Ross Goroshin",
            "Michael Mathieu",
            "Yann LeCun"
        ],
        "abstract": "Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03128",
        "title": "License Plate Recognition System Based on Color Coding Of License Plates",
        "authors": [
            "Jani Biju Babjan"
        ],
        "abstract": "License Plate Recognition Systems are used to determine the license plate number of a vehicle. The current system mainly uses Optical Character Recognition to recognize the number plate. There are several problems to this system. Some of them include interchanging of several letters or numbers (letter O with digit 0), difficulty in localizing the license plate, high error rate, use of different fonts in license plates etc. So a new system to recognize the license plate number using color coding of license plates is proposed in this paper. Easier localization of license plate can be done by searching for the start or stop patters of license plates. An eight segment display system along with traditional numbering with the first and last segments left for start or stop patterns is proposed in this paper. Practical applications include several areas under Internet of Things (IoT).\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03184",
        "title": "ICDAR 2015 Text Reading in the Wild Competition",
        "authors": [
            "Xinyu Zhou",
            "Shuchang Zhou",
            "Cong Yao",
            "Zhimin Cao",
            "Qi Yin"
        ],
        "abstract": "Recently, text detection and recognition in natural scenes are becoming increasing popular in the computer vision community as well as the document analysis community. However, majority of the existing ideas, algorithms and systems are specifically designed for English. This technical report presents the final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015) competition, which aims at establishing a benchmark for assessing detection and recognition algorithms devised for both Chinese and English scripts and providing a playground for researchers from the community. In this article, we describe in detail the dataset, tasks, evaluation protocols and participants of this competition, and report the performance of the participating methods. Moreover, promising directions for future research are discussed.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03301",
        "title": "Wide baseline stereo matching with convex bounded-distortion constraints",
        "authors": [
            "Meirav Galun",
            "Tal Amir",
            "Tal Hassner",
            "Ronen Basri",
            "Yaron Lipman"
        ],
        "abstract": "Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03365",
        "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop",
        "authors": [
            "Fisher Yu",
            "Ari Seff",
            "Yinda Zhang",
            "Shuran Song",
            "Thomas Funkhouser",
            "Jianxiong Xiao"
        ],
        "abstract": "While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2016-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03475",
        "title": "Image Tag Completion and Refinement by Subspace Clustering and Matrix Completion",
        "authors": [
            "Yuqing Hou",
            "Zhouchen Lin"
        ],
        "abstract": "Tag-based image retrieval (TBIR) has drawn much attention in recent years due to the explosive amount of digital images and crowdsourcing tags. However, the TBIR applications still suffer from the deficient and inaccurate tags provided by users. Inspired by the subspace clustering methods, we formulate the tag completion problem in a subspace clustering model which assumes that images are sampled from subspaces, and complete the tags using the state-of-the-art Low Rank Representation (LRR) method. And we propose a matrix completion algorithm to further refine the tags. Our empirical results on multiple benchmark datasets for image annotation show that the proposed algorithm outperforms state-of-the-art approaches when handling missing and noisy tags.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03495",
        "title": "BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis",
        "authors": [
            "Daniel Y. T. Chino",
            "Letricia P. S. Avalhais",
            "Jose F. Rodrigues Jr.",
            "Agma J. M. Traina"
        ],
        "abstract": "Emergency events involving fire are potentially harmful, demanding a fast and precise decision making. The use of crowdsourcing image and videos on crisis management systems can aid in these situations by providing more information than verbal/textual descriptions. Due to the usual high volume of data, automatic solutions need to discard non-relevant content without losing relevant information. There are several methods for fire detection on video using color-based models. However, they are not adequate for still image processing, because they can suffer on high false-positive results. These methods also suffer from parameters with little physical meaning, which makes fine tuning a difficult task. In this context, we propose a novel fire detection method for still images that uses classification based on color features combined with texture classification on superpixel regions. Our method uses a reduced number of parameters if compared to previous works, easing the process of fine tuning the method. Results show the effectiveness of our method of reducing false-positives while its precision remains compatible with the state-of-the-art methods.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03500",
        "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation",
        "authors": [
            "Angeliki Lazaridou",
            "Dat Tien Nguyen",
            "Raffaella Bernardi",
            "Marco Baroni"
        ],
        "abstract": "We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03607",
        "title": "P-CNN: Pose-based CNN Features for Action Recognition",
        "authors": [
            "Guilhem Ch\u00e9ron",
            "Ivan Laptev",
            "Cordelia Schmid"
        ],
        "abstract": "This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03648",
        "title": "Constrained Convolutional Neural Networks for Weakly Supervised Segmentation",
        "authors": [
            "Deepak Pathak",
            "Philipp Kr\u00e4henb\u00fchl",
            "Trevor Darrell"
        ],
        "abstract": "We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03799",
        "title": "Pose-Invariant 3D Face Alignment",
        "authors": [
            "Amin Jourabloo",
            "Xiaoming Liu"
        ],
        "abstract": "Face alignment aims to estimate the locations of a set of landmarks for a given image. This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D deformable model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks. Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normals. We gather a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03844",
        "title": "Techniques for effective and efficient fire detection from social media images",
        "authors": [
            "Marcos Bedo",
            "Gustavo Blanco",
            "Willian Oliveira",
            "Mirela Cazzolato",
            "Alceu Costa",
            "Jose Rodrigues",
            "Agma Traina",
            "Caetano Traina Jr"
        ],
        "abstract": "Social media could provide valuable information to support decision making in crisis management, such as in accidents, explosions and fires. However, much of the data from social media are images, which are uploaded in a rate that makes it impossible for human beings to analyze them. Despite the many works on image analysis, there are no fire detection studies on social media. To fill this gap, we propose the use and evaluation of a broad set of content-based image retrieval and classification techniques for fire detection. Our main contributions are: (i) the development of the Fast-Fire Detection method (FFDnR), which combines feature extractor and evaluation functions to support instance-based learning, (ii) the construction of an annotated set of images with ground-truth depicting fire occurrences -- the FlickrFire dataset, and (iii) the evaluation of 36 efficient image descriptors for fire detection. Using real data from Flickr, our results showed that FFDnR was able to achieve a precision for fire detection comparable to that of human annotators. Therefore, our work shall provide a solid basis for further developments on monitoring images from social media.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03936",
        "title": "A Novel Hybrid Approach for Cephalometric Landmark Detection",
        "authors": [
            "Mahshid Majd",
            "Farzaneh Shoeleh"
        ],
        "abstract": "Cephalometric analysis has an important role in dentistry and especially in orthodontics as a treatment planning tool to gauge the size and special relationships of the teeth, jaws and cranium. The first step of using such analyses is localizing some important landmarks known as cephalometric landmarks on craniofacial in x-ray image. The past decade has seen a growing interest in automating this process. In this paper, a novel hybrid approach is proposed for automatic detection of cephalometric landmarks. Here, the landmarks are categorized into three main sets according to their anatomical characteristics and usage in well-known cephalometric analyses. Consequently, to have a reliable and accurate detection system, three methods named edge tracing, weighted template matching, and analysis based estimation are designed, each of which is consistent and well-suited for one category. Edge tracing method is suggested to predict those landmarks which are located on edges. Weighted template matching method is well-suited for landmarks located in an obvious and specific structure which can be extracted or searchable in a given x-ray image. The last but not the least method is named analysis based estimation. This method is based on the fact that in cephalometric analyses the relations between landmarks are used and the locations of some landmarks are never used individually. Therefore the third suggested method has a novelty in estimating the desired relations directly. The effectiveness of the proposed approach is compared with the state of the art methods and the results were promising especially in real world applications.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03995",
        "title": "Technical Report: Image Captioning with Semantically Similar Images",
        "authors": [
            "Martin Kol\u00e1\u0159",
            "Michal Hradi\u0161",
            "Pavel Zem\u010d\u00edk"
        ],
        "abstract": "This report presents our submission to the MS COCO Captioning Challenge 2015. The method uses Convolutional Neural Network activations as an embedding to find semantically similar images. From these images, the most typical caption is selected based on unigram frequencies. Although the method received low scores with automated evaluation metrics and in human assessed average correctness, it is competitive in the ratio of captions which pass the Turing test and which are assessed as better or equal to human captions.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03998",
        "title": "Sparse Multi-layer Image Approximation: Facial Image Compression",
        "authors": [
            "Sohrab Ferdowsi",
            "Svyatoslav Voloshynovskiy",
            "Dimche Kostadinov"
        ],
        "abstract": "We propose a scheme for multi-layer representation of images. The problem is first treated from an information-theoretic viewpoint where we analyze the behavior of different sources of information under a multi-layer data compression framework and compare it with a single-stage (shallow) structure. We then consider the image data as the source of information and link the proposed representation scheme to the problem of multi-layer dictionary learning for visual data. For the current work we focus on the problem of image compression for a special class of images where we report a considerable performance boost in terms of PSNR at high compression ratios in comparison with the JPEG2000 codec.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04051",
        "title": "Towards Benchmarking Scene Background Initialization",
        "authors": [
            "Lucia Maddalena",
            "Alfredo Petrosino"
        ],
        "abstract": "Given a set of images of a scene taken at different times, the availability of an initial background model that describes the scene without foreground objects is the prerequisite for a wide range of applications, ranging from video surveillance to computational photography. Even though several methods have been proposed for scene background initialization, the lack of a common groundtruthed dataset and of a common set of metrics makes it difficult to compare their performance. To move first steps towards an easy and fair comparison of these methods, we assembled a dataset of sequences frequently adopted for background initialization, selected or created ground truths for quantitative evaluation through a selected suite of metrics, and compared results obtained by some existing methods, making all the material publicly available.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04130",
        "title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service",
        "authors": [
            "Harsh Agrawal",
            "Clint Solomon Mathialagan",
            "Yash Goyal",
            "Neelima Chavali",
            "Prakriti Banik",
            "Akrit Mohapatra",
            "Ahmed Osman",
            "Dhruv Batra"
        ],
        "abstract": "We are witnessing a proliferation of massive visual data. Unfortunately scaling existing computer vision algorithms to large datasets leaves researchers repeatedly solving the same algorithmic, logistical, and infrastructural problems. Our goal is to democratize computer vision; one should not have to be a computer vision, big data and distributed computing expert to have access to state-of-the-art distributed computer vision algorithms. We present CloudCV, a comprehensive system to provide access to state-of-the-art distributed computer vision algorithms as a cloud service through a Web Interface and APIs.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2017-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04191",
        "title": "Deep Structured Models For Group Activity Recognition",
        "authors": [
            "Zhiwei Deng",
            "Mengyao Zhai",
            "Lei Chen",
            "Yuhao Liu",
            "Srikanth Muralidharan",
            "Mehrsan Javan Roshtkhari",
            "Greg Mori"
        ],
        "abstract": "This paper presents a deep neural-network-based hierarchical graphical model for individual and group activity recognition in surveillance scenes. Deep networks are used to recognize the actions of individual people in a scene. Next, a neural-network-based hierarchical graphical model refines the predicted labels for each class by considering dependencies between the classes. This refinement step mimics a message-passing step similar to inference in a probabilistic graphical model. We show that this approach can be effective in group activity recognition, with the deep graphical model improving recognition rates over baseline methods.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04214",
        "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
        "authors": [
            "Xingjian Shi",
            "Zhourong Chen",
            "Hao Wang",
            "Dit-Yan Yeung",
            "Wai-kin Wong",
            "Wang-chun Woo"
        ],
        "abstract": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04304",
        "title": "Combinatorial Energy Learning for Image Segmentation",
        "authors": [
            "Jeremy Maitin-Shepard",
            "Viren Jain",
            "Michal Januszewski",
            "Peter Li",
            "Pieter Abbeel"
        ],
        "abstract": "We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2016-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04338",
        "title": "Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis",
        "authors": [
            "Wei Yang",
            "Haiting Lin",
            "Sing Bing Kang",
            "Jingyi Yu"
        ],
        "abstract": "In perspective cameras, images of a frontal-parallel 3D object preserve its aspect ratio invariant to its depth. Such an invariance is useful in photography but is unique to perspective projection. In this paper, we show that alternative non-perspective cameras such as the crossed-slit or XSlit cameras exhibit a different depth-dependent aspect ratio (DDAR) property that can be used to 3D recovery. We first conduct a comprehensive analysis to characterize DDAR, infer object depth from its AR, and model recoverable depth range, sensitivity, and error. We show that repeated shape patterns in real Manhattan World scenes can be used for 3D reconstruction using a single XSlit image. We also extend our analysis to model slopes of lines. Specifically, parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which can also be used to infer their depths. We validate our analyses using real XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that DDAR and DDS provide important depth cues and enable effective single-image scene reconstruction.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04340",
        "title": "Deep Secure Encoding: An Application to Face Recognition",
        "authors": [
            "Rohit Pandey",
            "Yingbo Zhou",
            "Venu Govindaraju"
        ],
        "abstract": "In this paper we present Deep Secure Encoding: a framework for secure classification using deep neural networks, and apply it to the task of biometric template protection for faces. Using deep convolutional neural networks (CNNs), we learn a robust mapping of face classes to high entropy secure codes. These secure codes are then hashed using standard hash functions like SHA-256 to generate secure face templates. The efficacy of the approach is shown on two face databases, namely, CMU-PIE and Extended Yale B, where we achieve state of the art matching performance, along with cancelability and high security with no unrealistic assumptions. Furthermore, the scheme can work in both identification and verification modes.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04356",
        "title": "The Artists who Forged Themselves: Detecting Creativity in Art",
        "authors": [
            "Milan Rajkovi\u0107",
            "Milo\u0161 Milovanovi\u0107"
        ],
        "abstract": "Creativity and the understanding of cognitive processes involved in the creative process are relevant to all of human activities. Comprehension of creativity in the arts is of special interest due to the involvement of many scientific and non scientific disciplines. Using digital representation of paintings, we show that creative process in painting art may be objectively recognized within the mathematical framework of self organization, a process characteristic of nonlinear dynamic systems and occurring in natural and social sciences. Unlike the artist identification process or the recognition of forgery, which presupposes the knowledge of the original work, our method requires no prior knowledge on the originality of the work of art. The original paintings are recognized as realizations of the creative process which, in general, is shown to correspond to self-organization of texture features which determine the aesthetic complexity of the painting. The method consists of the wavelet based statistical digital image processing and the measure of statistical complexity which represents the minimal (average) information necessary for optimal prediction. The statistical complexity is based on the properly defined causal states with optimal predictive properties. Two different time concepts related to the works of art are introduced: the internal time and the artistic time. The internal time of the artwork is determined by the span of causal dependencies between wavelet coefficients while the artistic time refers to the internal time during which complexity increases where complexity refers to compositional, aesthetic and structural arrangement of texture features. The method is illustrated by recognizing the original paintings from the copies made by the artists themselves, including the works of the famous surrealist painter Ren\u00e9 Magritte.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04395",
        "title": "Reading Scene Text in Deep Convolutional Sequences",
        "authors": [
            "Pan He",
            "Weilin Huang",
            "Yu Qiao",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "We develop a Deep-Text Recurrent Network (DTRN) that regards scene text reading as a sequence labelling problem. We leverage recent advances of deep convolutional neural networks to generate an ordered high-level sequence from a whole word image, avoiding the difficult character segmentation problem. Then a deep recurrent model, building on long short-term memory (LSTM), is developed to robustly recognize the generated CNN sequences, departing from most existing approaches recognising each character independently. Our model has a number of appealing properties in comparison to existing scene text recognition methods: (i) It can recognise highly ambiguous words by leveraging meaningful context information, allowing it to work reliably without either pre- or post-processing; (ii) the deep CNN feature is robust to various image distortions; (iii) it retains the explicit order information in word image, which is essential to discriminate word strings; (iv) the model does not depend on pre-defined dictionary, and it can process unknown words and arbitrary strings. Codes for the DTRN will be available.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04472",
        "title": "A Survey of Multithreading Image Analysis",
        "authors": [
            "Elham Sagheb Hossein Pour"
        ],
        "abstract": "Digital image analysis has made a big advance in many areas of enterprise applications including medicine, industry, and entertainment by assisting the extraction of semantic information from digital images. However, its large computational complexity has been a trouble to most real-time developments. While image analysis in general has been studied for a log period in computer science community, the use of multithreading strategy as the most efficient improving computational capacity technique has been limited so far. In this survey an attempt is made to explain the current knowledge and so far progresses in incorporating image analysis with multithreading approaches. The present work also provides insights and tendencies for the possible future enhancement of multithreading image analysis.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2017-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04500",
        "title": "Circle-based Eye Center Localization (CECL)",
        "authors": [
            "Yustinus Eko Soelistio",
            "Eric Postma",
            "Alfons Maes"
        ],
        "abstract": "We propose an improved eye center localization method based on the Hough transform, called Circle-based Eye Center Localization (CECL) that is simple, robust, and achieves accuracy on a par with typically more complex state-of-the-art methods. The CECL method relies on color and shape cues that distinguish the iris from other facial structures. The accuracy of the CECL method is demonstrated through a comparison with 15 state-of-the-art eye center localization methods against five error thresholds, as reported in the literature. The CECL method achieved an accuracy of 80.8% to 99.4% and ranked first for 2 of the 5 thresholds. It is concluded that the CECL method offers an attractive alternative to existing methods for automatic eye center localization.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04566",
        "title": "Optimising Spatial and Tonal Data for PDE-based Inpainting",
        "authors": [
            "Laurent Hoeltgen",
            "Markus Mainberger",
            "Sebastian Hoffmann",
            "Joachim Weickert",
            "Ching Hoo Tang",
            "Simon Setzer",
            "Daniel Johannsen",
            "Frank Neumann",
            "Benjamin Doerr"
        ],
        "abstract": "Some recent methods for lossy signal and image compression store only a few selected pixels and fill in the missing structures by inpainting with a partial differential equation (PDE). Suitable operators include the Laplacian, the biharmonic operator, and edge-enhancing anisotropic diffusion (EED). The quality of such approaches depends substantially on the selection of the data that is kept. Optimising this data in the domain and codomain gives rise to challenging mathematical problems that shall be addressed in our work.\n",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04579",
        "title": "ParseNet: Looking Wider to See Better",
        "authors": [
            "Wei Liu",
            "Andrew Rabinovich",
            "Alexander C. Berg"
        ],
        "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04608",
        "title": "Flow Segmentation in Dense Crowds",
        "authors": [
            "Javairia Nazir",
            "Mehreen Sirshar"
        ],
        "abstract": "A framework is proposed in this paper that is used to segment flow of dense crowds. The flow field that is generated by the movement in the crowd is treated just like an aperiodic dynamic system. On this flow field a grid of particles is put over for particle advection by the use of a numerical integration scheme. Then flow maps are generated which associates the initial position of the particles with final position. The gradient of the flow maps gives the amount of divergence of the neighboring particles. For forward integration and analysis forward Finite time Lyapunov Exponent is calculated and backward Finite time Lyapunov Exponent is also calculated it gives the Lagrangian coherent structures of the flow in crowd. Lagrangian Coherent Structures basically divides the flow in crowd into regions and these regions have different dynamics. These regions are then used to get the boundary in the different flow segments by using water shed algorithm. The experiment is conducted on the crowd dataset of UCF (University of central Florida).\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04654",
        "title": "Thin Structure Estimation with Curvature Regularization",
        "authors": [
            "Dmitrii Marin",
            "Yuri Boykov",
            "Yuchen Zhong"
        ],
        "abstract": "Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04655",
        "title": "Leveraging the Power of Gabor Phase for Face Identification: A Block Matching Approach",
        "authors": [
            "Yang Zhong",
            "Haibo Li"
        ],
        "abstract": "Different from face verification, face identification is much more demanding. To reach comparable performance, an identifier needs to be roughly N times better than a verifier. To expect a breakthrough in face identification, we need a fresh look at the fundamental building blocks of face recognition. In this paper we focus on the selection of a suitable signal representation and better matching strategy for face identification. We demonstrate how Gabor phase could be leveraged to improve the performance of face identification by using the Block Matching method. Compared to the existing approaches, the proposed method features much lower algorithmic complexity: face images are only filtered by a single-scale Gabor filter pair and the matching is performed between any pairs of face images at hand without involving any training process. Benchmark evaluations show that the proposed approach is totally comparable to and even better than state-of-the-art algorithms, which are typically based on more features extracted from a large set of Gabor faces and/or rely on heavy training processes.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04701",
        "title": "Multi-path Convolutional Neural Networks for Complex Image Classification",
        "authors": [
            "Mingming Wang"
        ],
        "abstract": "Convolutional Neural Networks demonstrate high performance on ImageNet Large-Scale Visual Recognition Challenges contest. Nevertheless, the published results only show the overall performance for all image classes. There is no further analysis why certain images get worse results and how they could be improved. In this paper, we provide deep performance analysis based on different types of images and point out the weaknesses of convolutional neural networks through experiment. We design a novel multiple paths convolutional neural network, which feeds different versions of images into separated paths to learn more comprehensive features. This model has better presentation for image than the traditional single path model. We acquire better classification results on complex validation set on both top 1 and top 5 scores than the best ILSVRC 2013 classification model.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04714",
        "title": "Slow and steady feature analysis: higher order temporal coherence in video",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "How can unlabeled video augment visual learning? Existing methods perform \"slow\" feature analysis, encouraging the representations of temporally close frames to exhibit only small differences. While this standard approach captures the fact that high-level visual signals change slowly over time, it fails to capture *how* the visual content changes. We propose to generalize slow feature analysis to \"steady\" feature analysis. The key idea is to impose a prior that higher order derivatives in the learned feature space must be small. To this end, we train a convolutional neural network with a regularizer on tuples of sequential frames from unlabeled video. It encourages feature changes over time to be smooth, i.e., similar to the most recent changes. Using five diverse datasets, including unlabeled YouTube and KITTI videos, we demonstrate our method's impact on object, scene, and action recognition tasks. We further show that our features learned from unlabeled video can even surpass a standard heavily supervised pretraining approach.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04721",
        "title": "Automatic Layer Separation using Light Field Imaging",
        "authors": [
            "Qiaosong Wang",
            "Haiting Lin",
            "Yi Ma",
            "Sing Bing Kang",
            "Jingyi Yu"
        ],
        "abstract": "We propose a novel approach that jointly removes reflection or translucent layer from a scene and estimates scene depth. The input data are captured via light field imaging. The problem is couched as minimizing the rank of the transmitted scene layer via Robust Principle Component Analysis (RPCA). We also impose regularization based on piecewise smoothness, gradient sparsity, and layer independence to simultaneously recover 3D geometry of the transmitted layer. Experimental results on synthetic and real data show that our technique is robust and reliable, and can handle a broad range of layer separation problems.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04723",
        "title": "Layered Interpretation of Street View Images",
        "authors": [
            "Ming-Yu Liu",
            "Shuoxin Lin",
            "Srikumar Ramalingam",
            "Oncel Tuzel"
        ],
        "abstract": "We propose a layered street view model to encode both depth and semantic information on street view images for autonomous driving. Recently, stixels, stix-mantics, and tiered scene labeling methods have been proposed to model street view images. We propose a 4-layer street view model, a compact representation over the recently proposed stix-mantics model. Our layers encode semantic classes like ground, pedestrians, vehicles, buildings, and sky in addition to the depths. The only input to our algorithm is a pair of stereo images. We use a deep neural network to extract the appearance features for semantic classes. We use a simple and an efficient inference algorithm to jointly estimate both semantic classes and layered depth values. Our method outperforms other competing approaches in Daimler urban scene segmentation dataset. Our algorithm is massively parallelizable, allowing a GPU implementation with a processing speed about 9 fps.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04757",
        "title": "Image-based Recommendations on Styles and Substitutes",
        "authors": [
            "Julian McAuley",
            "Christopher Targett",
            "Qinfeng Shi",
            "Anton van den Hengel"
        ],
        "abstract": "Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.\n    ",
        "submission_date": "2015-06-15T00:00:00",
        "last_modified_date": "2015-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04843",
        "title": "Evaluation of Denoising Techniques for EOG signals based on SNR Estimation",
        "authors": [
            "Anirban Dasgupta",
            "Suvodip Chakrborty",
            "Aritra Chaudhuri",
            "Aurobinda Routray"
        ],
        "abstract": "This paper evaluates four algorithms for denoising raw Electrooculography (EOG) data based on the Signal to Noise Ratio (SNR). The SNR is computed using the eigenvalue method. The filtering algorithms are a) Finite Impulse Response (FIR) bandpass filters, b) Stationary Wavelet Transform, c) Empirical Mode Decomposition (EMD) d) FIR Median Hybrid Filters. An EOG dataset has been prepared where the subject is asked to perform letter cancelation test on 20 subjects.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04878",
        "title": "End-to-end people detection in crowded scenes",
        "authors": [
            "Russell Stewart",
            "Mykhaylo Andriluka"
        ],
        "abstract": "Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04912",
        "title": "Subsampled terahertz data reconstruction based on spatio-temporal dictionary learning",
        "authors": [
            "Vahid Abolghasemi",
            "Hao Shen",
            "Yaochun Shen",
            "Lu Gan"
        ],
        "abstract": "In this paper, the problem of terahertz pulsed imaging and reconstruction is addressed. It is assumed that an incomplete (subsampled) three dimensional THz data set has been acquired and the aim is to recover all missing samples. A sparsity-inducing approach is proposed for this purpose. First, a simple interpolation is applied to incomplete noisy data. Then, we propose a spatio-temporal dictionary learning method to obtain an appropriate sparse representation of data based on a joint sparse recovery algorithm. Then, using the sparse coefficients and the learned dictionary, the 3D data is effectively denoised by minimizing a simple cost function. We consider two types of terahertz data to evaluate the performance of the proposed approach; THz data acquired for a model sample with clear layered structures (e.g., a T-shape plastic sheet buried in a polythene pellet), and pharmaceutical tablet data (with low spatial resolution). The achieved signal-to-noise-ratio for reconstruction of T-shape data, from only 5% observation was 19 dB. Moreover, the accuracies of obtained thickness and depth measurements for pharmaceutical tablet data after reconstruction from 10% observation were 98.8%, and 99.9%, respectively. These results, along with chemical mapping analysis, presented at the end of this paper, confirm the accuracy of the proposed method.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04924",
        "title": "Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation",
        "authors": [
            "Seunghoon Hong",
            "Hyeonwoo Noh",
            "Bohyung Han"
        ],
        "abstract": "We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches even with much less training images with strong annotations in PASCAL VOC dataset.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04935",
        "title": "Post-Reconstruction Deconvolution of PET Images by Total Generalized Variation Regularization",
        "authors": [
            "St\u00e9phanie Gu\u00e9rit",
            "Laurent Jacques",
            "Beno\u00eet Macq",
            "John A. Lee"
        ],
        "abstract": "Improving the quality of positron emission tomography (PET) images, affected by low resolution and high level of noise, is a challenging task in nuclear medicine and radiotherapy. This work proposes a restoration method, achieved after tomographic reconstruction of the images and targeting clinical situations where raw data are often not accessible. Based on inverse problem methods, our contribution introduces the recently developed total generalized variation (TGV) norm to regularize PET image deconvolution. Moreover, we stabilize this procedure with additional image constraints such as positivity and photometry invariance. A criterion for updating and adjusting automatically the regularization parameter in case of Poisson noise is also presented. Experiments are conducted on both synthetic data and real patient images.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04954",
        "title": "A Tensor-Based Dictionary Learning Approach to Tomographic Image Reconstruction",
        "authors": [
            "Sara Soltani",
            "Misha E. Kilmer",
            "Per Christian Hansen"
        ],
        "abstract": "We consider tomographic reconstruction using priors in the form of a dictionary learned from training images. The reconstruction has two stages: first we construct a tensor dictionary prior from our training data, and then we pose the reconstruction problem in terms of recovering the expansion coefficients in that dictionary. Our approach differs from past approaches in that a) we use a third-order tensor representation for our images and b) we recast the reconstruction problem using the tensor formulation. The dictionary learning problem is presented as a non-negative tensor factorization problem with sparsity constraints. The reconstruction problem is formulated in a convex optimization framework by looking for a solution with a sparse representation in the tensor dictionary. Numerical results show that our tensor formulation leads to very sparse representations of both the training images and the reconstructions due to the ability of representing repeated features compactly in the dictionary.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05001",
        "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection",
        "authors": [
            "Liliana Lo Presti",
            "Marco La Cascia"
        ],
        "abstract": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05032",
        "title": "Histopathological Image Classification using Discriminative Feature-oriented Dictionary Learning",
        "authors": [
            "Tiep Huu Vu",
            "Hojjat Seyed Mousavi",
            "Vishal Monga",
            "Arvind UK Rao",
            "Ganesh Rao"
        ],
        "abstract": "In histopathological image analysis, feature extraction for classification is a challenging task due to the diversity of histology features suitable for each problem as well as presence of rich geometrical structures. In this paper, we propose an automatic feature discovery framework via learning class-specific dictionaries and present a low-complexity method for classification and disease grading in histopathology. Essentially, our Discriminative Feature-oriented Dictionary Learning (DFDL) method learns class-specific dictionaries such that under a sparsity constraint, the learned dictionaries allow representing a new image sample parsimoniously via the dictionary corresponding to the class identity of the sample. At the same time, the dictionary is designed to be poorly capable of representing samples from other classes. Experiments on three challenging real-world image databases: 1) histopathological images of intraductal breast lesions, 2) mammalian kidney, lung and spleen images provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal the merits of our proposal over state-of-the-art alternatives. {Moreover, we demonstrate that DFDL exhibits a more graceful decay in classification accuracy against the number of training images which is highly desirable in practice where generous training is often not available\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05036",
        "title": "Depth Perception in Autostereograms: 1/f-Noise is Best",
        "authors": [
            "Yael Yankelevsky",
            "Ishai Shvartz",
            "Tamar Avraham",
            "Alfred M. Bruckstein"
        ],
        "abstract": "An autostereogram is a single image that encodes depth information that pops out when looking at it. The trick is achieved by replicating a vertical strip that sets a basic two-dimensional pattern with disparity shifts that encode a three-dimensional scene. It is of interest to explore the dependency between the ease of perceiving depth in autostereograms and the choice of the basic pattern used for generating them. In this work we confirm a theory proposed by Bruckstein et al. to explain the process of autostereographic depth perception, providing a measure for the ease of \"locking into\" the depth profile, based on the spectral properties of the basic pattern used. We report the results of three sets of psychophysical experiments using autostereograms generated from two-dimensional random noise patterns having power spectra of the form $1/f^\\beta$. The experiments were designed to test the ability of human subjects to identify smooth, low resolution surfaces, as well as detail, in the form of higher resolution objects in the depth profile, and to determine limits in identifying small objects as a function of their size. In accordance with the theory, we discover a significant advantage of the $1/f$ noise pattern (pink noise) for fast depth lock-in and fine detail detection, showing that such patterns are optimal choices for autostereogram design. Validating the theoretical model predictions strengthens its underlying assumptions, and contributes to a better understanding of the visual system's binocular disparity mechanisms.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05068",
        "title": "Extract an essential skeleton of a character as a graph from a character image",
        "authors": [
            "Kazuhisa Fujita"
        ],
        "abstract": "This paper aims to make a graph representing an essential skeleton of a character from an image that includes a machine printed or a handwritten character using growing neural gas (GNG) method and relative network graph (RNG) algorithm. The visual system in our brain can recognize printed characters and handwritten characters easily, robustly, and precisely. How does our brain robustly recognize characters? The visual processing in our brain uses the essential features of an object, such as crosses and corners. These features will be helpful for character recognition by a computer. However, extraction of the features is difficult. If the skeleton of a character is represented as a graph, we can more easily extract the features. To extract the skeleton of a character as a graph from an image, this paper proposes the new approach using GNG and RNG algorithm. I achieved to extract skeleton graphs from images including distorted, noisy, and handwritten characters.\n    ",
        "submission_date": "2015-06-13T00:00:00",
        "last_modified_date": "2022-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05187",
        "title": "Robust High Quality Image Guided Depth Upsampling",
        "authors": [
            "Wei Liu",
            "Yijun Li",
            "Xiaogang Chen",
            "Jie Yang",
            "Qiang Wu",
            "Jingyi Yu"
        ],
        "abstract": "Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at a high frame rate. However, its low resolution and sensitivity to the noise are always a concern. A popular solution is upsampling the obtained noisy low resolution depth map with the guidance of the companion high resolution color image. However, due to the constrains in the existing upsampling models, the high resolution depth map obtained in such way may suffer from either texture copy artifacts or blur of depth discontinuity. In this paper, a novel optimization framework is proposed with the brand new data term and smoothness term. The comprehensive experiments using both synthetic data and real data show that the proposed method well tackles the problem of texture copy artifacts and blur of depth discontinuity. It also demonstrates sufficient robustness to the noise. Moreover, a data driven scheme is proposed to adaptively estimate the parameter in the upsampling optimization framework. The encouraging performance is maintained even in the case of large upsampling e.g. $8\\times$ and $16\\times$.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05196",
        "title": "A Discriminative Representation of Convolutional Features for Indoor Scene Recognition",
        "authors": [
            "Salman H. Khan",
            "Munawar Hayat",
            "Mohammed Bennamoun",
            "Roberto Togneri",
            "Ferdous Sohel"
        ],
        "abstract": "Indoor scene recognition is a multi-faceted and challenging problem due to the diverse intra-class variations and the confusing inter-class similarities. This paper presents a novel approach which exploits rich mid-level convolutional features to categorize indoor scenes. Traditionally used convolutional features preserve the global spatial structure, which is a desirable property for general object recognition. However, we argue that this structuredness is not much helpful when we have large variations in scene layouts, e.g., in indoor scenes. We propose to transform the structured convolutional activations to another highly discriminative feature space. The representation in the transformed space not only incorporates the discriminative aspects of the target dataset, but it also encodes the features in terms of the general object categories that are present in indoor scenes. To this end, we introduce a new large-scale dataset of 1300 object categories which are commonly present in indoor scenes. Our proposed approach achieves a significant performance boost over previous state of the art approaches on five major scene classification datasets.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05257",
        "title": "CFORB: Circular FREAK-ORB Visual Odometry",
        "authors": [
            "Daniel J. Mankowitz",
            "Ehud Rivlin"
        ],
        "abstract": "We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB (CFORB). This algorithm detects features using the well-known ORB algorithm [12] and computes feature descriptors using the FREAK algorithm [14]. CFORB is invariant to both rotation and scale changes, and is suitable for use in environments with uneven terrain. Two visual geometric constraints have been utilized in order to remove invalid feature descriptor matches. These constraints have not previously been utilized in a Visual Odometry algorithm. A variation to circular matching [16] has also been implemented. This allows features to be matched between images without having to be dependent upon the epipolar constraint. This algorithm has been run on the KITTI benchmark dataset and achieves a competitive average translational error of $3.73 \\%$ and average rotational error of $0.0107 deg/m$. CFORB has also been run in an indoor environment and achieved an average translational error of $3.70 \\%$. After running CFORB in a highly textured environment with an approximately uniform feature spread across the images, the algorithm achieves an average translational error of $2.4 \\%$ and an average rotational error of $0.009 deg/m$.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05274",
        "title": "Partial Functional Correspondence",
        "authors": [
            "Emanuele Rodol\u00e0",
            "Luca Cosmo",
            "Michael M. Bronstein",
            "Andrea Torsello",
            "Daniel Cremers"
        ],
        "abstract": "In this paper, we propose a method for computing partial functional correspondence between non-rigid shapes. We use perturbation analysis to show how removal of shape parts changes the Laplace-Beltrami eigenfunctions, and exploit it as a prior on the spectral representation of the correspondence. Corresponding parts are optimization variables in our problem and are used to weight the functional correspondence; we are looking for the largest and most regular (in the Mumford-Shah sense) parts that minimize correspondence distortion. We show that our approach can cope with very challenging correspondence settings.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05532",
        "title": "A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification",
        "authors": [
            "Munawar Hayat",
            "Salman H. Khan",
            "Mohammed Bennamoun",
            "Senjian An"
        ],
        "abstract": "Unlike standard object classification, where the image to be classified contains one or multiple instances of the same object, indoor scene classification is quite different since the image consists of multiple distinct objects. Further, these objects can be of varying sizes and are present across numerous spatial locations in different layouts. For automatic indoor scene categorization, large scale spatial layout deformations and scale variations are therefore two major challenges and the design of rich feature descriptors which are robust to these challenges is still an open problem. This paper introduces a new learnable feature descriptor called \"spatial layout and scale invariant convolutional activations\" to deal with these challenges. For this purpose, a new Convolutional Neural Network architecture is designed which incorporates a novel 'Spatially Unstructured' layer to introduce robustness against spatial layout deformations. To achieve scale invariance, we present a pyramidal image representation. For feasible training of the proposed network for images of indoor scenes, the paper proposes a new methodology which efficiently adapts a trained network model (on a large scale data) for our task with only a limited amount of available training data. Compared with existing state of the art, the proposed approach achieves a relative performance improvement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8, Graz-02 and NYU datasets respectively.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05603",
        "title": "Point-wise Map Recovery and Refinement from Functional Correspondence",
        "authors": [
            "Emanuele Rodol\u00e0",
            "Michael Moeller",
            "Daniel Cremers"
        ],
        "abstract": "Since their introduction in the shape analysis community, functional maps have met with considerable success due to their ability to compactly represent dense correspondences between deformable shapes, with applications ranging from shape matching and image segmentation, to exploration of large shape collections. Despite the numerous advantages of such representation, however, the problem of converting a given functional map back to a point-to-point map has received a surprisingly limited interest. In this paper we analyze the general problem of point-wise map recovery from arbitrary functional maps. In doing so, we rule out many of the assumptions required by the currently established approach -- most notably, the limiting requirement of the input shapes being nearly-isometric. We devise an efficient recovery process based on a simple probabilistic model. Experiments confirm that this approach achieves remarkable accuracy improvements in very challenging cases.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05751",
        "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
        "authors": [
            "Emily Denton",
            "Soumith Chintala",
            "Arthur Szlam",
            "Rob Fergus"
        ],
        "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.\n    ",
        "submission_date": "2015-06-18T00:00:00",
        "last_modified_date": "2015-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05870",
        "title": "To Know Where We Are: Vision-Based Positioning in Outdoor Environments",
        "authors": [
            "Kuan-Wen Chen",
            "Chun-Hsin Wang",
            "Xiao Wei",
            "Qiao Liang",
            "Ming-Hsuan Yang",
            "Chu-Song Chen",
            "Yi-Ping Hung"
        ],
        "abstract": "Augmented reality (AR) displays become more and more popular recently, because of its high intuitiveness for humans and high-quality head-mounted display have rapidly developed. To achieve such displays with augmented information, highly accurate image registration or ego-positioning are required, but little attention have been paid for out-door environments. This paper presents a method for ego-positioning in outdoor environments with low cost monocular cameras. To reduce the computational and memory requirements as well as the communication overheads, we formulate the model compression algorithm as a weighted k-cover problem for better preserving model structures. Specifically for real-world vision-based positioning applications, we consider the issues with large scene change and propose a model update algorithm to tackle these problems. A long- term positioning dataset with more than one month, 106 sessions, and 14,275 images is constructed. Based on both local and up-to-date models constructed in our approach, extensive experimental results show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be achieved, which outperforms existing vision-based algorithms.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05920",
        "title": "New Descriptor for Glomerulus Detection in Kidney Microscopy Image",
        "authors": [
            "Tsuyoshi Kato",
            "Raissa Relator",
            "Hayliang Ngouv",
            "Yoshihiro Hirohashi",
            "Tetsuhiro Kakimoto",
            "Kinya Okada"
        ],
        "abstract": "Glomerulus detection is a key step in histopathological evaluation of microscopy images of kidneys. However, the task of automatic detection of glomeruli poses challenges due to the disparity in sizes and shapes of glomeruli in renal sections. Moreover, extensive variations of their intensities due to heterogeneity in immunohistochemistry staining are also encountered. Despite being widely recognized as a powerful descriptor for general object detection, the rectangular histogram of oriented gradients (Rectangular HOG) suffers from many false positives due to the aforementioned difficulties in the context of glomerulus detection.\n",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05929",
        "title": "Exploring the influence of scale on artist attribution",
        "authors": [
            "Nanne van Noord",
            "Eric Postma"
        ],
        "abstract": "Previous work has shown that the artist of an artwork can be identified by use of computational methods that analyse digital images. However, the digitised artworks are often investigated at a coarse scale discarding many of the important details that may define an artist's style. In recent years high resolution images of artworks have become available, which, combined with increased processing power and new computational techniques, allow us to analyse digital images of artworks at a very fine scale. In this work we train and evaluate a Convolutional Neural Network (CNN) on the task of artist attribution using artwork images of varying resolutions. To this end, we combine two existing methods to enable the application of high resolution images to CNNs. By comparing the attribution performances obtained at different scales, we find that in most cases finer scales are beneficial to the attribution performance, whereas for a minority of the artists, coarser scales appear to be preferable. We conclude that artist attribution would benefit from a multi-scale CNN approach which vastly expands the possibilities for computational art forensics.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05942",
        "title": "Scene-adaptive Coded Apertures Imaging",
        "authors": [
            "Xuehui Wang",
            "Jinli Suo",
            "Jingyi Yu",
            "Yongdong Zhang",
            "Qionghai Dai"
        ],
        "abstract": "Coded aperture imaging systems have recently shown great success in recovering scene depth and extending the depth-of-field. The ideal pattern, however, would have to serve two conflicting purposes: 1) be broadband to ensure robust deconvolution and 2) has sufficient zero-crossings for a high depth discrepancy. This paper presents a simple but effective scene-adaptive coded aperture solution to bridge this gap. We observe that the geometric structures in a natural scene often exhibit only a few edge directions, and the successive frames are closely correlated. Therefore we adopt a spatial partitioning and temporal propagation scheme. In each frame, we address one principal direction by applying depth-discriminative codes along it and broadband codes along its orthogonal direction. Since within a frame only the regions with edge direction corresponding to its aperture code behaves well, we utilize the close among-frame correlation to propagate the high quality single frame results temporally to obtain high performance over the whole image lattice. To physically implement this scheme, we use a Liquid Crystal on Silicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly, we capture the scene with a pinhole and analyze the scene content to determine primary edge orientations. Secondly, we sequentially apply the proposed coding scheme with these orientations in the following frames. Experiments on both synthetic and real scenes show that our technique is able to combine advantages of the state-of-the-art patterns for recovering better quality depth map and all-focus images.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06001",
        "title": "Stereoscopic Cinema",
        "authors": [
            "Fr\u00e9d\u00e9ric Devernay",
            "Paul Beardsley"
        ],
        "abstract": "Stereoscopic cinema has seen a surge of activity in recent years, and for the first time all of the major Hollywood studios released 3-D movies in 2009. This is happening alongside the adoption of 3-D technology for sports broadcasting, and the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-D cinema in the 1950s and the 1980s failed because the contemporary technology was immature and resulted in viewer discomfort. But current technologies -- such as accurately-adjustable 3-D camera rigs with onboard computers to automatically inform a camera operator of inappropriate stereoscopic shots, digital processing for post-shooting rectification of the 3-D imagery, digital projectors for accurate positioning of the two stereo projections on the cinema screen, and polarized silver screens to reduce cross-talk between the viewers left- and right-eyes -- mean that the viewer experience is at a much higher level of quality than in the past. Even so, creation of stereoscopic cinema is an open, active research area, and there are many challenges from acquisition to post-production to automatic adaptation for different-sized display. This chapter describes the current state-of-the-art in stereoscopic cinema, and directions of future work.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06006",
        "title": "Crowd Flow Segmentation in Compressed Domain using CRF",
        "authors": [
            "Srinivas S. S. Kruthiventi",
            "R. Venkatesh Babu"
        ],
        "abstract": "Crowd flow segmentation is an important step in many video surveillance tasks. In this work, we propose an algorithm for segmenting flows in H.264 compressed videos in a completely unsupervised manner. Our algorithm works on motion vectors which can be obtained by partially decoding the compressed video without extracting any additional features. Our approach is based on modelling the motion vector field as a Conditional Random Field (CRF) and obtaining oriented motion segments by finding the optimal labelling which minimises the global energy of CRF. These oriented motion segments are recursively merged based on gradient across their boundaries to obtain the final flow segments. This work in compressed domain can be easily extended to pixel domain by substituting motion vectors with motion based features like optical flow. The proposed algorithm is experimentally evaluated on a standard crowd flow dataset and its superior performance in both accuracy and computational time are demonstrated through quantitative results.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06039",
        "title": "moco: Fast Motion Correction for Calcium Imaging",
        "authors": [
            "Alexander Dubbs",
            "James Guevara",
            "Darcy S. Peterka",
            "Rafael Yuste"
        ],
        "abstract": "Motion correction is the first in a pipeline of algorithms to analyze calcium imaging videos and extract biologically relevant information, for example the network structure of the neurons therein. Fast motion correction would be especially critical for closed-loop activity triggered stimulation experiments, where accurate detection and targeting of specific cells in necessary. Our algorithm uses a Fourier-transform approach, and its efficiency derives from a combination of judicious downsampling and the accelerated computation of many $L_2$ norms using dynamic programming and two-dimensional, fft-accelerated convolutions. Its accuracy is comparable to that of established community-used algorithms, and it is more stable to large translational motions. It is programmed in Java and is compatible with ImageJ.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06046",
        "title": "Face Prediction Model for an Automatic Age-invariant Face Recognition System",
        "authors": [
            "Poonam Yadav"
        ],
        "abstract": "Automated face recognition and identification softwares are becoming part of our daily life; it finds its abode not only with Facebook's auto photo tagging, Apple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in Homeland Security Department's dedicated biometric face detection systems. Most of these automatic face identification systems fail where the effects of aging come into the picture. Little work exists in the literature on the subject of face prediction that accounts for aging, which is a vital part of the computer face recognition systems. In recent years, individual face components' (e.g. eyes, nose, mouth) features based matching algorithms have emerged, but these approaches are still not efficient. Therefore, in this work we describe a Face Prediction Model (FPM), which predicts human face aging or growth related image variation using Principle Component Analysis (PCA) and Artificial Neural Network (ANN) learning techniques. The FPM captures the facial changes, which occur with human aging and predicts the facial image with a few years of gap with an acceptable accuracy of face matching from 76 to 86%.\n    ",
        "submission_date": "2015-04-16T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06068",
        "title": "A general framework for the IT-based clustering methods",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Previously, we proposed a physically inspired rule to organize the data points in a sparse yet effective structure, called the in-tree (IT) graph, which is able to capture a wide class of underlying cluster structures in the datasets, especially for the density-based datasets. Although there are some redundant edges or lines between clusters requiring to be removed by computer, this IT graph has a big advantage compared with the k-nearest-neighborhood (k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in the IT graph are much more distinguishable and thus can be easily determined by several methods previously proposed by us.\n",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06096",
        "title": "Graph-based compression of dynamic 3D point cloud sequences",
        "authors": [
            "Dorina Thanou",
            "Philip A. Chou",
            "Pascal Frossard"
        ],
        "abstract": "This paper addresses the problem of compression of 3D point cloud sequences that are characterized by moving 3D positions and color attributes. As temporally successive point cloud frames are similar, motion estimation is key to effective compression of these sequences. It however remains a challenging problem as the point cloud frames have varying numbers of points without explicit correspondence information. We represent the time-varying geometry of these sequences with a set of graphs, and consider 3D positions and color attributes of the points clouds as signals on the vertices of the graphs. We then cast motion estimation as a feature matching problem between successive graphs. The motion is estimated on a sparse set of representative vertices using new spectral graph wavelet descriptors. A dense motion field is eventually interpolated by solving a graph-based regularization problem. The estimated motion is finally used for removing the temporal redundancy in the predictive coding of the 3D positions and the color characteristics of the point cloud sequences. Experimental results demonstrate that our method is able to accurately estimate the motion between consecutive frames. Moreover, motion estimation is shown to bring significant improvement in terms of the overall compression performance of the sequence. To the best of our knowledge, this is the first paper that exploits both the spatial correlation inside each frame (through the graph) and the temporal correlation between the frames (through the motion estimation) to compress the color and the geometry of 3D point cloud sequences in an efficient way.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06204",
        "title": "Learning to Segment Object Candidates",
        "authors": [
            "Pedro O. Pinheiro",
            "Ronan Collobert",
            "Piotr Dollar"
        ],
        "abstract": "Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.\n    ",
        "submission_date": "2015-06-20T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06272",
        "title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization",
        "authors": [
            "Junqi Jin",
            "Kun Fu",
            "Runpeng Cui",
            "Fei Sha",
            "Changshui Zhang"
        ],
        "abstract": "Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of \"abstract meaning\", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.\n    ",
        "submission_date": "2015-06-20T00:00:00",
        "last_modified_date": "2015-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06273",
        "title": "3D Reconstruction from Full-view Fisheye Camera",
        "authors": [
            "Chuiwen Ma",
            "Liang Shi",
            "Hanlu Huang",
            "Mengyuan Yan"
        ],
        "abstract": "In this report, we proposed a 3D reconstruction method for the full-view fisheye camera. The camera we used is Ricoh Theta, which captures spherical images and has a wide field of view (FOV). The conventional stereo apporach based on perspective camera model cannot be directly applied and instead we used a spherical camera model to depict the relation between 3D point and its corresponding observation in the image. We implemented a system that can reconstruct the 3D scene using captures from two or more cameras. A GUI is also created to allow users to control the view perspective and obtain a better intuition of how the scene is rebuilt. Experiments showed that our reconstruction results well preserved the structure of the scene in the real world.\n    ",
        "submission_date": "2015-06-20T00:00:00",
        "last_modified_date": "2015-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06274",
        "title": "Pose Estimation Based on 3D Models",
        "authors": [
            "Chuiwen Ma",
            "Hao Su",
            "Liang Shi"
        ],
        "abstract": "In this paper, we proposed a pose estimation system based on rendered image training set, which predicts the pose of objects in real image, with knowledge of object category and tight bounding box. We developed a patch-based multi-class classification algorithm, and an iterative approach to improve the accuracy. We achieved state-of-the-art performance on pose estimation task.\n    ",
        "submission_date": "2015-06-20T00:00:00",
        "last_modified_date": "2015-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06289",
        "title": "Filtrated Algebraic Subspace Clustering",
        "authors": [
            "Manolis C. Tsakiris",
            "Rene Vidal"
        ],
        "abstract": "Subspace clustering is the problem of clustering data that lie close to a union of linear subspaces. In the abstract form of the problem, where no noise or other corruptions are present, the data are assumed to lie in general position inside the algebraic variety of a union of subspaces, and the objective is to decompose the variety into its constituent subspaces. Prior algebraic-geometric approaches to this problem require the subspaces to be of equal dimension, or the number of subspaces to be known. Subspaces of arbitrary dimensions can still be recovered in closed form, in terms of all homogeneous polynomials of degree $m$ that vanish on their union, when an upper bound m on the number of the subspaces is given. In this paper, we propose an alternative, provably correct, algorithm for addressing a union of at most $m$ arbitrary-dimensional subspaces, based on the idea of descending filtrations of subspace arrangements. Our algorithm uses the gradient of a vanishing polynomial at a point in the variety to find a hyperplane containing the subspace S passing through that point. By intersecting the variety with this hyperplane, we obtain a subvariety that contains S, and recursively applying the procedure until no non-trivial vanishing polynomial exists, our algorithm eventually identifies S. By repeating this procedure for other points, our algorithm eventually identifies all the subspaces by returning a basis for their orthogonal complement. Finally, we develop a variant of the abstract algorithm, suitable for computations with noisy data. We show by experiments on synthetic and real data that the proposed algorithm outperforms state-of-the-art methods on several occasions, thus demonstrating the merit of the idea of filtrations.\n    ",
        "submission_date": "2015-06-20T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06343",
        "title": "Mining Mid-level Visual Patterns with Deep CNN Activations",
        "authors": [
            "Yao Li",
            "Lingqiao Liu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "The purpose of mid-level visual element discovery is to find clusters of image patches that are both representative and discriminative. Here we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). We observe that a fully-connected CNN activation extracted from an image patch typically possesses two appealing properties that enable its seamless integration with pattern mining techniques. The marriage between CNN activations and association rule mining, a well-known pattern mining technique in the literature, leads to fast and effective discovery of representative and discriminative patterns from a huge number of image patches. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent, and thus give rise to a mid-level visual element in our work. Given the patterns and retrieved mid-level visual elements, we propose two methods to generate image feature representations for each. The first method is to use the patterns as codewords in a dictionary, similar to the Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. The second one relies on the retrieved mid-level visual elements to construct a Bag-of-Elements representation. We evaluate the two encoding methods on scene and object classification tasks, and demonstrate that our approach outperforms or matches recent works using CNN activations for these tasks.\n    ",
        "submission_date": "2015-06-21T00:00:00",
        "last_modified_date": "2016-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06448",
        "title": "DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation",
        "authors": [
            "Holger R. Roth",
            "Le Lu",
            "Amal Farag",
            "Hoo-Chang Shin",
            "Jiamin Liu",
            "Evrim Turkbey",
            "Ronald M. Summers"
        ],
        "abstract": "Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-to-fine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via $P{-}\\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional ConvNet ($R_1{-}\\mathrm{ConvNet}$) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a \"zoom-out\" fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked $R_2{-}\\mathrm{ConvNet}$ leveraging the joint space of CT intensities and the $P{-}\\mathrm{ConvNet}$ dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6$\\pm$6.3% in training and 71.8$\\pm$10.7% in testing.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06579",
        "title": "Understanding Neural Networks Through Deep Visualization",
        "authors": [
            "Jason Yosinski",
            "Jeff Clune",
            "Anh Nguyen",
            "Thomas Fuchs",
            "Hod Lipson"
        ],
        "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06628",
        "title": "Modality-dependent Cross-media Retrieval",
        "authors": [
            "Yunchao Wei",
            "Yao Zhao",
            "Zhenfeng Zhu",
            "Shikui Wei",
            "Yanhui Xiao",
            "Jiashi Feng",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia dataset.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06659",
        "title": "Target Tracking In Real Time Surveillance Cameras and Videos",
        "authors": [
            "Nayyab Naseem",
            "Mehreen Sirshar"
        ],
        "abstract": "Security concerns has been kept on increasing, so it is important for everyone to keep their property safe from thefts and destruction. So the need for surveillance techniques are also increasing. The system has been developed to detect the motion in a video. A system has been developed for real time applications by using the techniques of background subtraction and frame differencing. In this system, motion is detected from the webcam or from the real time video. Background subtraction and frames differencing method has been used to detect the moving target. In background subtraction method, current frame is subtracted from the referenced frame and then the threshold is applied. If the difference is greater than the threshold then it is considered as the pixel from the moving object, otherwise it is considered as background pixel. Similarly, two frames difference method takes difference between two continuous frames. Then that resultant difference frame is thresholded and the amount of difference pixels is calculated.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06681",
        "title": "Adaptive Digital Scan Variable Pixels",
        "authors": [
            "Sherin Sugathan",
            "Reshma Scaria",
            "Alex Pappachen James"
        ],
        "abstract": "The square and rectangular shape of the pixels in the digital images for sensing and display purposes introduces several inaccuracies in the representation of digital images. The major disadvantage of square pixel shapes is the inability to accurately capture and display the details in the objects having variable orientations to edges, shapes and regions. This effect can be observed by the inaccurate representation of diagonal edges in low resolution square pixel images. This paper explores a less investigated idea of using variable shaped pixels for improving visual quality of image scans without increasing the square pixel resolution. The proposed adaptive filtering technique reports an improvement in image PSNR.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06724",
        "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
        "authors": [
            "Yukun Zhu",
            "Ryan Kiros",
            "Richard Zemel",
            "Ruslan Salakhutdinov",
            "Raquel Urtasun",
            "Antonio Torralba",
            "Sanja Fidler"
        ],
        "abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06825",
        "title": "DeepStereo: Learning to Predict New Views from the World's Imagery",
        "authors": [
            "John Flynn",
            "Ivan Neulander",
            "James Philbin",
            "Noah Snavely"
        ],
        "abstract": "Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision, but their use in graphics problems has been limited. In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. To verify our method we show that it can convincingly reproduce known test views from nearby imagery. Additionally we show images rendered from novel viewpoints. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.\n    ",
        "submission_date": "2015-06-22T00:00:00",
        "last_modified_date": "2015-06-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06868",
        "title": "Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data",
        "authors": [
            "Luping Zhou",
            "Lei Wang",
            "Lingqiao Liu",
            "Philip Ogunbona",
            "Dinggang Shen"
        ],
        "abstract": "Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables, BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06876",
        "title": "Autonomous 3D Reconstruction Using a MAV",
        "authors": [
            "Alexander Popov",
            "Dimitrios Zermas",
            "Nikolaos Papanikolopoulos"
        ],
        "abstract": "An approach is proposed for high resolution 3D reconstruction of an object using a Micro Air Vehicle (MAV). A system is described which autonomously captures images and performs a dense 3D reconstruction via structure from motion with no prior knowledge of the environment. Only the MAVs own sensors, the front facing camera and the Inertial Measurement Unit (IMU) are utilized. Precision agriculture is considered as an example application for the system.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06881",
        "title": "Automatic vehicle tracking and recognition from aerial image sequences",
        "authors": [
            "Ognjen Arandjelovic"
        ],
        "abstract": "This paper addresses the problem of automated vehicle tracking and recognition from aerial image sequences. Motivated by its successes in the existing literature focus on the use of linear appearance subspaces to describe multi-view object appearance and highlight the challenges involved in their application as a part of a practical system. A working solution which includes steps for data extraction and normalization is described. In experiments on real-world data the proposed methodology achieved promising results with a high correct recognition rate and few, meaningful errors (type II errors whereby genuinely similar targets are sometimes being confused with one another). Directions for future research and possible improvements of the proposed method are discussed.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06882",
        "title": "SALSA: A Novel Dataset for Multimodal Group Behavior Analysis",
        "authors": [
            "Xavier Alameda-Pineda",
            "Jacopo Staiano",
            "Ramanathan Subramanian",
            "Ligia Batrinca",
            "Elisa Ricci",
            "Bruno Lepri",
            "Oswald Lanz",
            "Nicu Sebe"
        ],
        "abstract": "Studying free-standing conversational groups (FCGs) in unstructured social settings (e.g., cocktail party ) is gratifying due to the wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties in the form of low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four static surveillance cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06905",
        "title": "Person re-identification via efficient inference in fully connected CRF",
        "authors": [
            "Jiuqing Wan",
            "Menglin Xing"
        ],
        "abstract": "In this paper, we address the problem of person re-identification problem, i.e., retrieving instances from gallery which are generated by the same person as the given probe image. This is very challenging because the person's appearance usually undergoes significant variations due to changes in illumination, camera angle and view, background clutter, and occlusion over the camera network. In this paper, we assume that the matched gallery images should not only be similar to the probe, but also be similar to each other, under suitable metric. We express this assumption with a fully connected CRF model in which each node corresponds to a gallery and every pair of nodes are connected by an edge. A label variable is associated with each node to indicate whether the corresponding image is from target person. We define unary potential for each node using existing feature calculation and matching techniques, which reflect the similarity between probe and gallery image, and define pairwise potential for each edge in terms of a weighed combination of Gaussian kernels, which encode appearance similarity between pair of gallery images. The specific form of pairwise potential allows us to exploit an efficient inference algorithm to calculate the marginal distribution of each label variable for this dense connected CRF. We show the superiority of our method by applying it to public datasets and comparing with the state of the art.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06981",
        "title": "R-CNN minus R",
        "authors": [
            "Karel Lenc",
            "Andrea Vedaldi"
        ],
        "abstract": "Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07062",
        "title": "Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with Constrained Spherical Deconvolution",
        "authors": [
            "J.M. Portegies",
            "R.H.J. Fick",
            "G.R. Sanguinetti",
            "S.P.L. Meesters",
            "G. Girard",
            "R. Duits"
        ],
        "abstract": "We propose two strategies to improve the quality of tractography results computed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Both methods are based on the same PDE framework, defined in the coupled space of positions and orientations, associated with a stochastic process describing the enhancement of elongated structures while preserving crossing structures. In the first method we use the enhancement PDE for contextual regularization of a fiber orientation distribution (FOD) that is obtained on individual voxels from high angular resolution diffusion imaging (HARDI) data via constrained spherical deconvolution (CSD). Thereby we improve the FOD as input for subsequent tractography. Secondly, we introduce the fiber to bundle coherence (FBC), a measure for quantification of fiber alignment. The FBC is computed from a tractography result using the same PDE framework and provides a criterion for removing the spurious fibers. We validate the proposed combination of CSD and enhancement on phantom data and on human data, acquired with different scanning protocols. On the phantom data we find that PDE enhancements improve both local metrics and global metrics of tractography results, compared to CSD without enhancements. On the human data we show that the enhancements allow for a better reconstruction of crossing fiber bundles and they reduce the variability of the tractography output with respect to the acquisition parameters. Finally, we show that both the enhancement of the FODs and the use of the FBC measure on the tractography improve the stability with respect to different stochastic realizations of probabilistic tractography. This is shown in a clinical application: the reconstruction of the optic radiation for epilepsy surgery planning.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07136",
        "title": "Segmentation of Three-dimensional Images with Parametric Active Surfaces and Topology Changes",
        "authors": [
            "Heike Benninghoff",
            "Harald Garcke"
        ],
        "abstract": "In this paper, we introduce a novel parametric method for segmentation of three-dimensional images. We consider a piecewise constant version of the Mumford-Shah and the Chan-Vese functionals and perform a region-based segmentation of 3D image data. An evolution law is derived from energy minimization problems which push the surfaces to the boundaries of 3D objects in the image. We propose a parametric scheme which describes the evolution of parametric surfaces. An efficient finite element scheme is proposed for a numerical approximation of the evolution equations. Since standard parametric methods cannot handle topology changes automatically, an efficient method is presented to detect, identify and perform changes in the topology of the surfaces. One main focus of this paper are the algorithmic details to handle topology changes like splitting and merging of surfaces and change of the genus of a surface. Different artificial images are studied to demonstrate the ability to detect the different types of topology changes. Finally, the parametric method is applied to segmentation of medical 3D images.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07224",
        "title": "Deep CNN Ensemble with Data Augmentation for Object Detection",
        "authors": [
            "Jian Guo",
            "Stephen Gould"
        ],
        "abstract": "We report on the methods used in our recent DeepEnsembleCoco submission to the PASCAL VOC 2012 challenge, which achieves state-of-the-art performance on the object detection task. Our method is a variant of the R-CNN model proposed Girshick:CVPR14 with two key improvements to training and evaluation. First, our method constructs an ensemble of deep CNN models with different architectures that are complementary to each other. Second, we augment the PASCAL VOC training set with images from the Microsoft COCO dataset to significantly enlarge the amount training data. Importantly, we select a subset of the Microsoft COCO images to be consistent with the PASCAL VOC task. Results on the PASCAL VOC evaluation server show that our proposed method outperform all previous methods on the PASCAL VOC 2012 detection task at time of submission.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07257",
        "title": "A Novel Feature Extraction Method for Scene Recognition Based on Centered Convolutional Restricted Boltzmann Machines",
        "authors": [
            "Jingyu Gao",
            "Jinfu Yang",
            "Guanghui Wang",
            "Mingai Li"
        ],
        "abstract": "Scene recognition is an important research topic in computer vision, while feature extraction is a key step of object recognition. Although classical Restricted Boltzmann machines (RBM) can efficiently represent complicated data, it is hard to handle large images due to its complexity in computation. In this paper, a novel feature extraction method, named Centered Convolutional Restricted Boltzmann Machines (CCRBM), is proposed for scene recognition. The proposed model is an improved Convolutional Restricted Boltzmann Machines (CRBM) by introducing centered factors in its learning strategy to reduce the source of instabilities. First, the visible units of the network are redefined using centered factors. Then, the hidden units are learned with a modified energy function by utilizing a distribution function, and the visible units are reconstructed using the learned hidden units. In order to achieve better generative ability, the Centered Convolutional Deep Belief Networks (CCDBN) is trained in a greedy layer-wise way. Finally, a softmax regression is incorporated for scene recognition. Extensive experimental evaluations using natural scenes, MIT-indoor scenes, and Caltech 101 datasets show that the proposed approach performs better than other counterparts in terms of stability, generalization, and discrimination. The CCDBN model is more suitable for natural scene image recognition by virtue of convolutional property.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07271",
        "title": "Natural Scene Recognition Based on Superpixels and Deep Boltzmann Machines",
        "authors": [
            "Jinfu Yang",
            "Jingyu Gao",
            "Guanghui Wang",
            "Shanshan Zhang"
        ],
        "abstract": "The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning model, which has been successfully applied to handwritten digit recognition and, as well as object recognition. However, the DBM is limited in scene recognition due to the fact that natural scene images are usually very large. In this paper, an efficient scene recognition approach is proposed based on superpixels and the DBMs. First, a simple linear iterative clustering (SLIC) algorithm is employed to generate superpixels of input images, where each superpixel is regarded as an input of a learning model. Then, a two-layer DBM model is constructed by stacking two restricted Boltzmann machines (RBMs), and a greedy layer-wise algorithm is applied to train the DBM model. Finally, a softmax regression is utilized to categorize scene images. The proposed technique can effectively reduce the computational complexity and enhance the performance for large natural image recognition. The approach is verified and evaluated by extensive experiments, including the fifteen-scene categories dataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to evaluate the proposed method. The experimental results show that the proposed approach outperforms other state-of-the-art methods in terms of recognition rate.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07310",
        "title": "Targeting Ultimate Accuracy: Face Recognition via Deep Embedding",
        "authors": [
            "Jingtuo Liu",
            "Yafeng Deng",
            "Tao Bai",
            "Zhengping Wei",
            "Chang Huang"
        ],
        "abstract": "Face Recognition has been studied for many decades. As opposed to traditional hand-crafted features such as LBP and HOG, much more sophisticated features can be learned automatically by deep learning methods in a data-driven way. In this paper, we propose a two-stage approach that combines a multi-patch deep CNN and deep metric learning, which extracts low dimensional but very discriminative features for face verification and recognition. Experiments show that this method outperforms other state-of-the-art methods on LFW dataset, achieving 99.77% pair-wise verification accuracy and significantly better accuracy under other two more practical protocols. This paper also discusses the importance of data size and the number of patches, showing a clear path to practical high-performance face recognition systems in real world.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07363",
        "title": "Salient Object Detection via Objectness Measure",
        "authors": [
            "Sai Srivatsa R",
            "R. Venkatesh Babu"
        ],
        "abstract": "Salient object detection has become an important task in many image processing applications. The existing approaches exploit background prior and contrast prior to attain state of the art results. In this paper, instead of using background cues, we estimate the foreground regions in an image using objectness proposals and utilize it to obtain smooth and accurate saliency maps. We propose a novel saliency measure called `foreground connectivity' which determines how tightly a pixel or a region is connected to the estimated foreground. We use the values assigned by this measure as foreground weights and integrate these in an optimization framework to obtain the final saliency maps. We extensively evaluate the proposed approach on two benchmark databases and demonstrate that the results obtained are better than the existing state of the art approaches.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07439",
        "title": "Kernel Cuts: MRF meets Kernel & Spectral Clustering",
        "authors": [
            "Meng Tang",
            "Dmitrii Marin",
            "Ismail Ben Ayed",
            "Yuri Boykov"
        ],
        "abstract": "We propose a new segmentation model combining common regularization energies, e.g. Markov Random Field (MRF) potentials, and standard pairwise clustering criteria like Normalized Cut (NC), average association (AA), etc. These clustering and regularization models are widely used in machine learning and computer vision, but they were not combined before due to significant differences in the corresponding optimization, e.g. spectral relaxation and combinatorial max-flow techniques. On the one hand, we show that many common applications using MRF segmentation energies can benefit from a high-order NC term, e.g. enforcing balanced clustering of arbitrary high-dimensional image features combining color, texture, location, depth, motion, etc. On the other hand, standard clustering applications can benefit from an inclusion of common pairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency, label cost, etc. To address joint energies like NC+MRF, we propose efficient Kernel Cut algorithms based on bound optimization. While focusing on graph cut and move-making techniques, our new unary (linear) kernel and spectral bound formulations for common pairwise clustering criteria allow to integrate them with any regularization functionals with existing discrete or continuous solvers.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07440",
        "title": "Unshredding of Shredded Documents: Computational Framework and Implementation",
        "authors": [
            "Lei Kristoffer R. Lactuan",
            "Jaderick P. Pabico"
        ],
        "abstract": "A shredded document $D$ is a document whose pages have been cut into strips for the purpose of destroying private, confidential, or sensitive information $I$ contained in $D$. Shredding has become a standard means of government organizations, businesses, and private individuals to destroy archival records that have been officially classified for disposal. It can also be used to destroy documentary evidence of wrongdoings by entities who are trying to hide $I$.\n",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07452",
        "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation",
        "authors": [
            "Marijn F. Stollenga",
            "Wonmin Byeon",
            "Marcus Liwicki",
            "Juergen Schmidhuber"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07597",
        "title": "Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping Fields of View",
        "authors": [
            "Michael J. Tribou",
            "David W. L. Wang",
            "Steven L. Waslander"
        ],
        "abstract": "An analysis of the relative motion and point feature model configurations leading to solution degeneracy is presented, for the case of a Simultaneous Localization and Mapping system using multicamera clusters with non-overlapping fields-of-view. The SLAM optimization system seeks to minimize image space reprojection error and is formulated for a cluster containing any number of component cameras, observing any number of point features over two keyframes. The measurement Jacobian is transformed to expose a reduced-dimension representation such that the degeneracy of the system can be determined by the rank of a dense submatrix. A set of relative motions sufficient for degeneracy are identified for certain cluster configurations, independent of target model geometry. Furthermore, it is shown that increasing the number of cameras within the cluster and observing features across different cameras over the two keyframes reduces the size of the degenerate motion sets significantly.\n    ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2015-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07613",
        "title": "Generalized Majorization-Minimization",
        "authors": [
            "Sobhan Naderi Parizi",
            "Kun He",
            "Reza Aghajani",
            "Stan Sclaroff",
            "Pedro Felzenszwalb"
        ],
        "abstract": "Non-convex optimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a powerful iterative procedure for optimizing non-convex functions that works by optimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to \\emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new optimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show empirically that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.\n    ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2019-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07656",
        "title": "DeepMatching: Hierarchical Deformable Dense Matching",
        "authors": [
            "Jerome Revaud",
            "Philippe Weinzaepfel",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al 2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets.  DeepMatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07704",
        "title": "AttentionNet: Aggregating Weak Directions for Accurate Object Detection",
        "authors": [
            "Donggeun Yoo",
            "Sunggyun Park",
            "Joon-Young Lee",
            "Anthony S. Paek",
            "In So Kweon"
        ],
        "abstract": "We present a novel detection method using a deep convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from the object proposal to the post bounding-box regression. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture only.\n    ",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2015-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07866",
        "title": "Camera Calibration from Dynamic Silhouettes Using Motion Barcodes",
        "authors": [
            "Gil Ben-Artzi",
            "Yoni Kasten",
            "Shmuel Peleg",
            "Michael Werman"
        ],
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences.\n",
        "submission_date": "2015-06-25T00:00:00",
        "last_modified_date": "2017-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08006",
        "title": "Spectral Collaborative Representation based Classification for Hand Gestures recognition on Electromyography Signals",
        "authors": [
            "Ali Boyali"
        ],
        "abstract": "In this study, we introduce a novel variant and application of the Collaborative Representation based Classification in spectral domain for recognition of the hand gestures using the raw surface Electromyography signals. The intuitive use of spectral features are explained via circulant matrices. The proposed Spectral Collaborative Representation based Classification (SCRC) is able to recognize gestures with higher levels of accuracy for a fairly rich gesture set. The worst recognition result which is the best in the literature is obtained as 97.3\\% among the four sets of the experiments for each hand gestures. The recognition results are reported with a substantial number of experiments and labeling computation.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08110",
        "title": "Nonnegative Matrix Factorization applied to reordered pixels of single images based on patches to achieve structured nonnegative dictionaries",
        "authors": [
            "Richard M. Charles",
            "Kye M. Taylor",
            "James H. Curry"
        ],
        "abstract": "Recent improvements in computing allow for the processing and analysis of very large datasets in a variety of fields. Often the analysis requires the creation of low-rank approximations to the datasets leading to efficient storage. This article presents and analyzes a novel approach for creating nonnegative, structured dictionaries using NMF applied to reordered pixels of single, natural images. We reorder the pixels based on patches and present our approach in general. We investigate our approach when using the Singular Value Decomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rank approximations. Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) are used to evaluate the algorithm. We report that while the SVD provides the best reconstructions, its dictionary of vectors lose both the sign structure of the original image and details of localized image content. In contrast, the dictionaries produced using NMF preserves the sign structure of the original image matrix and offer a nonnegative, parts-based dictionary.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08301",
        "title": "A Novel Approach for Stable Selection of Informative Redundant Features from High Dimensional fMRI Data",
        "authors": [
            "Yilun Wang",
            "Zhiqiang Li",
            "Yifeng Wang",
            "Xiaona Wang",
            "Junjie Zheng",
            "Xujuan Duan",
            "Huafu Chen"
        ],
        "abstract": "Feature selection is among the most important components because it not only helps enhance the classification accuracy, but also or even more important provides potential biomarker discovery. However, traditional multivariate methods is likely to obtain unstable and unreliable results in case of an extremely high dimensional feature space and very limited training samples, where the features are often correlated or redundant. In order to improve the stability, generalization and interpretations of the discovered potential biomarker and enhance the robustness of the resultant classifier, the redundant but informative features need to be also selected. Therefore we introduced a novel feature selection method which combines a recent implementation of the stability selection approach and the elastic net approach. The advantage in terms of better control of false discoveries and missed discoveries of our approach, and the resulted better interpretability of the obtained potential biomarker is verified in both synthetic and real fMRI experiments. In addition, we are among the first to demonstrate the robustness of feature selection benefiting from the incorporation of stability selection and also among the first to demonstrate the possible unrobustness of the classical univariate two-sample t-test method. Specifically, we show the robustness of our feature selection results in existence of noisy (wrong) training labels, as well as the robustness of the resulted classifier based on our feature selection results in the existence of data variation, demonstrated by a multi-center attention-deficit/hyperactivity disorder (ADHD) fMRI data.\n    ",
        "submission_date": "2015-06-27T00:00:00",
        "last_modified_date": "2016-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08347",
        "title": "Occlusion Coherence: Detecting and Localizing Occluded Faces",
        "authors": [
            "Golnaz Ghiasi",
            "Charless C. Fowlkes"
        ],
        "abstract": "The presence of occluders significantly impacts object recognition accuracy. However, occlusion is typically treated as an unstructured source of noise and explicit models for occluders have lagged behind those for object appearance and shape. In this paper we describe a hierarchical deformable part model for face detection and landmark localization that explicitly models part occlusion. The proposed model structure makes it possible to augment positive training data with large numbers of synthetically occluded instances. This allows us to easily incorporate the statistics of occlusion patterns in a discriminatively trained model. We test the model on several benchmarks for landmark localization and detection including challenging new data sets featuring significant occlusion. We find that the addition of an explicit occlusion model yields a detection system that outperforms existing approaches for occluded instances while maintaining competitive accuracy in detection and landmark localization for unoccluded instances.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08353",
        "title": "A note on patch-based low-rank minimization for fast image denoising",
        "authors": [
            "Haijuan Hu",
            "Jacques Froment",
            "Quansheng Liu"
        ],
        "abstract": "Patch-based low-rank minimization for image processing attracts much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising. The main denoising process is stated in three equivalent way: PCA, SVD and low-rank minimization. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is rather rapid, and it is effective for a variety of natural grayscale images and color images, especially for texture parts in images. Further improvements of this method are also given. In addition, due to the simplicity of this method, we could provide an explanation of the choice of the threshold parameter, estimation of PSNR values, and give other insights into this method.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2018-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08425",
        "title": "Deep-Plant: Plant Identification with convolutional neural networks",
        "authors": [
            "Sue Han Lee",
            "Chee Seng Chan",
            "Paul Wilkin",
            "Paolo Remagnino"
        ],
        "abstract": "This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2015-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08438",
        "title": "Unsupervised Semantic Parsing of Video Collections",
        "authors": [
            "Ozan Sener",
            "Amir Zamir",
            "Silvio Savarese",
            "Ashutosh Saxena"
        ],
        "abstract": "Human communication typically has an underlying structure. This is reflected in the fact that in many user generated videos, a starting point, ending, and certain objective steps between these two can be identified. In this paper, we propose a method for parsing a video into such semantic steps in an unsupervised way. The proposed method is capable of providing a semantic \"storyline\" of the video composed of its objective steps. We accomplish this using both visual and language cues in a joint generative model. The proposed method can also provide a textual description for each of the identified semantic steps and video segments. We evaluate this method on a large number of complex YouTube videos and show results of unprecedented quality for this intricate and impactful problem.\n    ",
        "submission_date": "2015-06-28T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08485",
        "title": "The Multi-Strand Graph for a PTZ Tracker",
        "authors": [
            "Shachaf Melman",
            "Yael Moses",
            "G\u00e9rard Medioni",
            "Yinghao Cai"
        ],
        "abstract": "High-resolution images can be used to resolve matching ambiguities between trajectory fragments (tracklets), which is one of the main challenges in multiple target tracking. A PTZ camera, which can pan, tilt and zoom, is a powerful and efficient tool that offers both close-up views and wide area coverage on demand. The wide-area view makes it possible to track many targets while the close-up view allows individuals to be identified from high-resolution images of their faces. A central component of a PTZ tracking system is a scheduling algorithm that determines which target to zoom in on.\n",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08529",
        "title": "Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes from Unstructured Text Descriptions",
        "authors": [
            "Mohamed Elhoseiny",
            "Ahmed Elgammal",
            "Babak Saleh"
        ],
        "abstract": "In this paper we propose a framework for predicting kernelized classifiers in the visual domain for categories with no training images where the knowledge comes from textual description about these categories. Through our optimization framework, the proposed approach is capable of embedding the class-level knowledge from the text domain as kernel classifiers in the visual domain. We also proposed a distributional semantic kernel between text descriptions which is shown to be effective in our setting. The proposed framework is not restricted to textual descriptions, and can also be applied to other forms knowledge representations. Our approach was applied for the challenging task of zero-shot learning of fine-grained categories from text descriptions of these categories.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08581",
        "title": "Variational Inference for Background Subtraction in Infrared Imagery",
        "authors": [
            "Konstantinos Makantasis",
            "Anastasios Doulamis",
            "Nikolaos Doulamis"
        ],
        "abstract": "We propose a Gaussian mixture model for background subtraction in infrared imagery. Following a Bayesian approach, our method automatically estimates the number of Gaussian components as well as their parameters, while simultaneously it avoids over/under fitting. The equations for estimating model parameters are analytically derived and thus our method does not require any sampling algorithm that is computationally and memory inefficient. The pixel density estimate is followed by an efficient and highly accurate updating mechanism, which permits our system to be automatically adapted to dynamically changing operation conditions. Experimental results and comparisons with other methods show that our method outperforms, in terms of precision and recall, while at the same time it keeps computational cost suitable for real-time applications.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08670",
        "title": "Automatic Channel Network Extraction from Remotely Sensed Images by Singularity Analysis",
        "authors": [
            "F. Isikdogan",
            "A.C. Bovik",
            "P. Passalacqua"
        ],
        "abstract": "Quantitative analysis of channel networks plays an important role in river studies. To provide a quantitative representation of channel networks, we propose a new method that extracts channels from remotely sensed images and estimates their widths. Our fully automated method is based on a recently proposed Multiscale Singularity Index that responds strongly to curvilinear structures but weakly to edges. The algorithm produces a channel map, using a single image where water and non-water pixels have contrast, such as a Landsat near-infrared band image or a water index defined on multiple bands. The proposed method provides a robust alternative to the procedures that are used in remote sensing of fluvial geomorphology and makes classification and analysis of channel networks easier. The source code of the algorithm is available at: ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08682",
        "title": "Human Shape Variation - An Efficient Implementation using Skeleton",
        "authors": [
            "Dhriti Sengupta",
            "Merina Kundu",
            "Jayati Ghosh Dastidar"
        ],
        "abstract": "It is at times important to detect human presence automatically in secure environments. This needs a shape recognition algorithm that is robust, fast and has low error rates. The algorithm needs to process camera images quickly to detect any human in the range of vision, and generate alerts, especially if the object under scrutiny is moving in certain directions. We present here a simple, efficient and fast algorithm using skeletons of the images, and simple features like posture and length of the object.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08704",
        "title": "An automatic and efficient foreground object extraction scheme",
        "authors": [
            "Subhajit Adhikari",
            "Joydeep Kar",
            "Jayati Ghosh Dastidar"
        ],
        "abstract": "This paper presents a method to differentiate the foreground objects from the background of a color image. Firstly a color image of any size is input for processing. The algorithm converts it to a grayscale image. Next we apply canny edge detector to find the boundary of the foreground object. We concentrate to find the maximum distance between each boundary pixel column wise and row wise and we fill the region that is bound by the edges. Thus we are able to extract the grayscale values of pixels that are in the bounded region and convert the grayscale image back to original color image containing only the foreground object.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08765",
        "title": "Spectral Motion Synchronization in SE(3)",
        "authors": [
            "Federica Arrigoni",
            "Andrea Fusiello",
            "Beatrice Rossi"
        ],
        "abstract": "This paper addresses the problem of motion synchronization (or averaging) and describes a simple, closed-form solution based on a spectral decomposition, which does not consider rotation and translation separately but works straight in SE(3), the manifold of rigid motions. Besides its theoretical interest, being the first closed form solution in SE(3), experimental results show that it compares favourably with the state of the art both in terms of precision and speed.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08815",
        "title": "Tracking Direction of Human Movement - An Efficient Implementation using Skeleton",
        "authors": [
            "Merina Kundu",
            "Dhriti Sengupta",
            "Jayati Ghosh Dastidar"
        ],
        "abstract": "Sometimes a simple and fast algorithm is required to detect human presence and movement with a low error rate in a controlled environment for security purposes. Here a light weight algorithm has been presented that generates alert on detection of human presence and its movement towards a certain direction. The algorithm uses fixed angle CCTV camera images taken over time and relies upon skeleton transformation of successive images and calculation of difference in their coordinates.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08891",
        "title": "Detecting Table Region in PDF Documents Using Distant Supervision",
        "authors": [
            "Miao Fan",
            "Doo Soon Kim"
        ],
        "abstract": "Superior to state-of-the-art approaches which compete in table recognition with 67 annotated government reports in PDF format released by {\\it ICDAR 2013 Table Competition}, this paper contributes a novel paradigm leveraging large-scale unlabeled PDF documents to open-domain table detection. We integrate the paradigm into our latest developed system ({\\it PdfExtra}) to detect the region of tables by means of 9,466 academic articles from the entire repository of {\\it ACL Anthology}, where almost all papers are archived by PDF format without annotation for tables. The paradigm first designs heuristics to automatically construct weakly labeled data. It then feeds diverse evidences, such as layouts of documents and linguistic features, which are extracted by {\\it Apache PDFBox} and processed by {\\it Stanford NLP} toolkit, into different canonical classifiers. We finally use these classifiers, i.e. {\\it Naive Bayes}, {\\it Logistic Regression} and {\\it Support Vector Machine}, to collaboratively vote on the region of tables. Experimental results show that {\\it PdfExtra} achieves a great leap forward, compared with the state-of-the-art approach. Moreover, we discuss the factors of different features, learning models and even domains of documents that may impact the performance. Extensive evaluations demonstrate that our paradigm is compatible enough to leverage various features and learning models for open-domain table region detection within PDF files.\n    ",
        "submission_date": "2015-06-29T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08959",
        "title": "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification",
        "authors": [
            "Linjie Yang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Updated on 24/09/2015: This update provides preliminary experiment results for fine-grained classification on the surveillance data of CompCars. The train/test splits are provided in the updated dataset. See details in Section 6.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09075",
        "title": "Long-Range Motion Trajectories Extraction of Articulated Human Using Mesh Evolution",
        "authors": [
            "Yuanyuan Wu",
            "Xiaohai He",
            "Byeongkeun Kang",
            "Haiying Song",
            "Truong Q. Nguyen"
        ],
        "abstract": "This letter presents a novel approach to extract reliable dense and long-range motion trajectories of articulated human in a video sequence. Compared with existing approaches that emphasize temporal consistency of each tracked point, we also consider the spatial structure of tracked points on the articulated human. We treat points as a set of vertices, and build a triangle mesh to join them in image space. The problem of extracting long-range motion trajectories is changed to the issue of consistency of mesh evolution over time. First, self-occlusion is detected by a novel mesh-based method and an adaptive motion estimation method is proposed to initialize mesh between successive frames. Furthermore, we propose an iterative algorithm to efficiently adjust vertices of mesh for a physically plausible deformation, which can meet the local rigidity of mesh and silhouette constraints. Finally, we compare the proposed method with the state-of-the-art methods on a set of challenging sequences. Evaluations demonstrate that our method achieves favorable performance in terms of both accuracy and integrity of extracted trajectories.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09110",
        "title": "Forming A Random Field via Stochastic Cliques: From Random Graphs to Fully Connected Random Fields",
        "authors": [
            "Mohammad Javad Shafiee",
            "Alexander Wong",
            "Paul Fieguth"
        ],
        "abstract": "Random fields have remained a topic of great interest over past decades for the purpose of structured inference, especially for problems such as image segmentation. The local nodal interactions commonly used in such models often suffer the short-boundary bias problem, which are tackled primarily through the incorporation of long-range nodal interactions. However, the issue of computational tractability becomes a significant issue when incorporating such long-range nodal interactions, particularly when a large number of long-range nodal interactions (e.g., fully-connected random fields) are modeled.\n",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09124",
        "title": "Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation",
        "authors": [
            "Saehoon Yi",
            "Vladimir Pavlovic"
        ],
        "abstract": "Video segmentation is a stepping stone to understanding video context. Video segmentation enables one to represent a video by decomposing it into coherent regions which comprise whole or parts of objects. However, the challenge originates from the fact that most of the video segmentation algorithms are based on unsupervised learning due to expensive cost of pixelwise video annotation and intra-class variability within similar unconstrained video classes. We propose a Markov Random Field model for unconstrained video segmentation that relies on tight integration of multiple cues: vertices are defined from contour based superpixels, unary potentials from temporal smooth label likelihood and pairwise potentials from global structure of a video. Multi-cue structure is a breakthrough to extracting coherent object regions for unconstrained videos in absence of supervision. Our experiments on VSB100 dataset show that the proposed model significantly outperforms competing state-of-the-art algorithms. Qualitative analysis illustrates that video segmentation result of the proposed model is consistent with human perception of objects.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09166",
        "title": "Aging display's effect on interpretation of digital pathology slides",
        "authors": [
            "Ali R. N. Avanaki",
            "Kathryn S. Espig",
            "Sameer Sawhney",
            "Liron Pantanowitz",
            "Anil V. Parwani",
            "Albert Xthona",
            "Tom R. L. Kimpe"
        ],
        "abstract": "It is our conjecture that the variability of colors in a pathology image effects the interpretation of pathology cases, whether it is diagnostic accuracy, diagnostic confidence, or workflow efficiency. In this paper, digital pathology images are analyzed to quantify the perceived difference in color that occurs due to display aging, in particular a change in the maximum luminance, white point, and color gamut. The digital pathology images studied include diagnostically important features, such as the conspicuity of nuclei. Three different display aging models are applied to images: aging of luminance & chrominance, aging of chrominance only, and a stabilized luminance & chrominance (i.e., no aging). These display models and images are then used to compare conspicuity of nuclei using CIE deltaE2000, a perceptual color difference metric. The effect of display aging using these display models and images is further analyzed through a human reader study designed to quantify the effects from a clinical perspective. Results from our reader study indicate significant impact of aged displays on workflow as well as diagnosis as follow. As compared to the originals (no-aging), slides with the effect of aging simulated were significantly more difficult to read (p-value of 0.0005) and took longer to score (p-value of 0.02). Moreover, luminance+chrominance aging significantly reduced inter-session percent agreement of diagnostic scores (p-value of 0.0418).\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09169",
        "title": "On anthropomorphic decision making in a model observer",
        "authors": [
            "Ali R. N. Avanaki",
            "Kathryn S. Espig",
            "Tom R. L. Kimpe",
            "Andrew D. A. Maidment"
        ],
        "abstract": "By analyzing human readers' performance in detecting small round lesions in simulated digital breast tomosynthesis background in a location known exactly scenario, we have developed a model observer that is a better predictor of human performance with different levels of background complexity (i.e., anatomical and quantum noise). Our analysis indicates that human observers perform a lesion detection task by combining a number of sub-decisions, each an indicator of the presence of a lesion in the image stack. This is in contrast to a channelized Hotelling observer, where the detection task is conducted holistically by thresholding a single decision variable, made from an optimally weighted linear combination of channels. However, it seems that the sub-par performance of human readers compared to the CHO cannot be fully explained by their reliance on sub-decisions, or perhaps we do not consider a sufficient number of sub-decisions. To bridge the gap between the performances of human readers and the model observer based upon sub-decisions, we use an additive noise model, the power of which is modulated with the level of background complexity. The proposed model observer better predicts the fast drop in human detection performance with background complexity.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09174",
        "title": "Discovering Characteristic Landmarks on Ancient Coins using Convolutional Networks",
        "authors": [
            "Jongpil Kim",
            "Vladimir Pavlovic"
        ],
        "abstract": "In this paper, we propose a novel method to find characteristic landmarks on ancient Roman imperial coins using deep convolutional neural network models (CNNs). We formulate an optimization problem to discover class-specific regions while guaranteeing specific controlled loss of accuracy. Analysis on visualization of the discovered region confirms that not only can the proposed method successfully find a set of characteristic regions per class, but also the discovered region is consistent with human expert annotations. We also propose a new framework to recognize the Roman coins which exploits hierarchical structure of the ancient Roman coins using the state-of-the-art classification power of the CNNs adopted to a new task of coin classification. Experimental results show that the proposed framework is able to effectively recognize the ancient Roman coins. For this research, we have collected a new Roman coin dataset where all coins are annotated and consist of observe (head) and reverse (tail) images.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09179",
        "title": "Learning to Detect Blue-white Structures in Dermoscopy Images with Weak Supervision",
        "authors": [
            "Ali Madooei",
            "Mark S. Drew",
            "Hossein Hajimirsadeghi"
        ],
        "abstract": "We propose a novel approach to identify one of the most significant dermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitish structure. In this paper, we achieve this goal in a Multiple Instance Learning framework using only image-level labels of whether the feature is present or not. As the output, we predict the image classification label and as well localize the feature in the image. Experiments are conducted on a challenging dataset with results outperforming state-of-the-art. This study provides an improvement on the scope of modelling for computerized image analysis of skin lesions, in particular in that it puts forward a framework for identification of dermoscopic local features from weakly-labelled data.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09215",
        "title": "Unsupervised Learning from Narrated Instruction Videos",
        "authors": [
            "Jean-Baptiste Alayrac",
            "Piotr Bojanowski",
            "Nishant Agrawal",
            "Josef Sivic",
            "Ivan Laptev",
            "Simon Lacoste-Julien"
        ],
        "abstract": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00019",
        "title": "Representing data by sparse combination of contextual data points for classification",
        "authors": [
            "Jingyan Wang",
            "Yihua Zhou",
            "Ming Yin",
            "Shaochang Chen",
            "Benjamin Edwards"
        ],
        "abstract": "In this paper, we study the problem of using contextual da- ta points of a data point for its classification problem. We propose to represent a data point as the sparse linear reconstruction of its context, and learn the sparse context to gather with a linear classifier in a su- pervised way to increase its discriminative ability. We proposed a novel formulation for context learning, by modeling the learning of context reconstruction coefficients and classifier in a unified objective. In this objective, the reconstruction error is minimized and the coefficient spar- sity is encouraged. Moreover, the hinge loss of the classifier is minimized and the complexity of the classifier is reduced. This objective is opti- mized by an alternative strategy in an iterative algorithm. Experiments on three benchmark data set show its advantage over state-of-the-art context-based data representation and classification methods.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00101",
        "title": "Supervised Learning of Semantics-Preserving Hash via Deep Convolutional Neural Networks",
        "authors": [
            "Huei-Fang Yang",
            "Kevin Lin",
            "Chu-Song Chen"
        ],
        "abstract": "This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off, and classification relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design, SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover, SSDH performs joint learning of image representations, hash codes, and classification in a point-wised manner, and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches, SSDH achieves higher retrieval accuracy, while the classification performance is not sacrificed.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00110",
        "title": "Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based PolSAR Image Classification",
        "authors": [
            "Fang Liu",
            "Junfei Shi",
            "Licheng Jiao",
            "Hongying Liu",
            "Shuyuan Yang",
            "Jie Wu",
            "Hongxia Hao",
            "Jialing Yuan"
        ],
        "abstract": "For polarimetric SAR (PolSAR) image classification, it is a challenge to classify the aggregated terrain types, such as the urban area, into semantic homogenous regions due to sharp bright-dark variations in intensity. The aggregated terrain type is formulated by the similar ground objects aggregated together. In this paper, a polarimetric hierarchical semantic model (PHSM) is firstly proposed to overcome this disadvantage based on the constructions of a primal-level and a middle-level semantic. The primal-level semantic is a polarimetric sketch map which consists of sketch segments as the sparse representation of a PolSAR image. The middle-level semantic is a region map which can extract semantic homogenous regions from the sketch map by exploiting the topological structure of sketch segments. Mapping the region map to the PolSAR image, a complex PolSAR scene is partitioned into aggregated, structural and homogenous pixel-level subspaces with the characteristics of relatively coherent terrain types in each subspace. Then, according to the characteristics of three subspaces above, three specific methods are adopted, and furthermore polarimetric information is exploited to improve the segmentation result. Experimental results on PolSAR data sets with different bands and sensors demonstrate that the proposed method is superior to the state-of-the-art methods in region homogeneity and edge preservation for terrain classification.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00136",
        "title": "Compressive Deconvolution in Medical Ultrasound Imaging",
        "authors": [
            "Zhouye Chen",
            "Adrian Basarab",
            "Denis Kouam\u00e9"
        ],
        "abstract": "The interest of compressive sampling in ultrasound imaging has been recently extensively evaluated by several research teams. Following the different application setups, it has been shown that the RF data may be reconstructed from a small number of measurements and/or using a reduced number of ultrasound pulse emissions. Nevertheless, RF image spatial resolution, contrast and signal to noise ratio are affected by the limited bandwidth of the imaging transducer and the physical phenomenon related to US wave propagation. To overcome these limitations, several deconvolution-based image processing techniques have been proposed to enhance the ultrasound images. In this paper, we propose a novel framework, named compressive deconvolution, that reconstructs enhanced RF images from compressed measurements. Exploiting an unified formulation of the direct acquisition model, combining random projections and 2D convolution with a spatially invariant point spread function, the benefit of our approach is the joint data volume reduction and image quality improvement. The proposed optimization method, based on the Alternating Direction Method of Multipliers, is evaluated on both simulated and in vivo data.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00302",
        "title": "Pose Embeddings: A Deep Architecture for Learning to Match Human Poses",
        "authors": [
            "Greg Mori",
            "Caroline Pantofaru",
            "Nisarg Kothari",
            "Thomas Leung",
            "George Toderici",
            "Alexander Toshev",
            "Weilong Yang"
        ],
        "abstract": "We present a method for learning an embedding that places images of humans in similar poses nearby. This embedding can be used as a direct method of comparing images based on human pose, avoiding potential challenges of estimating body joint positions. Pose embedding learning is formulated under a triplet-based distance criterion. A deep architecture is used to allow learning of a representation capable of making distinctions between different poses. Experiments on human pose matching and retrieval from video data demonstrate the potential of the method.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00410",
        "title": "Convolutional Color Constancy",
        "authors": [
            "Jonathan T. Barron"
        ],
        "abstract": "Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00448",
        "title": "Cross Modal Distillation for Supervision Transfer",
        "authors": [
            "Saurabh Gupta",
            "Judy Hoffman",
            "Jitendra Malik"
        ],
        "abstract": "In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00908",
        "title": "LogDet Rank Minimization with Application to Subspace Clustering",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Jie Cheng",
            "Qiang Chen"
        ],
        "abstract": "Low-rank matrix is desired in many machine learning and computer vision problems. Most of the recent studies use the nuclear norm as a convex surrogate of the rank operator. However, all singular values are simply added together by the nuclear norm, and thus the rank may not be well approximated in practical problems. In this paper, we propose to use a log-determinant (LogDet) function as a smooth and closer, though non-convex, approximation to rank for obtaining a low-rank representation in subspace clustering. Augmented Lagrange multipliers strategy is applied to iteratively optimize the LogDet-based non-convex objective function on potentially large-scale data. By making use of the angular information of principal directions of the resultant low-rank representation, an affinity graph matrix is constructed for spectral clustering. Experimental results on motion segmentation and face clustering data demonstrate that the proposed method often outperforms state-of-the-art subspace clustering algorithms.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00913",
        "title": "Fine-grained Recognition Datasets for Biodiversity Analysis",
        "authors": [
            "Erik Rodner",
            "Marcel Simon",
            "Gunnar Brehm",
            "Stephanie Pietsch",
            "J. Wolfgang W\u00e4gele",
            "Joachim Denzler"
        ],
        "abstract": "In the following paper, we present and discuss challenging applications for fine-grained visual classification (FGVC): biodiversity and species analysis. We not only give details about two challenging new datasets suitable for computer vision research with up to 675 highly similar classes, but also present first results with localized features using convolutional neural networks (CNN). We conclude with a list of challenging new research directions in the area of visual classification for biodiversity research.\n    ",
        "submission_date": "2015-07-03T00:00:00",
        "last_modified_date": "2015-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01208",
        "title": "Parsimonious Labeling",
        "authors": [
            "Puneet K. Dokania",
            "M. Pawan Kumar"
        ],
        "abstract": "We propose a new family of discrete energy minimization problems, which we call parsimonious labeling. Specifically, our energy functional consists of unary potentials and high-order clique potentials. While the unary potentials are arbitrary, the clique potentials are proportional to the {\\em diversity} of set of the unique labels assigned to the clique. Intuitively, our energy functional encourages the labeling to be parsimonious, that is, use as few labels as possible. This in turn allows us to capture useful cues for important computer vision applications such as stereo correspondence and image denoising. Furthermore, we propose an efficient graph-cuts based algorithm for the parsimonious labeling problem that provides strong theoretical guarantees on the quality of the solution. Our algorithm consists of three steps. First, we approximate a given diversity using a mixture of a novel hierarchical $P^n$ Potts model. Second, we use a divide-and-conquer approach for each mixture component, where each subproblem is solved using an effficient $\\alpha$-expansion algorithm. This provides us with a small number of putative labelings, one for each mixture component. Third, we choose the best putative labeling in terms of the energy value. Using both sythetic and standard real datasets, we show that our algorithm significantly outperforms other graph-cuts based approaches.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01209",
        "title": "TV News Commercials Detection using Success based Locally Weighted Kernel Combination",
        "authors": [
            "Raghvendra Kannao",
            "Prithwijit Guha"
        ],
        "abstract": "Commercial detection in news broadcast videos involves judicious selection of meaningful audio-visual feature combinations and efficient classifiers. And, this problem becomes much simpler if these combinations can be learned from the data. To this end, we propose an Multiple Kernel Learning based method for boosting successful kernel functions while ignoring the irrelevant ones. We adopt a intermediate fusion approach where, a SVM is trained with a weighted linear combination of different kernel functions instead of single kernel function. Each kernel function is characterized by a feature set and kernel type. We identify the feature sub-space locations of the prediction success of a particular classifier trained only with particular kernel function. We propose to estimate a weighing function using support vector regression (with RBF kernel) for each kernel function which has high values (near 1.0) where the classifier learned on kernel function succeeded and lower values (nearly 0.0) otherwise. Second contribution of this work is TV News Commercials Dataset of 150 Hours of News videos. Classifier trained with our proposed scheme has outperformed the baseline methods on 6 of 8 benchmark dataset and our own TV commercials dataset.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01238",
        "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit",
        "authors": [
            "Chong You",
            "Daniel P. Robinson",
            "Rene Vidal"
        ],
        "abstract": "Subspace clustering methods based on $\\ell_1$, $\\ell_2$ or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, $\\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, $\\ell_2$ and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01251",
        "title": "Autoencoding the Retrieval Relevance of Medical Images",
        "authors": [
            "Zehra Camlica",
            "H.R. Tizhoosh",
            "Farzad Khalvati"
        ],
        "abstract": "Content-based image retrieval (CBIR) of medical images is a crucial task that can contribute to a more reliable diagnosis if applied to big data. Recent advances in feature extraction and classification have enormously improved CBIR results for digital images. However, considering the increasing accessibility of big data in medical imaging, we are still in need of reducing both memory requirements and computational expenses of image retrieval systems. This work proposes to exclude the features of image blocks that exhibit a low encoding error when learned by a $n/p/n$ autoencoder ($p\\!<\\!n$). We examine the histogram of autoendcoding errors of image blocks for each image class to facilitate the decision which image regions, or roughly what percentage of an image perhaps, shall be declared relevant for the retrieval task. This leads to reduction of feature dimensionality and speeds up the retrieval process. To validate the proposed scheme, we employ local binary patterns (LBP) and support vector machines (SVM) which are both well-established approaches in CBIR research community. As well, we use IRMA dataset with 14,410 x-ray images as test data. The results show that the dimensionality of annotated feature vectors can be reduced by up to 50% resulting in speedups greater than 27% at expense of less than 1% decrease in the accuracy of retrieval when validating the precision and recall of the top 20 hits.\n    ",
        "submission_date": "2015-07-05T00:00:00",
        "last_modified_date": "2015-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01330",
        "title": "Visual Data Deblocking using Structural Layer Priors",
        "authors": [
            "Xiaojie Guo"
        ],
        "abstract": "The blocking artifact frequently appears in compressed real-world images or video sequences, especially coded at low bit rates, which is visually annoying and likely hurts the performance of many computer vision algorithms. A compressed frame can be viewed as the superimposition of an intrinsic layer and an artifact one. Recovering the two layers from such frames seems to be a severely ill-posed problem since the number of unknowns to recover is twice as many as the given measurements. In this paper, we propose a simple and robust method to separate these two layers, which exploits structural layer priors including the gradient sparsity of the intrinsic layer, and the independence of the gradient fields of the two layers. A novel Augmented Lagrangian Multiplier based algorithm is designed to efficiently and effectively solve the recovery problem. Extensive experimental results demonstrate the superior performance of our method over the state of the arts, in terms of visual quality and simplicity.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01422",
        "title": "End-to-end Convolutional Network for Saliency Prediction",
        "authors": [
            "Junting Pan",
            "Xavier Gir\u00f3-i-Nieto"
        ],
        "abstract": "The prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles. This paper however addresses the problem with a completely data-driven approach by training a convolutional network. The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate. The convolutional network in this paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction with a superior performance in all considered metrics.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01442",
        "title": "Learning Better Encoding for Approximate Nearest Neighbor Search with Dictionary Annealing",
        "authors": [
            "Shicong Liu",
            "Hongtao Lu"
        ],
        "abstract": "We introduce a novel dictionary optimization method for high-dimensional vector quantization employed in approximate nearest neighbor (ANN) search. Vector quantization methods first seek a series of dictionaries, then approximate each vector by a sum of elements selected from these dictionaries. An optimal series of dictionaries should be mutually independent, and each dictionary should generate a balanced encoding for the target dataset. Existing methods did not explicitly consider this. To achieve these goals along with minimizing the quantization error (residue), we propose a novel dictionary optimization method called \\emph{Dictionary Annealing} that alternatively \"heats up\" a single dictionary by generating an intermediate dataset with residual vectors, \"cools down\" the dictionary by fitting the intermediate dataset, then extracts the new residual vectors for the next iteration. Better codes can be learned by DA for the ANN search tasks. DA is easily implemented on GPU to utilize the latest computing technology, and can easily extended to an online dictionary learning scheme. We show by experiments that our optimized dictionaries substantially reduce the overall quantization error. Jointly used with residual vector quantization, our optimized dictionaries lead to a better approximate nearest neighbor search performance compared to the state-of-the-art methods.\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01578",
        "title": "Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video",
        "authors": [
            "Subarna Tripathi",
            "Serge Belongie",
            "Truong Nguyen"
        ],
        "abstract": "We explore the efficiency of the CRF inference module beyond image level semantic segmentation. The key idea is to combine the best of two worlds of semantic co-labeling and exploiting more expressive models. Similar to [Alvarez14] our formulation enables us perform inference over ten thousand images within seconds. On the other hand, it can handle higher-order clique potentials similar to [vineet2014] in terms of region-level label consistency and context in terms of co-occurrences. We follow the mean-field updates for higher order potentials similar to [vineet2014] and extend the spatial smoothness and appearance kernels [DenseCRF13] to address video data inspired by [Alvarez14]; thus making the system amenable to perform video semantic segmentation most effectively.\n    ",
        "submission_date": "2015-07-01T00:00:00",
        "last_modified_date": "2015-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01581",
        "title": "Joint Calibration for Semantic Segmentation",
        "authors": [
            "Holger Caesar",
            "Jasper Uijlings",
            "Vittorio Ferrari"
        ],
        "abstract": "Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively).\n    ",
        "submission_date": "2015-07-06T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02049",
        "title": "DCTNet : A Simple Learning-free Approach for Face Recognition",
        "authors": [
            "Cong Jie Ng",
            "Andrew Beng Jin Teoh"
        ],
        "abstract": "PCANet was proposed as a lightweight deep learning network that mainly leverages Principal Component Analysis (PCA) to learn multistage filter banks followed by binarization and block-wise histograming. PCANet was shown worked surprisingly well in various image classification tasks. However, PCANet is data-dependence hence inflexible. In this paper, we proposed a data-independence network, dubbed DCTNet for face recognition in which we adopt Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is motivated by the fact that 2D DCT basis is indeed a good approximation for high ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is free from learning as 2D DCT bases can be computed in advance. Besides that, we also proposed an effective method to regulate the block-wise histogram feature vector of DCTNet for robustness. It is shown to provide surprising performance boost when the probe image is considerably different in appearance from the gallery image. We evaluate the performance of DCTNet extensively on a number of benchmark face databases and being able to achieve on par with or often better accuracy performance than PCANet.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02144",
        "title": "Spotlight the Negatives: A Generalized Discriminative Latent Model",
        "authors": [
            "Hossein Azizpour",
            "Mostafa Arefiyan",
            "Sobhan Naderi Parizi",
            "Stefan Carlsson"
        ],
        "abstract": "Discriminative latent variable models (LVM) are frequently applied to various visual recognition tasks. In these systems the latent (hidden) variables provide a formalism for modeling structured variation of visual features. Conventionally, latent variables are de- fined on the variation of the foreground (positive) class. In this work we augment LVMs to include negative latent variables corresponding to the background class. We formalize the scoring function of such a generalized LVM (GLVM). Then we discuss a framework for learning a model based on the GLVM scoring function. We theoretically showcase how some of the current visual recognition methods can benefit from this generalization. Finally, we experiment on a generalized form of Deformable Part Models with negative latent variables and show significant improvements on two different detection tasks.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02154",
        "title": "Double-Base Asymmetric AdaBoost",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "Based on the use of different exponential bases to define class-dependent error bounds, a new and highly efficient asymmetric boosting scheme, coined as AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical derivation procedure, unlike most of the other approaches in the literature, our algorithm preserves all the formal guarantees and properties of original (cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel derivation scheme enables an extremely efficient conditional search procedure, dramatically improving and simplifying the training phase of the algorithm. Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is able to save over 99% training time with regard to Cost-Sensitive AdaBoost, providing the same cost-sensitive results. This computational advantage of AdaBoostDB can make a difference in problems managing huge pools of weak classifiers in which boosting techniques are commonly used.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02159",
        "title": "Towards Good Practices for Very Deep Two-Stream ConvNets",
        "authors": [
            "Limin Wang",
            "Yuanjun Xiong",
            "Zhe Wang",
            "Yu Qiao"
        ],
        "abstract": "Deep convolutional networks have achieved great success for object recognition in still images. However, for action recognition in videos, the improvement of deep convolutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream ConvNets) are relatively shallow compared with those very deep models in image domain (e.g. VGGNet, GoogLeNet), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the ImageNet dataset, and thus it will be easy to over-fit on the training dataset.\n",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02177",
        "title": "Iris Recognition Using Scattering Transform and Textural Features",
        "authors": [
            "Shervin Minaee",
            "AmirAli Abdolrashidi",
            "Yao Wang"
        ],
        "abstract": "Iris recognition has drawn a lot of attention since the mid-twentieth century. Among all biometric features, iris is known to possess a rich set of features. Different features have been used to perform iris recognition in the past. In this paper, two powerful sets of features are introduced to be used for iris recognition: scattering transform-based features and textural features. PCA is also applied on the extracted features to reduce the dimensionality of the feature vector while preserving most of the information of its initial value. Minimum distance classifier is used to perform template matching for each new test sample. The proposed scheme is tested on a well-known iris database, and showed promising results with the best accuracy rate of 99.2%.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02313",
        "title": "Feature Representation in Convolutional Neural Networks",
        "authors": [
            "Ben Athiwaratkun",
            "Keegan Kang"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) are powerful models that achieve impressive results for image classification. In addition, pre-trained CNNs are also useful for other computer vision tasks as generic feature extractors. This paper aims to gain insight into the feature aspect of CNN and demonstrate other uses of CNN features. Our results show that CNN feature maps can be used with Random Forests and SVM to yield classification results that outperforms the original CNN. A CNN that is less than optimal (e.g. not fully trained or overfitting) can also extract features for Random Forest/SVM that yield competitive classification accuracy. In contrast to the literature which uses the top-layer activations as feature representation of images for other tasks, using lower-layer features can yield better results for classification.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02346",
        "title": "Neural Network Classifiers for Natural Food Products",
        "authors": [
            "Jaderick P. Pabico",
            "Alona V. De Grano",
            "Alan L. Zarsuela"
        ],
        "abstract": "Two cheap, off-the-shelf machine vision systems (MVS), each using an artificial neural network (ANN) as classifier, were developed, improved and evaluated to automate the classification of tomato ripeness and acceptability of eggs, respectively. Six thousand color images of human-graded tomatoes and 750 images of human-graded eggs were used to train, test, and validate several multi-layered ANNs. The ANNs output the corresponding grade of the produce by accepting as inputs the spectral patterns of the background-less image. In both MVS, the ANN with the highest validation rate was automatically chosen by a heuristic and its performance compared to that of the human graders'. Using the validation set, the MVS correctly graded 97.00\\% and 86.00\\% of the tomato and egg data, respectively. The human grader's, however, were measured to perform at a daily average of 92.65\\% and 72.67\\% for tomato and egg grading, respectively. This results show that an ANN-based MVS is a potential alternative to manual grading.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02379",
        "title": "Understanding Intra-Class Knowledge Inside CNN",
        "authors": [
            "Donglai Wei",
            "Bolei Zhou",
            "Antonio Torrabla",
            "William Freeman"
        ],
        "abstract": "Convolutional Neural Network (CNN) has been successful in image recognition tasks, and recent works shed lights on how CNN separates different classes with the learned inter-class knowledge through visualization. In this work, we instead visualize the intra-class knowledge inside CNN to better understand how an object class is represented in the fully-connected layers.\n",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02380",
        "title": "Learning Structured Ordinal Measures for Video based Face Recognition",
        "authors": [
            "Ran He",
            "Tieniu Tan",
            "Larry Davis",
            "Zhenan Sun"
        ],
        "abstract": "This paper presents a structured ordinal measure method for video-based face recognition that simultaneously learns ordinal filters and structured ordinal features. The problem is posed as a non-convex integer program problem that includes two parts. The first part learns stable ordinal filters to project video data into a large-margin ordinal space. The second seeks self-correcting and discrete codes by balancing the projected data and a rank-one ordinal matrix in a structured low-rank way. Unsupervised and supervised structures are considered for the ordinal matrix. In addition, as a complement to hierarchical structures, deep feature representations are integrated into our method to enhance coding stability. An alternating minimization method is employed to handle the discrete and low-rank constraints, yielding high-quality codes that capture prior structures well. Experimental results on three commonly used face video databases show that our method with a simple voting classifier can achieve state-of-the-art recognition rates using fewer features and samples.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02385",
        "title": "Towards Effective Codebookless Model for Image Classification",
        "authors": [
            "Qilong Wang",
            "Peihua Li",
            "Lei Zhang",
            "Wangmeng Zuo"
        ],
        "abstract": "The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02438",
        "title": "Generalized Video Deblurring for Dynamic Scenes",
        "authors": [
            "Tae Hyun Kim",
            "Kyoung Mu Lee"
        ],
        "abstract": "Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02558",
        "title": "Multi-Type Activity Recognition in Robot-Centric Scenarios",
        "authors": [
            "Ilaria Gori",
            "J. K. Aggarwal",
            "Larry Matthies",
            "Michael S. Ryoo"
        ],
        "abstract": "Activity recognition is very useful in scenarios where robots interact with, monitor or assist humans. In the past years many types of activities -- single actions, two persons interactions or ego-centric activities, to name a few -- have been analyzed. Whereas traditional methods treat such types of activities separately, an autonomous robot should be able to detect and recognize multiple types of activities to effectively fulfill its tasks. We propose a method that is intrinsically able to detect and recognize activities of different types that happen in sequence or concurrently. We present a new unified descriptor, called Relation History Image (RHI), which can be extracted from all the activity types we are interested in. We then formulate an optimization procedure to detect and recognize activities of different types. We apply our approach to a new dataset recorded from a robot-centric perspective and systematically evaluate its quality compared to multiple baselines. Finally, we show the efficacy of the RHI descriptor on publicly available datasets performing extensive comparisons.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02620",
        "title": "Deep filter banks for texture recognition, description, and segmentation",
        "authors": [
            "Mircea Cimpoi",
            "Subhransu Maji",
            "Iasonas Kokkinos",
            "Andrea Vedaldi"
        ],
        "abstract": "Visual textures have played a key role in image understanding because they convey important semantics of images, and because texture representations that pool local image descriptors in an orderless manner have had a tremendous impact in diverse applications. In this paper we make several contributions to texture understanding. First, instead of focusing on texture instance and material category recognition, we propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking. Second, we look at the problem of recognizing materials and texture attributes in realistic imaging conditions, including when textures appear in clutter, developing corresponding benchmarks on top of the recently proposed OpenSurfaces dataset. Third, we revisit classic texture representations, including bag-of-visual-words and the Fisher vectors, in the context of deep learning and show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks. We obtain in this manner state-of-the-art performance in numerous datasets well beyond textures, an efficient method to apply deep features to image regions, as well as benefit in transferring features from one domain to another.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02703",
        "title": "Robot In a Room: Toward Perfect Object Recognition in Closed Environments",
        "authors": [
            "Shuran Song",
            "Linguang Zhang",
            "Jianxiong Xiao"
        ],
        "abstract": "While general object recognition is still far from being solved, this paper proposes a way for a robot to recognize every object at an almost human-level accuracy. Our key observation is that many robots will stay in a relatively closed environment (e.g. a house or an office). By constraining a robot to stay in a limited territory, we can ensure that the robot has seen most objects before and the speed of introducing a new object is slow. Furthermore, we can build a 3D map of the environment to reliably subtract the background to make recognition easier. We propose extremely robust algorithms to obtain a 3D map and enable humans to collectively annotate objects. During testing time, our algorithm can recognize all objects very reliably, and query humans from crowd sourcing platform if confidence is low or new objects are identified. This paper explains design decisions in building such a system, and constructs a benchmark for extensive evaluation. Experiments suggest that making robot vision appear to be working from an end user's perspective is a reachable goal today, as long as the robot stays in a closed environment. By formulating this task, we hope to lay the foundation of a new direction in vision for robotics. Code and data will be available upon acceptance.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02772",
        "title": "Riemannian Dictionary Learning and Sparse Coding for Positive Definite Matrices",
        "authors": [
            "Anoop Cherian",
            "Suvrit Sra"
        ],
        "abstract": "Data encoded as symmetric positive definite (SPD) matrices frequently arise in many areas of computer vision and machine learning. While these matrices form an open subset of the Euclidean space of symmetric matrices, viewing them through the lens of non-Euclidean Riemannian geometry often turns out to be better suited in capturing several desirable data properties. However, formulating classical machine learning algorithms within such a geometry is often non-trivial and computationally expensive. Inspired by the great success of dictionary learning and sparse coding for vector-valued data, our goal in this paper is to represent data in the form of SPD matrices as sparse conic combinations of SPD atoms from a learned dictionary via a Riemannian geometric approach. To that end, we formulate a novel Riemannian optimization objective for dictionary learning and sparse coding in which the representation loss is characterized via the affine invariant Riemannian metric. We also present a computationally simple algorithm for optimizing our model. Experiments on several computer vision datasets demonstrate superior classification and retrieval performance using our approach when compared to sparse coding via alternative non-Riemannian formulations.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02779",
        "title": "Robust Performance-driven 3D Face Tracking in Long Range Depth Scenes",
        "authors": [
            "Hai X. Pham",
            "Chongyu Chen",
            "Luc N. Dao",
            "Vladimir Pavlovic",
            "Jianfei Cai",
            "Tat-jen Cham"
        ],
        "abstract": "We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker, capable of handling a wide range of target scene depths, beyond those that can be afforded by traditional depth or RGB face trackers. Lastly, since the blendshape is not able to accurately recover the real facial shape, we use the tracked 3D face model as a prior in a novel filtering process to further refine the depth map for use in other tasks, such as 3D reconstruction.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02879",
        "title": "Deep Perceptual Mapping for Thermal to Visible Face Recognition",
        "authors": [
            "M. Saquib Sarfraz",
            "Rainer Stiefelhagen"
        ],
        "abstract": "Cross modal face matching between the thermal and visible spectrum is a much de- sired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-to-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship be- tween the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from visible to thermal spectrum while preserving the identity in- formation. We show substantive performance improvement on a difficult thermal-visible face dataset. The presented approach improves the state-of-the-art by more than 10% in terms of Rank-1 identification and bridge the drop in performance due to the modality gap by more than 40%.\n    ",
        "submission_date": "2015-07-10T00:00:00",
        "last_modified_date": "2015-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03060",
        "title": "LooseCut: Interactive Image Segmentation with Loosely Bounded Boxes",
        "authors": [
            "Hongkai Yu",
            "Youjie Zhou",
            "Hui Qian",
            "Min Xian",
            "Yuewei Lin",
            "Dazhou Guo",
            "Kang Zheng",
            "Kareem Abdelfatah",
            "Song Wang"
        ],
        "abstract": "One popular approach to interactively segment the foreground object of interest from an image is to annotate a bounding box that covers the foreground object. Then, a binary labeling is performed to achieve a refined segmentation. One major issue of the existing algorithms for such interactive image segmentation is their preference of an input bounding box that tightly encloses the foreground object. This increases the annotation burden, and prevents these algorithms from utilizing automatically detected bounding boxes. In this paper, we develop a new LooseCut algorithm that can handle cases where the input bounding box only loosely covers the foreground object. We propose a new Markov Random Fields (MRF) model for segmentation with loosely bounded boxes, including a global similarity constraint to better distinguish the foreground and background, and an additional energy term to encourage consistent labeling of similar-appearance pixels. This MRF model is then solved by an iterated max-flow algorithm. In the experiments, we evaluate LooseCut in three publicly-available image datasets, and compare its performance against several state-of-the-art interactive image segmentation algorithms. We also show that LooseCut can be used for enhancing the performance of unsupervised video segmentation and image saliency detection.\n    ",
        "submission_date": "2015-07-11T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03148",
        "title": "Face Alignment Assisted by Head Pose Estimation",
        "authors": [
            "Heng Yang",
            "Wenxuan Mou",
            "Yichi Zhang",
            "Ioannis Patras",
            "Hatice Gunes",
            "Peter Robinson"
        ],
        "abstract": "In this paper we propose a supervised initialization scheme for cascaded face alignment based on explicit head pose estimation. We first investigate the failure cases of most state of the art face alignment approaches and observe that these failures often share one common global property, i.e. the head pose variation is usually large. Inspired by this, we propose a deep convolutional network model for reliable and accurate head pose estimation. Instead of using a mean face shape, or randomly selected shapes for cascaded face alignment initialisation, we propose two schemes for generating initialisation: the first one relies on projecting a mean 3D face shape (represented by 3D facial landmarks) onto 2D image under the estimated head pose; the second one searches nearest neighbour shapes from the training set according to head pose distance. By doing so, the initialisation gets closer to the actual shape, which enhances the possibility of convergence and in turn improves the face alignment performance. We demonstrate the proposed method on the benchmark 300W dataset and show very competitive performance in both head pose estimation and face alignment.\n    ",
        "submission_date": "2015-07-11T00:00:00",
        "last_modified_date": "2015-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03196",
        "title": "DeepFont: Identify Your Font from An Image",
        "authors": [
            "Zhangyang Wang",
            "Jianchao Yang",
            "Hailin Jin",
            "Eli Shechtman",
            "Aseem Agarwala",
            "Jonathan Brandt",
            "Thomas S. Huang"
        ],
        "abstract": "As font is one of the core design concepts, automatic font identification and similar font suggestion from an image or photo has been on the wish list of many designers. We study the Visual Font Recognition (VFR) problem, and advance the state-of-the-art remarkably by developing the DeepFont system. First of all, we build up the first available large-scale VFR dataset, named AdobeVFR, consisting of both labeled synthetic data and partially labeled real-world data. Next, to combat the domain mismatch between available training and testing data, we introduce a Convolutional Neural Network (CNN) decomposition approach, using a domain adaptation technique based on a Stacked Convolutional Auto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. Moreover, we study a novel learning-based model compression approach, in order to reduce the DeepFont model size without sacrificing its performance. The DeepFont system achieves an accuracy of higher than 80% (top-5) on our collected dataset, and also produces a good font similarity measure for font selection and suggestion. We also achieve around 6 times compression of the model without any visible loss of recognition accuracy.\n    ",
        "submission_date": "2015-07-12T00:00:00",
        "last_modified_date": "2015-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03360",
        "title": "Sparsity assisted solution to the twin image problem in phase retrieval",
        "authors": [
            "Charu Gaur",
            "Baranidharan Mohan",
            "Kedar Khare"
        ],
        "abstract": "The iterative phase retrieval problem for complex-valued objects from Fourier transform magnitude data is known to suffer from the twin image problem. In particular, when the object support is centro-symmetric, the iterative solution often stagnates such that the resultant complex image contains the features of both the desired solution and its inverted and complex-conjugated replica. The conventional approach to address the twin image problem is to modify the object support during initial iterations which can possibly lead to elimination of one of the twin images. However, at present there seems to be no deterministic procedure to make sure that the twin image will always be very weak or absent. In this work we make an important observation that the ideal solution without the twin image is typically more sparse (in some suitable transform domain) as compared to the stagnated solution containing the twin image. We further show that introducing a sparsity enhancing step in the iterative algorithm can address the twin image problem without the need to change the object support throughout the iterative process even when the object support is centro-symmetric. In a simulation study, we use binary and gray-scale pure phase objects and illustrate the effectiveness of the sparsity assisted phase recovery in the context of the twin image problem. The results have important implications for a wide range of topics in Physics where the phase retrieval problem plays a central role.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2015-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03409",
        "title": "Unconstrained Facial Landmark Localization with Backbone-Branches Fully-Convolutional Networks",
        "authors": [
            "Zhujin Liang",
            "Shengyong Ding",
            "Liang Lin"
        ],
        "abstract": "This paper investigates how to rapidly and accurately localize facial landmarks in unconstrained, cluttered environments rather than in the well segmented face images. We present a novel Backbone-Branches Fully-Convolutional Neural Network (BB-FCN), which produces facial landmark response maps directly from raw images without relying on pre-process or sliding window approaches. BB-FCN contains one backbone and a number of network branches with each corresponding to one landmark type, and it operates in a progressive manner. Specifically, the backbone roughly detects the locations of facial landmarks by taking the whole image as input, and the branches further refine the localizations based on a local observation from the backbone's intermediate feature map. Moreover, our backbone-branches architecture does not contain full-connection layers for location regression, leading to efficient learning and inference. Our extensive experiments show that our model achieves superior performances over other state-of-the-arts under both the constrained (i.e. with face regions) and the \"in the wild\" scenarios.\n    ",
        "submission_date": "2015-07-13T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03698",
        "title": "Lifting GIS Maps into Strong Geometric Context for Scene Understanding",
        "authors": [
            "Ra\u00fal D\u00edaz",
            "Minhaeng Lee",
            "Jochen Schubert",
            "Charless C. Fowlkes"
        ],
        "abstract": "  Contextual information can have a substantial impact on the performance of visual tasks such as semantic segmentation, object detection, and geometric estimation. Data stored in Geographic Information Systems (GIS) offers a rich source of contextual information that has been largely untapped by computer vision. We propose to leverage such information for scene understanding by combining GIS resources with large sets of unorganized photographs using Structure from Motion (SfM) techniques. We present a pipeline to quickly generate strong 3D geometric priors from 2D GIS data using SfM models aligned with minimal user input. Given an image resectioned against this model, we generate robust predictions of depth, surface normals, and semantic labels. We show that the precision of the predicted geometry is substantially more accurate other single-image depth estimation methods. We then demonstrate the utility of these contextual constraints for re-scoring pedestrian detections, and use these GIS contextual features alongside object detection score maps to improve a CRF-based semantic segmentation framework, boosting accuracy over baseline models.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03751",
        "title": "Closed Curves and Elementary Visual Object Identification",
        "authors": [
            "Manfred Harringer"
        ],
        "abstract": "For two closed curves on a plane (discrete version) and local criteria for similarity of points on the curves one gets a potential, which describes the similarity between curve points. This is the base for a global similarity measure of closed curves (Fr\u00e9chet distance). I use borderlines of handwritten digits to demonstrate an area of application. I imagine, measuring the similarity of closed curves is an essential and elementary task performed by a visual system. This approach to similarity measures may be used by visual systems.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.03811",
        "title": "Ensemble of Hankel Matrices for Face Emotion Recognition",
        "authors": [
            "Liliana Lo Presti",
            "Marco La Cascia"
        ],
        "abstract": "In this paper, a face emotion is considered as the result of the composition of multiple concurrent signals, each corresponding to the movements of a specific facial muscle. These concurrent signals are represented by means of a set of multi-scale appearance features that might be correlated with one or more concurrent signals. The extraction of these appearance features from a sequence of face images yields to a set of time series. This paper proposes to use the dynamics regulating each appearance feature time series to recognize among different face emotions. To this purpose, an ensemble of Hankel matrices corresponding to the extracted time series is used for emotion classification within a framework that combines nearest neighbor and a majority vote schema. Experimental results on a public available dataset shows that the adopted representation is promising and yields state-of-the-art accuracy in emotion classification.\n    ",
        "submission_date": "2015-07-14T00:00:00",
        "last_modified_date": "2015-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04060",
        "title": "Unsupervised Decision Forest for Data Clustering and Density Estimation",
        "authors": [
            "Hayder Albehadili",
            "Naz Islam"
        ],
        "abstract": "An algorithm to improve performance parameter for unsupervised decision forest clustering and density estimation is presented. Specifically, a dual assignment parameter is introduced as a density estimator by combining Random Forest and Gaussian Mixture Model. The Random Forest method has been specifically applied to construct a robust affinity graph that provides information on the underlying structure of data objects used in clustering. The proposed algorithm differs from the commonly used spectral clustering methods where the computed distance metric is used to find similarities between data points. Experiments were conducted using five datasets. A comparison with six other state-of-the-art methods shows that our model is superior to existing approaches. Efficiency of the proposed model is in capturing the underlying structure for a given set of data points. The proposed method is also robust, and can discriminate between the complex features of data points among different clusters.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2015-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04125",
        "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part I: Theoretical Perspective",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "Boosting algorithms have been widely used to tackle a plethora of problems. In the last few years, a lot of approaches have been proposed to provide standard AdaBoost with cost-sensitive capabilities, each with a different focus. However, for the researcher, these algorithms shape a tangled set with diffuse differences and properties, lacking a unifying analysis to jointly compare, classify, evaluate and discuss those approaches on a common basis. In this series of two papers we aim to revisit the various proposals, both from theoretical (Part I) and practical (Part II) perspectives, in order to analyze their specific properties and behavior, with the final goal of identifying the algorithm providing the best and soundest results.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04126",
        "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part II: Empirical Analysis",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "A lot of approaches, each following a different strategy, have been proposed in the literature to provide AdaBoost with cost-sensitive properties. In the first part of this series of two papers, we have presented these algorithms in a homogeneous notational framework, proposed a clustering scheme for them and performed a thorough theoretical analysis of those approaches with a fully theoretical foundation. The present paper, in order to complete our analysis, is focused on the empirical study of all the algorithms previously presented over a wide range of heterogeneous classification problems. The results of our experiments, confirming the theoretical conclusions, seem to reveal that the simplest approach, just based on cost-sensitive weight initialization, is the one showing the best and soundest results, despite having been recurrently overlooked in the literature.\n    ",
        "submission_date": "2015-07-15T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04437",
        "title": "A Deep Hashing Learning Network",
        "authors": [
            "Guoqiang Zhong",
            "Pan Yang",
            "Sijiang Wang",
            "Junyu Dong"
        ],
        "abstract": "Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods, an image is first encoded as a vector of hand-crafted visual feature, followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input, the feature may not preserve the semantic similarities of images pairs. Meanwhile, the hashing function learning process is independent with the feature representation, so the feature may not be optimal for the hashing projection. In this paper, we propose a supervised hashing method based on a well designed deep convolutional neural network, which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04512",
        "title": "Diagnosing State-Of-The-Art Object Proposal Methods",
        "authors": [
            "Hongyuan Zhu",
            "Shijian Lu",
            "Jianfei Cai",
            "Quangqing Lee"
        ],
        "abstract": "Object proposal has become a popular paradigm to replace exhaustive sliding window search in current top-performing methods in PASCAL VOC and ImageNet. Recently, Hosang et al. conduct the first unified study of existing methods' in terms of various image-level degradations. On the other hand, the vital question \"what object-level characteristics really affect existing methods' performance?\" is not yet answered. Inspired by Hoiem et al.'s work in categorical object detection, this paper conducts the first meta-analysis of various object-level characteristics' impact on state-of-the-art object proposal methods. Specifically, we examine the effects of object size, aspect ratio, iconic view, color contrast, shape regularity and texture. We also analyse existing methods' localization accuracy and latency for various PASCAL VOC object classes. Our study reveals the limitations of existing methods in terms of non-iconic view, small object size, low color contrast, shape regularity etc. Based on our observations, lessons are also learned and shared with respect to the selection of existing object proposal technologies as well as the design of the future ones.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04576",
        "title": "Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos",
        "authors": [
            "Maedeh Aghaei",
            "Mariella Dimiccoli",
            "Petia Radeva"
        ],
        "abstract": "Wearable cameras offer a hands-free way to record egocentric images of daily experiences, where social events are of special interest. The first step towards detection of social events is to track the appearance of multiple persons involved in it. In this paper, we propose a novel method to find correspondences of multiple faces in low temporal resolution egocentric videos acquired through a wearable camera. This kind of photo-stream imposes additional challenges to the multi-tracking problem with respect to conventional videos. Due to the free motion of the camera and to its low temporal resolution, abrupt changes in the field of view, in illumination condition and in the target location are highly frequent. To overcome such difficulties, we propose a multi-face tracking method that generates a set of tracklets through finding correspondences along the whole sequence for each detected face and takes advantage of the tracklets redundancy to deal with unreliable ones. Similar tracklets are grouped into the so called extended bag-of-tracklets (eBoT), which is aimed to correspond to a specific person. Finally, a prototype tracklet is extracted for each eBoT, where the occurred occlusions are estimated by relying on a new measure of confidence. We validated our approach over an extensive dataset of egocentric photo-streams and compared it to state of the art methods, demonstrating its effectiveness and robustness.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2016-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04760",
        "title": "Driver Gaze Region Estimation Without Using Eye Movement",
        "authors": [
            "Lex Fridman",
            "Philipp Langhans",
            "Joonbum Lee",
            "Bryan Reimer"
        ],
        "abstract": "Automated estimation of the allocation of a driver's visual attention may be a critical component of future Advanced Driver Assistance Systems. In theory, vision-based tracking of the eye can provide a good estimate of gaze location. In practice, eye tracking from video is challenging because of sunglasses, eyeglass reflections, lighting conditions, occlusions, motion blur, and other factors. Estimation of head pose, on the other hand, is robust to many of these effects, but cannot provide as fine-grained of a resolution in localizing the gaze. However, for the purpose of keeping the driver safe, it is sufficient to partition gaze into regions. In this effort, we propose a system that extracts facial features and classifies their spatial configuration into six regions in real-time. Our proposed method achieves an average accuracy of 91.4% at an average decision rate of 11 Hz on a dataset of 50 drivers from an on-road study.\n    ",
        "submission_date": "2015-07-16T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04816",
        "title": "RBIR Based on Signature Graph",
        "authors": [
            "Thanh The Van",
            "Thanh Manh Le"
        ],
        "abstract": "This paper approaches the image retrieval system on the base of visual features local region RBIR (region-based image retrieval). First of all, the paper presents a method for extracting the interest points based on Harris-Laplace to create the feature region of the image. Next, in order to reduce the storage space and speed up query image, the paper builds the binary signature structure to describe the visual content of image. Based on the image's binary signature, the paper builds the SG (signature graph) to classify and store image's binary signatures. Since then, the paper builds the image retrieval algorithm on SG through the similar measure EMD (earth mover's distance) between the image's binary signatures. Last but not least, the paper gives an image retrieval model RBIR, experiments and assesses the image retrieval method on Corel image database over 10,000 images.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04831",
        "title": "Deep Multimodal Speaker Naming",
        "authors": [
            "Yongtao Hu",
            "Jimmy Ren",
            "Jingwen Dai",
            "Chang Yuan",
            "Li Xu",
            "Wenping Wang"
        ],
        "abstract": "Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04835",
        "title": "Multiscale Adaptive Representation of Signals: I. The Basic Framework",
        "authors": [
            "Cheng Tai",
            "Weinan E"
        ],
        "abstract": "We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04844",
        "title": "Learning Robust Deep Face Representation",
        "authors": [
            "Xiang Wu"
        ],
        "abstract": "With the development of convolution neural network, more and more researchers focus their attention on the advantage of CNN for face recognition task. In this paper, we propose a deep convolution network for learning a robust face representation. The deep convolution net is constructed by 4 convolution layers, 4 max pooling layers and 2 fully connected layers, which totally contains about 4M parameters. The Max-Feature-Map activation function is used instead of ReLU because the ReLU might lead to the loss of information due to the sparsity while the Max-Feature-Map can get the compact and discriminative feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for single net.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04908",
        "title": "Analysis of the South Slavic Scripts by Run-Length Features of the Image Texture",
        "authors": [
            "Darko Brodic",
            "Zoran N. Milivojevic",
            "Alessia Amelio"
        ],
        "abstract": "The paper proposes an algorithm for the script recognition based on the texture characteristics. The image texture is achieved by coding each letter with the equivalent script type (number code) according to its position in the text line. Each code is transformed into equivalent gray level pixel creating an 1-D image. Then, the image texture is subjected to the run-length analysis. This analysis extracts the run-length features, which are classified to make a distinction between the scripts under consideration. In the experiment, a custom oriented database is subject to the proposed algorithm. The database consists of some text documents written in Cyrillic, Latin and Glagolitic scripts. Furthermore, it is divided into training and test parts. The results of the experiment show that 3 out of 5 run-length features can be used for effective differentiation between the analyzed South Slavic scripts.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05033",
        "title": "Classification of Complex Wishart Matrices with a Diffusion-Reaction System guided by Stochastic Distances",
        "authors": [
            "Luis Gomez",
            "Luis Alvarez",
            "Luis Mazorra",
            "Alejandro C. Frery"
        ],
        "abstract": "We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar) imagery classification based on stochastic distances in the space of random matrices obeying complex Wishart distributions. Given a collection of prototypes $\\{Z_m\\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classify any random matrix $X$ using two criteria in an iterative setup. Firstly, we associate $X$ to the class which minimizes the weighted stochastic distance $w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize the class discrimination power. Secondly, we improve the result by embedding the classification problem into a diffusion-reaction partial differential system where the diffusion term smooths the patches within the image, and the reaction term tends to move the pixel values towards the closest class prototype. In particular, the method inherits the benefits of speckle reduction by diffusion-like methods. Results on synthetic and real PolSAR data show the performance of the method.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05053",
        "title": "Massively Deep Artificial Neural Networks for Handwritten Digit Recognition",
        "authors": [
            "Keiron O'Shea"
        ],
        "abstract": "Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on the famous MNIST database of handwritten digits. All that was required to achieve this result was a high number of hidden layers consisting of many neurons, and a graphics card to greatly speed up the rate of learning.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05243",
        "title": "Hand Gesture Recognition Library",
        "authors": [
            "Jonathan Fidelis Paul",
            "Dibyabiva Seth",
            "Cijo Paul",
            "Jayati Ghosh Dastidar"
        ],
        "abstract": "In this paper we have presented a hand gesture recognition library. Various functions include detecting cluster count, cluster orientation, finger pointing direction, etc. To use these functions first the input image needs to be processed into a logical array for which a function has been developed. The library has been developed keeping flexibility in mind and thus provides application developers a wide range of options to develop custom gestures.\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05244",
        "title": "Handwriting Recognition",
        "authors": [
            "Jayati Ghosh Dastidar",
            "Surabhi Sarkar",
            "Rick Punyadyuti Sinha",
            "Kasturi Basu"
        ],
        "abstract": "This paper describes the method to recognize offline handwritten characters. A robust algorithm for handwriting segmentation is described here with the help of which individual characters can be segmented from a selected word from a paragraph of handwritten text image which is given as input.\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05348",
        "title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection",
        "authors": [
            "Zhaowei Cai",
            "Mohammad Saberian",
            "Nuno Vasconcelos"
        ],
        "abstract": "The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.\n    ",
        "submission_date": "2015-07-19T00:00:00",
        "last_modified_date": "2015-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05409",
        "title": "A Parameter-free Affinity Based Clustering",
        "authors": [
            "Bhaskar Mukhoty",
            "Ruchir Gupta",
            "Y. N. Singh"
        ],
        "abstract": "Several methods have been proposed to estimate the number of clusters in a dataset; the basic ideal behind all of them has been to study an index that measures inter-cluster separation and intra-cluster cohesion over a range of cluster numbers and report the number which gives an optimum value of the index. In this paper we propose a simple, parameter free approach that is like human cognition to form clusters, where closely lying points are easily identified to form a cluster and total number of clusters are revealed. To identify closely lying points, affinity of two points is defined as a function of distance and a threshold affinity is identified, above which two points in a dataset are likely to be in the same cluster. Well separated clusters are identified even in the presence of outliers, whereas for not so well separated dataset, final number of clusters are estimated and the detected clusters are merged to produce the final clusters. Experiments performed with several large dimensional synthetic and real datasets show good results with robustness to noise and density variation within dataset.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2016-01-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05489",
        "title": "Efficient moving point handling for incremental 3D manifold reconstruction",
        "authors": [
            "Andrea Romanoni",
            "Matteo Matteucci"
        ],
        "abstract": "As incremental Structure from Motion algorithms become effective, a good sparse point cloud representing the map of the scene becomes available frame-by-frame. From the 3D Delaunay triangulation of these points, state-of-the-art algorithms build a manifold rough model of the scene. These algorithms integrate incrementally new points to the 3D reconstruction only if their position estimate does not change. Indeed, whenever a point moves in a 3D Delaunay triangulation, for instance because its estimation gets refined, a set of tetrahedra have to be removed and replaced with new ones to maintain the Delaunay property; the management of the manifold reconstruction becomes thus complex and it entails a potentially big overhead. In this paper we investigate different approaches and we propose an efficient policy to deal with moving points in the manifold estimation process. We tested our approach with four sequences of the KITTI dataset and we show the effectiveness of our proposal in comparison with state-of-the-art approaches.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05532",
        "title": "Clustering Tree-structured Data on Manifold",
        "authors": [
            "Na Lu",
            "Hongyu Miao"
        ],
        "abstract": "Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of Euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Fr\u00e9chet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05578",
        "title": "Subspace Alignment Based Domain Adaptation for RCNN Detector",
        "authors": [
            "Anant Raj",
            "Vinay P. Namboodiri",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper, we propose subspace alignment based domain adaptation of the state of the art RCNN based object detector. The aim is to be able to achieve high quality object detection in novel, real world target scenarios without requiring labels from the target domain. While, unsupervised domain adaptation has been studied in the case of object classification, for object detection it has been relatively unexplored. In subspace based domain adaptation for objects, we need access to source and target subspaces for the bounding box features. The absence of supervision (labels and bounding boxes are absent) makes the task challenging. In this paper, we show that we can still adapt sub- spaces that are localized to the object by obtaining detections from the RCNN detector trained on source and applied on target. Then we form localized subspaces from the detections and show that subspace alignment based adaptation between these subspaces yields improved object detection. This evaluation is done by considering challenging real world datasets of PASCAL VOC as source and validation set of Microsoft COCO dataset as target for various categories.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05670",
        "title": "Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries",
        "authors": [
            "Yuke Zhu",
            "Ce Zhang",
            "Christopher R\u00e9",
            "Li Fei-Fei"
        ],
        "abstract": "The complexity of the visual world creates significant challenges for comprehensive visual understanding. In spite of recent successes in visual recognition, today's vision systems would still struggle to deal with visual queries that require a deeper reasoning. We propose a knowledge base (KB) framework to handle an assortment of visual queries, without the need to train new classifiers for new tasks. Building such a large-scale multimodal KB presents a major challenge of scalability. We cast a large-scale MRF into a KB representation, incorporating visual, textual and structured data, as well as their diverse relations. We introduce a scalable knowledge base construction system that is capable of building a KB with half billion variables and millions of parameters in a few hours. Our system achieves competitive results compared to purpose-built models on standard recognition and retrieval tasks, while exhibiting greater flexibility in answering richer visual queries.\n    ",
        "submission_date": "2015-07-20T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05699",
        "title": "Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians",
        "authors": [
            "Peiyun Hu",
            "Deva Ramanan"
        ],
        "abstract": "Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a unidirectional bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores bidirectional architectures that also reason with top-down feedback: neural units are influenced by both lower and higher-level units.\n",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2016-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05717",
        "title": "An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition",
        "authors": [
            "Baoguang Shi",
            "Xiang Bai",
            "Cong Yao"
        ],
        "abstract": "Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05726",
        "title": "Rule Of Thumb: Deep derotation for improved fingertip detection",
        "authors": [
            "Aaron Wetzler",
            "Ron Slossberg",
            "Ron Kimmel"
        ],
        "abstract": "We investigate a novel global orientation regression approach for articulated objects using a deep convolutional neural network. This is integrated with an in-plane image derotation scheme, DeROT, to tackle the problem of per-frame fingertip detection in depth images. The method reduces the complexity of learning in the space of articulated poses which is demonstrated by using two distinct state-of-the-art learning based hand pose estimation methods applied to fingertip detection. Significant classification improvements are shown over the baseline implementation. Our framework involves no tracking, kinematic constraints or explicit prior model of the articulated object in hand. To support our approach we also describe a new pipeline for high accuracy magnetic annotation and labeling of objects imaged by a depth camera.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05737",
        "title": "Online Metric-Weighted Linear Representations for Robust Visual Tracking",
        "authors": [
            "Xi Li",
            "Chunhua Shen",
            "Anthony Dick",
            "Zhongfei Zhang",
            "Yueting Zhuang"
        ],
        "abstract": "In this paper, we propose a visual tracker based on a metric-weighted linear representation of appearance. In order to capture the interdependence of different feature dimensions, we develop two online distance metric learning methods using proximity comparison information and structured output learning. The learned metric is then incorporated into a linear representation of appearance.\n",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05738",
        "title": "Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos",
        "authors": [
            "Serena Yeung",
            "Olga Russakovsky",
            "Ning Jin",
            "Mykhaylo Andriluka",
            "Greg Mori",
            "Li Fei-Fei"
        ],
        "abstract": "Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory (LSTM) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2017-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05936",
        "title": "The Cumulative Distribution Transform and Linear Pattern Classification",
        "authors": [
            "Se Rim Park",
            "Soheil Kolouri",
            "Shinjini Kundu",
            "Gustavo Rohde"
        ],
        "abstract": "Discriminating data classes emanating from sensors is an important problem with many applications in science and technology. We describe a new transform for pattern identification that interprets patterns as probability density functions, and has special properties with regards to classification. The transform, which we denote as the Cumulative Distribution Transform (CDT) is invertible, with well defined forward and inverse operations. We show that it can be useful in `parsing out' variations (confounds) that are `Lagrangian' (displacement and intensity variations) by converting these to `Eulerian' (intensity variations) in transform space. This conversion is the basis for our main result that describes when the CDT can allow for linear classification to be possible in transform space. We also describe several properties of the transform and show, with computational experiments that used both real and simulated data, that the CDT can help render a variety of real world problems simpler to solve.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2017-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06120",
        "title": "Towards Storytelling from Visual Lifelogging: An Overview",
        "authors": [
            "Marc Bola\u00f1os",
            "Mariella Dimiccoli",
            "Petia Radeva"
        ],
        "abstract": "Visual lifelogging consists of acquiring images that capture the daily experiences of the user by wearing a camera over a long period of time. The pictures taken offer considerable potential for knowledge mining concerning how people live their lives, hence, they open up new opportunities for many potential applications in fields including healthcare, security, leisure and the quantified self. However, automatically building a story from a huge collection of unstructured egocentric data presents major challenges. This paper provides a thorough review of advances made so far in egocentric data analysis, and in view of the current state of the art, indicates new lines of research to move us towards storytelling from visual lifelogging.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2016-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06149",
        "title": "Data-free parameter pruning for Deep Neural Networks",
        "authors": [
            "Suraj Srinivas",
            "R. Venkatesh Babu"
        ],
        "abstract": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\\% of the total parameters in an MNIST-trained network, and about 35\\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06173",
        "title": "Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo",
        "authors": [
            "Amit Adam",
            "Christoph Dann",
            "Omer Yair",
            "Shai Mazor",
            "Sebastian Nowozin"
        ],
        "abstract": "We propose a computational model for shape, illumination and albedo inference in a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based on phase modulation, our camera enables general exposure profiles. This results in added flexibility and requires novel computational approaches.\n",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06266",
        "title": "Particle detection and tracking in fluorescence time-lapse imaging: a contrario approach",
        "authors": [
            "Mariella Dimiccoli",
            "Jean-Pascal Jacob",
            "Lionel Moisan"
        ],
        "abstract": "This paper proposes a probabilistic approach for the detection and the tracking of particles in fluorescent time-lapse imaging. In the presence of a very noised and poor-quality data, particles and trajectories can be characterized by an a contrario model, that estimates the probability of observing the structures of interest in random data. This approach, first introduced in the modeling of human visual perception and then successfully applied in many image processing tasks, leads to algorithms that neither require a previous learning stage, nor a tedious parameter tuning and are very robust to noise. Comparative evaluations against a well-established baseline show that the proposed approach outperforms the state of the art.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06332",
        "title": "Part Localization using Multi-Proposal Consensus for Fine-Grained Categorization",
        "authors": [
            "Kevin J. Shih",
            "Arun Mallya",
            "Saurabh Singh",
            "Derek Hoiem"
        ],
        "abstract": "We present a simple deep learning framework to simultaneously predict keypoint locations and their respective visibilities and use those to achieve state-of-the-art performance for fine-grained classification. We show that by conditioning the predictions on object proposals with sufficient image support, our method can do well without complicated spatial reasoning. Instead, inference methods with robustness to outliers, yield state-of-the-art for keypoint localization. We demonstrate the effectiveness of our accurate keypoint localization and visibility prediction on the fine-grained bird recognition task with and without ground truth bird bounding boxes, and outperform existing state-of-the-art methods by over 2%.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06397",
        "title": "Multi-Target Tracking with Time-Varying Clutter Rate and Detection Profile: Application to Time-lapse Cell Microscopy Sequences",
        "authors": [
            "Seyed Hamid Rezatofighi",
            "Stephen Gould",
            "Ba Tuong Vo",
            "Ba-Ngu Vo",
            "Katarina Mele",
            "Richard Hartley"
        ],
        "abstract": "Quantitative analysis of the dynamics of tiny cellular and sub-cellular structures, known as particles, in time-lapse cell microscopy sequences requires the development of a reliable multi-target tracking method capable of tracking numerous similar targets in the presence of high levels of noise, high target density, complex motion patterns and intricate interactions. In this paper, we propose a framework for tracking these structures based on the random finite set Bayesian filtering framework. We focus on challenging biological applications where image characteristics such as noise and background intensity change during the acquisition process. Under these conditions, detection methods usually fail to detect all particles and are often followed by missed detections and many spurious measurements with unknown and time-varying rates. To deal with this, we propose a bootstrap filter composed of an estimator and a tracker. The estimator adaptively estimates the required meta parameters for the tracker such as clutter rate and the detection probability of the targets, while the tracker estimates the state of the targets. Our results show that the proposed approach can outperform state-of-the-art particle trackers on both synthetic and real data in this regime.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06429",
        "title": "Deep Fishing: Gradient Features from Deep Nets",
        "authors": [
            "Albert Gordo",
            "Adrien Gaidon",
            "Florent Perronnin"
        ],
        "abstract": "Convolutional Networks (ConvNets) have recently improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels. Deep learning is a marked departure from the previous state of the art, the Fisher Vector (FV), which relied on gradient-based encoding of local hand-crafted features. In this paper, we discuss a novel connection between these two approaches. First, we show that one can derive gradient representations from ConvNets in a similar fashion to the FV. Second, we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation. We experimentally study the benefits of transferring this representation over the outputs of ConvNet layers, and find consistent improvements on the Pascal VOC 2007 and 2012 datasets.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06504",
        "title": "Active skeleton for bacteria modeling",
        "authors": [
            "Jean-Pascal Jacob",
            "Mariella Dimiccoli",
            "Lionel Moisan"
        ],
        "abstract": "The investigation of spatio-temporal dynamics of bacterial cells and their molecular components requires automated image analysis tools to track cell shape properties and molecular component locations inside the cells. In the study of bacteria aging, the molecular components of interest are protein aggregates accumulated near bacteria boundaries. This particular location makes very ambiguous the correspondence between aggregates and cells, since computing accurately bacteria boundaries in phase-contrast time-lapse imaging is a challenging task. This paper proposes an active skeleton formulation for bacteria modeling which provides several advantages: an easy computation of shape properties (perimeter, length, thickness, orientation), an improved boundary accuracy in noisy images, and a natural bacteria-centered coordinate system that permits the intrinsic location of molecular components inside the cell. Starting from an initial skeleton estimate, the medial axis of the bacterium is obtained by minimizing an energy function which incorporates bacteria shape constraints. Experimental results on biological images and comparative evaluation of the performances validate the proposed approach for modeling cigar-shaped bacteria like Escherichia coli. The Image-J plugin of the proposed method can be found online at ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06535",
        "title": "Manitest: Are classifiers really invariant?",
        "authors": [
            "Alhussein Fawzi",
            "Pascal Frossard"
        ],
        "abstract": "Invariance to geometric transformations is a highly desirable property of automatic classifiers in many image recognition tasks. Nevertheless, it is unclear to which extent state-of-the-art classifiers are invariant to basic transformations such as rotations and translations. This is mainly due to the lack of general methods that properly measure such an invariance. In this paper, we propose a rigorous and systematic approach for quantifying the invariance to geometric transformations of any classifier. Our key idea is to cast the problem of assessing a classifier's invariance as the computation of geodesics along the manifold of transformed images. We propose the Manitest method, built on the efficient Fast Marching algorithm to compute the invariance of classifiers. Our new method quantifies in particular the importance of data augmentation for learning invariance from data, and the increased invariance of convolutional neural networks with depth. We foresee that the proposed generic tool for measuring invariance to a large class of geometric transformations and arbitrary classifiers will have many applications for evaluating and comparing classifiers based on their invariance, and help improving the invariance of existing classifiers.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2015-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06550",
        "title": "Human Pose Estimation with Iterative Error Feedback",
        "authors": [
            "Joao Carreira",
            "Pulkit Agrawal",
            "Katerina Fragkiadaki",
            "Jitendra Malik"
        ],
        "abstract": "Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2016-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06617",
        "title": "Fourier descriptors based on the structure of the human primary visual cortex with applications to object recognition",
        "authors": [
            "Amine Bohi",
            "Dario Prandi",
            "Vincente Guis",
            "Fr\u00e9d\u00e9ric Bouchara",
            "Jean-Paul Gauthier"
        ],
        "abstract": "In this paper we propose a supervised object recognition method using new global features and inspired by the model of the human primary visual cortex V1 as the semidiscrete roto-translation group $SE(2,N) = \\mathbb Z_N\\rtimes \\mathbb R^2$. The proposed technique is based on generalized Fourier descriptors on the latter group, which are invariant to natural geometric transformations (rotations, translations). These descriptors are then used to feed an SVM classifier. We have tested our method against the COIL-100 image database and the ORL face database, and compared it with other techniques based on traditional descriptors, global and local. The obtained results have shown that our approach looks extremely efficient and stable to noise, in presence of which it outperforms the other techniques analyzed in the paper.\n    ",
        "submission_date": "2015-07-23T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06821",
        "title": "Multimodal Deep Learning for Robust RGB-D Object Recognition",
        "authors": [
            "Andreas Eitel",
            "Jost Tobias Springenberg",
            "Luciano Spinello",
            "Martin Riedmiller",
            "Wolfram Burgard"
        ],
        "abstract": "Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06838",
        "title": "Descriptors and regions of interest fusion for gender classification in the wild. Comparison and combination with Convolutional Neural Networks",
        "authors": [
            "M. Castrill\u00f3n-Santana",
            "J. Lorenzo-Navarro",
            "E. Ram\u00f3n-Balmaseda"
        ],
        "abstract": "Gender classification (GC) has achieved high accuracy in different experimental evaluations based mostly on inner facial details. However, these results do not generalize well in unrestricted datasets and particularly in cross-database experiments, where the performance drops drastically. In this paper, we analyze the state-of-the-art GC accuracy on three large datasets: MORPH, LFW and GROUPS. We discuss their respective difficulties and bias, concluding that the most challenging and wildest complexity is present in GROUPS. This dataset covers hard conditions such as low resolution imagery and cluttered background. Firstly, we analyze in depth the performance of different descriptors extracted from the face and its local context on this dataset. Selecting the bests and studying their most suitable combination allows us to design a solution that beats any previously published results for GROUPS with the Dago's protocol, reaching an accuracy over 94.2%, reducing the gap with other simpler datasets. The chosen solution based on local descriptors is later evaluated in a cross-database scenario with the three mentioned datasets, and full dataset 5-fold cross validation. The achieved results are compared with a Convolutional Neural Network approach, achieving rather similar marks. Finally, a solution is proposed combining both focuses, exhibiting great complementarity, boosting GC performance to beat previously published results in GC both cross-database, and full in-database evaluations.\n    ",
        "submission_date": "2015-07-24T00:00:00",
        "last_modified_date": "2016-02-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07073",
        "title": "Efficient Face Alignment via Locality-constrained Representation for Robust Recognition",
        "authors": [
            "Yandong Wen",
            "Weiyang Liu",
            "Meng Yang",
            "Zhifeng Li"
        ],
        "abstract": "Practical face recognition has been studied in the past decades, but still remains an open challenge. Current prevailing approaches have already achieved substantial breakthroughs in recognition accuracy. However, their performance usually drops dramatically if face samples are severely misaligned. To address this problem, we propose a highly efficient misalignment-robust locality-constrained representation (MRLR) algorithm for practical real-time face recognition. Specifically, the locality constraint that activates the most correlated atoms and suppresses the uncorrelated ones, is applied to construct the dictionary for face alignment. Then we simultaneously align the warped face and update the locality-constrained dictionary, eventually obtaining the final alignment. Moreover, we make use of the block structure to accelerate the derived analytical solution. Experimental results on public data sets show that MRLR significantly outperforms several state-of-the-art approaches in terms of efficiency and scalability with even better performance.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07075",
        "title": "A Study of Morphological Filtering Using Graph and Hypergraphs",
        "authors": [
            "Keerthana S. Prakash",
            "R. P. Prakash",
            "V. P. Binu"
        ],
        "abstract": "Mathematical morphology (MM) helps to describe and analyze shapes using set theory. MM can be effectively applied to binary images which are treated as sets. Basic morphological operators defined can be used as an effective tool in image processing. Morphological operators are also developed based on graph and hypergraph. These operators have found better performance and applications in image processing. Bino et al. [8], [9] developed the theory of morphological operators on hypergraph. A hypergraph structure is considered and basic morphological operation erosion/dilation is defined. Several new operators opening/closing and filtering are also defined on the hypergraphs. Hypergraph based filtering have found comparatively better performance with morphological filters based on graph. In this paper we evaluate the effectiveness of hypergraph based ASF on binary images. Experimental results shows that hypergraph based ASF filters have outperformed graph based ASF.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2015-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07096",
        "title": "Thinning Algorithm Using Hypergraph Based Morphological Operators",
        "authors": [
            "R. P. Prakash",
            "Keerthana S. Prakash",
            "V. P. Binu"
        ],
        "abstract": "The object recognition is a complex problem in the image processing. Mathematical morphology is Shape oriented operations, that simplify image data, preserving their essential shape characteristics and eliminating irrelevancies. This paper briefly describes morphological operators using hypergraph and its applications for thinning algorithms. The morphological operators using hypergraph method is used to preventing errors and irregularities in skeleton, and is an important step recognizing line objects. The morphological operators using hypergraph such as dilation, erosion, opening, closing is a novel approach in image processing and it act as a filter remove the noise and errors in the images.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2015-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07203",
        "title": "Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision System",
        "authors": [
            "Louie Vincent A. Ngoho",
            "Jaderick P. Pabico"
        ],
        "abstract": "We developed a machine vision system to automatically capture the dynamics of pedestrians under four different traffic scenarios. By considering the overhead view of each pedestrian as a digital object, the system processes the image sequences to track the pedestrians. Considering the perspective effect of the camera lens and the projected area of the hallway at the top-view scene, the distance of each tracked object from its original position to its current position is approximated every video frame. Using the approximated distance and the video frame rate (30 frames per second), the respective velocity and acceleration of each tracked object are later derived. The quantified motion characteristics of the pedestrians are displayed by the system through 2-dimensional graphs of the kinematics of motion. The system also outputs video images of the pedestrians with superimposed markers for tracking. These visual markers were used to visually describe and quantify the behavior of the pedestrians under different traffic scenarios.\n    ",
        "submission_date": "2015-07-26T00:00:00",
        "last_modified_date": "2015-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07242",
        "title": "Face Search at Scale: 80 Million Gallery",
        "authors": [
            "Dayong Wang",
            "Charles Otto",
            "Anil K. Jain"
        ],
        "abstract": "Due to the prevalence of social media websites, one challenge facing computer vision researchers is to devise methods to process and search for persons of interest among the billions of shared photos on these websites. Facebook revealed in a 2013 white paper that its users have uploaded more than 250 billion photos, and are uploading 350 million new photos each day. Due to this humongous amount of data, large-scale face search for mining web images is both important and challenging. Despite significant progress in face recognition, searching a large collection of unconstrained face images has not been adequately addressed. To address this challenge, we propose a face search system which combines a fast search procedure, coupled with a state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe face, we first filter the large gallery of photos to find the top-k most similar faces using deep features generated from a convolutional neural network. The k candidates are re-ranked by combining similarities from deep features and the COTS matcher. We evaluate the proposed face search system on a gallery containing 80 million web-downloaded face images. Experimental results demonstrate that the deep features are competitive with state-of-the-art methods on unconstrained face recognition benchmarks (LFW and IJB-A). Further, the proposed face search system offers an excellent trade-off between accuracy and scalability on datasets consisting of millions of images. Additionally, in an experiment involving searching for face images of the Tsarnaev brothers, convicted of the Boston Marathon bombing, the proposed face search system could find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5M gallery and at rank 8 in 7 seconds on an 80M gallery.\n    ",
        "submission_date": "2015-07-26T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07458",
        "title": "Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization",
        "authors": [
            "Xun Xu",
            "Timothy Hospedales",
            "Shaogang Gong"
        ],
        "abstract": "The growing rate of public space CCTV installations has generated a need for automated methods for exploiting video surveillance data including scene understanding, query, behaviour annotation and summarization. For this reason, extensive research has been performed on surveillance scene understanding and analysis. However, most studies have considered single scenes, or groups of adjacent scenes. The semantic similarity between different but related scenes (e.g., many different traffic scenes of similar layout) is not generally exploited to improve any automated surveillance tasks and reduce manual effort. Exploiting commonality, and sharing any supervised annotations, between different scenes is however challenging due to: Some scenes are totally un-related -- and thus any information sharing between them would be detrimental; while others may only share a subset of common activities -- and thus information sharing is only useful if it is selective. Moreover, semantically similar activities which should be modelled together and shared across scenes may have quite different pixel-level appearance in each scene. To address these issues we develop a new framework for distributed multiple-scene global understanding that clusters surveillance scenes by their ability to explain each other's behaviours; and further discovers which subset of activities are shared versus scene-specific within each cluster. We show how to use this structured representation of multiple scenes to improve common surveillance tasks including scene activity understanding, cross-scene query-by-example, behaviour classification with reduced supervised labelling requirements, and video summarization. In each case we demonstrate how our multi-scene model improves on a collection of standard single scene models and a flat model of all scenes.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07505",
        "title": "Real-time 2D/3D Registration via CNN Regression",
        "authors": [
            "Shun Miao",
            "Z. Jane Wang",
            "Rui Liao"
        ],
        "abstract": "In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07508",
        "title": "Fast Segmentation of Left Ventricle in CT Images by Explicit Shape Regression using Random Pixel Difference Features",
        "authors": [
            "Peng Sun",
            "Haoyin Zhou",
            "Devon Lundine",
            "James K. Min",
            "Guanglei Xiong"
        ],
        "abstract": "Recently, machine learning has been successfully applied to model-based left ventricle (LV) segmentation. The general framework involves two stages, which starts with LV localization and is followed by boundary delineation. Both are driven by supervised learning techniques. When compared to previous non-learning-based methods, several advantages have been shown, including full automation and improved accuracy. However, the speed is still slow, in the order of several seconds, for applications involving a large number of cases or case loads requiring real-time performance. In this paper, we propose a fast LV segmentation algorithm by joint localization and boundary delineation via training explicit shape regressor with random pixel difference features. Tested on 3D cardiac computed tomography (CT) image volumes, the average running time of the proposed algorithm is 1.2 milliseconds per case. On a dataset consisting of 139 CT volumes, a 5-fold cross validation shows the segmentation error is $1.21 \\pm 0.11$ for LV endocardium and $1.23 \\pm 0.11$ millimeters for epicardium. Compared with previous work, the proposed method is more stable (lower standard deviation) without significant compromise to the accuracy.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07583",
        "title": "Mapping Auto-context Decision Forests to Deep ConvNets for Semantic Segmentation",
        "authors": [
            "David L. Richmond",
            "Dagmar Kainmueller",
            "Michael Y. Yang",
            "Eugene W. Myers",
            "Carsten Rother"
        ],
        "abstract": "We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2018-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07646",
        "title": "Learning 3D Deformation of Animals from 2D Images",
        "authors": [
            "Angjoo Kanazawa",
            "Shahar Kovalsky",
            "Ronen Basri",
            "David W. Jacobs"
        ],
        "abstract": "Understanding how an animal can deform and articulate is essential for a realistic modification of its 3D model. In this paper, we show that such information can be learned from user-clicked 2D images and a template 3D model of the target animal. We present a volumetric deformation framework that produces a set of new 3D models by deforming a template 3D model according to a set of user-clicked images. Our framework is based on a novel locally-bounded deformation energy, where every local region has its own stiffness value that bounds how much distortion is allowed at that location. We jointly learn the local stiffness bounds as we deform the template 3D mesh to match each user-clicked image. We show that this seemingly complex task can be solved as a sequence of convex optimization problems. We demonstrate the effectiveness of our approach on cats and horses, which are highly deformable and articulated animals. Our framework produces new 3D models of animals that are significantly more plausible than methods without learned stiffness.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07800",
        "title": "SynapCountJ --- a Tool for Analyzing Synaptic Densities in Neurons",
        "authors": [
            "Gadea Mata",
            "J\u00f3nathan Heras",
            "Miguel Morales",
            "Ana Romero",
            "Julio Rubio"
        ],
        "abstract": "The quantification of synapses is instrumental to measure the evolution of synaptic densities of neurons under the effect of some physiological conditions, neuronal diseases or even drug treatments. However, the manual quantification of synapses is a tedious, error-prone, time-consuming and subjective task; therefore, tools that might automate this process are desirable. In this paper, we present SynapCountJ, an ImageJ plugin, that can measure synaptic density of individual neurons obtained by immunofluorescence techniques, and also can be applied for batch processing of neurons that have been obtained in the same experiment or using the same setting. The procedure to quantify synapses implemented in SynapCountJ is based on the colocalization of three images of the same neuron (the neuron marked with two antibody markers and the structure of the neuron) and is inspired by methods coming from Computational Algebraic Topology. SynapCountJ provides a procedure to semi-automatically quantify the number of synapses of neuron cultures; as a result, the time required for such an analysis is greatly reduced.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07815",
        "title": "A Multi-Camera Image Processing and Visualization System for Train Safety Assessment",
        "authors": [
            "Giuseppe Lisanti",
            "Svebor Karaman",
            "Daniele Pezzatini",
            "Alberto Del Bimbo"
        ],
        "abstract": "In this paper we present a machine vision system to efficiently monitor, analyze and present visual data acquired with a railway overhead gantry equipped with multiple cameras. This solution aims to improve the safety of daily life railway transportation in a two- fold manner: (1) by providing automatic algorithms that can process large imagery of trains (2) by helping train operators to keep attention on any possible malfunction. The system is designed with the latest cutting edge, high-rate visible and thermal cameras that ob- serve a train passing under an railway overhead gantry. The machine vision system is composed of three principal modules: (1) an automatic wagon identification system, recognizing the wagon ID according to the UIC classification of railway coaches; (2) a temperature monitoring system; (3) a system for the detection, localization and visualization of the pantograph of the train. These three machine vision modules process batch trains sequences and their resulting analysis are presented to an operator using a multitouch user interface. We detail all technical aspects of our multi-camera portal: the hardware requirements, the software developed to deal with the high-frame rate cameras and ensure reliable acquisition, the algorithms proposed to solve each computer vision task, and the multitouch interaction and visualization interface. We evaluate each component of our system on a dataset recorded in an ad-hoc railway test-bed, showing the potential of our proposed portal for train safety assessment.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07882",
        "title": "Occlusion-Aware Object Localization, Segmentation and Pose Estimation",
        "authors": [
            "Samarth Brahmbhatt",
            "Heni Ben Amor",
            "Henrik Christensen"
        ],
        "abstract": "We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of im- age segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion.\n    ",
        "submission_date": "2015-07-27T00:00:00",
        "last_modified_date": "2015-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07909",
        "title": "Offline Handwritten Signature Verification - Literature Review",
        "authors": [
            "Luiz G. Hafemann",
            "Robert Sabourin",
            "Luiz S. Oliveira"
        ],
        "abstract": "The area of Handwritten Signature Verification has been broadly researched in the last decades, but remains an open research problem. The objective of signature verification systems is to discriminate if a given signature is genuine (produced by the claimed individual), or a forgery (produced by an impostor). This has demonstrated to be a challenging task, in particular in the offline (static) scenario, that uses images of scanned signatures, where the dynamic information about the signing process is not available. Many advancements have been proposed in the literature in the last 5-10 years, most notably the application of Deep Learning methods to learn feature representations from signature images. In this paper, we present how the problem has been handled in the past few decades, analyze the recent advancements in the field, and the potential directions for future research.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08030",
        "title": "Adapted sampling for 3D X-ray computed tomography",
        "authors": [
            "Anthony Cazasnoves",
            "Fanny Buyens",
            "Sylvie Sevestre"
        ],
        "abstract": "In this paper, we introduce a method to build an adapted mesh representation of a 3D object for X-Ray tomography reconstruction. Using this representation, we provide means to reduce the computational cost of reconstruction by way of iterative algorithms. The adapted sampling of the reconstruction space is directly obtained from the projection dataset and prior to any reconstruction. It is built following two stages : firstly, 2D structural information is extracted from the projection images and is secondly merged in 3D to obtain a 3D pointcloud sampling the interfaces of the object. A relevant mesh is then built from this cloud by way of tetrahedralization. Critical parameters selections have been automatized through a statistical framework, thus avoiding dependence on users expertise. Applying this approach on geometrical shapes and on a 3D Shepp-Logan phantom, we show the relevance of such a sampling - obtained in a few seconds - and the drastic decrease in cells number to be estimated during reconstruction when compared to the usual regular voxel lattice. A first iterative reconstruction of the Shepp-Logan using this kind of sampling shows the relevant advantages in terms of low dose or sparse acquisition sampling contexts. The method can also prove useful for other applications such as finite element method computations.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08064",
        "title": "Collaborative Representation Classification Ensemble for Face Recognition",
        "authors": [
            "Xiaochao Qu",
            "Suah Kim",
            "Run Cui",
            "Hyoung Joong Kim"
        ],
        "abstract": "Collaborative Representation Classification (CRC) for face recognition attracts a lot attention recently due to its good recognition performance and fast speed. Compared to Sparse Representation Classification (SRC), CRC achieves a comparable recognition performance with 10-1000 times faster speed. In this paper, we propose to ensemble several CRC models to promote the recognition rate, where each CRC model uses different and divergent randomly generated biologically-inspired features as the face representation. The proposed ensemble algorithm calculates an ensemble weight for each CRC model that guided by the underlying classification rule of CRC. The obtained weights reflect the confidences of those CRC models where the more confident CRC models have larger weights. The proposed weighted ensemble method proves to be very effective and improves the performance of each CRC model significantly. Extensive experiments are conducted to show the superior performance of the proposed method.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08076",
        "title": "Cross-pose Face Recognition by Canonical Correlation Analysis",
        "authors": [
            "Annan Li",
            "Shiguang Shan",
            "Xilin Chen",
            "Bingpeng Ma",
            "Shuicheng Yan",
            "Wen Gao"
        ],
        "abstract": "The pose problem is one of the bottlenecks in automatic face recognition. We argue that one of the diffculties in this problem is the severe misalignment in face images or feature vectors with different poses. In this paper, we propose that this problem can be statistically solved or at least mitigated by maximizing the intra-subject across-pose correlations via canonical correlation analysis (CCA). In our method, based on the data set with coupled face images of the same identities and across two different poses, CCA learns simultaneously two linear transforms, each for one pose. In the transformed subspace, the intra-subject correlations between the different poses are maximized, which implies pose-invariance or pose-robustness is achieved. The experimental results show that our approach could considerably improve the recognition performance. And if further enhanced with holistic+local feature representation, the performance could be comparable to the state-of-the-art.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08085",
        "title": "Tracking Randomly Moving Objects on Edge Box Proposals",
        "authors": [
            "Gao Zhu",
            "Fatih Porikli",
            "Hongdong Li"
        ],
        "abstract": "Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of \"high-quality\" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08173",
        "title": "Fast Robust PCA on Graphs",
        "authors": [
            "Nauman Shahid",
            "Nathanael Perraudin",
            "Vassilis Kalofolias",
            "Gilles Puy",
            "Pierre Vandergheynst"
        ],
        "abstract": "Mining useful clusters from high dimensional data has received significant attention of the computer vision and pattern recognition community in the recent years. Linear and non-linear dimensionality reduction has played an important role to overcome the curse of dimensionality. However, often such methods are accompanied with three different problems: high computational complexity (usually associated with the nuclear norm minimization), non-convexity (for matrix factorization methods) and susceptibility to gross corruptions in the data. In this paper we propose a principal component analysis (PCA) based solution that overcomes these three issues and approximates a low-rank recovery method for high dimensional datasets. We target the low-rank recovery by enforcing two types of graph smoothness assumptions, one on the data samples and the other on the features by designing a convex optimization problem. The resulting algorithm is fast, efficient and scalable for huge datasets with O(nlog(n)) computational complexity in the number of data samples. It is also robust to gross corruptions in the dataset as well as to the model parameters. Clustering experiments on 7 benchmark datasets with different types of corruptions and background separation experiments on 3 video datasets show that our proposed model outperforms 10 state-of-the-art dimensionality reduction models. Our theoretical analysis proves that the proposed model is able to recover approximate low-rank representations with a bounded error for clusterable data.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08184",
        "title": "Beamforming through regularized inverse problems in ultrasound medical imaging",
        "authors": [
            "Teodora Szasz",
            "Adrian Basarab",
            "Denis Kouam\u00e9"
        ],
        "abstract": "Beamforming in ultrasound imaging has significant impact on the quality of the final image, controlling its resolution and contrast. Despite its low spatial resolution and contrast, delay-and-sum is still extensively used nowadays in clinical applications, due to its real-time capabilities. The most common alternatives are minimum variance method and its variants, which overcome the drawbacks of delay-and-sum, at the cost of higher computational complexity that limits its utilization in real-time applications.\n",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08286",
        "title": "Deep Learning for Single-View Instance Recognition",
        "authors": [
            "David Held",
            "Sebastian Thrun",
            "Silvio Savarese"
        ],
        "abstract": "Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding.\n    ",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08363",
        "title": "Action recognition in still images by latent superpixel classification",
        "authors": [
            "Shaukat Abidi",
            "Massimo Piccardi",
            "Mary-Anne Williams"
        ],
        "abstract": "Action recognition from still images is an important task of computer vision applications such as image annotation, robotic navigation, video surveillance and several others. Existing approaches mainly rely on either bag-of-feature representations or articulated body-part models. However, the relationship between the action and the image segments is still substantially unexplored. For this reason, in this paper we propose to approach action recognition by leveraging an intermediate layer of \"superpixels\" whose latent classes can act as attributes of the action. In the proposed approach, the action class is predicted by a structural model(learnt by Latent Structural SVM) based on measurements from the image superpixels and their latent classes. Experimental results over the challenging Stanford 40 Actions dataset report a significant average accuracy of 74.06% for the positive class and 88.50% for the negative class, giving evidence to the performance of the proposed approach.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08373",
        "title": "When VLAD met Hilbert",
        "authors": [
            "Mehrtash Harandi",
            "Mathieu Salzmann",
            "Fatih Porikli"
        ],
        "abstract": "Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerful image/video representations that compete with or even outperform state-of-the-art approaches on many challenging visual recognition tasks. In this paper, we address two fundamental limitations of VLAD: its requirement for the local descriptors to have vector form and its restriction to linear classifiers due to its high-dimensionality. To this end, we introduce a kernelized version of VLAD. This not only lets us inherently exploit more sophisticated classification schemes, but also enables us to efficiently aggregate non-vector descriptors (e.g., tensors) in the VLAD framework. Furthermore, we propose three approximate formulations that allow us to accelerate the coding process while still benefiting from the properties of kernel VLAD. Our experiments demonstrate the effectiveness of our approach at handling manifold-valued data, such as covariance descriptors, on several classification tasks. Our results also evidence the benefits of our nonlinear VLAD descriptors against the linear ones in Euclidean space using several standard benchmark datasets.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08429",
        "title": "Multilinear Map Layer: Prediction Regularization by Structural Constraint",
        "authors": [
            "Shuchang Zhou",
            "Yuxin Wu"
        ],
        "abstract": "In this paper we propose and study a technique to impose structural constraints on the output of a neural network, which can reduce amount of computation and number of parameters besides improving prediction accuracy when the output is known to approximately conform to the low-rankness prior. The technique proceeds by replacing the output layer of neural network with the so-called MLM layers, which forces the output to be the result of some Multilinear Map, like a hybrid-Kronecker-dot product or Kronecker Tensor Product. In particular, given an \"autoencoder\" model trained on SVHN dataset, we can construct a new model with MLM layer achieving 62\\% reduction in total number of parameters and reduction of $\\ell_2$ reconstruction error from 0.088 to 0.004. Further experiments on other autoencoder model variants trained on SVHN datasets also demonstrate the efficacy of MLM layers.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08445",
        "title": "People Counting in High Density Crowds from Still Images",
        "authors": [
            "Ankan Bansal",
            "K.S. Venkatesh"
        ],
        "abstract": "We present a method of estimating the number of people in high density crowds from still images. The method estimates counts by fusing information from multiple sources. Most of the existing work on crowd counting deals with very small crowds (tens of individuals) and use temporal information from videos. Our method uses only still images to estimate the counts in high density images (hundreds to thousands of individuals). At this scale, we cannot rely on only one set of features for count estimation. We, therefore, use multiple sources, viz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM features and low confidence head detections, to estimate the counts. Each of these sources gives a separate estimate of the count along with confidences and other statistical measures which are then combined to obtain the final estimate. We test our method on an existing dataset of fifty images containing over 64000 individuals. Further, we added another fifty annotated images of crowds and tested on the complete dataset of hundred images containing over 87000 individuals. The counts per image range from 81 to 4633. We report the performance in terms of mean absolute error, which is a measure of accuracy of the method, and mean normalised absolute error, which is a measure of the robustness.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08571",
        "title": "Agglomerative clustering and collectiveness measure via exponent generating function",
        "authors": [
            "Wei-Ya Ren",
            "Shuo-Hao Li",
            "Qiang Guo",
            "Guo-Hui Li",
            "Jun Zhang"
        ],
        "abstract": "The key in agglomerative clustering is to define the affinity measure between two sets. A novel agglomerative clustering method is proposed by utilizing the path integral to define the affinity measure. Firstly, the path integral descriptor of an edge, a node and a set is computed by path integral and exponent generating function. Then, the affinity measure between two sets is obtained by path integral descriptor of sets. Several good properties of the path integral descriptor is proposed in this paper. In addition, we give the physical interpretation of the proposed path integral descriptor of a set. The proposed path integral descriptor of a set can be regard as the collectiveness measure of a set, which can be a moving system such as human crowd, sheep herd and so on. Self-driven particle (SDP) model is used to test the ability of the proposed method in measuring collectiveness.\n    ",
        "submission_date": "2015-07-30T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08711",
        "title": "Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs",
        "authors": [
            "Mehrtash Harandi",
            "Mathieu Salzmann",
            "Mahsa Baktashmotlagh"
        ],
        "abstract": "State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution. Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets. Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08754",
        "title": "Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification",
        "authors": [
            "Fa Wu",
            "Peijun Hu",
            "Dexing Kong"
        ],
        "abstract": "This paper presents a new version of Dropout called Split Dropout (sDropout) and rotational convolution techniques to improve CNNs' performance on image classification. The widely used standard Dropout has advantage of preventing deep neural networks from overfitting by randomly dropping units during training. Our sDropout randomly splits the data into two subsets and keeps both rather than discards one subset. We also introduce two rotational convolution techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling convolution (FRPC) to boost CNNs' performance on the robustness for rotation transformation. These two techniques encode rotation invariance into the network without adding extra parameters. Experimental evaluations on ImageNet2012 classification task demonstrate that sDropout not only enhances the performance but also converges faster. Additionally, RPC and FRPC make CNNs more robust for rotation transformations. Overall, FRPC together with sDropout bring $1.18\\%$ (model of Zeiler and Fergus~\\cite{zeiler2013visualizing}, 10-view, top-1) accuracy increase in ImageNet 2012 classification task compared to the original network.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08761",
        "title": "Multimodal Multipart Learning for Action Recognition in Depth Videos",
        "authors": [
            "Amir Shahroudy",
            "Gang Wang",
            "Tian-Tsong Ng",
            "Qingxiong Yang"
        ],
        "abstract": "The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08818",
        "title": "A Visual Embedding for the Unsupervised Extraction of Abstract Semantics",
        "authors": [
            "D. Garcia-Gasulla",
            "J. B\u00e9jar",
            "U. Cort\u00e9s",
            "E. Ayguad\u00e9",
            "J. Labarta",
            "T. Suzumura",
            "R. Chen"
        ],
        "abstract": "Vector-space word representations obtained from neural network models have been shown to enable semantic operations based on vector arithmetic. In this paper, we explore the existence of similar information on vector representations of images. For that purpose we define a methodology to obtain large, sparse vector representations of image classes, and generate vectors through the state-of-the-art deep learning architecture GoogLeNet for 20K images obtained from ImageNet. We first evaluate the resultant vector-space semantics through its correlation with WordNet distances, and find vector distances to be strongly correlated with linguistic semantics. We then explore the location of images within the vector space, finding elements close in WordNet to be clustered together, regardless of significant visual variances (e.g. 118 dog types). More surprisingly, we find that the space unsupervisedly separates complex classes without prior knowledge (e.g. living things). Afterwards, we consider vector arithmetics. Although we are unable to obtain meaningful results on this regard, we discuss the various problem we encountered, and how we consider to solve them. Finally, we discuss the impact of our research for cognitive systems, focusing on the role of the architecture being used.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2016-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08905",
        "title": "Deep Networks for Image Super-Resolution with Sparse Prior",
        "authors": [
            "Zhaowen Wang",
            "Ding Liu",
            "Jianchao Yang",
            "Wei Han",
            "Thomas Huang"
        ],
        "abstract": "Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08958",
        "title": "SnowWatch: Snow Monitoring through Acquisition and Analysis of User-Generated Content",
        "authors": [
            "Roman Fedorov",
            "Piero Fraternali",
            "Chiara Pasini",
            "Marco Tagliasacchi"
        ],
        "abstract": "We present a system for complementing snow phenomena monitoring with virtual measurements extracted from public visual content. The proposed system integrates an automatic acquisition and analysis of photographs and webcam images depicting Alpine mountains. In particular, the technical demonstration consists in a web portal that interfaces the whole system with the population. It acts as an entertaining photo-sharing social web site, acquiring at the same time visual content necessary for environmental monitoring.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00092",
        "title": "Land Use Classification in Remote Sensing Images by Convolutional Neural Networks",
        "authors": [
            "Marco Castelluccio",
            "Giovanni Poggi",
            "Carlo Sansone",
            "Luisa Verdoliva"
        ],
        "abstract": "We explore the use of convolutional neural networks for the semantic classification of remote sensing scenes. Two recently proposed architectures, CaffeNet and GoogLeNet, are adopted, with three different learning modalities. Besides conventional training from scratch, we resort to pre-trained networks that are only fine-tuned on the target data, so as to avoid overfitting problems and reduce design time. Experiments on two remote sensing datasets, with markedly different characteristics, testify on the effectiveness and wide applicability of the proposed solution, which guarantees a significant performance improvement over all state-of-the-art references.\n    ",
        "submission_date": "2015-08-01T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00102",
        "title": "Towards Distortion-Predictable Embedding of Neural Networks",
        "authors": [
            "Axel Angel"
        ],
        "abstract": "Current research in Computer Vision has shown that Convolutional Neural Networks (CNN) give state-of-the-art performance in many classification tasks and Computer Vision problems. The embedding of CNN, which is the internal representation produced by the last layer, can indirectly learn topological and relational properties. Moreover, by using a suitable loss function, CNN models can learn invariance to a wide range of non-linear distortions such as rotation, viewpoint angle or lighting condition. In this work, new insights are discovered about CNN embeddings and a new loss function is proposed, derived from the contrastive loss, that creates models with more predicable mappings and also quantifies distortions. In typical distortion-dependent methods, there is no simple relation between the features corresponding to one image and the features of this image distorted. Therefore, these methods require to feed-forward inputs under every distortions in order to find the corresponding features representations. Our contribution makes a step towards embeddings where features of distorted inputs are related and can be derived from each others by the intensity of the distortion.\n    ",
        "submission_date": "2015-08-01T00:00:00",
        "last_modified_date": "2015-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00217",
        "title": "Indexing of CNN Features for Large Scale Image Search",
        "authors": [
            "Ruoyu Liu",
            "Yao Zhao",
            "Shikui Wei",
            "Yi Yang"
        ],
        "abstract": "The convolutional neural network (CNN) features can give a good description of image content, which usually represent images with unique global vectors. Although they are compact compared to local descriptors, they still cannot efficiently deal with large-scale image retrieval due to the cost of the linear incremental computation and storage. To address this issue, we build a simple but effective indexing framework based on inverted table, which significantly decreases both the search time and memory usage. In addition, several strategies are fully investigated under an indexing framework to adapt it to CNN features and compensate for quantization errors. First, we use multiple assignment for the query and database images to increase the probability of relevant images' co-existing in the same Voronoi cells obtained via the clustering algorithm. Then, we introduce embedding codes to further improve precision by removing false matches during a search. We demonstrate that by using hashing schemes to calculate the embedding codes and by changing the ranking rule, indexing framework speeds can be greatly improved. Extensive experiments conducted on several unsupervised and supervised benchmarks support these results and the superiority of the proposed indexing framework. We also provide a fair comparison between the popular CNN features.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2018-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00239",
        "title": "Partial matching face recognition method for rehabilitation nursing robots beds",
        "authors": [
            "Dongmei Liang",
            "Wushan Cheng"
        ],
        "abstract": "In order to establish face recognition system in rehabilitation nursing robots beds and achieve real-time monitor the patient on the bed. We propose a face recognition method based on partial matching Hu moments which apply for rehabilitation nursing robots beds. Firstly we using Haar classifier to detect human faces automatically in dynamic video frames. Secondly we using Otsu threshold method to extract facial features (eyebrows, eyes, mouth) in the face image and its Hu moments. Finally, we using Hu moment feature set to achieve the automatic face recognition. Experimental results show that this method can efficiently identify face in a dynamic video and it has high practical value (the accuracy rate is 91% and the average recognition time is 4.3s).\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00271",
        "title": "Recurrent Network Models for Human Dynamics",
        "authors": [
            "Katerina Fragkiadaki",
            "Sergey Levine",
            "Panna Felsen",
            "Jitendra Malik"
        ],
        "abstract": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00278",
        "title": "Dictionary and Image Recovery from Incomplete and Random Measurements",
        "authors": [
            "Mohammad Aghagolzadeh",
            "Hayder Radha"
        ],
        "abstract": "This paper tackles algorithmic and theoretical aspects of dictionary learning from incomplete and random block-wise image measurements and the performance of the adaptive dictionary for sparse image recovery. This problem is related to blind compressed sensing in which the sparsifying dictionary or basis is viewed as an unknown variable and subject to estimation during sparse recovery. However, unlike existing guarantees for a successful blind compressed sensing, our results do not rely on additional structural constraints on the learned dictionary or the measured signal. In particular, we rely on the spatial diversity of compressive measurements to guarantee that the solution is unique with a high probability. Moreover, our distinguishing goal is to measure and reduce the estimation error with respect to the ideal dictionary that is based on the complete image. Using recent results from random matrix theory, we show that applying a slightly modified dictionary learning algorithm over compressive measurements results in accurate estimation of the ideal dictionary for large-scale images. Empirically, we experiment with both space-invariant and space-varying sensing matrices and demonstrate the critical role of spatial diversity in measurements. Simulation results confirm that the presented algorithm outperforms the typical non-adaptive sparse recovery based on offline-learned universal dictionaries.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00282",
        "title": "On Hyperspectral Classification in the Compressed Domain",
        "authors": [
            "Mohammad Aghagolzadeh",
            "Hayder Radha"
        ],
        "abstract": "In this paper, we study the problem of hyperspectral pixel classification based on the recently proposed architectures for compressive whisk-broom hyperspectral imagers without the need to reconstruct the complete data cube. A clear advantage of classification in the compressed domain is its suitability for real-time on-site processing of the sensed data. Moreover, it is assumed that the training process also takes place in the compressed domain, thus, isolating the classification unit from the recovery unit at the receiver's side. We show that, perhaps surprisingly, using distinct measurement matrices for different pixels results in more accuracy of the learned classifier and consistent classification performance, supporting the role of information diversity in learning.\n    ",
        "submission_date": "2015-08-02T00:00:00",
        "last_modified_date": "2015-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00307",
        "title": "Local Color Contrastive Descriptor for Image Classification",
        "authors": [
            "Sheng Guo",
            "Weilin Huang",
            "Yu Qiao"
        ],
        "abstract": "Image representation and classification are two fundamental tasks towards multimedia content retrieval and understanding. The idea that shape and texture information (e.g. edge or orientation) are the key features for visual representation is ingrained and dominated in current multimedia and computer vision communities. A number of low-level features have been proposed by computing local gradients (e.g. SIFT, LBP and HOG), and have achieved great successes on numerous multimedia applications. In this paper, we present a simple yet efficient local descriptor for image classification, referred as Local Color Contrastive Descriptor (LCCD), by leveraging the neural mechanisms of color contrast. The idea originates from the observation in neural science that color and shape information are linked inextricably in visual cortical processing. The color contrast yields key information for visual color perception and provides strong linkage between color and shape. We propose a novel contrastive mechanism to compute the color contrast in both spatial location and multiple channels. The color contrast is computed by measuring \\emph{f}-divergence between the color distributions of two regions. Our descriptor enriches local image representation with both color and contrast information. We verified experimentally that it can compensate strongly for the shape based descriptor (e.g. SIFT), while keeping computationally simple. Extensive experimental results on image classification show that our descriptor improves the performance of SIFT substantially by combinations, and achieves the state-of-the-art performance on three challenging benchmark datasets. It improves recent Deep Learning model (DeCAF) [1] largely from the accuracy of 40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will be available.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00330",
        "title": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units",
        "authors": [
            "Zhibin Liao",
            "Gustavo Carneiro"
        ],
        "abstract": "Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00413",
        "title": "Identifying Emotion from Natural Walking",
        "authors": [
            "Liqing Cui",
            "Shun Li",
            "Wan Zhang",
            "Zhan Zhang",
            "Tingshao Zhu"
        ],
        "abstract": "Emotion identification from gait aims to automatically determine persons affective state, it has attracted a great deal of interests and offered immense potential value in action tendency, health care, psychological detection and human-computer(robot) ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00430",
        "title": "Kernelized Multiview Projection",
        "authors": [
            "Mengyang Yu",
            "Li Liu",
            "Ling Shao"
        ],
        "abstract": "Conventional vision algorithms adopt a single type of feature or a simple concatenation of multiple features, which is always represented in a high-dimensional space. In this paper, we propose a novel unsupervised spectral embedding algorithm called Kernelized Multiview Projection (KMP) to better fuse and embed different feature representations. Computing the kernel matrices from different features/views, KMP can encode them with the corresponding weights to achieve a low-dimensional and semantically meaningful subspace where the distribution of each view is sufficiently smooth and discriminative. More crucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) and solves the out-of-sample problem, which allows it to be competent for various practical applications. Extensive experiments on three popular image datasets demonstrate the effectiveness of our multiview embedding algorithm.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00537",
        "title": "Evaluating software-based fingerprint liveness detection using Convolutional Networks and Local Binary Patterns",
        "authors": [
            "Rodrigo Frassetto Nogueira",
            "Roberto de Alencar Lotufo",
            "Rubens Campos Machado"
        ],
        "abstract": "With the growing use of biometric authentication systems in the past years, spoof fingerprint detection has become increasingly important. In this work, we implement and evaluate two different feature extraction techniques for software-based fingerprint liveness detection: Convolutional Networks with random weights and Local Binary Patterns. Both techniques were used in conjunction with a Support Vector Machine (SVM) classifier. Dataset Augmentation was used to increase classifier's performance and a variety of preprocessing operations were tested, such as frequency filtering, contrast equalization, and region of interest filtering. The experiments were made on the datasets used in The Liveness Detection Competition of years 2009, 2011 and 2013, which comprise almost 50,000 real and fake fingerprints' images. Our best method achieves an overall rate of 95.2% of correctly classified samples - an improvement of 35% in test error when compared with the best previously published results.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2015-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00776",
        "title": "Online Domain Adaptation for Multi-Object Tracking",
        "authors": [
            "Adrien Gaidon",
            "Eleonora Vig"
        ],
        "abstract": "Automatically detecting, labeling, and tracking objects in videos depends first and foremost on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work is the first to investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from the pre-trained one, and (ii) we also adapt the pre-trained model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap \"mini-Fisher Vectors\" and expensive \"off-the-shelf\" ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00835",
        "title": "Semantic Pose using Deep Networks Trained on Synthetic RGB-D",
        "authors": [
            "Jeremie Papon",
            "Markus Schoeler"
        ],
        "abstract": "In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel on a GPU in seconds.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00966",
        "title": "3D Automatic Segmentation Method for Retinal Optical Coherence Tomography Volume Data Using Boundary Surface Enhancement",
        "authors": [
            "Yankui Sun",
            "Tian Zhang",
            "Yue Zhao",
            "Yufan He"
        ],
        "abstract": "With the introduction of spectral-domain optical coherence tomography (SDOCT), much larger image datasets are routinely acquired compared to what was possible using the previous generation of time-domain OCT. Thus, there is a critical need for the development of 3D segmentation methods for processing these data. We present here a novel 3D automatic segmentation method for retinal OCT volume data. Briefly, to segment a boundary surface, two OCT volume datasets are obtained by using a 3D smoothing filter and a 3D differential filter. Their linear combination is then calculated to generate new volume data with an enhanced boundary surface, where pixel intensity, boundary position information, and intensity changes on both sides of the boundary surface are used simultaneously. Next, preliminary discrete boundary points are detected from the A-Scans of the volume data. Finally, surface smoothness constraints and a dynamic threshold are applied to obtain a smoothed boundary surface by correcting a small number of error points. Our method can extract retinal layer boundary surfaces sequentially with a decreasing search region of volume data. We performed automatic segmentation on eight human OCT volume datasets acquired from a commercial Spectralis OCT system, where each volume of data consisted of 97 OCT images with a resolution of 496 512; experimental results show that this method can accurately segment seven layer boundary surfaces in normal as well as some abnormal eyes.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00998",
        "title": "Single and Multiple Illuminant Estimation Using Convolutional Neural Networks",
        "authors": [
            "Simone Bianco",
            "Claudio Cusano",
            "Raimondo Schettini"
        ],
        "abstract": "In this paper we present a method for the estimation of the color of the illuminant in RAW images. The method includes a Convolutional Neural Network that has been specially designed to produce multiple local estimates. A multiple illuminant detector determines whether or not the local outputs of the network must be aggregated into a single estimate. We evaluated our method on standard datasets with single and multiple illuminants, obtaining lower estimation errors with respect to those obtained by other general purpose methods in the state of the art.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01057",
        "title": "On the convergence of the sparse possibilistic c-means algorithm",
        "authors": [
            "Spyridoula D. Xenaki",
            "Konstantinos D. Koutroumbas",
            "Athanasios A. Rontogiannis"
        ],
        "abstract": "In this paper, a convergence proof for the recently proposed sparse possibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated Zangwill convergence theorem. It is shown that the iterative sequence generated by SPCM converges to a stationary point or there exists a subsequence of it that converges to a stationary point of the cost function of the algorithm.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2017-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01081",
        "title": "Detection of Critical Number of People in Interlocked Doors for Security Access Control by Exploiting a Microwave Transceiver-Array",
        "authors": [
            "Paolo Nesi",
            "Gianni Pantaleo"
        ],
        "abstract": "Counting the number of people is something many security application focus on, when dealing with controlling accesses in restricted areas, as it occurs with banks, airports, railway stations and governmental offices. This paper presents an automated solution for detecting the presence of more than one person into interlocked doors adopted in many accesses. In most cases, interlocked doors are small areas where other pieces of information and sensors are placed in order to detect the presence of guns, explosive, etc. The general goals and the required environmental condition, allowed us to implement a detection system at lower costs and complexity, with respect to other existing techniques. The system consists of a fixed array of microwave transceiver modules, whose received signals are processed to collect information related to a sort of volume occupied in the interlocked door cabin. The proposed solution has been statistically validated by using statistical analysis. The whole solution has been also implemented to be used in a real time environment and thus validated against real experimental measures.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01108",
        "title": "Evaluating color texture descriptors under large variations of controlled lighting conditions",
        "authors": [
            "Claudio Cusano",
            "Paolo Napoletano",
            "Raimondo Schettini"
        ],
        "abstract": "The recognition of color texture under varying lighting conditions is still an open issue. Several features have been proposed for this purpose, ranging from traditional statistical descriptors to features extracted with neural networks. Still, it is not completely clear under what circumstances a feature performs better than the others. In this paper we report an extensive comparison of old and new texture features, with and without a color normalization step, with a particular focus on how they are affected by small and large variation in the lighting conditions. The evaluation is performed on a new texture database including 68 samples of raw food acquired under 46 conditions that present single and combined variations of light color, direction and intensity. The database allows to systematically investigate the robustness of texture descriptors across a large range of variations of imaging conditions.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01128",
        "title": "Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning for Anterior Visual Pathway Segmentation",
        "authors": [
            "Awais Mansoor",
            "Juan J. Cerrolaza",
            "Robert A. Avery",
            "Marius G. Linguraru"
        ],
        "abstract": "MRI quantification of cranial nerves such as anterior visual pathway (AVP) in MRI is challenging due to their thin small size, structural variation along its path, and adjacent anatomic structures. Segmentation of pathologically abnormal optic nerve (e.g. optic nerve glioma) poses additional challenges due to changes in its shape at unpredictable locations. In this work, we propose a partitioned joint statistical shape model approach with sparse appearance learning for the segmentation of healthy and pathological AVP. Our main contributions are: (1) optimally partitioned statistical shape models for the AVP based on regional shape variations for greater local flexibility of statistical shape model; (2) refinement model to accommodate pathological regions as well as areas of subtle variation by training the model on-the-fly using the initial segmentation obtained in (1); (3) hierarchical deformable framework to incorporate scale information in partitioned shape and appearance models. Our method, entitled PAScAL (PArtitioned Shape and Appearance Learning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) from pediatric patients (ages 2-17). The experimental results show that the proposed localized shape and sparse appearance-based learning approach significantly outperforms segmentation approaches in the analysis of pathological data.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01158",
        "title": "Socially Constrained Structural Learning for Groups Detection in Crowd",
        "authors": [
            "Francesco Solera",
            "Simone Calderara",
            "Rita Cucchiara"
        ],
        "abstract": "Modern crowd theories agree that collective behavior is the result of the underlying interactions among small groups of individuals. In this work, we propose a novel algorithm for detecting social groups in crowds by means of a Correlation Clustering procedure on people trajectories. The affinity between crowd members is learned through an online formulation of the Structural SVM framework and a set of specifically designed features characterizing both their physical and social identity, inspired by Proxemic theory, Granger causality, DTW and Heat-maps. To adhere to sociological observations, we introduce a loss function (G-MITRE) able to deal with the complexity of evaluating group detection performances. We show our algorithm achieves state-of-the-art results when relying on both ground truth trajectories and tracklets previously extracted by available detector/tracker systems.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01176",
        "title": "HFirst: A Temporal Approach to Object Recognition",
        "authors": [
            "Garrick Orchard",
            "Cedric Meyer",
            "Ralph Etienne-Cummings",
            "Christoph Posch",
            "Nitish Thakor",
            "Ryad Benosman"
        ],
        "abstract": "This paper introduces a spiking hierarchical model for object recognition which utilizes the precise timing information inherently present in the output of biologically inspired asynchronous Address Event Representation (AER) vision sensors. The asynchronous nature of these systems frees computation and communication from the rigid predetermined timing enforced by system clocks in conventional systems. Freedom from rigid timing constraints opens the possibility of using true timing to our advantage in computation. We show not only how timing can be used in object recognition, but also how it can in fact simplify computation. Specifically, we rely on a simple temporal-winner-take-all rather than more computationally intensive synchronous operations typically used in biologically inspired neural networks for object recognition. This approach to visual computation represents a major paradigm shift from conventional clocked systems and can find application in other sensory modalities and computational tasks. We showcase effectiveness of the approach by achieving the highest reported accuracy to date (97.5\\%$\\pm$3.5\\%) for a previously published four class card pip recognition task and an accuracy of 84.9\\%$\\pm$1.9\\% for a new more difficult 36 class character recognition task.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01244",
        "title": "TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile Tablets",
        "authors": [
            "Qiong Huang",
            "Ashok Veeraraghavan",
            "Ashutosh Sabharwal"
        ],
        "abstract": "We study gaze estimation on tablets, our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet is not constrained. We collected the first large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. Driven by our observations on the collected data, we present a TabletGaze algorithm for automatic gaze estimation using multi-level HoG feature and Random Forests regressor. The TabletGaze algorithm achieves a mean error of 3.17 cm. We perform extensive evaluation on the impact of various factors such as dataset size, race, wearing glasses and user posture on the gaze estimation accuracy and make important observations about the impact of these factors.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2016-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01292",
        "title": "Compact Convolutional Neural Network Cascade for Face Detection",
        "authors": [
            "Ilya Kalinovskii",
            "Vladimir Spitsyn"
        ],
        "abstract": "The problem of faces detection in images or video streams is a classical problem of computer vision. The multiple solutions of this problem have been proposed, but the question of their optimality is still open. Many algorithms achieve a high quality face detection, but at the cost of high computational complexity. This restricts their application in the real-time systems. This paper presents a new solution of the frontal face detection problem based on compact convolutional neural networks cascade. The test results on FDDB dataset show that it is competitive with state-of-the-art algorithms. This proposed detector is implemented using three technologies: SSE/AVX/AVX2 instruction sets for Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach considerably exceeds all the existing CPU-based and GPU-based algorithms. Because of high computational efficiency, our detector can processing 4K Ultra HD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy Bridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension 60x60 pixels or higher. At the same time its performance weakly dependent on the background and number of objects in scene. This is achieved by the asynchronous computation of stages in the cascade.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01308",
        "title": "Collaborative Total Variation: A General Framework for Vectorial TV Models",
        "authors": [
            "Joan Duran",
            "Michael Moeller",
            "Catalina Sbert",
            "Daniel Cremers"
        ],
        "abstract": "Even after over two decades, the total variation (TV) remains one of the most popular regularizations for image processing problems and has sparked a tremendous amount of research, particularly to move from scalar to vector-valued functions. In this paper, we consider the gradient of a color image as a three dimensional matrix or tensor with dimensions corresponding to the spatial extend, the differences to other pixels, and the spectral channels. The smoothness of this tensor is then measured by taking different norms along the different dimensions. Depending on the type of these norms one obtains very different properties of the regularization, leading to novel models for color images. We call this class of regularizations collaborative total variation (CTV). On the theoretical side, we characterize the dual norm, the subdifferential and the proximal mapping of the proposed regularizers. We further prove, with the help of the generalized concept of singular vectors, that an $\\ell^{\\infty}$ channel coupling makes the most prior assumptions and has the greatest potential to reduce color artifacts. Our practical contributions consist of an extensive experimental section where we compare the performance of a large number of collaborative TV methods for inverse problems like denoising, deblurring and inpainting.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01521",
        "title": "Automatic 3D Liver Segmentation Using Sparse Representation of Global and Local Image Information via Level Set Formulation",
        "authors": [
            "Saif Dawood Salman Al-Shaikhli",
            "Michael Ying Yang",
            "Bodo Rosenhahn"
        ],
        "abstract": "In this paper, a novel framework for automated liver segmentation via a level set formulation is presented. A sparse representation of both global (region-based) and local (voxel-wise) image information is embedded in a level set formulation to innovate a new cost function. Two dictionaries are build: A region-based feature dictionary and a voxel-wise dictionary. These dictionaries are learned, using the K-SVD method, from a public database of liver segmentation challenge (MICCAI-SLiver07). The learned dictionaries provide prior knowledge to the level set formulation. For the quantitative evaluation, the proposed method is evaluated using the testing data of MICCAI-SLiver07 database. The results are evaluated using different metric scores computed by the challenge organizers. The experimental results demonstrate the superiority of the proposed framework by achieving the highest segmentation accuracy (79.6\\%) in comparison to the state-of-the-art methods.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01667",
        "title": "Places205-VGGNet Models for Scene Recognition",
        "authors": [
            "Limin Wang",
            "Sheng Guo",
            "Weilin Huang",
            "Yu Qiao"
        ],
        "abstract": "VGGNets have turned out to be effective for object recognition in still images. However, it is unable to yield good performance by directly adapting the VGGNet models trained on the ImageNet dataset for scene recognition. This report describes our implementation of training the VGGNets on the large-scale Places205 dataset. Specifically, we train three VGGNet models, namely VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe toolbox with high computational efficiency. We verify the performance of trained Places205-VGGNet models on three datasets: MIT67, SUN397, and Places205. Our trained models achieve the state-of-the-art performance on these datasets and are made public available.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2015-08-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01722",
        "title": "Unconstrained Face Verification using Deep CNN Features",
        "authors": [
            "Jun-Cheng Chen",
            "Vishal M. Patel",
            "Rama Chellappa"
        ],
        "abstract": "In this paper, we present an algorithm for unconstrained face verification based on deep convolutional features and evaluate it on the newly released IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the traditional Labeled Face in the Wild (LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network (DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the IJB-A dataset are provided.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01859",
        "title": "Simulation of optical flow and fuzzy based obstacle avoidance system for mobile robots",
        "authors": [
            "G.D. Illeperuma",
            "D.U.J. Sonnadara"
        ],
        "abstract": "Honey bees use optical flow to avoid obstacles effectively. In this research work similar methodology was tested on a simulated mobile robot. Simulation framework was based on VRML and Simulink in a 3D world. Optical flow vectors were calculated from a video scene captured by a virtual camera which was used as inputs to a fuzzy logic controller. Fuzzy logic controller decided the locomotion of the robot. Different fuzzy logic rules were evaluated. The robot was able to navigate through complex static and dynamic environments effectively, avoiding obstacles on its path.\n    ",
        "submission_date": "2015-08-08T00:00:00",
        "last_modified_date": "2015-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01887",
        "title": "Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning in Hierarchy",
        "authors": [
            "Zhanglin Peng",
            "Ya Li",
            "Zhaoquan Cai",
            "Liang Lin"
        ],
        "abstract": "This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches.\n    ",
        "submission_date": "2015-08-08T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01983",
        "title": "Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance",
        "authors": [
            "Amr Bakry",
            "Mohamed Elhoseiny",
            "Tarek El-Gaaly",
            "Ahmed Elgammal"
        ],
        "abstract": "This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2016-06-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02171",
        "title": "Automatic Extraction of the Passing Strategies of Soccer Teams",
        "authors": [
            "Laszlo Gyarmati",
            "Xavier Anguera"
        ],
        "abstract": "Technology offers new ways to measure the locations of the players and of the ball in sports. This translates to the trajectories the ball takes on the field as a result of the tactics the team applies. The challenge professionals in soccer are facing is to take the reverse path: given the trajectories of the ball is it possible to infer the underlying strategy/tactic of a team? We propose a method based on Dynamic Time Warping to reveal the tactics of a team through the analysis of repeating series of events. Based on the analysis of an entire season, we derive insights such as passing strategies for maintaining ball possession or counter attacks, and passing styles with a focus on the team or on the capabilities of the individual players.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02246",
        "title": "Feature Learning for Interaction Activity Recognition in RGBD Videos",
        "authors": [
            "Ngu Nguyen"
        ],
        "abstract": "This paper proposes a human activity recognition method which is based on features learned from 3D video data without incorporating domain knowledge. The experiments on data collected by RGBD cameras produce results outperforming other techniques. Our feature encoding method follows the bag-of-visual-word model, then we use a SVM classifier to recognise the activities. We do not use skeleton or tracking information and the same technique is applied on color and depth data.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02405",
        "title": "Gait Assessment for Multiple Sclerosis Patients Using Microsoft Kinect",
        "authors": [
            "Farnood Gholami",
            "Daria A. Trojan",
            "Jozsef K\u007fovecses",
            "Wassim M. Haddad",
            "Behnood Gholami"
        ],
        "abstract": "Gait analysis of patients with neurological disorders, including multiple sclerosis (MS), is important for rehabilitation and treatment. The Mircrosoft Kinect sensor, which was developed for motion recognition in gaming applications, is an ideal candidate for an inexpensive system providing the capability for human gait analysis. In this research, we develop a framework to quantify the gait abnormality of MS patients using a Kinect for Windows camera. In addition to the previously introduced gait indices, a novel set of MS gait indices based on the concept of dynamic time warping is introduced. The newly introduced indices can characterize a patient's gait pattern as a whole and quantify a subject's gait distance from the healthy population. We will investigate the correlation of gait indices with the multiple sclerosis walking scale (MSWS) and the clinical ambulation score. This work establishes the feasibility of using the Kinect sensor for clinical gait assessment for MS patients.\n    ",
        "submission_date": "2015-08-10T00:00:00",
        "last_modified_date": "2015-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02496",
        "title": "A Practical Guide to CNNs and Fisher Vectors for Image Instance Retrieval",
        "authors": [
            "Vijay Chandrasekhar",
            "Jie Lin",
            "Olivier Mor\u00e8re",
            "Hanlin Goh",
            "Antoine Veillard"
        ],
        "abstract": "With deep learning becoming the dominant approach in computer vision, the use of representations extracted from Convolutional Neural Nets (CNNs) is quickly gaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global image descriptors for image instance retrieval. While the good performance of CNNs for image classification are unambiguously recognised, which of the two has the upper hand in the image retrieval context is not entirely clear yet. In this work, we propose a comprehensive study that systematically evaluates FVs and CNNs for image retrieval. The first part compares the performances of FVs and CNNs on multiple publicly available data sets. We investigate a number of details specific to each method. For FVs, we compare sparse descriptors based on interest point detectors with dense single-scale and multi-scale variants. For CNNs, we focus on understanding the impact of depth, architecture and training data on retrieval results. Our study shows that no descriptor is systematically better than the other and that performance gains can usually be obtained by using both types together. The second part of the study focuses on the impact of geometrical transformations such as rotations and scale changes. FVs based on interest point detectors are intrinsically resilient to such transformations while CNNs do not have a built-in mechanism to ensure such invariance. We show that performance of CNNs can quickly degrade in presence of rotations while they are far less affected by changes in scale. We then propose a number of ways to incorporate the required invariances in the CNN pipeline. Overall, our work is intended as a reference guide offering practically useful and simply implementable guidelines to anyone looking for state-of-the-art global descriptors best suited to their specific image instance retrieval problem.\n    ",
        "submission_date": "2015-08-11T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02606",
        "title": "InAR:Inverse Augmented Reality",
        "authors": [
            "Hao Hu",
            "Hainan Cui"
        ],
        "abstract": "Augmented reality is the art to seamlessly fuse virtual objects into real ones. In this short note, we address the opposite problem, the inverse augmented reality, that is, given a perfectly augmented reality scene where human is unable to distinguish real objects from virtual ones, how the machine could help do the job. We show by structure from motion (SFM), a simple 3D reconstruction technique from images in computer vision, the real and virtual objects can be easily separated in the reconstructed 3D scene.\n    ",
        "submission_date": "2015-08-11T00:00:00",
        "last_modified_date": "2015-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02844",
        "title": "What is Holding Back Convnets for Detection?",
        "authors": [
            "Bojan Pepik",
            "Rodrigo Benenson",
            "Tobias Ritschel",
            "Bernt Schiele"
        ],
        "abstract": "Convolutional neural networks have recently shown excellent results in general object detection and many other tasks. Albeit very effective, they involve many user-defined design choices. In this paper we want to better understand these choices by inspecting two key aspects \"what did the network learn?\", and \"what can the network learn?\". We exploit new annotations (Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despite common belief, our results indicate that existing state-of-the-art convnet architectures are not invariant to various appearance factors. In fact, all considered networks have similar weak points which cannot be mitigated by simply increasing the training data (architectural changes are needed). We show that overall performance can improve when using image renderings for data augmentation. We report the best known results on the Pascal3D+ detection and view-point estimation tasks.\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02848",
        "title": "Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration",
        "authors": [
            "Yunjin Chen",
            "Thomas Pock"
        ],
        "abstract": "Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (\\ie, linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD -- \\textit{Trainable Nonlinear Reaction Diffusion}. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2016-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02959",
        "title": "Mountain Peak Detection in Online Social Media",
        "authors": [
            "Roman Fedorov"
        ],
        "abstract": "We present a system for the classification of mountain panoramas from user-generated photographs followed by identification and extraction of mountain peaks from those panoramas. We have developed an automatic technique that, given as input a geo-tagged photograph, estimates its FOV (Field Of View) and the direction of the camera using a matching algorithm on the photograph edge maps and a rendered view of the mountain silhouettes that should be seen from the observer's point of view. The extraction algorithm then identifies the mountain peaks present in the photograph and their profiles. We discuss possible applications in social fields such as photograph peak tagging on social portals, augmented reality on mobile devices when viewing a mountain panorama, and generation of collective intelligence systems (such as environmental models) from massive social media collections (e.g. snow water availability maps based on mountain peak states extracted from photograph hosting services).\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02977",
        "title": "A massively parallel multi-level approach to a domain decomposition method for the optical flow estimation with varying illumination",
        "authors": [
            "Diane Gilliocq-Hirtz",
            "Zakaria Belhachmi"
        ],
        "abstract": "We consider a variational method to solve the optical flow problem with varying illumination. We apply an adaptive control of the regularization parameter which allows us to preserve the edges and fine features of the computed flow. To reduce the complexity of the estimation for high resolution images and the time of computations, we implement a multi-level parallel approach based on the domain decomposition with the Schwarz overlapping method. The second level of parallelism uses the massively parallel solver MUMPS. We perform some numerical simulations to show the efficiency of our approach and to validate it on classical and real-world image sequences.\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03269",
        "title": "A New Approach to an Old Problem: The Reconstruction of a Go Game through a Series of Photographs",
        "authors": [
            "Mario Corsolini",
            "Andrea Carta"
        ],
        "abstract": "Given a series of photographs taken during a Go game, we describe the techniques we successfully employ for pinpointing the grid lines of the Go board and for tracking their small movements between consecutive photographs; then we discuss how to approximate the location and orientation of the observer's point of view, in order to compensate for projection effects. Finally we describe the different criteria that jointly form the algorithm for stones' detection, thus enabling us to automatically reconstruct the whole move sequence.\n    ",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03422",
        "title": "Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data",
        "authors": [
            "Salman H. Khan",
            "Munawar Hayat",
            "Mohammed Bennamoun",
            "Ferdous Sohel",
            "Roberto Togneri"
        ],
        "abstract": "Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multi-class problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2017-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03498",
        "title": "Lensless Compressive Imaging",
        "authors": [
            "Xin Yuan",
            "Hong Jiang",
            "Gang Huang",
            "Paul Wilford"
        ],
        "abstract": "We develop a lensless compressive imaging architecture, which consists of an aperture assembly and a single sensor, without using any lens. An anytime algorithm is proposed to reconstruct images from the compressive measurements; the algorithm produces a sequence of solutions that monotonically converge to the true signal (thus, anytime). The algorithm is developed based on the sparsity of local overlapping patches (in the transformation domain) and state-of-the-art results have been obtained. Experiments on real data demonstrate that encouraging results are obtained by measuring about 10% (of the image pixels) compressive measurements. The reconstruction results of the proposed algorithm are compared with the JPEG compression (based on file sizes) and the reconstructed image quality is close to the JPEG compression, in particular at a high compression rate.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03710",
        "title": "A Novel Approach For Finger Vein Verification Based on Self-Taught Learning",
        "authors": [
            "Mohsen Fayyaz",
            "Masoud PourReza",
            "Mohammad Hajizadeh Saffar",
            "Mohammad Sabokrou",
            "Mahmood Fathy"
        ],
        "abstract": "In this paper, we propose a method for user Finger Vein Authentication (FVA) as a biometric system. Using the discriminative features for classifying theses finger veins is one of the main tips that make difference in related works, Thus we propose to learn a set of representative features, based on autoencoders. We model the user finger vein using a Gaussian distribution. Experimental results show that our algorithm perform like a state-of-the-art on SDUMLA-HMT benchmark.\n    ",
        "submission_date": "2015-08-15T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03755",
        "title": "Beat-Event Detection in Action Movie Franchises",
        "authors": [
            "Danila Potapov",
            "Matthijs Douze",
            "Jerome Revaud",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "While important advances were recently made towards temporally localizing and recognizing specific human actions or activities in videos, efficient detection and classification of long video chunks belonging to semantically defined categories such as \"pursuit\" or \"romance\" remains ",
        "submission_date": "2015-08-15T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03881",
        "title": "Pose-Guided Human Parsing with Deep Learned Features",
        "authors": [
            "Fangting Xia",
            "Jun Zhu",
            "Peng Wang",
            "Alan Yuille"
        ],
        "abstract": "Parsing human body into semantic regions is crucial to human-centric analysis. In this paper, we propose a segment-based parsing pipeline that explores human pose information, i.e. the joint location of a human model, which improves the part proposal, accelerates the inference and regularizes the parsing process at the same time. Specifically, we first generate part segment proposals with respect to human joints predicted by a deep model, then part- specific ranking models are trained for segment selection using both pose-based features and deep-learned part potential features. Finally, the best ensemble of the proposed part segments are inferred though an And-Or Graph.\n",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03928",
        "title": "LCNN: Low-level Feature Embedded CNN for Salient Object Detection",
        "authors": [
            "Hongyang Li",
            "Huchuan Lu",
            "Zhe Lin",
            "Xiaohui Shen",
            "Brian Price"
        ],
        "abstract": "In this paper, we propose a novel deep neural network framework embedded with low-level features (LCNN) for salient object detection in complex images. We utilise the advantage of convolutional neural networks to automatically learn the high-level features that capture the structured information and semantic context in the image. In order to better adapt a CNN model into the saliency task, we redesign the network architecture based on the small-scale datasets. Several low-level features are extracted, which can effectively capture contrast and spatial information in the salient regions, and incorporated to compensate with the learned high-level features at the output of the last fully connected layer. The concatenated feature vector is further fed into a hinge-loss SVM detector in a joint discriminative learning manner and the final saliency score of each region within the bounding box is obtained by the linear combination of the detector's weights. Experiments on three challenging benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be effective and superior than most low-level oriented state-of-the-arts in terms of P-R curves, F-measure and mean absolute errors.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03929",
        "title": "Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition",
        "authors": [
            "Saeed Reza Kheradpisheh",
            "Masoud Ghodrati",
            "Mohammad Ganjtabesh",
            "Timoth\u00e9e Masquelier"
        ],
        "abstract": "Deep convolutional neural networks (DCNNs) have attracted much attention recently, and have shown to be able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Yet it is unknown whether DCNNs match human performance at the task of view-invariant object recognition, whether they make similar errors and use similar representations for this task, and whether the answers depend on the magnitude of the viewpoint variations. To investigate these issues, we benchmarked eight state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and compared their results to those of humans with backward masking. Unlike in all previous DCNN studies, we carefully controlled the magnitude of the viewpoint variations to demonstrate that shallow nets can outperform deep nets and humans when variations are weak. When facing larger variations, however, more layers were needed to match human performance and error distributions, and to have representations that are consistent with human behavior. A very deep net with 18 layers even outperformed humans at the highest variation level, using the most human-like representations.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2016-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03953",
        "title": "Sense Beyond Expressions: Cuteness",
        "authors": [
            "Kang Wang",
            "Tam V. Nguyen",
            "Jiashi Feng",
            "Jose Sepulveda"
        ],
        "abstract": "With the development of Internet culture, cuteness has become a popular concept. Many people are curious about what factors making a person look cute. However, there is rare research to answer this interesting question. In this work, we construct a dataset of personal images with comprehensively annotated cuteness scores and facial attributes to investigate this high-level concept in depth. Based on this dataset, through an automatic attributes mining process, we find several critical attributes determining the cuteness of a person. We also develop a novel Continuous Latent Support Vector Machine (C-LSVM) method to predict the cuteness score of one person given only his image. Extensive evaluations validate the effectiveness of the proposed method for cuteness prediction.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04028",
        "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze Classification",
        "authors": [
            "Lex Fridman",
            "Joonbum Lee",
            "Bryan Reimer",
            "Trent Victor"
        ],
        "abstract": "Accurate, robust, inexpensive gaze tracking in the car can help keep a driver safe by facilitating the more effective study of how to improve (1) vehicle interfaces and (2) the design of future Advanced Driver Assistance Systems. In this paper, we estimate head pose and eye pose from monocular video using methods developed extensively in prior work and ask two new interesting questions. First, how much better can we classify driver gaze using head and eye pose versus just using head pose? Second, are there individual-specific gaze strategies that strongly correlate with how much gaze classification improves with the addition of eye pose information? We answer these questions by evaluating data drawn from an on-road study of 40 drivers. The main insight of the paper is conveyed through the analogy of an \"owl\" and \"lizard\" which describes the degree to which the eyes and the head move when shifting gaze. When the head moves a lot (\"owl\"), not much classification improvement is attained by estimating eye pose on top of head pose. On the other hand, when the head stays still and only the eyes move (\"lizard\"), classification accuracy increases significantly from adding in eye pose. We characterize how that accuracy varies between people, gaze strategies, and gaze regions.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2016-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04035",
        "title": "A Generative Model for Multi-Dialect Representation",
        "authors": [
            "Emmanuel N. Osegi"
        ],
        "abstract": "In the era of deep learning several unsupervised models have been developed to capture the key features in unlabeled handwritten data. Popular among them is the Restricted Boltzmann Machines RBM. However, due to the novelty in handwritten multidialect data, the RBM may fail to generate an efficient representation. In this paper we propose a generative model, the Mode Synthesizing Machine MSM for on-line representation of real life handwritten multidialect language data. The MSM takes advantage of the hierarchical representation of the modes of a data distribution using a two-point error update to learn a sequence of representative multidialects in a generative way. Experiments were performed to evaluate the performance of the MSM over the RBM with the former attaining much lower error values than the latter on both independent and mixed data set.\n    ",
        "submission_date": "2015-08-17T00:00:00",
        "last_modified_date": "2015-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04190",
        "title": "Action Recognition based on Subdivision-Fusion Model",
        "authors": [
            "Hao Zongbo",
            "Lu Linlin",
            "Zhang Qianni",
            "Wu Jie",
            "Izquierdo Ebroul",
            "Yang Juanyu",
            "Zhao Jun"
        ],
        "abstract": "This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize human actions. In most action recognition tasks, overlapping feature distribution is a common problem leading to overfitting. In the subdivision stage of the proposed SFM, samples in each category are clustered. Then, such samples are grouped into multiple more concentrated subcategories. Boundaries for the subcategories are easier to find and as consequence overfitting is avoided. In the subsequent fusion stage, the multi-subcategories classification results are converted back to the original category recognition problem. Two methods to determine the number of clusters are provided. The proposed model has been thoroughly tested with four popular datasets. In the Hollywood2 dataset, an accuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of 64.3%. The performance on the YouTube Action dataset has been improved from 75.8% to 82.5%, while considerably improvements are also observed on the KTH and UCF50 datasets.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04198",
        "title": "Low Rank Representation on Riemannian Manifold of Square Root Densities",
        "authors": [
            "Yifan Fu",
            "Junbin Gao",
            "Xia Hong",
            "David Tien"
        ],
        "abstract": "In this paper, we present a novel low rank representation (LRR) algorithm for data lying on the manifold of square root densities. Unlike traditional LRR methods which rely on the assumption that the data points are vectors in the Euclidean space, our new algorithm is designed to incorporate the intrinsic geometric structure and geodesic distance of the manifold. Experiments on several computer vision datasets showcase its noise robustness and superior performance on classification and subspace clustering compared to other state-of-the-art approaches.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04224",
        "title": "Image tag completion by local learning",
        "authors": [
            "Jingyan Wang",
            "Yihua Zhou",
            "Haoxiang Wang",
            "Xiaohong Yang",
            "Feng Yang",
            "Austin Peterson"
        ],
        "abstract": "The problem of tag completion is to learn the missing tags of an image. In this paper, we propose to learn a tag scoring vector for each image by local linear learning. A local linear function is used in the neighborhood of each image to predict the tag scoring vectors of its neighboring images. We construct a unified objective function for the learning of both tag scoring vectors and local linear function parame- ters. In the objective, we impose the learned tag scoring vectors to be consistent with the known associations to the tags of each image, and also minimize the prediction error of each local linear function, while reducing the complexity of each local function. The objective function is optimized by an alternate optimization strategy and gradient descent methods in an iterative algorithm. We compare the proposed algorithm against different state-of-the-art tag completion methods, and the results show its advantages.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04238",
        "title": "Preprint ARPPS Augmented Reality Pipeline Prospect System",
        "authors": [
            "Xiaolei Zhang",
            "Yong Han",
            "DongSheng Hao",
            "Zhihan Lv"
        ],
        "abstract": "This is the preprint version of our paper on ICONIP. Outdoor augmented reality geographic information system (ARGIS) is the hot application of augmented reality over recent years. This paper concludes the key solutions of ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS), and respectively realizes the machine vision based pipeline prospect system (MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the MVBPPS's realization, this paper studies the neural network based 3D features matching method.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04326",
        "title": "Cascade Learning by Optimally Partitioning",
        "authors": [
            "Yanwei Pang",
            "Jiale Cao",
            "Xuelong Li"
        ],
        "abstract": "Cascaded AdaBoost classifier is a well-known efficient object detection algorithm. The cascade structure has many parameters to be determined. Most of existing cascade learning algorithms are designed by assigning detection rate and false positive rate to each stage either dynamically or statically. Their objective functions are not directly related to minimum computation cost. These algorithms are not guaranteed to have optimal solution in the sense of minimizing computation cost. On the assumption that a strong classifier is given, in this paper we propose an optimal cascade learning algorithm (we call it iCascade) which iteratively partitions the strong classifiers into two parts until predefined number of stages are generated. iCascade searches the optimal number ri of weak classifiers of each stage i by directly minimizing the computation cost of the cascade. Theorems are provided to guarantee the existence of the unique optimal solution. Theorems are also given for the proposed efficient algorithm of searching optimal parameters ri. Once a new stage is added, the parameter ri for each stage decreases gradually as iteration proceeds, which we call decreasing phenomenon. Moreover, with the goal of minimizing computation cost, we develop an effective algorithm for setting the optimal threshold of each stage classifier. In addition, we prove in theory why more new weak classifiers are required compared to the last stage. Experimental results on face detection demonstrate the effectiveness and efficiency of the proposed algorithm.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04389",
        "title": "A Deep Pyramid Deformable Part Model for Face Detection",
        "authors": [
            "Rajeev Ranjan",
            "Vishal M. Patel",
            "Rama Chellappa"
        ],
        "abstract": "We present a face detection algorithm based on Deformable Part Models and deep pyramidal features. The proposed method called DP2MFD is able to detect faces of various sizes and poses in unconstrained conditions. It reduces the gap in training and testing of DPM on deep features by adding a normalization layer to the deep convolutional neural network (CNN). Extensive experiments on four publicly available unconstrained face detection datasets show that our method is able to capture the meaningful structure of faces and performs significantly better than many competitive face detection algorithms.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04458",
        "title": "Multiresolution Approach to Acceleration of Iterative Image Reconstruction for X-Ray Imaging for Security Applications",
        "authors": [
            "S. Degirmenci",
            "Joseph A. O'Sullivan",
            "David G. Politte"
        ],
        "abstract": "Three-dimensional x-ray CT image reconstruction in baggage scanning in security applications is an important research field. The variety of materials to be reconstructed is broader than medical x-ray imaging. Presence of high attenuating materials such as metal may cause artifacts if analytical reconstruction methods are used. Statistical modeling and the resultant iterative algorithms are known to reduce these artifacts and present good quantitative accuracy in estimates of linear attenuation coefficients. However, iterative algorithms may require computations in order to achieve quantitatively accurate results. For the case of baggage scanning, in order to provide fast accurate inspection throughput, they must be accelerated drastically. There are many approaches proposed in the literature to increase speed of convergence. This paper presents a new method that estimates the wavelet coefficients of the images in the discrete wavelet transform domain instead of the image space itself. Initially, surrogate functions are created around approximation coefficients only. As the iterations proceed, the wavelet tree on which the updates are made is expanded based on a criterion and detail coefficients at each level are updated and the tree is expanded this way. For example, in the smooth regions of the image the detail coefficients are not updated while the coefficients that represent the high-frequency component around edges are being updated, thus saving time by focusing computations where they are needed. This approach is implemented on real data from a SureScan (TM) x1000 Explosive Detection System and compared to straightforward implementation of the unregularized alternating minimization of O'Sullivan and Benac [1].\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04467",
        "title": "Robust Subspace Clustering via Smoothed Rank Approximation",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Qiang Cheng"
        ],
        "abstract": "Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04535",
        "title": "Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification",
        "authors": [
            "Ruimao Zhang",
            "Liang Lin",
            "Rui Zhang",
            "Wangmeng Zuo",
            "Lei Zhang"
        ],
        "abstract": "Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately, e.g., learning hash functions from a predefined hand-crafted feature space. Meanwhile, the bit lengths of output hashing codes are preset in most previous methods, neglecting the significance level of different bits and restricting their practical flexibility. To address these issues, we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically, we organize the training images into a batch of triplet samples, each sample containing two images with the same label and one with a different label. With these triplet samples, we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition, a regularization term is introduced to enforce the adjacency consistency, i.e., images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion, where discriminative image features and hash functions are simultaneously optimized. Furthermore, each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04546",
        "title": "Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images",
        "authors": [
            "Alexander Krull",
            "Eric Brachmann",
            "Frank Michel",
            "Michael Ying Yang",
            "Stefan Gumhold",
            "Carsten Rother"
        ],
        "abstract": "Analysis-by-synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that \"learns to compare\", while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares an observed and rendered image. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects, and it can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04586",
        "title": "Saliency maps on image hierarchies",
        "authors": [
            "Ver\u00f3nica Vilaplana"
        ],
        "abstract": "In this paper we propose two saliency models for salient object segmentation based on a hierarchical image segmentation, a tree-like structure that represents regions at different scales from the details to the whole image (e.g. gPb-UCM, BPT). The first model is based on a hierarchy of image partitions. The saliency at each level is computed on a region basis, taking into account the contrast between regions. The maps obtained for the different partitions are then integrated into a final saliency map. The second model directly works on the structure created by the segmentation algorithm, computing saliency at each node and integrating these cues in a straightforward manner into a single saliency map. We show that the proposed models produce high quality saliency maps. Objective evaluation demonstrates that the two methods achieve state-of-the-art performance in several benchmark datasets.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04785",
        "title": "Who are the Devils Wearing Prada in New York City?",
        "authors": [
            "KuanTing Chen",
            "Kezhen Chen",
            "Peizhong Cong",
            "Winston H. Hsu",
            "Jiebo Luo"
        ],
        "abstract": "Fashion is a perpetual topic in human social life, and the mass has the penchant to emulate what large city residents and celebrities wear. Undeniably, New York City is such a bellwether large city with all kinds of fashion leadership. Consequently, to study what the fashion trends are during this year, it is very helpful to learn the fashion trends of New York City. Discovering fashion trends in New York City could boost many applications such as clothing recommendation and advertising. Does the fashion trend in the New York Fashion Show actually influence the clothing styles on the public? To answer this question, we design a novel system that consists of three major components: (1) constructing a large dataset from the New York Fashion Shows and New York street chic in order to understand the likely clothing fashion trends in New York, (2) utilizing a learning-based approach to discover fashion attributes as the representative characteristics of fashion trends, and (3) comparing the analysis results from the New York Fashion Shows and street-chic images to verify whether the fashion shows have actual influence on the people in New York City. Through the preliminary experiments over a large clothing dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on fashion trends and fashion influence.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04843",
        "title": "Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection",
        "authors": [
            "Kisuk Lee",
            "Aleksandar Zlateski",
            "Ashwin Vishwanathan",
            "H. Sebastian Seung"
        ],
        "abstract": "Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04887",
        "title": "Multi-criteria Similarity-based Anomaly Detection using Pareto Depth Analysis",
        "authors": [
            "Ko-Jen Hsiao",
            "Kevin S. Xu",
            "Jeff Calder",
            "Alfred O. Hero III"
        ],
        "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. Similarity-based anomaly detection algorithms detect abnormally large amounts of similarity or dissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between a test sample and the training samples. In many application domains there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such cases, multiple dissimilarity measures can be defined, including non-metric measures, and one can test for anomalies by scalarizing using a non-negative linear combination of them. If the relative importance of the different dissimilarity measures are not known in advance, as in many anomaly detection applications, the anomaly detection algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we propose a method for similarity-based anomaly detection using a novel multi-criteria dissimilarity measure, the Pareto depth. The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach is provably better than using linear combinations of the criteria and shows superior performance on experiments with synthetic and real data sets.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04945",
        "title": "DeepWriterID: An End-to-end Online Text-independent Writer Identification System",
        "authors": [
            "Weixin Yang",
            "Lianwen Jin",
            "Manfei Liu"
        ],
        "abstract": "Owing to the rapid growth of touchscreen mobile terminals and pen-based interfaces, handwriting-based writer identification systems are attracting increasing attention for personal authentication, digital forensics, and other applications. However, most studies on writer identification have not been satisfying because of the insufficiency of data and difficulty of designing good features under various conditions of handwritings. Hence, we introduce an end-to-end system, namely DeepWriterID, employed a deep convolutional neural network (CNN) to address these problems. A key feature of DeepWriterID is a new method we are proposing, called DropSegment. It designs to achieve data augmentation and improve the generalized applicability of CNN. For sufficient feature representation, we further introduce path signature feature maps to improve performance. Experiments were conducted on the NLPR handwriting database. Even though we only use pen-position information in the pen-down state of the given handwriting samples, we achieved new state-of-the-art identification rates of 95.72% for Chinese text and 98.51% for English text.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04955",
        "title": "Introducing Geometry in Active Learning for Image Segmentation",
        "authors": [
            "Ksenia Konyushkova",
            "Raphael Sznitman",
            "Pascal Fua"
        ],
        "abstract": "We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04981",
        "title": "High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging",
        "authors": [
            "Changsoo Je",
            "Sang Wook Lee",
            "Rae-Hong Park"
        ],
        "abstract": "For structured-light range imaging, color stripes can be used for increasing the number of distinguishable light patterns compared to binary BW stripes. Therefore, an appropriate use of color patterns can reduce the number of light projections and range imaging is achievable in single video frame or in \"one shot\". On the other hand, the reliability and range resolution attainable from color stripes is generally lower than those from multiply projected binary BW patterns since color contrast is affected by object color reflectance and ambient light. This paper presents new methods for selecting stripe colors and designing multiple-stripe patterns for \"one-shot\" and \"two-shot\" imaging. We show that maximizing color contrast between the stripes in one-shot imaging reduces the ambiguities resulting from colored object surfaces and limitations in sensor/projector resolution. Two-shot imaging adds an extra video frame and maximizes the color contrast between the first and second video frames to diminish the ambiguities even further. Experimental results demonstrate the effectiveness of the presented one-shot and two-shot color-stripe imaging schemes.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05028",
        "title": "Using User Generated Online Photos to Estimate and Monitor Air Pollution in Major Cities",
        "authors": [
            "Yuncheng Li",
            "Jifei Huang",
            "Jiebo Luo"
        ],
        "abstract": "With the rapid development of economy in China over the past decade, air pollution has become an increasingly serious problem in major cities and caused grave public health concerns in China. Recently, a number of studies have dealt with air quality and air pollution. Among them, some attempt to predict and monitor the air quality from different sources of information, ranging from deployed physical sensors to social media. These methods are either too expensive or unreliable, prompting us to search for a novel and effective way to sense the air quality. In this study, we propose to employ the state of the art in computer vision techniques to analyze photos that can be easily acquired from online social media. Next, we establish the correlation between the haze level computed directly from photos with the official PM 2.5 record of the taken city at the taken time. Our experiments based on both synthetic and real photos have shown the promise of this image-based approach to estimating and monitoring air pollution.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05038",
        "title": "Seeing Behind the Camera: Identifying the Authorship of a Photograph",
        "authors": [
            "Christopher Thomas",
            "Adriana Kovashka"
        ],
        "abstract": "We introduce the novel problem of identifying the photographer behind a photograph. To explore the feasibility of current computer vision techniques to address this problem, we created a new dataset of over 180,000 images taken by 41 well-known photographers. Using this dataset, we examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer. We also trained a new deep convolutional neural network for this task. Our results show that high-level features greatly outperform low-level features. We provide qualitative results using these learned models that give insight into our method's ability to distinguish between photographers, and allow us to draw interesting conclusions about what specific photographers shoot. We also demonstrate two applications of our method.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2016-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05046",
        "title": "Improving Image Restoration with Soft-Rounding",
        "authors": [
            "Xing Mei",
            "Honggang Qi",
            "Bao-Gang Hu",
            "Siwei Lyu"
        ],
        "abstract": "Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05151",
        "title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation",
        "authors": [
            "Christian Bailer",
            "Bertram Taetz",
            "Didier Stricker"
        ],
        "abstract": "Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05306",
        "title": "Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification",
        "authors": [
            "Zhen Zuo",
            "Gang Wang",
            "Bing Shuai",
            "Lifan Zhao",
            "Qingxiong Yang"
        ],
        "abstract": "In order to encode the class correlation and class specific information in image representation, we propose a new local feature learning approach named Deep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to hierarchically learn feature transformation filter banks to transform raw pixel image patches to features. The learned filter banks are expected to: (1) encode common visual patterns of a flexible number of categories; (2) encode discriminative information; and (3) hierarchically extract patterns at different visual levels. Particularly, in each single layer of DDSFL, shareable filters are jointly learned for classes which share the similar patterns. Discriminative power of the filters is achieved by enforcing the features from the same category to be close, while features from different categories to be far away from each other. Furthermore, we also propose two exemplar selection methods to iteratively select training data for more efficient and effective learning. Based on the experimental results, DDSFL can achieve very promising performance, and it also shows great complementary effect to the state-of-the-art Caffe features.\n    ",
        "submission_date": "2015-08-21T00:00:00",
        "last_modified_date": "2015-08-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05463",
        "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity",
        "authors": [
            "Mohammad Javad Shafiee",
            "Parthipan Siva",
            "Alexander Wong"
        ],
        "abstract": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances.\n    ",
        "submission_date": "2015-08-22T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05581",
        "title": "Learning Sampling Distributions for Efficient Object Detection",
        "authors": [
            "Yanwei Pang",
            "Jiale Cao",
            "Xuelong Li"
        ],
        "abstract": "Object detection is an important task in computer vision and learning systems. Multistage particle windows (MPW), proposed by Gualdi et al., is an algorithm of fast and accurate object detection. By sampling particle windows from a proposal distribution (PD), MPW avoids exhaustively scanning the image. Despite its success, it is unknown how to determine the number of stages and the number of particle windows in each stage. Moreover, it has to generate too many particle windows in the initialization step and it redraws unnecessary too many particle windows around object-like regions. In this paper, we attempt to solve the problems of MPW. An important fact we used is that there is large probability for a randomly generated particle window not to contain the object because the object is a sparse event relevant to the huge number of candidate windows. Therefore, we design the proposal distribution so as to efficiently reject the huge number of non-object windows. Specifically, we propose the concepts of rejection, acceptance, and ambiguity windows and regions. This contrasts to MPW which utilizes only on region of support. The PD of MPW is acceptance-oriented whereas the PD of our method (called iPW) is rejection-oriented. Experimental results on human and face detection demonstrate the efficiency and effectiveness of the iPW algorithm. The source code is publicly accessible.\n    ",
        "submission_date": "2015-08-23T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05683",
        "title": "Morphometry-Based Longitudinal Neurodegeneration Simulation with MR Imaging",
        "authors": [
            "Siqi Liu",
            "Sidong Liu",
            "Sonia Pujol",
            "Ron Kikinis",
            "Dagan Feng",
            "Michael Fulham",
            "Weidong Cai"
        ],
        "abstract": "We present a longitudinal MR simulation framework which simulates the future neurodegenerative progression by outputting the predicted follow-up MR image and the voxel-based morphometry (VBM) map. This framework expects the patients to have at least 2 historical MR images available. The longitudinal and cross-sectional VBM maps are extracted to measure the affinity between the target subject and the template subjects collected for simulation. Then the follow-up simulation is performed by resampling the latest available target MR image with a weighted sum of non-linear transformations derived from the best-matched templates. The leave-one-out strategy was used to compare different simulation methods. Compared to the state-of-the-art voxel-based method, our proposed morphometry-based simulation achieves better accuracy in most cases.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05704",
        "title": "Iterative Thresholded Bi-Histogram Equalization for Medical Image Enhancement",
        "authors": [
            "Muhammad Ali Qadar",
            "Yan Zhaowen",
            "Li Hua"
        ],
        "abstract": "Enhancement of human vision to get an insight to information content is of vital importance. The traditional histogram equalization methods have been suffering from amplified contrast with the addition of artifacts and a surprising unnatural visibility of the processed images. In order to overcome these drawbacks, this paper proposes interative, mean, and multi-threshold selection criterion with plateau limits, which consist of histogram segmentation, clipping and transformation modules. The histogram partition consists of multiple thresholding processes that divide the histogram into two parts, whereas the clipping process nicely enhances the contrast by having a check on the rate of enhancement that could be tuned. Histogram equalization to each segmented sub-histogram provides the output image with preserved brightness and enhanced contrast. Results of the present study showed that the proposed method efficiently handles the noise amplification. Further, it also preserves the brightness by retaining natural look of targeted image.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05879",
        "title": "Optical images-based edge detection in Synthetic Aperture Radar images",
        "authors": [
            "Gilberto P. Silva Junior",
            "Alejandro C. Frery",
            "Sandra Sandri",
            "Humberto Bustince",
            "Edurne Barrenechea",
            "C\u00e9dric Marco-Detchart"
        ],
        "abstract": "We address the issue of adapting optical images-based edge detection techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery. We modify the gravitational edge detection technique (inspired by the Law of Universal Gravity) proposed by Lopez-Molina et al, using the non-standard neighbourhood configuration proposed by Fu et al, to reduce the speckle noise in polarimetric SAR imagery. We compare the modified and unmodified versions of the gravitational edge detection technique with the well-established one proposed by Canny, as well as with a recent multiscale fuzzy-based technique proposed by Lopez-Molina et Alejandro We also address the issues of aggregation of gray level images before and after edge detection and of filtering. All techniques addressed here are applied to a mosaic built using class distributions obtained from a real scene, as well as to the true PolSAR image; the mosaic results are assessed using Baddeley's Delta Metric. Our experiments show that modifying the gravitational edge detection technique with a non-standard neighbourhood configuration produces better results than the original technique, as well as the other techniques used for comparison. The experiments show that adapting edge detection methods from Computational Intelligence for use in PolSAR imagery is a new field worthy of exploration.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05995",
        "title": "An algorithm for Left Atrial Thrombi detection using Transesophageal Echocardiography",
        "authors": [
            "Jianrui Ding",
            "Min Xian",
            "H.D.Cheng",
            "Yang Li",
            "Fei Xu",
            "Yingtao Zhang"
        ],
        "abstract": "Transesophageal echocardiography (TEE) is widely used to detect left atrium (LA)/left atrial appendage (LAA) thrombi. In this paper, the local binary pattern variance (LBPV) features are extracted from region of interest (ROI). And the dynamic features are formed by using the information of its neighbor frames in the sequence. The sequence is viewed as a bag, and the images in the sequence are considered as the instances. Multiple-instance learning (MIL) method is employed to solve the LAA thrombi detection. The experimental results show that the proposed method can achieve better performance than that by using other methods.\n    ",
        "submission_date": "2015-08-24T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06010",
        "title": "Wavelet subspace decomposition of thermal infrared images for defect detection in artworks",
        "authors": [
            "Muhammad Zubair Ahmad",
            "Amir Ali Khan",
            "Sihem Mezghani",
            "Eric Perrin",
            "Kamel Mouhoubi",
            "Jean-Luc Bodnar",
            "Valeriu Vrabie"
        ],
        "abstract": "Monitoring the health of ancient artworks requires adequate prudence because of the sensitive nature of these materials. Classical techniques for identifying the development of faults rely on acoustic testing. These techniques, being invasive, may result in causing permanent damage to the material, especially if the material is inspected periodically. Non destructive testing has been carried out for different materials since long. In this regard, non-invasive systems were developed based on infrared thermometry principle to identify the faults in artworks. The test artwork is heated and the thermal response of the different layers is captured with the help of a thermal infrared camera. However, prolonged heating risks overheating and thus causing damage to artworks and an alternate approach is to use pseudo-random binary sequence excitations. The faults in the artwork, though, cannot be detected on the captured images, especially if their strength is weak. The weaker faults are either masked by the stronger ones, by the pictorial layer of the artwork or by the non-uniform heating. This work addresses the detection and localization of the faults through a wavelet based subspace decomposition scheme. The proposed scheme, on one hand, allows to remove the background while, on the other hand, removes the undesired high frequency noise. It is shown that the detection parameter is proportional to the diameter and the depth of the fault. A criterion is proposed to select the optimal wavelet basis along with suitable level selection for wavelet decomposition and reconstruction. The proposed approach is tested on a laboratory developed test sample with known fault locations and dimensions as well as real artworks. A comparison with a previously reported method demonstrates the efficacy of the proposed approach for fault detection in artworks.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06073",
        "title": "Cooking in the kitchen: Recognizing and Segmenting Human Activities in Videos",
        "authors": [
            "Hilde Kuehne",
            "Juergen Gall",
            "Thomas Serre"
        ],
        "abstract": "As research on action recognition matures, the focus is shifting away from categorizing basic task-oriented actions using hand-segmented video datasets to understanding complex goal-oriented daily human activities in real-world settings. Temporally structured models would seem obvious to tackle this set of problems, but so far, cases where these models have outperformed simpler unstructured bag-of-word types of models are scarce. With the increasing availability of large human activity datasets, combined with the development of novel feature coding techniques that yield more compact representations, it is time to revisit structured generative approaches.\n",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06163",
        "title": "Accurate Urban Road Centerline Extraction from VHR Imagery via Multiscale Segmentation and Tensor Voting",
        "authors": [
            "Guangliang Cheng",
            "Feiyun Zhu",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "abstract": "It is very useful and increasingly popular to extract accurate road centerlines from very-high-resolution (VHR) re- mote sensing imagery for various applications, such as road map generation and updating etc. There are three shortcomings of current methods: (a) Due to the noise and occlusions (owing to vehicles and trees), most road extraction methods bring in heterogeneous classification results; (b) Morphological thinning algorithm is widely used to extract road centerlines, while it pro- duces small spurs around the centerlines; (c) Many methods are ineffective to extract centerlines around the road intersections. To address the above three issues, we propose a novel method to ex- tract smooth and complete road centerlines via three techniques: the multiscale joint collaborative representation (MJCR) & graph cuts (GC), tensor voting (TV) & non-maximum suppression (NMS) and fitting based connection algorithm. Specifically, a MJCR-GC based road area segmentation method is proposed by incorporating mutiscale features and spatial information. In this way, a homogenous road segmentation result is achieved. Then, to obtain a smooth and correct road centerline network, a TV-NMS based centerline extraction method is introduced. This method not only extracts smooth road centerlines, but also connects the discontinuous road centerlines. Finally, to overcome the ineffectiveness of current methods in the road intersection, a fitting based road centerline connection algorithm is proposed. As a result, we can get a complete road centerline network. Extensive experiments on two datasets demonstrate that our method achieves higher quantitative results, as well as more satisfactory visual performances by comparing with state-of-the- art methods.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06171",
        "title": "BREN: Body Reflection Essence-Neuter Model for Separation of Reflection Components",
        "authors": [
            "Changsoo Je",
            "Hyung-Min Park"
        ],
        "abstract": "We propose a novel reflection color model consisting of body essence and (mixed) neuter, and present an effective method for separating dichromatic reflection components using a single image. Body essence is an entity invariant to interface reflection, and has two degrees of freedom unlike hue and maximum chromaticity. As a result, the proposed method is insensitive to noise and proper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red, green, and blue), contrary to the maximum chromaticity-based methods. Interface reflection is separated by using a Gaussian function, which removes a critical thresholding problem. Furthermore, the method does not require any region segmentation. Experimental results show the efficacy of the proposed model and method.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06464",
        "title": "SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter",
        "authors": [
            "Osamu Hirose",
            "Shotaro Kawaguchi",
            "Terumasa Tokunaga",
            "Yu Toyoshima",
            "Takayuki Teramoto",
            "Sayuri Kuge",
            "Takeshi Ishihara",
            "Yuichi Iino",
            "Ryo Yoshida"
        ],
        "abstract": "Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014).\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2016-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06535",
        "title": "Deep Convolutional Neural Networks for Smile Recognition",
        "authors": [
            "Patrick O. Glauner"
        ],
        "abstract": "This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06576",
        "title": "A Neural Algorithm of Artistic Style",
        "authors": [
            "Leon A. Gatys",
            "Alexander S. Ecker",
            "Matthias Bethge"
        ],
        "abstract": "In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06585",
        "title": "Towards universal neural nets: Gibbs machines and ACE",
        "authors": [
            "Galin Georgiev"
        ],
        "abstract": "We study from a physics viewpoint a class of generative neural nets, Gibbs machines, designed for gradual learning. While including variational auto-encoders, they offer a broader universal platform for incrementally adding newly learned features, including physical symmetries. Their direct connection to statistical physics and information geometry is established. A variational Pythagorean theorem justifies invoking the exponential/Gibbs class of probabilities for creating brand new objects. Combining these nets with classifiers, gives rise to a brand of universal generative neural nets - stochastic auto-classifier-encoders (ACE). ACE have state-of-the-art performance in their class, both for classification and density estimation for the MNIST data set.\n    ",
        "submission_date": "2015-08-26T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06708",
        "title": "Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation",
        "authors": [
            "Sijin Li",
            "Weichen Zhang",
            "Antoni B. Chan"
        ],
        "abstract": "This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06725",
        "title": "Image Type Water Meter Character Recognition Based on Embedded DSP",
        "authors": [
            "Ying Liu",
            "Yan-bin Han",
            "Yu-lin Zhang"
        ],
        "abstract": "In the paper, we combined DSP processor with image processing algorithm and studied the method of water meter character recognition. We collected water meter image through camera at a fixed angle, and the projection method is used to recognize those digital images. The experiment results show that the method can recognize the meter characters accurately and artificial meter reading is replaced by automatic digital recognition, which improves working efficiency.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06728",
        "title": "A Comparative Analysis of Retrieval Techniques In Content Based Image Retrieval",
        "authors": [
            "Mohini P. Sardey",
            "G. K. Kharate"
        ],
        "abstract": "Basic group of visual techniques such as color, shape, texture are used in Content Based Image Retrievals (CBIR) to retrieve query image or subregion of image to find similar images in image database. To improve query result, relevance feedback is used many times in CBIR to help user to express their preference and improve query ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06853",
        "title": "Shopper Analytics: a customer activity recognition system using a distributed RGB-D camera network",
        "authors": [
            "Daniele Liciotti",
            "Marco Contigiani",
            "Emanuele Frontoni",
            "Adriano Mancini",
            "Primo Zingaretti",
            "Valerio Placidi"
        ],
        "abstract": "The aim of this paper is to present an integrated system consisted of a RGB-D camera and a software able to monitor shoppers in intelligent retail environments. We propose an innovative low cost smart system that can understand the shoppers' behavior and, in particular, their interactions with the products in the shelves, with the aim to develop an automatic RGB-D technique for video analysis. The system of cameras detects the presence of people and univocally identifies them. Through the depth frames, the system detects the interactions of the shoppers with the products on the shelf and determines if a product is picked up or if the product is taken and then put back and finally, if there is not contact with the products. The system is low cost and easy to install, and experimental results demonstrated that its performances are satisfactory also in real environments.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07148",
        "title": "Discrete Hashing with Deep Neural Network",
        "authors": [
            "Thanh-Toan Do",
            "Anh-Zung Doan",
            "Ngai-Man Cheung"
        ],
        "abstract": "This paper addresses the problem of learning binary hash codes for large scale image search by proposing a novel hashing method based on deep neural network. The advantage of our deep model over previous deep model used in hashing is that our model contains necessary criteria for producing good codes such as similarity preserving, balance and independence. Another advantage of our method is that instead of relaxing the binary constraint of codes during the learning process as most previous works, in this paper, by introducing the auxiliary variable, we reformulate the optimization into two sub-optimization steps allowing us to efficiently solve binary constraints without any relaxation.\n",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07415",
        "title": "Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via Adaptive Local and Nonlocal Statistical Priors",
        "authors": [
            "Nasser Eslahi",
            "Hami Mahdavinataj",
            "Ali Aghagolzadeh"
        ],
        "abstract": "The motivation of this paper is to introduce a novel framework for the restoration of images corrupted by mixed Gaussian-impulse noise. To this aim, first, an adaptive curvelet thresholding criterion is proposed which tries to adaptively remove the perturbations appeared during denoising process. Then, a new statistical regularization term, called joint adaptive statistical prior (JASP), is established which enforces both the local and nonlocal statistical consistencies, simultaneously, in a unified manner. Furthermore, a novel technique for mixed Gaussian plus impulse noise removal using JASP in a variational scheme is developed--we refer to it as De-JASP. To efficiently solve the above variational scheme, an efficient alternating minimization algorithm based on split Bregman iterative framework is developed. Extensive experimental results manifest the effectiveness of the proposed method comparing with the current state-of-the-art methods in mixed Gaussian-impulse noise removal.\n    ",
        "submission_date": "2015-08-29T00:00:00",
        "last_modified_date": "2015-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07468",
        "title": "Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors",
        "authors": [
            "Yuqing Hou"
        ],
        "abstract": "Tag-based image retrieval (TBIR) has drawn much attention in recent years due to the explosive amount of digital images and crowdsourcing tags. However, TBIR is still suffering from the incomplete and inaccurate tags provided by users, posing a great challenge for tag-based image management applications. In this work, we proposed a novel method for image annotation, incorporating several priors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors. Highly representative CNN feature vectors are adopt to model the tag-visual correlation and narrow the semantic gap. And we extract word vectors for tags to measure similarity between tags in the semantic level, which is more accurate than traditional frequency-based or graph-based methods. We utilize the accelerated proximal gradient (APG) method to solve our model efficiently. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and robustness of the proposed method.\n    ",
        "submission_date": "2015-08-29T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07647",
        "title": "Love Thy Neighbors: Image Annotation by Exploiting Image Metadata",
        "authors": [
            "Justin Johnson",
            "Lamberto Ballan",
            "Fei-Fei Li"
        ],
        "abstract": "Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically, in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata.\n    ",
        "submission_date": "2015-08-30T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07654",
        "title": "Action Recognition by Hierarchical Mid-level Action Elements",
        "authors": [
            "Tian Lan",
            "Yuke Zhu",
            "Amir Roshan Zamir",
            "Silvio Savarese"
        ],
        "abstract": "Realistic videos of human actions exhibit rich spatiotemporal structures at multiple levels of granularity: an action can always be decomposed into multiple finer-grained elements in both space and time. To capture this intuition, we propose to represent videos by a hierarchy of mid-level action elements (MAEs), where each MAE corresponds to an action-related spatiotemporal segment in the video. We introduce an unsupervised method to generate this representation from videos. Our method is capable of distinguishing action-related segments from background segments and representing actions at multiple spatiotemporal resolutions. Given a set of spatiotemporal segments generated from the training data, we introduce a discriminative clustering algorithm that automatically discovers MAEs at multiple levels of granularity. We develop structured models that capture a rich set of spatial, temporal and hierarchical relations among the segments, where the action label and multiple levels of MAE labels are jointly inferred. The proposed model achieves state-of-the-art performance in multiple action recognition benchmarks. Moreover, we demonstrate the effectiveness of our model in real-world applications such as action recognition in large-scale untrimmed videos and action parsing.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07680",
        "title": "Domain Generalization for Object Recognition with Multi-task Autoencoders",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang",
            "David Balduzzi"
        ],
        "abstract": "The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.\n",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07859",
        "title": "Multi-Projector Color Structured-Light Vision",
        "authors": [
            "Changsoo Je",
            "Kwang Hee Lee",
            "Sang Wook Lee"
        ],
        "abstract": "Research interest in rapid structured-light imaging has grown increasingly for the modeling of moving objects, and a number of methods have been suggested for the range capture in a single video frame. The imaging area of a 3D object using a single projector is restricted since the structured light is projected only onto a limited area of the object surface. Employing additional projectors to broaden the imaging area is a challenging problem since simultaneous projection of multiple patterns results in their superposition in the light-intersected areas and the recognition of original patterns is by no means trivial. This paper presents a novel method of multi-projector color structured-light vision based on projector-camera triangulation. By analyzing the behavior of superposed-light colors in a chromaticity domain, we show that the original light colors cannot be properly extracted by the conventional direct estimation. We disambiguate multiple projectors by multiplexing the orientations of projector patterns so that the superposed patterns can be separated by explicit derivative computations. Experimental studies are carried out to demonstrate the validity of the presented method. The proposed method increases the efficiency of range acquisition compared to conventional active stereo using multiple projectors.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07902",
        "title": "Maximum Persistency via Iterative Relaxed Inference with Graphical Models",
        "authors": [
            "Alexander Shekhovtsov",
            "Paul Swoboda",
            "Bogdan Savchynskyy"
        ],
        "abstract": "We consider the NP-hard problem of MAP-inference for undirected discrete graphical models. We propose a polynomial time and practically efficient algorithm for finding a part of its optimal solution. Specifically, our algorithm marks some labels of the considered graphical model either as (i) optimal, meaning that they belong to all optimal solutions of the inference problem; (ii) non-optimal if they provably do not belong to any solution. With access to an exact solver of a linear programming relaxation to the MAP-inference problem, our algorithm marks the maximal possible (in a specified sense) number of labels. We also present a version of the algorithm, which has access to a suboptimal dual solver only and still can ensure the (non-)optimality for the marked labels, although the overall number of the marked labels may decrease. We propose an efficient implementation, which runs in time comparable to a single run of a suboptimal dual solver. Our method is well-scalable and shows state-of-the-art results on computational benchmarks from machine learning and computer vision.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2017-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07953",
        "title": "Approximate Nearest Neighbor Fields in Video",
        "authors": [
            "Nir Ben-Zrihem",
            "Lihi Zelnik-Manor"
        ],
        "abstract": "We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search), an algorithm for matching patches of a video to a set of reference patches in real-time. For each query, RIANN finds potential matches by intersecting rings around key points in appearance space. Its search complexity is reversely correlated to the amount of temporal change, making it a good fit for videos, where typically most patches change slowly with time. Experiments show that RIANN is up to two orders of magnitude faster than previous ANN methods, and is the only solution that operates in real-time. We further demonstrate how RIANN can be used for real-time video processing and provide examples for a range of real-time video applications, including colorization, denoising, and several artistic effects.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00111",
        "title": "Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection",
        "authors": [
            "Audrey G. Chung",
            "Mohammad Javad Shafiee",
            "Devinder Kumar",
            "Farzad Khalvati",
            "Masoom A. Haider",
            "Alexander Wong"
        ],
        "abstract": "Prostate cancer is the most diagnosed form of cancer in Canadian men, and is the third leading cause of cancer death. Despite these statistics, prognosis is relatively good with a sufficiently early diagnosis, making fast and reliable prostate cancer detection crucial. As imaging-based prostate cancer screening, such as magnetic resonance imaging (MRI), requires an experienced medical professional to extensively review the data and perform a diagnosis, radiomics-driven methods help streamline the process and has the potential to significantly improve diagnostic accuracy and efficiency, and thus improving patient survival rates. These radiomics-driven methods currently rely on hand-crafted sets of quantitative imaging-based features, which are selected manually and can limit their ability to fully characterize unique prostate cancer tumour phenotype. In this study, we propose a novel \\textit{discovery radiomics} framework for generating custom radiomic sequences tailored for prostate cancer detection. Discovery radiomics aims to uncover abstract imaging-based features that capture highly unique tumour traits and characteristics beyond what can be captured using predefined feature models. In this paper, we discover new custom radiomic sequencers for generating new prostate radiomic sequences using multi-parametric MRI data. We evaluated the performance of the discovered radiomic sequencer against a state-of-the-art hand-crafted radiomic sequencer for computer-aided prostate cancer detection with a feedforward neural network using real clinical prostate multi-parametric MRI data. Results for the discovered radiomic sequencer demonstrate good performance in prostate cancer detection and clinical decision support relative to the hand-crafted radiomic sequencer. The use of discovery radiomics shows potential for more efficient and reliable automatic prostate cancer detection.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00116",
        "title": "FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation",
        "authors": [
            "M. Salman Asif",
            "Ali Ayremlou",
            "Aswin Sankaranarayanan",
            "Ashok Veeraraghavan",
            "Richard Baraniuk"
        ],
        "abstract": "FlatCam is a thin form-factor lensless camera that consists of a coded mask placed on top of a bare, conventional sensor array. Unlike a traditional, lens-based camera where an image of the scene is directly recorded on the sensor pixels, each pixel in FlatCam records a linear combination of light from multiple scene elements. A computational algorithm is then used to demultiplex the recorded measurements and reconstruct an image of the scene. FlatCam is an instance of a coded aperture imaging system; however, unlike the vast majority of related work, we place the coded mask extremely close to the image sensor that can enable a thin system. We employ a separable mask to ensure that both calibration and image reconstruction are scalable in terms of memory requirements and computational complexity. We demonstrate the potential of the FlatCam design using two prototypes: one at visible wavelengths and one at infrared wavelengths.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00117",
        "title": "Discovery Radiomics for Pathologically-Proven Computed Tomography Lung Cancer Prediction",
        "authors": [
            "Devinder Kumar",
            "Mohammad Javad Shafiee",
            "Audrey G. Chung",
            "Farzad Khalvati",
            "Masoom A. Haider",
            "Alexander Wong"
        ],
        "abstract": "Lung cancer is the leading cause for cancer related deaths. As such, there is an urgent need for a streamlined process that can allow radiologists to provide diagnosis with greater efficiency and accuracy. A powerful tool to do this is radiomics: a high-dimension imaging feature set. In this study, we take the idea of radiomics one step further by introducing the concept of discovery radiomics for lung cancer prediction using CT imaging data. In this study, we realize these custom radiomic sequencers as deep convolutional sequencers using a deep convolutional neural network learning architecture. To illustrate the prognostic power and effectiveness of the radiomic sequences produced by the discovered sequencer, we perform cancer prediction between malignant and benign lesions from 97 patients using the pathologically-proven diagnostic data from the LIDC-IDRI dataset. Using the clinically provided pathologically-proven data as ground truth, the proposed framework provided an average accuracy of 77.52% via 10-fold cross-validation with a sensitivity of 79.06% and specificity of 76.11%, surpassing the state-of-the art method.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2017-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00154",
        "title": "Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and Active Contour",
        "authors": [
            "Jalil Rasekhi"
        ],
        "abstract": "In this paper we present a new method for motion tracking of tumors in liver ultrasound image sequences. Our algorithm has two main steps. In the first step, we apply mean shift algorithm with multiple features to estimate the center of the target in each frame. Target in the first frame is defined using an ellipse. Edge, texture, and intensity features are extracted from the first frame, and then mean shift algorithm is applied to each feature separately to find the center of ellipse related to that feature in the next frame. The center of ellipse will be the weighted average of these centers. By using mean shift actually we estimate the target movement between two consecutive frames. Once the correct ellipsoid in each frame is known, in the second step we apply the Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour models, in order to find the correct boundary of tumors. We sample a few points on the boundary of active contour then translate those points based on the translation of the center of ellipsoid in two consecutive frames to determine the target movement. We use these translated sample points as an initial guess for active contour in the next frame. Our experimental results show that, the suggested method provides a reliable performance for liver tumor tracking in ultrasound image sequences.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00244",
        "title": "Robust Face Recognition via Multimodal Deep Face Representation",
        "authors": [
            "Changxing Ding",
            "Dacheng Tao"
        ],
        "abstract": "Face images appeared in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefited from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00296",
        "title": "Fast Randomized Singular Value Thresholding for Low-rank Optimization",
        "authors": [
            "Tae-Hyun Oh",
            "Yasuyuki Matsushita",
            "Yu-Wing Tai",
            "In So Kweon"
        ],
        "abstract": "Rank minimization can be converted into tractable surrogate problems, such as Nuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related to NNM, or WNNM, can be solved iteratively by applying a closed-form proximal operator, called Singular Value Thresholding (SVT), or Weighted SVT, but they suffer from high computational cost of Singular Value Decomposition (SVD) at each iteration. We propose a fast and accurate approximation method for SVT, that we call fast randomized SVT (FRSVT), with which we avoid direct computation of SVD. The key idea is to extract an approximate basis for the range of the matrix from its compressed matrix. Given the basis, we compute partial singular values of the original matrix from the small factored matrix. In addition, by developping a range propagation method, our method further speeds up the extraction of approximate basis at each iteration. Our theoretical analysis shows the relationship between the approximation bound of SVD and its effect to NNM via SVT. Along with the analysis, our empirical results quantitatively and qualitatively show that our approximation rarely harms the convergence of the host algorithms. We assess the efficiency and accuracy of the proposed method on various computer vision problems, e.g., subspace clustering, weather artifact removal, and simultaneous multi-image alignment and rectification.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2016-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00313",
        "title": "Iterative hypothesis testing for multi-object tracking in presence of features with variable reliability",
        "authors": [
            "Amit Kumar K.C.",
            "Damien Delannay",
            "Christophe De Vleeschouwer"
        ],
        "abstract": "This paper assumes prior detections of multiple targets at each time instant, and uses a graph-based approach to connect those detections across time, based on their position and appearance estimates. In contrast to most earlier works in the field, our framework has been designed to exploit the appearance features, even when they are only sporadically available, or affected by a non-stationary noise, along the sequence of detections. This is done by implementing an iterative hypothesis testing strategy to progressively aggregate the detections into short trajectories, named tracklets. Specifically, each iteration considers a node, named key-node, and investigates how to link this key-node with other nodes in its neighborhood, under the assumption that the target appearance is defined by the key-node appearance estimate. This is done through shortest path computation in a temporal neighborhood of the key-node. The approach is conservative in that it only aggregates the shortest paths that are sufficiently better compared to alternative paths. It is also multi-scale in that the size of the investigated neighborhood is increased proportionally to the number of detections already aggregated into the key-node. The multi-scale nature of the process and the progressive relaxation of its conservativeness makes it both computationally efficient and effective.\n",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00552",
        "title": "DAG-Recurrent Neural Networks For Scene Labeling",
        "authors": [
            "Bing Shuai",
            "Zhen Zuo",
            "Gang Wang",
            "Bing Wang"
        ],
        "abstract": "In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Specifically, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which significantly benefits the local classification. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00568",
        "title": "Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach",
        "authors": [
            "Michael Fire",
            "Jonathan Schler"
        ],
        "abstract": "Online advertising is a huge, rapidly growing advertising market in today's world. One common form of online advertising is using image ads. A decision is made (often in real time) every time a user sees an ad, and the advertiser is eager to determine the best ad to display. Consequently, many algorithms have been developed that calculate the optimal ad to show to the current user at the present time. Typically, these algorithms focus on variations of the ad, optimizing among different properties such as background color, image size, or set of images. However, there is a more fundamental layer. Our study looks at new qualities of ads that can be determined before an ad is shown (rather than online optimization) and defines which ads are most likely to be successful.\n",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00651",
        "title": "Manipulated Object Proposal: A Discriminative Object Extraction and Feature Fusion Framework for First-Person Daily Activity Recognition",
        "authors": [
            "Changzhi Luo",
            "Bingbing Ni",
            "Jun Yuan",
            "Jianfeng Wang",
            "Shuicheng Yan",
            "Meng Wang"
        ],
        "abstract": "Detecting and recognizing objects interacting with humans lie in the center of first-person (egocentric) daily activity recognition. However, due to noisy camera motion and frequent changes in viewpoint and scale, most of the previous egocentric action recognition methods fail to capture and model highly discriminative object features. In this work, we propose a novel pipeline for first-person daily activity recognition, aiming at more discriminative object feature representation and object-motion feature fusion. Our object feature extraction and representation pipeline is inspired by the recent success of object hypotheses and deep convolutional neural network based detection frameworks. Our key contribution is a simple yet effective manipulated object proposal generation scheme. This scheme leverages motion cues such as motion boundary and motion magnitude (in contrast, camera motion is usually considered as \"noise\" for most previous methods) to generate a more compact and discriminative set of object proposals, which are more closely related to the objects which are being manipulated. Then, we learn more discriminative object detectors from these manipulated object proposals based on region-based convolutional neural network (R-CNN). Meanwhile, we develop a network based feature fusion scheme which better combines object and motion features. We show in experiments that the proposed framework significantly outperforms the state-of-the-art recognition performance on a challenging first-person daily activity benchmark.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00714",
        "title": "Dictionary based Approach to Edge Detection",
        "authors": [
            "Nitish Chandra",
            "Kedar Khare"
        ],
        "abstract": "Edge detection is a very essential part of image processing, as quality and accuracy of detection determines the success of further processing. We have developed a new self learning technique for edge detection using dictionary comprised of eigenfilters constructed using features of the input image. The dictionary based method eliminates the need of pre or post processing of the image and accounts for noise, blurriness, class of image and variation of illumination during the detection process itself. Since, this method depends on the characteristics of the image, the new technique can detect edges more accurately and capture greater detail than existing algorithms such as Sobel, Prewitt Laplacian of Gaussian, Canny method etc which use generic filters and operators. We have demonstrated its application on various classes of images such as text, face, barcodes, traffic and cell images. An application of this technique to cell counting in a microscopic image is also presented.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00816",
        "title": "Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging",
        "authors": [
            "Suren Jayasuriya",
            "Adithya Pediredla",
            "Sriram Sivaramakrishnan",
            "Alyosha Molnar",
            "Ashok Veeraraghavan"
        ],
        "abstract": "A variety of techniques such as light field, structured illumination, and time-of-flight (TOF) are commonly used for depth acquisition in consumer imaging, robotics and many other applications. Unfortunately, each technique suffers from its individual limitations preventing robust depth sensing. In this paper, we explore the strengths and weaknesses of combining light field and time-of-flight imaging, particularly the feasibility of an on-chip implementation as a single hybrid depth sensor. We refer to this combination as depth field imaging. Depth fields combine light field advantages such as synthetic aperture refocusing with TOF imaging advantages such as high depth resolution and coded signal processing to resolve multipath interference. We show applications including synthesizing virtual apertures for TOF imaging, improved depth mapping through partial and scattering occluders, and single frequency TOF phase unwrapping. Utilizing space, angle, and temporal coding, depth fields can improve depth sensing in the wild and generate new insights into the dimensions of light's plenoptic function.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01074",
        "title": "A Novice Guide towards Human Motion Analysis and Understanding",
        "authors": [
            "Ahmed Nabil Mohamed"
        ],
        "abstract": "Human motion analysis and understanding has been, and is still, the focus of attention of many disciplines which is considered an obvious indicator of the wide and massive importance of the subject. The purpose of this article is to shed some light on this very important subject, so it can be a good insight for a novice computer vision researcher in this field by providing him/her with a wealth of knowledge about the subject covering many directions. There are two main contributions of this article. The first one investigates various aspects of some disciplines (e.g., arts, philosophy, psychology, and neuroscience) that are interested in the subject and review some of their contributions stressing on those that can be useful for computer vision researchers. Moreover, many examples are illustrated to indicate the benefits of integrating concepts and results among different disciplines. The second contribution is concerned with the subject from the computer vision aspect where we discuss the following issues. First, we explore many demanding and promising applications to reveal the wide and massive importance of the field. Second, we list various types of sensors that may be used for acquiring various data. Third, we review different taxonomies used for classifying motions. Fourth, we review various processes involved in motion analysis. Fifth, we exhibit how different surveys are structured. Sixth, we examine many of the most cited and recent reviews in the field that have been published during the past two decades to reveal various approaches used for implementing different stages of the problem and refer to various algorithms and their suitability for different situations. Moreover, we provide a long list of public datasets and discuss briefly some examples of these datasets. Finally, we provide a general discussion of the subject from the aspect of computer vision.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01122",
        "title": "Vision-Based Road Detection using Contextual Blocks",
        "authors": [
            "Caio C\u00e9sar Teodoro Mendes",
            "Vincent Fr\u00e9mont",
            "Denis Fernando Wolf"
        ],
        "abstract": "Road detection is a fundamental task in autonomous navigation systems. In this paper, we consider the case of monocular road detection, where images are segmented into road and non-road regions. Our starting point is the well-known machine learning approach, in which a classifier is trained to distinguish road and non-road regions based on hand-labeled images. We proceed by introducing the use of \"contextual blocks\" as an efficient way of providing contextual information to the classifier. Overall, the proposed methodology, including its image feature selection and classifier, was conceived with computational cost in mind, leaving room for optimized implementations. Regarding experiments, we perform a sensible evaluation of each phase and feature subset that composes our system. The results show a great benefit from using contextual blocks and demonstrate their computational efficiency. Finally, we submit our results to the KITTI road detection benchmark achieving scores comparable with state of the art methods.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01277",
        "title": "A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place",
        "authors": [
            "Colin Rennie",
            "Rahul Shome",
            "Kostas E. Bekris",
            "Alberto F. De Souza"
        ],
        "abstract": "An important logistics application of robotics involves manipulators that pick-and-place objects placed in warehouse shelves. A critical aspect of this task corre- sponds to detecting the pose of a known object in the shelf using visual data. Solving this problem can be assisted by the use of an RGB-D sensor, which also provides depth information beyond visual data. Nevertheless, it remains a challenging problem since multiple issues need to be addressed, such as low illumination inside shelves, clutter, texture-less and reflective objects as well as the limitations of depth sensors. This paper provides a new rich data set for advancing the state-of-the-art in RGBD- based 3D object pose estimation, which is focused on the challenges that arise when solving warehouse pick- and-place tasks. The publicly available data set includes thousands of images and corresponding ground truth data for the objects used during the first Amazon Picking Challenge at different poses and clutter conditions. Each image is accompanied with ground truth information to assist in the evaluation of algorithms for object detection. To show the utility of the data set, a recent algorithm for RGBD-based pose estimation is evaluated in this paper. Based on the measured performance of the algorithm on the data set, various modifications and improvements are applied to increase the accuracy of detection. These steps can be easily applied to a variety of different methodologies for object pose detection and improve performance in the domain of warehouse pick-and-place.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2016-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01287",
        "title": "Image Classification with Rejection using Contextual Information",
        "authors": [
            "Filipe Condessa",
            "Jos\u00e9 Bioucas-Dias",
            "Carlos Castro",
            "John Ozolek",
            "Jelena Kova\u010devi\u0107"
        ],
        "abstract": "We introduce a new supervised algorithm for image classification with rejection using multiscale contextual information. Rejection is desired in image-classification applications that require a robust classifier but not the classification of the entire image. The proposed algorithm combines local and multiscale contextual information with rejection, improving the classification performance. As a probabilistic model for classification, we adopt a multinomial logistic regression. The concept of rejection with contextual information is implemented by modeling the classification problem as an energy minimization problem over a graph representing local and multiscale similarities of the image. The rejection is introduced through an energy data term associated with the classification risk and the contextual information through an energy smoothness term associated with the local and multiscale similarities within the image. We illustrate the proposed method on the classification of images of H&E-stained teratoma tissues.\n    ",
        "submission_date": "2015-09-03T00:00:00",
        "last_modified_date": "2015-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01329",
        "title": "Semantic Amodal Segmentation",
        "authors": [
            "Yan Zhu",
            "Yuandong Tian",
            "Dimitris Mexatas",
            "Piotr Doll\u00e1r"
        ],
        "abstract": "Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition?\n",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2016-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01343",
        "title": "Learning Temporal Alignment Uncertainty for Efficient Event Detection",
        "authors": [
            "Iman Abbasnejad",
            "Sridha Sridharan",
            "Simon Denman",
            "Clinton Fookes",
            "Simon Lucey"
        ],
        "abstract": "In this paper we tackle the problem of efficient video event detection. We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation. A popular approach in this regard is to represent a sequence using a bag of words (BOW) representation due to its: (i) fixed dimensionality irrespective of the sequence length, and (ii) its ability to compactly model the statistics in the sequence. A drawback to the BOW representation, however, is the intrinsic destruction of the temporal ordering information. In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering. Our representation, like BOW, is of a fixed dimensionality making it easily integrated with a linear detection function. Extensive experiments on CK+, 6DMG, and UvA-NEMO databases show significant performance improvements across both isolated and continuous event detection tasks.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01354",
        "title": "CNN Based Hashing for Image Retrieval",
        "authors": [
            "Jinma Guo",
            "Jianmin Li"
        ],
        "abstract": "Along with data on the web increasing dramatically, hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix, we proposed a straightforward CNN-based hashing method, i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples, and that the co-adaption between feature extractor and hash functions is important for hashing.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01509",
        "title": "EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis",
        "authors": [
            "Israel D. Gebru",
            "Xavier Alameda-Pineda",
            "Florence Forbes",
            "Radu Horaud"
        ],
        "abstract": "Data clustering has received a lot of attention and numerous methods, algorithms and software packages are available. Among these techniques, parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization (EM). In this paper we propose a new mixture model that associates a weight with each observed point. We introduce the weighted-data Gaussian mixture and we derive two EM algorithms. The first one considers a fixed weight for each observation. The second one treats each weight as a random variable following a gamma distribution. We propose a model selection method based on a minimum message length criterion, provide a weight initialization strategy, and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques. We also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data, namely audio-visual scene analysis.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01514",
        "title": "Conjugate Gradient Acceleration of Non-Linear Smoothing Filters",
        "authors": [
            "Andrew Knyazev",
            "Alexander Malyshev"
        ],
        "abstract": "The most efficient signal edge-preserving smoothing filters, e.g., for denoising, are non-linear. Thus, their acceleration is challenging and is often performed in practice by tuning filter parameters, such as by increasing the width of the local smoothing neighborhood, resulting in more aggressive smoothing of a single sweep at the cost of increased edge blurring. We propose an alternative technology, accelerating the original filters without tuning, by running them through a special conjugate gradient method, not affecting their quality. The filter non-linearity is dealt with by careful freezing and restarting. Our initial numerical experiments on toy one-dimensional signals demonstrate 20x acceleration of the classical bilateral filter and 3-5x acceleration of the recently developed guided filter.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01520",
        "title": "An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes",
        "authors": [
            "Sileye Ba",
            "Xavier Alameda-Pineda",
            "Alessio Xompero",
            "Radu Horaud"
        ],
        "abstract": "Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01602",
        "title": "Object Recognition from Short Videos for Robotic Perception",
        "authors": [
            "Ivan Bogun",
            "Anelia Angelova",
            "Navdeep Jaitly"
        ],
        "abstract": "Deep neural networks have become the primary learning technique for object recognition. Videos, unlike still images, are temporally coherent which makes the application of deep networks non-trivial. Here, we investigate how motion can aid object recognition in short videos. Our approach is based on Long Short-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs, we implement each gate as a convolution. We show that convolutional-based LSTM models are capable of learning motion dependencies and are able to improve the recognition accuracy when more frames in a sequence are available. We evaluate our approach on the Washington RGBD Object dataset and on the Washington RGBD Scenes dataset. Our approach outperforms deep nets applied to still images and sets a new state-of-the-art in this domain.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01624",
        "title": "Chebyshev and Conjugate Gradient Filters for Graph Image Denoising",
        "authors": [
            "Dong Tian",
            "Hassan Mansour",
            "Andrew Knyazev",
            "Anthony Vetro"
        ],
        "abstract": "In 3D image/video acquisition, different views are often captured with varying noise levels across the views. In this paper, we propose a graph-based image enhancement technique that uses a higher quality view to enhance a degraded view. A depth map is utilized as auxiliary information to match the perspectives of the two views. Our method performs graph-based filtering of the noisy image by directly computing a projection of the image to be filtered onto a lower dimensional Krylov subspace of the graph Laplacian. We discuss two graph spectral denoising methods: first using Chebyshev polynomials, and second using iterations of the conjugate gradient algorithm. Our framework generalizes previously known polynomial graph filters, and we demonstrate through numerical simulations that our proposed technique produces subjectively cleaner images with about 1-3 dB improvement in PSNR over existing polynomial graph filters.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01654",
        "title": "Co-interest Person Detection from Multiple Wearable Camera Videos",
        "authors": [
            "Yuewei Lin",
            "Kareem Ezzeldeen",
            "Youjie Zhou",
            "Xiaochuan Fan",
            "Hongkai Yu",
            "Hui Qian",
            "Song Wang"
        ],
        "abstract": "Wearable cameras, such as Google Glass and Go Pro, enable video data collection over larger areas and from different views. In this paper, we tackle a new problem of locating the co-interest person (CIP), i.e., the one who draws attention from most camera wearers, from temporally synchronized videos taken by multiple wearable cameras. Our basic idea is to exploit the motion patterns of people and use them to correlate the persons across different videos, instead of performing appearance-based matching as in traditional video co-segmentation/localization. This way, we can identify CIP even if a group of people with similar appearance are present in the view. More specifically, we detect a set of persons on each frame as the candidates of the CIP and then build a Conditional Random Field (CRF) model to select the one with consistent motion patterns in different videos and high spacial-temporal consistency in each video. We collect three sets of wearable-camera videos for testing the proposed algorithm. All the involved people have similar appearances in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm.\n    ",
        "submission_date": "2015-09-05T00:00:00",
        "last_modified_date": "2015-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01719",
        "title": "Unsupervised Cross-Domain Recognition by Identifying Compact Joint Subspaces",
        "authors": [
            "Yuewei Lin",
            "Jing Chen",
            "Yu Cao",
            "Youjie Zhou",
            "Lingfeng Zhang",
            "Yuan Yan Tang",
            "Song Wang"
        ],
        "abstract": "This paper introduces a new method to solve the cross-domain recognition problem. Different from the traditional domain adaption methods which rely on a global domain shift for all classes between source and target domain, the proposed method is more flexible to capture individual class variations across domains. By adopting a natural and widely used assumption -- \"the data samples from the same class should lay on a low-dimensional subspace, even if they come from different domains\", the proposed method circumvents the limitation of the global domain shift, and solves the cross-domain recognition by finding the compact joint subspaces of source and target domain. Specifically, given labeled samples in source domain, we construct subspaces for each of the classes. Then we construct subspaces in the target domain, called anchor subspaces, by collecting unlabeled samples that are close to each other and highly likely all fall into the same class. The corresponding class label is then assigned by minimizing a cost function which reflects the overlap and topological structure consistency between subspaces across source and target domains, and within anchor subspaces, ",
        "submission_date": "2015-09-05T00:00:00",
        "last_modified_date": "2015-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01788",
        "title": "Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM) for unsupervised RGB-D image segmentation",
        "authors": [
            "Md. Abul Hasnat",
            "Olivier Alata",
            "Alain Tr\u00e9meau"
        ],
        "abstract": "Recent advances in depth imaging sensors provide easy access to the synchronized depth with color, called RGB-D image. In this paper, we propose an unsupervised method for indoor RGB-D image segmentation and analysis. We consider a statistical image generation model based on the color and geometry of the scene. Our method consists of a joint color-spatial-directional clustering method followed by a statistical planar region merging method. We evaluate our method on the NYU depth database and compare it with existing unsupervised RGB-D segmentation methods. Results show that, it is comparable with the state of the art methods and it needs less computation time. Moreover, it opens interesting perspectives to fuse color and geometry in an unsupervised manner.\n    ",
        "submission_date": "2015-09-06T00:00:00",
        "last_modified_date": "2015-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01947",
        "title": "An end-to-end generative framework for video segmentation and recognition",
        "authors": [
            "Hilde Kuehne",
            "Juergen Gall",
            "Thomas Serre"
        ],
        "abstract": "We describe an end-to-end generative approach for the segmentation and recognition of human activities. In this approach, a visual representation based on reduced Fisher Vectors is combined with a structured temporal model for recognition. We show that the statistical properties of Fisher Vectors make them an especially suitable front-end for generative models such as Gaussian mixtures. The system is evaluated for both the recognition of complex activities as well as their parsing into action units. Using a variety of video datasets ranging from human cooking activities to animal behaviors, our experiments demonstrate that the resulting architecture outperforms state-of-the-art approaches for larger datasets, i.e. when sufficient amount of data is available for training structured generative models.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01951",
        "title": "Hierarchical Deep Learning Architecture For 10K Objects Classification",
        "authors": [
            "Atul Laxman Katole",
            "Krishna Prasad Yellapragada",
            "Amish Kumar Bedi",
            "Sehaj Singh Kalra",
            "Mynepalli Siva Chaitanya"
        ],
        "abstract": "Evolution of visual object recognition architectures based on Convolutional Neural Networks & Convolutional Deep Belief Networks paradigms has revolutionized artificial Vision Science. These architectures extract & learn the real world hierarchical visual features utilizing supervised & unsupervised learning approaches respectively. Both the approaches yet cannot scale up realistically to provide recognition for a very large number of objects as high as 10K. We propose a two level hierarchical deep learning architecture inspired by divide & conquer principle that decomposes the large scale recognition architecture into root & leaf level model architectures. Each of the root & leaf level models is trained exclusively to provide superior results than possible by any 1-level deep learning architecture prevalent today. The proposed architecture classifies objects in two steps. In the first step the root level model classifies the object in a high level category. In the second step, the leaf level recognition model for the recognized high level category is selected among all the leaf models. This leaf level model is presented with the same input object image which classifies it in a specific category. Also we propose a blend of leaf level models trained with either supervised or unsupervised learning approaches. Unsupervised learning is suitable whenever labelled data is scarce for the specific leaf level models. Currently the training of leaf level models is in progress; where we have trained 25 out of the total 47 leaf level models as of now. We have trained the leaf models with the best case top-5 error rate of 3.2% on the validation data set for the particular leaf models. Also we demonstrate that the validation error of the leaf level models saturates towards the above mentioned accuracy as the number of epochs are increased to more than sixty.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01978",
        "title": "An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture",
        "authors": [
            "Darko Brodic",
            "Alessia Amelio",
            "Zoran N. Milivojevic"
        ],
        "abstract": "The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and hand-printed in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02027",
        "title": "A New Low-Rank Tensor Model for Video Completion",
        "authors": [
            "Wenrui Hu",
            "Dacheng Tao",
            "Wensheng Zhang",
            "Yuan Xie",
            "Yehui Yang"
        ],
        "abstract": "In this paper, we propose a new low-rank tensor model based on the circulant algebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensor denotes a 3-way tensor representation to laterally store 2D data slices in order. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twist tensor in the Fourier domain, which allows an efficient computation using FFT. On the other, t-TNN is equal to the nuclear norm of block circulant matricization of the twist tensor in the original domain, which extends the traditional matrix nuclear norm in a block circulant way. We test the t-TNN model on a video completion application that aims to fill missing values and the experiment results validate its effectiveness, especially when dealing with video recorded by a non-stationary panning camera. The block circulant matricization of the twist tensor can be transformed into a circulant block representation with nuclear norm invariance. This representation, after transformation, exploits the horizontal translation relationship between the frames in a video, and endows the t-TNN model with a more powerful ability to reconstruct panning videos than the existing state-of-the-art low-rank models.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02094",
        "title": "Future Localization from an Egocentric Depth Image",
        "authors": [
            "Hyun Soo Park",
            "Yedong Niu",
            "Jianbo Shi"
        ],
        "abstract": "This paper presents a method for future localization: to predict a set of plausible trajectories of ego-motion given a depth image. We predict paths avoiding obstacles, between objects, even paths turning around a corner into space behind objects. As a byproduct of the predicted trajectories of ego-motion, we discover in the image the empty space occluded by foreground objects. We use no image based features such as semantic labeling/segmentation or object detection/recognition for this algorithm. Inspired by proxemics, we represent the space around a person using an EgoSpace map, akin to an illustrated tourist map, that measures a likelihood of occlusion at the egocentric coordinate system. A future trajectory of ego-motion is modeled by a linear combination of compact trajectory bases allowing us to constrain the predicted trajectory. We learn the relationship between the EgoSpace map and trajectory from the EgoMotion dataset providing in-situ measurements of the future trajectory. A cost function that takes into account partial occlusion due to foreground objects is minimized to predict a trajectory. This cost function generates a trajectory that passes through the occluded space, which allows us to discover the empty space behind the foreground objects. We quantitatively evaluate our method to show predictive validity and apply to various real world scenes including walking, shopping, and social interactions.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02122",
        "title": "Convexity Shape Constraints for Image Segmentation",
        "authors": [
            "Loic A. Royer",
            "David L. Richmond",
            "Carsten Rother",
            "Bjoern Andres",
            "Dagmar Kainmueller"
        ],
        "abstract": "Segmenting an image into multiple components is a central task in computer vision. In many practical scenarios, prior knowledge about plausible components is available. Incorporating such prior knowledge into models and algorithms for image segmentation is highly desirable, yet can be non-trivial. In this work, we introduce a new approach that allows, for the first time, to constrain some or all components of a segmentation to have convex shapes. Specifically, we extend the Minimum Cost Multicut Problem by a class of constraints that enforce convexity. To solve instances of this APX-hard integer linear program to optimality, we separate the proposed constraints in the branch-and-cut loop of a state-of-the-art ILP solver. Results on natural and biological images demonstrate the effectiveness of the approach as well as its advantage over the state-of-the-art heuristic.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02130",
        "title": "Structured Prediction with Output Embeddings for Semantic Image Annotation",
        "authors": [
            "Ariadna Quattoni",
            "Arnau Ramisa",
            "Pranava Swaroop Madhyastha",
            "Edgar Simo-Serra",
            "Francesc Moreno-Noguer"
        ],
        "abstract": "We address the task of annotating images with semantic tuples. Solving this problem requires an algorithm which is able to deal with hundreds of classes for each argument of the tuple. In such contexts, data sparsity becomes a key challenge, as there will be a large number of classes for which only a few examples are available. We propose handling this by incorporating feature representations of both the inputs (images) and outputs (argument classes) into a factorized log-linear model, and exploiting the flexibility of scoring functions based on bilinear forms. Experiments show that integrating feature representations of the outputs in the structured prediction model leads to better overall predictions. We also conclude that the best output representation is specific for each type of argument.\n    ",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2015-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02223",
        "title": "Diffusion tensor imaging with deterministic error bounds",
        "authors": [
            "Artur Gorokh",
            "Yury Korolev",
            "Tuomo Valkonen"
        ],
        "abstract": "Errors in the data and the forward operator of an inverse problem can be handily modelled using partial order in Banach lattices. We present some existing results of the theory of regularisation in this novel framework, where errors are represented as bounds by means of the appropriate partial order.\n",
        "submission_date": "2015-09-07T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02317",
        "title": "Object Proposals for Text Extraction in the Wild",
        "authors": [
            "Lluis Gomez",
            "Dimosthenis Karatzas"
        ],
        "abstract": "Object Proposals is a recent computer vision technique receiving increasing interest from the research community. Its main objective is to generate a relatively small set of bounding box proposals that are most likely to contain objects of interest. The use of Object Proposals techniques in the scene text understanding field is innovative. Motivated by the success of powerful while expensive techniques to recognize words in a holistic way, Object Proposals techniques emerge as an alternative to the traditional text detectors.\n",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02320",
        "title": "HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A Pre-processing Approach",
        "authors": [
            "Xianbiao Qi",
            "Guoying Zhao",
            "Jie Chen",
            "Matti Pietik\u00e4inen"
        ],
        "abstract": "\\textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2} (HEp-2) cells is an effective way to identify the presence of Anti-Nuclear Antibody (ANA). Most existing works on HEp-2 cell classification mainly focus on feature extraction, feature encoding and classifier design. Very few efforts have been devoted to study the importance of the pre-processing techniques. In this paper, we analyze the importance of the pre-processing, and investigate the role of Gaussian Scale Space (GSS) theory as a pre-processing approach for the HEp-2 cell classification task. We validate the GSS pre-processing under the Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under the BoW framework, the introduced pre-processing approach, using only one Local Orientation Adaptive Descriptor (LOAD), achieved superior performance on the Executable Thematic on Pattern Recognition Techniques for Indirect Immunofluorescence (ET-PRT-IIF) image analysis. Our system, using only one feature, outperformed the winner of the ICPR 2014 contest that combined four types of features. Meanwhile, the proposed pre-processing method is not restricted to this work; it can be generalized to many existing works.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02441",
        "title": "Semantic Video Segmentation : Exploring Inference Efficiency",
        "authors": [
            "Subarna Tripathi",
            "Serge Belongie",
            "Youngbae Hwang",
            "Truong Nguyen"
        ],
        "abstract": "We explore the efficiency of the CRF inference beyond image level semantic segmentation and perform joint inference in video frames. The key idea is to combine best of two worlds: semantic co-labeling and more expressive models. Our formulation enables us to perform inference over ten thousand images within seconds and makes the system amenable to perform video semantic segmentation most effectively. On CamVid dataset, with TextonBoost unaries, our proposed method achieves up to 8% improvement in accuracy over individual semantic image segmentation without additional time overhead. The source code is available at ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02468",
        "title": "Accelerated graph-based spectral polynomial filters",
        "authors": [
            "Andrew Knyazev",
            "Alexander Malyshev"
        ],
        "abstract": "Graph-based spectral denoising is a low-pass filtering using the eigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomial filtering avoids costly computation of the eigendecomposition by projections onto suitable Krylov subspaces. Polynomial filters can be based, e.g., on the bilateral and guided filters. We propose constructing accelerated polynomial filters by running flexible Krylov subspace based linear and eigenvalue solvers such as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG) method.\n    ",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02470",
        "title": "Deep Attributes from Context-Aware Regional Neural Codes",
        "authors": [
            "Jianwei Luo",
            "Jianguo Li",
            "Jun Wang",
            "Zhiguo Jiang",
            "Yurong Chen"
        ],
        "abstract": "Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As soft-max layer directly corresponds to semantic concepts, this representation is named \"deep attributes\". Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build context-aware classifiers.\n",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02491",
        "title": "Edge-enhancing Filters with Negative Weights",
        "authors": [
            "Andrew Knyazev"
        ],
        "abstract": "In [DOI:",
        "submission_date": "2015-09-08T00:00:00",
        "last_modified_date": "2015-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02587",
        "title": "A Dual Fast and Slow Feature Interaction in Biologically Inspired Visual Recognition of Human Action",
        "authors": [
            "Bardia Yousefi",
            "C.K. Loo"
        ],
        "abstract": "Computational neuroscience studies that have examined human visual system through functional magnetic resonance imaging (fMRI) have identified a model where the mammalian brain pursues two distinct pathways (for recognition of biological movement tasks). In the brain, dorsal stream analyzes the information of motion (optical flow), which is the fast features, and ventral stream (form pathway) analyzes form information (through active basis model based incremental slow feature analysis ) as slow features. The proposed approach suggests the motion perception of the human visual system composes of fast and slow feature interactions that identifies biological movements. Form features in the visual system biologically follows the application of active basis model with incremental slow feature analysis for the extraction of the slowest form features of human objects movements in the ventral stream. Applying incremental slow feature analysis provides an opportunity to use the action prototypes. To extract the slowest features episodic observation is required but the fast features updates the processing of motion information in every frames. Experimental results have shown promising accuracy for the proposed model and good performance with two datasets (KTH and Weizmann).\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2015-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02634",
        "title": "Semantic Image Segmentation via Deep Parsing Network",
        "authors": [
            "Ziwei Liu",
            "Xiaoxiao Li",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02636",
        "title": "Proposal-free Network for Instance-level Object Segmentation",
        "authors": [
            "Xiaodan Liang",
            "Yunchao Wei",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "Instance-level object segmentation is an important yet under-explored task. The few existing studies are almost all based on region proposal methods to extract candidate segments and then utilize object classification to produce final results. Nonetheless, generating accurate region proposals itself is quite challenging. In this work, we propose a Proposal-Free Network (PFN ) to address the instance-level object segmentation problem, which outputs the instance numbers of different categories and the pixel-level information on 1) the coordinates of the instance bounding box each pixel belongs to, and 2) the confidences of different categories for each pixel, based on pixel-to-pixel deep convolutional neural network. All the outputs together, by using any off-the-shelf clustering method for simple post-processing, can naturally generate the ultimate instance-level object segmentation results. The whole PFN can be easily trained in an end-to-end way without the requirement of a proposal generation stage. Extensive evaluations on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate that the proposed PFN solution well beats the state-of-the-arts for instance-level object segmentation. In particular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN, significantly higher than 43.8% and 46.3% by the state-of-the-art algorithms, SDS [9] and [16], respectively.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02649",
        "title": "Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering with Corrupted and Incomplete Data",
        "authors": [
            "Pan Ji",
            "Mathieu Salzmann",
            "Hongdong Li"
        ],
        "abstract": "The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corrupted and missing measurements.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02970",
        "title": "Dictionary Learning and Sparse Coding for Third-order Super-symmetric Tensors",
        "authors": [
            "Piotr Koniusz",
            "Anoop Cherian"
        ],
        "abstract": "Super-symmetric tensors - a higher-order extension of scatter matrices - are becoming increasingly popular in machine learning and computer vision for modelling data statistics, co-occurrences, or even as visual descriptors. However, the size of these tensors are exponential in the data dimensionality, which is a significant concern. In this paper, we study third-order super-symmetric tensor descriptors in the context of dictionary learning and sparse coding. Our goal is to approximate these tensors as sparse conic combinations of atoms from a learned dictionary, where each atom is a symmetric positive semi-definite matrix. Apart from the significant benefits to tensor compression that this framework provides, our experiments demonstrate that the sparse coefficients produced by the scheme lead to better aggregation of high-dimensional data, and showcases superior performance on two common computer vision tasks compared to the state-of-the-art.\n    ",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2015-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03001",
        "title": "Real-time Sign Language Fingerspelling Recognition using Convolutional Neural Networks from Depth map",
        "authors": [
            "Byeongkeun Kang",
            "Subarna Tripathi",
            "Truong Q. Nguyen"
        ],
        "abstract": "Sign language recognition is important for natural and convenient communication between deaf community and hearing majority. We take the highly efficient initial step of automatic fingerspelling recognition system using convolutional neural networks (CNNs) from depth maps. In this work, we consider relatively larger number of classes compared with the previous literature. We train CNNs for the classification of 31 alphabets and numbers using a subset of collected depth data from multiple subjects. While using different learning configurations, such as hyper-parameter selection with and without validation, we achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracy for new signers. The result shows that accuracy improves as we include more data from different subjects during training. The processing time is 3 ms for the prediction of a single image. To the best of our knowledge, the system achieves the highest accuracy and speed. The trained model and dataset is available on our repository.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03150",
        "title": "STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation",
        "authors": [
            "Yunchao Wei",
            "Xiaodan Liang",
            "Yunpeng Chen",
            "Xiaohui Shen",
            "Ming-Ming Cheng",
            "Jiashi Feng",
            "Yao Zhao",
            "Shuicheng Yan"
        ],
        "abstract": "Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with supervision from the predicted segmentation masks of simple images based on the Initial-DCNN as well as the image-level annotations. Finally, more pixel-level segmentation masks of complex images (two or more categories of objects with cluttered background), which are inferred by using Enhanced-DCNN and image-level annotations, are utilized as the supervision information to learn the Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple images from ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2016-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03248",
        "title": "A deep matrix factorization method for learning attribute representations",
        "authors": [
            "George Trigeorgis",
            "Konstantinos Bousmalis",
            "Stefanos Zafeiriou",
            "Bjoern W.Schuller"
        ],
        "abstract": "Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03371",
        "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems",
        "authors": [
            "Fabian Tschopp"
        ],
        "abstract": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once.\n",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03413",
        "title": "Learning Sparse Feature Representations using Probabilistic Quadtrees and Deep Belief Nets",
        "authors": [
            "Saikat Basu",
            "Manohar Karki",
            "Sangram Ganguly",
            "Robert DiBiano",
            "Supratik Mukhopadhyay",
            "Ramakrishna Nemani"
        ],
        "abstract": "Learning sparse feature representations is a useful instrument for solving an unsupervised learning problem. In this paper, we present three labeled handwritten digit datasets, collectively called n-MNIST. Then, we propose a novel framework for the classification of handwritten digits that learns sparse representations using probabilistic quadtrees and Deep Belief Nets. On the MNIST and n-MNIST datasets, our framework shows promising results and significantly outperforms traditional Deep Belief Networks.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03453",
        "title": "A reliable order-statistics-based approximate nearest neighbor search algorithm",
        "authors": [
            "Luisa Verdoliva",
            "Davide Cozzolino",
            "Giovanni Poggi"
        ],
        "abstract": "We propose a new algorithm for fast approximate nearest neighbor search based on the properties of ordered vectors. Data vectors are classified based on the index and sign of their largest components, thereby partitioning the space in a number of cones centered in the origin. The query is itself classified, and the search starts from the selected cone and proceeds to neighboring ones. Overall, the proposed algorithm corresponds to locality sensitive hashing in the space of directions, with hashing based on the order of components. Thanks to the statistical features emerging through ordering, it deals very well with the challenging case of unstructured data, and is a valuable building block for more complex techniques dealing with structured data. Experiments on both simulated and real-world data prove the proposed algorithm to provide a state-of-the-art performance.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2016-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03456",
        "title": "OCR accuracy improvement on document images through a novel pre-processing approach",
        "authors": [
            "Abdeslam El Harraj",
            "Naoufal Raissouni"
        ],
        "abstract": "Digital camera and mobile document image acquisition are new trends arising in the world of Optical Character Recognition and text detection. In some cases, such process integrates many distortions and produces poorly scanned text or text-photo images and natural images, leading to an unreliable OCR digitization. In this paper, we present a novel nonparametric and unsupervised method to compensate for undesirable document image distortions aiming to optimally improve OCR accuracy. Our approach relies on a very efficient stack of document image enhancing techniques to recover deformation of the entire document image. First, we propose a local brightness and contrast adjustment method to effectively handle lighting variations and the irregular distribution of image illumination. Second, we use an optimized greyscale conversion algorithm to transform our document image to greyscale level. Third, we sharpen the useful information in the resulting greyscale image using Un-sharp Masking method. Finally, an optimal global binarization approach is used to prepare the final document image to OCR recognition. The proposed approach can significantly improve text detection rate and optical character recognition accuracy. To demonstrate the efficiency of our approach, an exhaustive experimentation on a standard dataset is presented.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03502",
        "title": "Person Recognition in Personal Photo Collections",
        "authors": [
            "Seong Joon Oh",
            "Rodrigo Benenson",
            "Mario Fritz",
            "Bernt Schiele"
        ],
        "abstract": "Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA).\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03542",
        "title": "Fingerprint Recognition Using Translation Invariant Scattering Network",
        "authors": [
            "Shervin Minaee",
            "Yao Wang"
        ],
        "abstract": "Fingerprint recognition has drawn a lot of attention during last decades. Different features and algorithms have been used for fingerprint recognition in the past. In this paper, a powerful image representation called scattering transform/network, is used for recognition. Scattering network is a convolutional network where its architecture and filters are predefined wavelet transforms. The first layer of scattering representation is similar to sift descriptors and the higher layers capture higher frequency content of the signal. After extraction of scattering features, their dimensionality is reduced by applying principal component analysis (PCA). At the end, multi-class SVM is used to perform template matching for the recognition task. The proposed scheme is tested on a well-known fingerprint database and has shown promising results with the best accuracy rate of 98\\%.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03557",
        "title": "Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils",
        "authors": [
            "Martin Uecker",
            "Michael Lustig"
        ],
        "abstract": "Purpose: To develop an ESPIRiT-based method to estimate coil sensitivities with image phase as a building block for efficient and robust image reconstruction with phase constraints. Theory and Methods: ESPIRiT is a new framework for calibration of the coil sensitivities and reconstruction in parallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined set of physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploits conjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, a new post-processing step is proposed for the explicit computation of coil sensitivities that include the absolute phase of the image. The accuracy of the computed maps is directly validated using a test based on projection onto fully sampled coil images and also indirectly in phase-constrained parallel-imaging reconstructions. Results: The proposed method can estimate accurate sensitivities which include low-resolution image phase. In case of high-frequency phase variations VCC-ESPIRiT yields an additional set of maps that indicates the existence of a high-frequency phase component. Taking this additional set of maps into account can improve the robustness of phase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is a useful tool for phase-constrained imaging.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2016-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03602",
        "title": "DeepSat - A Learning framework for Satellite Imagery",
        "authors": [
            "Saikat Basu",
            "Sangram Ganguly",
            "Supratik Mukhopadhyay",
            "Robert DiBiano",
            "Manohar Karki",
            "Ramakrishna Nemani"
        ],
        "abstract": "Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold - (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ~11%. On SAT-6, it produces a classification accuracy of 93.9% and outperforms the other algorithms by ~15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.\n    ",
        "submission_date": "2015-09-11T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03660",
        "title": "Oracle MCG: A first peek into COCO Detection Challenges",
        "authors": [
            "Jordi Pont-Tuset",
            "Pablo Arbel\u00e1ez",
            "Luc Van Gool"
        ],
        "abstract": "The recently presented COCO detection challenge will most probably be the reference benchmark in object detection in the next years. COCO is two orders of magnitude larger than Pascal and has four times the number of categories; so in all likelihood researchers will be faced with a number of new challenges. At this point, without any finished round of the competition, it is difficult for researchers to put their techniques in context, or in other words, to know how good their results are. In order to give a little context, this note evaluates a hypothetical object detector consisting in an oracle picking the best object proposal from a state-of-the-art technique. This oracle achieves a AP=0.292 in segmented objects and AP=0.317 in bounding boxes, showing that indeed the database is challenging, given that this value is the best one can expect if working on object proposals without refinement.\n    ",
        "submission_date": "2015-08-14T00:00:00",
        "last_modified_date": "2015-08-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03877",
        "title": "Learning Contextual Dependencies with Convolutional Hierarchical Recurrent Neural Networks",
        "authors": [
            "Zhen Zuo",
            "Bing Shuai",
            "Gang Wang",
            "Xiao Liu",
            "Xingxing Wang",
            "Bing Wang"
        ],
        "abstract": "Existing deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependencies among different image regions. However, such dependencies are very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information among sequential data, and they only require a limited number of network parameters. General RNNs can hardly be directly applied on non-sequential data. Thus, we proposed the hierarchical RNNs (HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two recurrent neural network models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost; and 2) hierarchical long-short term memory recurrent network (HLSTM), which performs better than HSRN with the price of more computational cost.\n",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03891",
        "title": "On Binary Classification with Single-Layer Convolutional Neural Networks",
        "authors": [
            "Soroush Mehri"
        ],
        "abstract": "Convolutional neural networks are becoming standard tools for solving object recognition and visual tasks. However, most of the design and implementation of these complex models are based on trail-and-error. In this report, the main focus is to consider some of the important factors in designing convolutional networks to perform better. Specifically, classification with wide single-layer networks with large kernels as a general framework is considered. Particularly, we will show that pre-training using unsupervised schemes is vital, reasonable regularization is beneficial and applying of strong regularizers like dropout could be devastating. Pool size is also could be as important as learning procedure itself. In addition, it has been presented that using such a simple and relatively fast model for classifying cats and dogs, performance is close to state-of-the-art achievable by a combination of SVM models on color and texture features.\n    ",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2015-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03936",
        "title": "Learning Social Relation Traits from Face Images",
        "authors": [
            "Zhanpeng Zhang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Social relation defines the association, e.g, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03956",
        "title": "Learning to Divide and Conquer for Online Multi-Target Tracking",
        "authors": [
            "Francesco Solera",
            "Simone Calderara",
            "Rita Cucchiara"
        ],
        "abstract": "Online Multiple Target Tracking (MTT) is often addressed within the tracking-by-detection paradigm. Detections are previously extracted independently in each frame and then objects trajectories are built by maximizing specifically designed coherence functions. Nevertheless, ambiguities arise in presence of occlusions or detection errors. In this paper we claim that the ambiguities in tracking could be solved by a selective use of the features, by working with more reliable features if possible and exploiting a deeper representation of the target only if necessary. To this end, we propose an online divide and conquer tracker for static camera scenes, which partitions the assignment problem in local subproblems and solves them by selectively choosing and combining the best features. The complete framework is cast as a structural learning task that unifies these phases and learns tracker parameters from examples. Experiments on two different datasets highlights a significant improvement of tracking performances (MOTA +10%) over the state of the art.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04115",
        "title": "Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range Imaging",
        "authors": [
            "Changsoo Je",
            "Sang Wook Lee",
            "Rae-Hong Park"
        ],
        "abstract": "Active range sensing using structured-light is the most accurate and reliable method for obtaining 3D information. However, most of the work has been limited to range sensing of static objects, and range sensing of dynamic (moving or deforming) objects has been investigated recently only by a few researchers. Sinusoidal structured-light is one of the well-known optical methods for 3D measurement. In this paper, we present a novel method for rapid high-resolution range imaging using color sinusoidal pattern. We consider the real-world problem of nonlinearity and color-band crosstalk in the color light projector and color camera, and present methods for accurate recovery of color-phase. For high-resolution ranging, we use high-frequency patterns and describe new unwrapping algorithms for reliable range recovery. The experimental results demonstrate the effectiveness of our methods.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04186",
        "title": "Expanded Parts Model for Semantic Description of Humans in Still Images",
        "authors": [
            "Gaurav Sharma",
            "Frederic Jurie",
            "Cordelia Schmid"
        ],
        "abstract": "We introduce an Expanded Parts Model (EPM) for recognizing human attributes (e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) in still images. An EPM is a collection of part templates which are learnt discriminatively to explain specific scale-space regions in the images (in human centric coordinates). This is in contrast to current models which consist of a relatively few (i.e. a mixture of) 'average' templates. EPM uses only a subset of the parts to score an image and scores the image sparsely in space, i.e. it ignores redundant and random background in an image. To learn our model, we propose an algorithm which automatically mines parts and learns corresponding discriminative templates together with their respective locations from a large number of candidate parts. We validate our method on three recent challenging datasets of human attributes and actions. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04232",
        "title": "gSLICr: SLIC superpixels at over 250Hz",
        "authors": [
            "Carl Yuheng Ren",
            "Victor Adrian Prisacariu",
            "Ian D Reid"
        ],
        "abstract": "We introduce a parallel GPU implementation of the Simple Linear Iterative Clustering (SLIC) superpixel segmentation. Using a single graphic card, our implementation achieves speedups of up to $83\\times$ from the standard sequential implementation. Our implementation is fully compatible with the standard sequential implementation and the software is now available online and is open source.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04237",
        "title": "A Total Fractional-Order Variation Model for Image Restoration with Non-homogeneous Boundary Conditions and its Numerical Solution",
        "authors": [
            "Jianping Zhang",
            "Ke Chen"
        ],
        "abstract": "To overcome the weakness of a total variation based model for image restoration, various high order (typically second order) regularization models have been proposed and studied recently. In this paper we analyze and test a fractional-order derivative based total $\\alpha$-order variation model, which can outperform the currently popular high order regularization models. There exist several previous works using total $\\alpha$-order variations for image restoration; however first no analysis is done yet and second all tested formulations, differing from each other, utilize the zero Dirichlet boundary conditions which are not realistic (while non-zero boundary conditions violate definitions of fractional-order derivatives). This paper first reviews some results of fractional-order derivatives and then analyzes the theoretical properties of the proposed total $\\alpha$-order variational model rigorously. It then develops four algorithms for solving the variational problem, one based on the variational Split-Bregman idea and three based on direct solution of the discretise-optimization problem. Numerical experiments show that, in terms of restoration quality and solution efficiency, the proposed model can produce highly competitive results, for smooth images, to two established high order models: the mean curvature and the total generalized variation.\n    ",
        "submission_date": "2015-09-06T00:00:00",
        "last_modified_date": "2015-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04309",
        "title": "Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach",
        "authors": [
            "Xiaowei Zhou",
            "Menglong Zhu",
            "Spyridon Leonardos",
            "Kostas Daniilidis"
        ],
        "abstract": "We investigate the problem of estimating the 3D shape of an object defined by a set of 3D landmarks, given their 2D correspondences in a single image. A successful approach to alleviating the reconstruction ambiguity is the 3D deformable shape model and a sparse representation is often used to capture complex shape variability. But the model inference is still a challenge due to the nonconvexity in optimization resulted from joint estimation of shape and viewpoint. In contrast to prior work that relies on a alternating scheme with solutions depending on initialization, we propose a convex approach to addressing this challenge and develop an efficient algorithm to solve the proposed convex program. Moreover, we propose a robust model to handle gross errors in the 2D correspondences. We demonstrate the exact recovery property of the proposed method, the advantage compared to the nonconvex baseline methods and the applicability to recover 3D human poses and car models from single images.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2017-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04399",
        "title": "Analyzing structural characteristics of object category representations from their semantic-part distributions",
        "authors": [
            "Ravi Kiran Sarvadevabhatla",
            "Venkatesh Babu R"
        ],
        "abstract": "Studies from neuroscience show that part-mapping computations are employed by human visual system in the process of object recognition. In this work, we present an approach for analyzing semantic-part characteristics of object category representations. For our experiments, we use category-epitome, a recently proposed sketch-based spatial representation for objects. To enable part-importance analysis, we first obtain semantic-part annotations of hand-drawn sketches originally used to construct the corresponding epitomes. We then examine the extent to which the semantic-parts are present in the epitomes of a category and visualize the relative importance of parts as a word cloud. Finally, we show how such word cloud visualizations provide an intuitive understanding of category-level structural trends that exist in the category-epitome object representations.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04420",
        "title": "Neuron detection in stack images: a persistent homology interpretation",
        "authors": [
            "J\u00f3nathan Heras",
            "Gadea Mata",
            "Germ\u00e1n Cuesto",
            "Julio Rubio",
            "Miguel Morales"
        ],
        "abstract": "Automation and reliability are the two main requirements when computers are applied in Life Sciences. In this paper we report on an application to neuron recognition, an important step in our long-term project of providing software systems to the study of neural morphology and functionality from biomedical images. Our algorithms have been implemented in an ImageJ plugin called NeuronPersistentJ, which has been validated experimentally. The soundness and reliability of our approach are based on the interpretation of our processing methods with respect to persistent homology, a well-known tool in computational mathematics.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04581",
        "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images",
        "authors": [
            "Zhen Liu"
        ],
        "abstract": "With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demon- strated promising performance in various vision-based ap- plications, such as classification, recognition, and objec- t detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves lim- ited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the s- cenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motiva- tion by an experimental study to demonstrate the sensitivi- ty of the global CNN feature to the basic geometric trans- formations. Then, we propose to represent visual content with approximate invariance to the above geometric trans- formations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial repre- sentation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04619",
        "title": "Medical Image Classification via SVM using LBP Features from Saliency-Based Folded Data",
        "authors": [
            "Zehra Camlica",
            "H.R. Tizhoosh",
            "Farzad Khalvati"
        ],
        "abstract": "Good results on image classification and retrieval using support vector machines (SVM) with local binary patterns (LBPs) as features have been extensively reported in the literature where an entire image is retrieved or classified. In contrast, in medical imaging, not all parts of the image may be equally significant or relevant to the image retrieval application at hand. For instance, in lung x-ray image, the lung region may contain a tumour, hence being highly significant whereas the surrounding area does not contain significant information from medical diagnosis perspective. In this paper, we propose to detect salient regions of images during training and fold the data to reduce the effect of irrelevant regions. As a result, smaller image areas will be used for LBP features calculation and consequently classification by SVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the performance of the proposed approach. The results demonstrate the benefits of saliency-based folding approach that delivers comparable classification accuracies with state-of-the-art but exhibits lower computational cost and storage requirements, factors highly important for big data analytics.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04664",
        "title": "Self-Configuring and Evolving Fuzzy Image Thresholding",
        "authors": [
            "A. Othman",
            "H.R. Tizhoosh",
            "F. Khalvati"
        ],
        "abstract": "Every segmentation algorithm has parameters that need to be adjusted in order to achieve good results. Evolving fuzzy systems for adjustment of segmentation parameters have been proposed recently (Evolving fuzzy image segmentation -- EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few limitations when used in practice. As a major drawback, EFIS depends on detection of the object of interest for feature calculation, a task that is highly application-dependent. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to auto-configure the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require the detection of a region of interest (ROI).\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04706",
        "title": "Direct high-order edge-preserving regularization for tomographic image reconstruction",
        "authors": [
            "Daniil Kazantsev",
            "Evgueni Ovtchinnikov",
            "William R. B. Lionheart",
            "Philip J. Withers",
            "Peter D. Lee"
        ],
        "abstract": "In this paper we present a new two-level iterative algorithm for tomographic image reconstruction. The algorithm uses a regularization technique, which we call edge-preserving Laplacian, that preserves sharp edges between objects while damping spurious oscillations in the areas where the reconstructed image is smooth. Our numerical simulations demonstrate that the proposed method outperforms total variation (TV) regularization and it is competitive with the combined TV-L2 penalty. Obtained reconstructed images show increased signal-to-noise ratio and visually appealing structural features. Computer implementation and parameter control of the proposed technique is straightforward, which increases the feasibility of it across many tomographic applications. In this paper, we applied our method to the under-sampled computed tomography (CT) projection data and also considered a case of reconstruction in emission tomography The MATLAB code is provided to support obtained results.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04767",
        "title": "Zero-Shot Learning via Semantic Similarity Embedding",
        "authors": [
            "Ziming Zhang",
            "Venkatesh Saligrama"
        ],
        "abstract": "In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (\\eg attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04783",
        "title": "Group Membership Prediction",
        "authors": [
            "Ziming Zhang",
            "Yuting Chen",
            "Venkatesh Saligrama"
        ],
        "abstract": "The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a {\\it familial} relationship. In this context we propose a novel probability model and introduce latent {\\em view-specific} and {\\em view-shared} random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04853",
        "title": "SPECFACE - A Dataset of Human Faces Wearing Spectacles",
        "authors": [
            "Anirban Dasgupta",
            "Shubhobrata Bhattacharya",
            "Aurobinda Routray"
        ],
        "abstract": "This paper presents a database of human faces for persons wearing spectacles. The database consists of images of faces having significant variations with respect to illumination, head pose, skin color, facial expressions and sizes, and nature of spectacles. The database contains data of 60 subjects. This database is expected to be a precious resource for the development and evaluation of algorithms for face detection, eye detection, head tracking, eye gaze tracking, etc., for subjects wearing spectacles. As such, this can be a valuable contribution to the computer vision community.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04874",
        "title": "DenseBox: Unifying Landmark Localization with End to End Object Detection",
        "authors": [
            "Lichao Huang",
            "Yi Yang",
            "Yafeng Deng",
            "Yinan Yu"
        ],
        "abstract": "How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04887",
        "title": "An Improved Algorithm for Eye Corner Detection",
        "authors": [
            "Anirban Dasgupta",
            "Anshit Mandloi",
            "Anjith George",
            "Aurobinda Routray"
        ],
        "abstract": "In this paper, a modified algorithm for the detection of nasal and temporal eye corners is presented. The algorithm is a modification of the Santos and Proenka Method. In the first step, we detect the face and the eyes using classifiers based on Haar-like features. We then segment out the sclera, from the detected eye region. From the segmented sclera, we segment out an approximate eyelid contour. Eye corner candidates are obtained using Harris and Stephens corner detector. We introduce a post-pruning of the Eye corner candidates to locate the eye corners, finally. The algorithm has been tested on Yale, JAFFE databases as well as our created database.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04916",
        "title": "Projection Bank: From High-dimensional Data to Medium-length Binary Codes",
        "authors": [
            "Li Liu",
            "Mengyang Yu",
            "Ling Shao"
        ],
        "abstract": "Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional single linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but with a significantly smaller memory footprint and lower coding complexity.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04942",
        "title": "Guiding Long-Short Term Memory for Image Caption Generation",
        "authors": [
            "Xu Jia",
            "Efstratios Gavves",
            "Basura Fernando",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search in order to prevent from favoring short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or even outperform the current state-of-the-art.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04954",
        "title": "Human and Sheep Facial Landmarks Localisation by Triplet Interpolated Features",
        "authors": [
            "Heng Yang",
            "Renqiao Zhang",
            "Peter Robinson"
        ],
        "abstract": "In this paper we present a method for localisation of facial landmarks on human and sheep. We introduce a new feature extraction scheme called triplet-interpolated feature used at each iteration of the cascaded shape regression framework. It is able to extract features from similar semantic location given an estimated shape, even when head pose variations are large and the facial landmarks are very sparsely distributed. Furthermore, we study the impact of training data imbalance on model performance and propose a training sample augmentation scheme that produces more initialisations for training samples from the minority. More specifically, the augmentation number for a training sample is made to be negatively correlated to the value of the fitted probability density function at the sample's position. We evaluate the proposed scheme on both human and sheep facial landmarks localisation. On the benchmark 300w human face dataset, we demonstrate the benefits of our proposed methods and show very competitive performance when comparing to other methods. On a newly created sheep face dataset, we get very good performance despite the fact that we only have a limited number of training samples and a set of sparse landmarks are annotated.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05016",
        "title": "Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture",
        "authors": [
            "Ashesh Jain",
            "Avi Singh",
            "Hema S Koppula",
            "Shane Soh",
            "Ashutosh Saxena"
        ],
        "abstract": "Anticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4% to 90.5% and recall from 71.2% to 87.4%.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05054",
        "title": "Overcomplete Dictionary Learning with Jacobi Atom Updates",
        "authors": [
            "Paul Irofti",
            "Bogdan Dumitrescu"
        ],
        "abstract": "Dictionary learning for sparse representations is traditionally approached with sequential atom updates, in which an optimized atom is used immediately for the optimization of the next atoms. We propose instead a Jacobi version, in which groups of atoms are updated independently, in parallel. Extensive numerical evidence for sparse image representation shows that the parallel algorithms, especially when all atoms are updated simultaneously, give better dictionaries than their sequential counterparts.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05064",
        "title": "Exact simultaneous recovery of locations and structure from known orientations and corrupted point correspondences",
        "authors": [
            "Paul Hand",
            "Choongbum Lee",
            "Vladislav Voroninski"
        ],
        "abstract": "Let $t_1,\\ldots,t_{n_l} \\in \\mathbb{R}^d$ and $p_1,\\ldots,p_{n_s} \\in \\mathbb{R}^d$ and consider the bipartite location recovery problem: given a subset of pairwise direction observations $\\{(t_i - p_j) / \\|t_i - p_j\\|_2\\}_{i,j \\in [n_l] \\times [n_s]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\\{t_i\\}_{i \\in [n_ll]}$ and $\\{p_j\\}_{j \\in [n_s]}$ up to a global translation and scale. We study the recently introduced ShapeFit algorithm as a method for solving this bipartite location recovery problem. In this case, ShapeFit consists of a simple convex program over $d(n_l + n_s)$ real variables. We prove that this program recovers a set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by a bipartite Erd\u0151s-R\u00e9nyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. This recovery theorem is based on a set of deterministic conditions that we prove are sufficient for exact recovery. Finally, we propose a modified pipeline for the Structure for Motion problem, based on this bipartite location recovery problem.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05186",
        "title": "Accelerated Distance Computation with Encoding Tree for High Dimensional Data",
        "authors": [
            "Shicong Liu",
            "Junru Shao",
            "Hongtao Lu"
        ],
        "abstract": "We propose a novel distance to calculate distance between high dimensional vector pairs, utilizing vector quantization generated encodings. Vector quantization based methods are successful in handling large scale high dimensional data. These methods compress vectors into short encodings, and allow efficient distance computation between an uncompressed vector and compressed dataset without decompressing explicitly. However for large datasets, these distance computing methods perform excessive computations. We avoid excessive computations by storing the encodings on an Encoding Tree(E-Tree), interestingly the memory consumption is also lowered. We also propose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree and E-Forest is compatible with various existing quantization-based methods. We show by experiments our methods speed-up distance computing for high dimensional data drastically, and various existing algorithms can benefit from our methods.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05194",
        "title": "HCLAE: High Capacity Locally Aggregating Encodings for Approximate Nearest Neighbor Search",
        "authors": [
            "Shicong Liu",
            "Junru Shao",
            "Hongtao Lu"
        ],
        "abstract": "Vector quantization-based approaches are successful to solve Approximate Nearest Neighbor (ANN) problems which are critical to many applications. The idea is to generate effective encodings to allow fast distance approximation. We propose quantization-based methods should partition the data space finely and exhibit locality of the dataset to allow efficient non-exhaustive search. In this paper, we introduce the concept of High Capacity Locality Aggregating Encodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn HCLAE by a simulated annealing procedure. The quantization error is lower than other state-of-the-art. The algorithms of DA can be easily extended to an online learning scheme, allowing effective handle of large scale data. Further, we propose Aggregating-Tree (A-Tree), a non-exhaustive search method using HCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up on ANN-Search tasks, compared to the state-of-the-art.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05195",
        "title": "Improved Residual Vector Quantization for High-dimensional Approximate Nearest Neighbor Search",
        "authors": [
            "Shicong Liu",
            "Hongtao Lu",
            "Junru Shao"
        ],
        "abstract": "Quantization methods have been introduced to perform large scale approximate nearest search tasks. Residual Vector Quantization (RVQ) is one of the effective quantization methods. RVQ uses a multi-stage codebook learning scheme to lower the quantization error stage by stage. However, there are two major limitations for RVQ when applied to on high-dimensional approximate nearest neighbor search: 1. The performance gain diminishes quickly with added stages. 2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an improved residual vector quantization (IRVQ) method, our IRVQ learns codebook with a hybrid method of subspace clustering and warm-started k-means on each stage to prevent performance gain from dropping, and uses a multi-path encoding scheme to encode a vector with lower distortion. Experimental results on the benchmark datasets show that our method gives substantially improves RVQ and delivers better performance compared to the state-of-the-art.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05251",
        "title": "Hand-held Video Deblurring via Efficient Fourier Aggregation",
        "authors": [
            "Mauricio Delbracio",
            "Guillermo Sapiro"
        ],
        "abstract": "Videos captured with hand-held cameras often suffer from a significant amount of blur, mainly caused by the inevitable natural tremor of the photographer's hand. In this work, we present an algorithm that removes blur due to camera shake by combining information in the Fourier domain from nearby frames in a video. The dynamic nature of typical videos with the presence of multiple moving objects and occlusions makes this problem of camera shake removal extremely challenging, in particular when low complexity is needed. Given an input video frame, we first create a consistent registered version of temporally adjacent frames. Then, the set of consistently registered frames is block-wise fused in the Fourier domain with weights depending on the Fourier spectrum magnitude. The method is motivated from the physiological fact that camera shake blur has a random nature and therefore, nearby video frames are generally blurred differently. Experiments with numerous videos recorded in the wild, along with extensive comparisons, show that the proposed algorithm achieves state-of-the-art results while at the same time being much faster than its competitors.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05267",
        "title": "Deep Multi-task Learning for Railway Track Inspection",
        "authors": [
            "Xavier Gibert",
            "Vishal M. Patel",
            "Rama Chellappa"
        ],
        "abstract": "Railroad tracks need to be periodically inspected and monitored to ensure safe transportation. Automated track inspection using computer vision and pattern recognition methods have recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors. Achieving full automation is still very challenging due to the number of different possible failure modes as well as the broad range of image variations that can potentially trigger false alarms. Also, the number of defective components is very small, so not many training examples are available for the machine to learn a robust anomaly detector. In this paper, we show that detection performance can be improved by combining multiple detectors within a multi-task learning framework. We show that this approach results in better accuracy in detecting defects on railway ties and fasteners.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05329",
        "title": "Recurrent Spatial Transformer Networks",
        "authors": [
            "S\u00f8ren Kaae S\u00f8nderby",
            "Casper Kaae S\u00f8nderby",
            "Lars Maal\u00f8e",
            "Ole Winther"
        ],
        "abstract": "We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05360",
        "title": "Geometry-aware Deep Transform",
        "authors": [
            "Jiaji Huang",
            "Qiang Qiu",
            "Robert Calderbank",
            "Guillermo Sapiro"
        ],
        "abstract": "Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network; therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal $(K,\\epsilon)$-robustness analysis.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05366",
        "title": "Facial Descriptors for Human Interaction Recognition In Still Images",
        "authors": [
            "Gokhan Tanisik",
            "Cemil Zalluhoglu",
            "Nazli Ikizler-Cinbis"
        ],
        "abstract": "This paper presents a novel approach in a rarely studied area of computer vision: Human interaction recognition in still images. We explore whether the facial regions and their spatial configurations contribute to the recognition of interactions. In this respect, our method involves extraction of several visual features from the facial regions, as well as incorporation of scene characteristics and deep features to the recognition. Extracted multiple features are utilized within a discriminative learning framework for recognizing interactions between people. Our designed facial descriptors are based on the observation that relative positions, size and locations of the faces are likely to be important for characterizing human interactions. Since there is no available dataset in this relatively new domain, a comprehensive new dataset which includes several images of human interactions is collected. Our experimental results show that faces and scene characteristics contain important information to recognize interactions between people.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05371",
        "title": "DeXpression: Deep Convolutional Neural Network for Expression Recognition",
        "authors": [
            "Peter Burkert",
            "Felix Trier",
            "Muhammad Zeshan Afzal",
            "Andreas Dengel",
            "Marcus Liwicki"
        ],
        "abstract": "We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05463",
        "title": "Learning from Synthetic Data Using a Stacked Multichannel Autoencoder",
        "authors": [
            "Xi Zhang",
            "Yanwei Fu",
            "Shanshan Jiang",
            "Leonid Sigal",
            "Gady Agam"
        ],
        "abstract": "Learning from synthetic data has many important and practical applications. An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework-Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05520",
        "title": "An Experimental Survey on Correlation Filter-based Tracking",
        "authors": [
            "Zhe Chen",
            "Zhibin Hong",
            "Dacheng Tao"
        ],
        "abstract": "Over these years, Correlation Filter-based Trackers (CFTs) have aroused increasing interests in the field of visual object tracking, and have achieved extremely compelling results in different competitions and benchmarks. In this paper, our goal is to review the developments of CFTs with extensive experimental results. 11 trackers are surveyed in our work, based on which a general framework is summarized. Furthermore, we investigate different training schemes for correlation filters, and also discuss various effective improvements that have been made recently. Comprehensive experiments have been conducted to evaluate the effectiveness and efficiency of the surveyed CFTs, and comparisons have been made with other competing trackers. The experimental results have shown that state-of-art performance, in terms of robustness, speed and accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF. We find that further improvements for correlation filter-based tracking can be made on estimating scales, applying part-based tracking strategy and cooperating with long-term tracking methods.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05536",
        "title": "Efficient Clustering on Riemannian Manifolds: A Kernelised Random Projection Approach",
        "authors": [
            "Kun Zhao",
            "Azadeh Alavi",
            "Arnold Wiliem",
            "Brian C. Lovell"
        ],
        "abstract": "Reformulating computer vision problems over Riemannian manifolds has demonstrated superior performance in various computer vision applications. This is because visual data often forms a special structure lying on a lower dimensional space embedded in a higher dimensional space. However, since these manifolds belong to non-Euclidean topological spaces, exploiting their structures is computationally expensive, especially when one considers the clustering analysis of massive amounts of data. To this end, we propose an efficient framework to address the clustering problem on Riemannian manifolds. This framework implements random projections for manifold points via kernel space, which can preserve the geometric structure of the original space, but is computationally efficient. Here, we introduce three methods that follow our framework. We then validate our framework on several computer vision applications by comparing against popular clustering methods on Riemannian manifolds. Experimental results demonstrate that our framework maintains the performance of the clustering whilst massively reducing computational complexity by over two orders of magnitude in some cases.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05592",
        "title": "Color-Stripe Structured Light Robust to Surface Color and Discontinuity",
        "authors": [
            "Kwang Hee Lee",
            "Changsoo Je",
            "Sang Wook Lee"
        ],
        "abstract": "Multiple color stripes have been employed for structured light-based rapid range imaging to increase the number of uniquely identifiable stripes. The use of multiple color stripes poses two problems: (1) object surface color may disturb the stripe color and (2) the number of adjacent stripes required for identifying a stripe may not be maintained near surface discontinuities such as occluding boundaries. In this paper, we present methods to alleviate those problems. Log-gradient filters are employed to reduce the influence of object colors, and color stripes in two and three directions are used to increase the chance of identifying correct stripes near surface discontinuities. Experimental results demonstrate the effectiveness of our methods.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05634",
        "title": "Linearized Kernel Dictionary Learning",
        "authors": [
            "Alona Golts",
            "Michael Elad"
        ],
        "abstract": "In this paper we present a new approach of incorporating kernels into dictionary learning. The kernel K-SVD algorithm (KKSVD), which has been introduced recently, shows an improvement in classification performance, with relation to its linear counterpart K-SVD. However, this algorithm requires the storage and handling of a very large kernel matrix, which leads to high computational cost, while also limiting its use to setups with small number of training examples. We address these problems by combining two ideas: first we approximate the kernel matrix using a cleverly sampled subset of its columns using the Nystr\u00f6m method; secondly, as we wish to avoid using this matrix altogether, we decompose it by SVD to form new \"virtual samples,\" on which any linear dictionary learning can be employed. Our method, termed \"Linearized Kernel Dictionary Learning\" (LKDL) can be seamlessly applied as a pre-processing stage on top of any efficient off-the-shelf dictionary learning scheme, effectively \"kernelizing\" it. We demonstrate the effectiveness of our method on several tasks of both supervised and unsupervised classification and show the efficiency of the proposed scheme, its easy integration and performance boosting properties.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05844",
        "title": "Similar Handwritten Chinese Character Discrimination by Weakly Supervised Learning",
        "authors": [
            "Zhibo Yang",
            "Huanle Xu",
            "Keda Fu",
            "Yong Xia"
        ],
        "abstract": "Traditional approaches for handwritten Chinese character recognition suffer in classifying similar characters. In this paper, we propose to discriminate similar handwritten Chinese characters by using weakly supervised learning. Our approach learns a discriminative SVM for each similar pair which simultaneously localizes the discriminative region of similar character and makes the classification. For the first time, similar handwritten Chinese character recognition (SHCCR) is formulated as an optimization problem extended from SVM. We also propose a novel feature descriptor, Gradient Context, and apply bag-of-words model to represent regions with different scales. In our method, we do not need to select a sized-fixed sub-window to differentiate similar characters. The unconstrained property makes our method well adapted to high variance in the size and position of discriminative regions in similar handwritten Chinese characters. We evaluate our proposed approach over the CASIA Chinese character data set and the results show that our method outperforms the state of the art.\n    ",
        "submission_date": "2015-09-19T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05897",
        "title": "Face Photo Sketch Synthesis via Larger Patch and Multiresolution Spline",
        "authors": [
            "Xu Yang"
        ],
        "abstract": "Face photo sketch synthesis has got some researchers' attention in recent years because of its potential applications in digital entertainment and law enforcement. Some patches based methods have been proposed to solve this problem. These methods usually focus more on how to get a sketch patch for a given photo patch than how to blend these generated patches. However, without appropriately blending method, some jagged parts and mottled points will appear in the entire face sketch. In order to get a smoother sketch, we propose a new method to reduce such jagged parts and mottled points. In our system, we resort to an existed method, which is Markov Random Fields (MRF), to train a crude face sketch firstly. Then this crude sketch face sketch will be divided into some larger patches again and retrained by Non-Negative Matrix Factorization (NMF). At last, we use Multiresolution Spline and a blend trick named full-coverage trick to blend these retrained patches. The experiment results show that compared with some previous method, we can get a smoother face sketch.\n    ",
        "submission_date": "2015-09-19T00:00:00",
        "last_modified_date": "2015-09-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05909",
        "title": "Modelling Uncertainty in Deep Learning for Camera Relocalization",
        "authors": [
            "Alex Kendall",
            "Roberto Cipolla"
        ],
        "abstract": "We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 degrees accuracy for very large scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.\n    ",
        "submission_date": "2015-09-19T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06003",
        "title": "Robust Visual Tracking via Inverse Nonnegative Matrix Factorization",
        "authors": [
            "Fanghui Liu",
            "Tao Zhou",
            "Keren Fu",
            "Irene Y.H. Gu",
            "Jie Yang"
        ],
        "abstract": "The establishment of robust target appearance model over time is an overriding concern in visual tracking. In this paper, we propose an inverse nonnegative matrix factorization (NMF) method for robust appearance modeling. Rather than using a linear combination of nonnegative basis matrices for each target image patch in the conventional NMF, the proposed method is a reverse thought to conventional NMF tracker. It utilizes both the foreground and background information, and imposes a local coordinate constraint, where the basis matrix is sparse matrix from the linear combination of candidates with corresponding nonnegative coefficient vectors. Inverse NMF is used as a feature encoder, where the resulting coefficient vectors are fed into a SVM classifier for separating the target from the background. The proposed method is tested on several videos and compared with seven state-of-the-art methods. Our results have provided further support to the effectiveness and robustness of the proposed method.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06004",
        "title": "A Parallel Framework for Parametric Maximum Flow Problems in Image Segmentation",
        "authors": [
            "Vlad Olaru",
            "Mihai Florea",
            "Cristian Sminchisescu"
        ],
        "abstract": "This paper presents a framework that supports the implementation of parallel solutions for the widespread parametric maximum flow computational routines used in image segmentation algorithms. The framework is based on supergraphs, a special construction combining several image graphs into a larger one, and works on various architectures (multi-core or GPU), either locally or remotely in a cluster of computing nodes. The framework can also be used for performance evaluation of parallel implementations of maximum flow algorithms. We present the case study of a state-of-the-art image segmentation algorithm based on graph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallel framework to solve parametric maximum flow problems, based on a GPU implementation of the well-known push-relabel algorithm. Our results indicate that real-time implementations based on the proposed techniques are possible.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06016",
        "title": "Image Set Querying Based Localization",
        "authors": [
            "Lei Deng",
            "Siyuan Huang",
            "Yueqi Duan",
            "Baohua Chen",
            "Jie Zhou"
        ],
        "abstract": "Conventional single image based localization methods usually fail to localize a querying image when there exist large variations between the querying image and the pre-built scene. To address this, we propose an image-set querying based localization approach. When the localization by a single image fails to work, the system will ask the user to capture more auxiliary images. First, a local 3D model is established for the querying image set. Then, the pose of the querying image set is estimated by solving a nonlinear optimization problem, which aims to match the local 3D model against the pre-built scene. Experiments have shown the effectiveness and feasibility of the proposed approach.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06033",
        "title": "Deep Convolutional Features for Image Based Retrieval and Scene Categorization",
        "authors": [
            "Arsalan Mousavian",
            "Jana Kosecka"
        ],
        "abstract": "Several recent approaches showed how the representations learned by Convolutional Neural Networks can be repurposed for novel tasks. Most commonly it has been shown that the activation features of the last fully connected layers (fc7 or fc6) of the network, followed by a linear classifier outperform the state-of-the-art on several recognition challenge datasets. Instead of recognition, this paper focuses on the image retrieval problem and proposes a examines alternative pooling strategies derived for CNN features. The presented scheme uses the features maps from an earlier layer 5 of the CNN architecture, which has been shown to preserve coarse spatial information and is semantically meaningful. We examine several pooling strategies and demonstrate superior performance on the image retrieval task (INRIA Holidays) at the fraction of the computational cost, while using a relatively small memory requirements. In addition to retrieval, we see similar efficiency gains on the SUN397 scene categorization dataset, demonstrating wide applicability of this simple strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from different geographical locations in the world for image retrieval that stresses more dramatic changes in appearance and viewpoint.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06035",
        "title": "Image Retrieval Based on LBP Pyramidal Multiresolution using Reversible Watermarking",
        "authors": [
            "H.Ouahi",
            "K.Afdel",
            "M.Machkour"
        ],
        "abstract": "In the medical field, images are increasingly used to facilitate diagnosis of diseases. These images are stored in multimedia databases accompanied by doctor s prescriptions and other information related to ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06041",
        "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks",
        "authors": [
            "Quanzeng You",
            "Jiebo Luo",
            "Hailin Jin",
            "Jianchao Yang"
        ],
        "abstract": "Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06066",
        "title": "On Large-Scale Retrieval: Binary or n-ary Coding?",
        "authors": [
            "Mahyar Najibi",
            "Mohammad Rastegari",
            "Larry S. Davis"
        ],
        "abstract": "The growing amount of data available in modern-day datasets makes the need to efficiently search and retrieve information. To make large-scale search feasible, Distance Estimation and Subset Indexing are the main approaches. Although binary coding has been popular for implementing both techniques, n-ary coding (known as Product Quantization) is also very effective for Distance Estimation. However, their relative performance has not been studied for Subset Indexing. We investigate whether binary or n-ary coding works better under different retrieval strategies. This leads to the design of a new n-ary coding method, \"Linear Subspace Quantization (LSQ)\" which, unlike other n-ary encoders, can be used as a similarity-preserving embedding. Experiments on image retrieval show that when Distance Estimation is used, n-ary LSQ outperforms other methods. However, when Subset Indexing is applied, interestingly, binary codings are more effective and binary LSQ achieves the best accuracy.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2015-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06086",
        "title": "Fusing Multi-Stream Deep Networks for Video Classification",
        "authors": [
            "Zuxuan Wu",
            "Yu-Gang Jiang",
            "Xi Wang",
            "Hao Ye",
            "Xiangyang Xue",
            "Jun Wang"
        ],
        "abstract": "This paper studies deep network architectures to address the problem of video classification. A multi-stream framework is proposed to fully utilize the rich multimodal information in videos. Specifically, we first train three Convolutional Neural Networks to model spatial, short-term motion and audio clues respectively. Long Short Term Memory networks are then adopted to explore long-term temporal dynamics. With the outputs of the individual streams, we propose a simple and effective fusion method to generate the final predictions, where the optimal fusion weights are learned adaptively for each class, and the learning process is regularized by automatically estimated class relationships. Our contributions are two-fold. First, the proposed multi-stream framework is able to exploit multimodal features that are more comprehensive than those previously attempted. Second, we demonstrate that the adaptive fusion method using the class relationship as a regularizer outperforms traditional alternatives that estimate the weights in a \"free\" fashion. Our framework produces significantly better results than the state of the arts on two popular benchmarks, 92.2\\% on UCF-101 (without using audio) and 84.9\\% on Columbia Consumer Videos.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06114",
        "title": "Vision System and Depth Processing for DRC-HUBO+",
        "authors": [
            "Inwook Shim",
            "Seunghak Shin",
            "Yunsu Bok",
            "Kyungdon Joo",
            "Dong-Geol Choi",
            "Joon-Young Lee",
            "Jaesik Park",
            "Jun-Ho Oh",
            "In So Kweon"
        ],
        "abstract": "This paper presents a vision system and a depth processing algorithm for DRC-HUBO+, the winner of the DRC finals 2015. Our system is designed to reliably capture 3D information of a scene and objects robust to challenging environment conditions. We also propose a depth-map upsampling method that produces an outliers-free depth map by explicitly handling depth outliers. Our system is suitable for an interactive robot with real-world that requires accurate object detection and pose estimation. We evaluate our depth processing algorithm over state-of-the-art algorithms on several synthetic and real-world datasets.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06161",
        "title": "On 3D Face Reconstruction via Cascaded Regression in Shape Space",
        "authors": [
            "Feng Liu",
            "Dan Zeng",
            "Jing Li",
            "Qijun Zhao"
        ],
        "abstract": "Cascaded regression has been recently applied to reconstructing 3D faces from single 2D images directly in shape space, and achieved state-of-the-art performance. This paper investigates thoroughly such cascaded regression based 3D face reconstruction approaches from four perspectives that are not well studied yet: (i) The impact of the number of 2D landmarks; (ii) the impact of the number of 3D vertices; (iii) the way of using standalone automated landmark detection methods; and (iv) the convergence property. To answer these questions, a simplified cascaded regression based 3D face reconstruction method is devised, which can be integrated with standalone automated landmark detection methods and reconstruct 3D face shapes that have the same pose and expression as the input face images, rather than normalized pose and expression. Moreover, an effective training method is proposed by disturbing the automatically detected landmarks. Comprehensive evaluation experiments have been done with comparison to other 3D face reconstruction methods. The results not only deepen the understanding of cascaded regression based 3D face reconstruction approaches, but also prove the effectiveness of proposed method.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2017-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06243",
        "title": "LEWIS: Latent Embeddings for Word Images and their Semantics",
        "authors": [
            "Albert Gordo",
            "Jon Almazan",
            "Naila Murray",
            "Florent Perronnin"
        ],
        "abstract": "The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration.\n",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06321",
        "title": "Evaluating the visualization of what a Deep Neural Network has learned",
        "authors": [
            "Wojciech Samek",
            "Alexander Binder",
            "Gr\u00e9goire Montavon",
            "Sebastian Bach",
            "Klaus-Robert M\u00fcller"
        ],
        "abstract": "Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ''importance'' of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2015-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06451",
        "title": "From Facial Parts Responses to Face Detection: A Deep Learning Approach",
        "authors": [
            "Shuo Yang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06470",
        "title": "Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks",
        "authors": [
            "Yiyi Liao",
            "Sarath Kodagoda",
            "Yue Wang",
            "Lei Shi",
            "Yong Liu"
        ],
        "abstract": "Scene classification is a fundamental perception task for environmental understanding in today's robotics. In this paper, we have attempted to exploit the use of popular machine learning technique of deep learning to enhance scene understanding, particularly in robotics applications. As scene images have larger diversity than the iconic object images, it is more challenging for deep learning methods to automatically learn features from scene images with less samples. Inspired by human scene understanding based on object knowledge, we address the problem of scene classification by encouraging deep neural networks to incorporate object-level information. This is implemented with a regularization of semantic segmentation. With only 5 thousand training images, as opposed to 2.5 million images, we show the proposed deep architecture achieves superior scene classification results to the state-of-the-art on a publicly available SUN RGB-D dataset. In addition, performance of semantic segmentation, the regularizer, also reaches a new record with refinement derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset trained model to a mobile robot captured images to classify scenes in our university demonstrating the generalization ability of the proposed algorithm.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06557",
        "title": "Local Multi-Grouped Binary Descriptor with Ring-based Pooling Configuration and Optimization",
        "authors": [
            "Yongqiang Gao",
            "Weilin Huang",
            "Yu Qiao"
        ],
        "abstract": "Local binary descriptors are attracting increasingly attention due to their great advantages in computational speed, which are able to achieve real-time performance in numerous image/vision applications. Various methods have been proposed to learn data-dependent binary descriptors. However, most existing binary descriptors aim overly at computational simplicity at the expense of significant information loss which causes ambiguity in similarity measure using Hamming distance. In this paper, by considering multiple features might share complementary information, we present a novel local binary descriptor, referred as Ring-based Multi-Grouped Descriptor (RMGD), to successfully bridge the performance gap between current binary and floated-point descriptors. Our contributions are two-fold. Firstly, we introduce a new pooling configuration based on spatial ring-region sampling, allowing for involving binary tests on the full set of pairwise regions with different shapes, scales and distances. This leads to a more meaningful description than existing methods which normally apply a limited set of pooling configurations. Then, an extended Adaboost is proposed for efficient bit selection by emphasizing high variance and low correlation, achieving a highly compact representation. Secondly, the RMGD is computed from multiple image properties where binary strings are extracted. We cast multi-grouped features integration as rankSVM or sparse SVM learning problem, so that different features can compensate strongly for each other, which is the key to discriminativeness and robustness. The performance of RMGD was evaluated on a number of publicly available benchmarks, where the RMGD outperforms the state-of-the-art binary descriptors significantly.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06658",
        "title": "Attribute-Graph: A Graph based approach to Image Ranking",
        "authors": [
            "Nikita Prabhu",
            "R. Venkatesh Babu"
        ],
        "abstract": "We propose a novel image representation, termed Attribute-Graph, to rank images by their semantic similarity to a given query image. An Attribute-Graph is an undirected fully connected graph, incorporating both local and global image characteristics. The graph nodes characterise objects as well as the overall scene context using mid-level semantic attributes, while the edges capture the object topology. We demonstrate the effectiveness of Attribute-Graphs by applying them to the problem of image ranking. We benchmark the performance of our algorithm on the 'rPascal' and 'rImageNet' datasets, which we have created in order to evaluate the ranking performance on complex queries containing multiple objects. Our experimental evaluation shows that modelling images as Attribute-Graphs results in improved ranking performance over existing techniques.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06720",
        "title": "A Dual-Source Approach for 3D Pose Estimation from a Single Image",
        "authors": [
            "Hashim Yasin",
            "Umar Iqbal",
            "Bj\u00f6rn Kr\u00fcger",
            "Andreas Weber",
            "Juergen Gall"
        ],
        "abstract": "One major challenge for 3D pose estimation from a single RGB image is the acquisition of sufficient training data. In particular, collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of images with annotated 2D poses and the second source consists of accurate 3D motion capture data. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient and robust 3D pose retrieval. In our experiments, we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2016-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06729",
        "title": "Algebraic Clustering of Affine Subspaces",
        "authors": [
            "Manolis C. Tsakiris",
            "Rene Vidal"
        ],
        "abstract": "Subspace clustering is an important problem in machine learning with many applications in computer vision and pattern recognition. Prior work has studied this problem using algebraic, iterative, statistical, low-rank and sparse representation techniques. While these methods have been applied to both linear and affine subspaces, theoretical results have only been established in the case of linear subspaces. For example, algebraic subspace clustering (ASC) is guaranteed to provide the correct clustering when the data points are in general position and the union of subspaces is transversal. In this paper we study in a rigorous fashion the properties of ASC in the case of affine subspaces. Using notions from algebraic geometry, we prove that the homogenization trick, which embeds points in a union of affine subspaces into points in a union of linear subspaces, preserves the general position of the points and the transversality of the union of subspaces in the embedded space, thus establishing the correctness of ASC for affine subpaces.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2017-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06853",
        "title": "New Fuzzy LBP Features for Face Recognition",
        "authors": [
            "Abdullah Gubbi",
            "Mohammed Fazle Azeem",
            "Zahid Ansari"
        ],
        "abstract": "There are many Local texture features each very in way they implement and each of the Algorithm trying improve the performance. An attempt is made in this paper to represent a theoretically very simple and computationally effective approach for face recognition. In our implementation the face image is divided into 3x3 sub-regions from which the features are extracted using the Local Binary Pattern (LBP) over a window, fuzzy membership function and at the central pixel. The LBP features possess the texture discriminative property and their computational cost is very low. By utilising the information from LBP, membership function, and central pixel, the limitations of traditional LBP is eliminated. The bench mark database like ORL and Sheffield Databases are used for the evaluation of proposed features with SVM classifier. For the proposed approach K-fold and ROC curves are obtained and results are compared.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06925",
        "title": "Robust Object Tracking with a Hierarchical Ensemble Framework",
        "authors": [
            "Mengmeng Wang",
            "Yong Liu"
        ],
        "abstract": "Autonomous robots enjoy a wide popularity nowadays and have been applied in many applications, such as home security, entertainment, delivery, navigation and guidance. It is vital to robots to track objects accurately in these applications, so it is necessary to focus on tracking algorithms to improve the robustness and accuracy. In this paper, we propose a robust object tracking algorithm based on a hierarchical ensemble framework which can incorporate information including individual pixel features, local patches and holistic target models. The framework combines multiple ensemble models simultaneously instead of using a single ensemble model individually. A discriminative model which accounts for the matching degree of local patches is adopted via a bottom ensemble layer, and a generative model which exploits holistic templates is used to search for the object through the middle ensemble layer as well as an adaptive Kalman filter. We test the proposed tracker on challenging benchmark image sequences. Both qualitative and quantitative evaluations demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms, especially when the appearance changes dramatically and the occlusions occur.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2016-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07009",
        "title": "Is Image Super-resolution Helpful for Other Vision Tasks?",
        "authors": [
            "Dengxin Dai",
            "Yujian Wang",
            "Yuhua Chen",
            "Luc Van Gool"
        ],
        "abstract": "Despite the great advances made in the field of image super-resolution (ISR) during the last years, the performance has merely been evaluated perceptually. Thus, it is still unclear whether ISR is helpful for other vision tasks. In this paper, we present the first comprehensive study and analysis of the usefulness of ISR for other vision applications. In particular, six ISR methods are evaluated on four popular vision tasks, namely edge detection, semantic image segmentation, digit recognition, and scene recognition. We show that applying ISR to input images of other vision systems does improve their performance when the input images are of low-resolution. We also study the correlation between four standard perceptual evaluation criteria (namely PSNR, SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments show that they correlate well with each other in general, but perceptual criteria are still not accurate enough to be used as full proxies for the usefulness. We hope this work will inspire the community to evaluate ISR methods also in real vision applications, and to adopt ISR as a pre-processing step of other vision tasks if the resolution of their input images is low.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07075",
        "title": "3D Scan Registration using Curvelet Features in Planetary Environments",
        "authors": [
            "Siddhant Ahuja",
            "Peter Iles",
            "Steven L. Waslander"
        ],
        "abstract": "Topographic mapping in planetary environments relies on accurate 3D scan registration methods. However, most global registration algorithms relying on features such as FPFH and Harris-3D show poor alignment accuracy in these settings due to the poor structure of the Mars-like terrain and variable resolution, occluded, sparse range data that is hard to register without some a-priori knowledge of the environment. In this paper, we propose an alternative approach to 3D scan registration using the curvelet transform that performs multi-resolution geometric analysis to obtain a set of coefficients indexed by scale (coarsest to finest), angle and spatial position. Features are detected in the curvelet domain to take advantage of the directional selectivity of the transform. A descriptor is computed for each feature by calculating the 3D spatial histogram of the image gradients, and nearest neighbor based matching is used to calculate the feature correspondences. Correspondence rejection using Random Sample Consensus identifies inliers, and a locally optimal Singular Value Decomposition-based estimation of the rigid-body transformation aligns the laser scans given the re-projected correspondences in the metric space. Experimental results on a publicly available data-set of planetary analogue indoor facility, as well as simulated and real-world scans from Neptec Design Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard demonstrates improved performance over existing methods in the challenging sparse Mars-like terrain.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07225",
        "title": "Automatic Concept Discovery from Parallel Text and Visual Corpora",
        "authors": [
            "Chen Sun",
            "Chuang Gan",
            "Ram Nevatia"
        ],
        "abstract": "Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora; it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07244",
        "title": "Multi-Region Probabilistic Dice Similarity Coefficient using the Aitchison Distance and Bipartite Graph Matching",
        "authors": [
            "Shawn Andrews",
            "Ghassan Hamarneh"
        ],
        "abstract": "Validation of image segmentation methods is of critical importance. Probabilistic image segmentation is increasingly popular as it captures uncertainty in the results. Image segmentation methods that support multi-region (as opposed to binary) delineation are more favourable as they capture interactions between the different objects in the image. The Dice similarity coefficient (DSC) has been a popular metric for evaluating the accuracy of automated or semi-automated segmentation methods by comparing their results to the ground truth. In this work, we develop an extension of the DSC to multi-region probabilistic segmentations (with unordered labels). We use bipartite graph matching to establish label correspondences and propose two functions that extend the DSC, one based on absolute probability differences and one based on the Aitchison distance. These provide a robust and accurate measure of multi-region probabilistic segmentation accuracy.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07473",
        "title": "Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences",
        "authors": [
            "Andreas Veit",
            "Balazs Kovacs",
            "Sean Bell",
            "Julian McAuley",
            "Kavita Bala",
            "Serge Belongie"
        ],
        "abstract": "With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like `What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible. We model compatibility based on co-occurrence in large-scale user behavior data; in particular co-purchase data from ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07479",
        "title": "Learning Concept Embeddings with Combined Human-Machine Expertise",
        "authors": [
            "Michael J. Wilber",
            "Iljung S. Kwak",
            "David Kriegman",
            "Serge Belongie"
        ],
        "abstract": "This paper presents our work on \"SNaCK,\" a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07611",
        "title": "Incremental Loop Closure Verification by Guided Sampling",
        "authors": [
            "Kanji Tanaka"
        ],
        "abstract": "Loop closure detection, the task of identifying locations revisited by a robot in a sequence of odometry and perceptual observations, is typically formulated as a combination of two subtasks: (1) bag-of-words image retrieval and (2) post-verification using RANSAC geometric verification. The main contribution of this study is the proposal of a novel post-verification framework that achieves good precision recall trade-off in loop closure detection. This study is motivated by the fact that not all loop closure hypotheses are equally plausible (e.g., owing to mutual consistency between loop closure constraints) and that if we have evidence that one hypothesis is more plausible than the others, then it should be verified more frequently. We demonstrate that the problem of loop closure detection can be viewed as an instance of a multi-model hypothesize-and-verify framework and build guided sampling strategies on the framework where loop closures proposed using image retrieval are verified in a planned order (rather than in a conventional uniform order) to operate in a constant time. Experimental results using a stereo SLAM system confirm that the proposed strategy, the use of loop closure constraints and robot trajectory hypotheses as a guide, achieves promising results despite the fact that there exists a significant number of false positive constraints and hypotheses.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07615",
        "title": "Discriminative Map Retrieval Using View-Dependent Map Descriptor",
        "authors": [
            "Enfu Liu",
            "Kanji Tanaka"
        ],
        "abstract": "Map retrieval, the problem of similarity search over a large collection of 2D pointset maps previously built by mobile robots, is crucial for autonomous navigation in indoor and outdoor environments. Bag-of-words (BoW) methods constitute a popular approach to map retrieval; however, these methods have extremely limited descriptive ability because they ignore the spatial layout information of the local features. The main contribution of this paper is an extension of the bag-of-words map retrieval method to enable the use of spatial information from local features. Our strategy is to explicitly model a unique viewpoint of an input local map; the pose of the local feature is defined with respect to this unique viewpoint, and can be viewed as an additional invariant feature for discriminative map retrieval. Specifically, we wish to determine a unique viewpoint that is invariant to moving objects, clutter, occlusions, and actual viewpoints. Hence, we perform scene parsing to analyze the scene structure, and consider the \"center\" of the scene structure to be the unique viewpoint. Our scene parsing is based on a Manhattan world grammar that imposes a quasi-Manhattan world constraint to enable the robust detection of a scene structure that is invariant to clutter and moving objects. Experimental results using the publicly available radish dataset validate the efficacy of the proposed approach.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07618",
        "title": "Self-localization Using Visual Experience Across Domains",
        "authors": [
            "Taisho Tsukamoto",
            "Kanji Tanaka"
        ],
        "abstract": "In this study, we aim to solve the single-view robot self-localization problem by using visual experience across domains. Although the bag-of-words method constitutes a popular approach to single-view localization, it fails badly when it's visual vocabulary is learned and tested in different domains. Further, we are interested in using a cross-domain setting, in which the visual vocabulary is learned in different seasons and routes from the input query/database scenes. Our strategy is to mine a cross-domain visual experience, a library of raw visual images collected in different domains, to discover the relevant visual patterns that effectively explain the input scene, and use them for scene retrieval. In particular, we show that the appearance and the pose of the mined visual patterns of a query scene can be efficiently and discriminatively matched against those of the database scenes by employing image-to-class distance and spatial pyramid matching. Experimental results obtained using a novel cross-domain dataset show that our system achieves promising results despite our visual vocabulary being learned and tested in different domains.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07627",
        "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection",
        "authors": [
            "Hirokatsu Kataoka",
            "Kenji Iwata",
            "Yutaka Satoh"
        ],
        "abstract": "In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture and very deep convolutional network (VGGNet) architecture. To date, most CNN researchers have employed the last layers before output, which were extracted from the fully connected feature layers. However, since it is unlikely that feature representation effectiveness is dependent on the problem, this study evaluates additional convolutional layers that are adjacent to fully connected layers, in addition to executing simple tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and transformation, using tools such as principal component analysis. In our experiments, we carried out detection and classification tasks using the Caltech 101 and Daimler Pedestrian Benchmark Datasets.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07838",
        "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation",
        "authors": [
            "Catalin Ionescu",
            "Orestis Vantzos",
            "Cristian Sminchisescu"
        ],
        "abstract": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. The proposed matrix backpropagation methodology applies broadly to a variety of problems in machine learning or computational perception. Here we illustrate it by performing visual segmentation experiments using the BSDS and MSCOCO benchmarks, where we show that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07845",
        "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval",
        "authors": [
            "Bharat Singh",
            "Xintong Han",
            "Zhe Wu",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query--some combinations of concepts may be visually compact but irrelevant--and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07979",
        "title": "Anomaly Detection in Unstructured Environments using Bayesian Nonparametric Scene Modeling",
        "authors": [
            "Yogesh Girdhar",
            "Walter Cho",
            "Matthew Campbell",
            "Jesus Pineda",
            "Elizabeth Clarke",
            "Hanumant Singh"
        ],
        "abstract": "This paper explores the use of a Bayesian non-parametric topic modeling technique for the purpose of anomaly detection in video data. We present results from two experiments. The first experiment shows that the proposed technique is automatically able characterize the underlying terrain, and detect anomalous flora in image data collected by an underwater robot. The second experiment shows that the same technique can be used on images from a static camera in a dynamic unstructured environment. In the second dataset, consisting of video data from a static seafloor camera capturing images of a busy coral reef, the proposed technique was able to detect all three instances of an underwater vehicle passing in front of the camera, amongst many other observations of fishes, debris, lighting changes due to surface waves, and benthic flora.\n    ",
        "submission_date": "2015-09-26T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08067",
        "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs",
        "authors": [
            "Tianfu Wu",
            "Yang Lu",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. %The AOG captures both structural and appearance variations of a target object in a principled way. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2016-09-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08075",
        "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing",
        "authors": [
            "Hamid Izadinia",
            "Fereshteh Sadeghi",
            "Santosh Kumar Divvala",
            "Yejin Choi",
            "Ali Farhadi"
        ],
        "abstract": "We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08082",
        "title": "Multivariate Median Filters and Partial Differential Equations",
        "authors": [
            "Martin Welk"
        ],
        "abstract": "Multivariate median filters have been proposed as generalisations of the well-established median filter for grey-value images to multi-channel images. As multivariate median, most of the recent approaches use the $L^1$ median, i.e.\\ the minimiser of an objective function that is the sum of distances to all input points. Many properties of univariate median filters generalise to such a filter. However, the famous result by Guichard and Morel about approximation of the mean curvature motion PDE by median filtering does not have a comparably simple counterpart for $L^1$ multivariate median filtering. We discuss the affine equivariant Oja median and the affine equivariant transformation--retransformation $L^1$ median as alternatives to $L^1$ median filtering. We analyse multivariate median filters in a space-continuous setting, including the formulation of a space-continuous version of the transformation--retransformation $L^1$ median, and derive PDEs approximated by these filters in the cases of bivariate planar images, three-channel volume images and three-channel planar images. The PDEs for the affine equivariant filters can be interpreted geometrically as combinations of a diffusion and a principal-component-wise curvature motion contribution with a cross-effect term based on torsions of principal components. Numerical experiments are presented that demonstrate the validity of the approximation results.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08147",
        "title": "Amodal Completion and Size Constancy in Natural Scenes",
        "authors": [
            "Abhishek Kar",
            "Shubham Tulsiani",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08182",
        "title": "Robust video object tracking using particle filter with likelihood based feature fusion and adaptive template updating",
        "authors": [
            "Yi Dai",
            "Bin Liu"
        ],
        "abstract": "A robust algorithm solution is proposed for tracking an object in complex video scenes. In this solution, the bootstrap particle filter (PF) is initialized by an object detector, which models the time-evolving background of the video signal by an adaptive Gaussian mixture. The motion of the object is expressed by a Markov model, which defines the state transition prior. The color and texture features are used to represent the object, and a marginal likelihood based feature fusion approach is proposed. A corresponding object template model updating procedure is developed to account for possible scale changes of the object in the tracking process. Experimental results show that our algorithm beats several existing alternatives in tackling challenging scenarios in video tracking tasks.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08197",
        "title": "Fast Non-local Stereo Matching based on Hierarchical Disparity Prediction",
        "authors": [
            "Xuan Luo",
            "Xuejiao Bai",
            "Shuo Li",
            "Hongtao Lu",
            "Sei-ichiro Kamata"
        ],
        "abstract": "Stereo matching is the key step in estimating depth from two or more images. Recently, some tree-based non-local stereo matching methods have been proposed, which achieved state-of-the-art performance. The algorithms employed some tree structures to aggregate cost and thus improved the performance and reduced the coputation load of the stereo matching. However, the computational complexity of these tree-based algorithms is still high because they search over the entire disparity range. In addition, the extreme greediness of the minimum spanning tree (MST) causes the poor performance in large areas with similar colors but varying disparities. In this paper, we propose an efficient stereo matching method using a hierarchical disparity prediction (HDP) framework to dramatically reduce the disparity search range so as to speed up the tree-based non-local stereo methods. Our disparity prediction scheme works on a graph pyramid derived from an image whose disparity to be estimated. We utilize the disparity of a upper graph to predict a small disparity range for the lower graph. Some independent disparity trees (DT) are generated to form a disparity prediction forest (HDPF) over which the cost aggregation is made. When combined with the state-of-the-art tree-based methods, our scheme not only dramatically speeds up the original methods but also improves their performance by alleviating the second drawback of the tree-based methods. This is partially because our DTs overcome the extreme greediness of the MST. Extensive experimental results on some benchmark datasets demonstrate the effectiveness and efficiency of our framework. For example, the segment-tree based stereo matching becomes about 25.57 times faster and 2.2% more accurate over the Middlebury 2006 full-size dataset.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08379",
        "title": "Learning FRAME Models Using CNN Filters",
        "authors": [
            "Yang Lu",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ],
        "abstract": "The convolutional neural network (ConvNet or CNN) has proven to be very successful in many tasks such as those in computer vision. In this conceptual paper, we study the generative perspective of the discriminative CNN. In particular, we propose to learn the generative FRAME (Filters, Random field, And Maximum Entropy) model using the highly expressive filters pre-learned by the CNN at the convolutional layers. We show that the learning algorithm can generate realistic and rich object and texture patterns in natural scenes. We explain that each learned model corresponds to a new CNN unit at a layer above the layer of filters employed by the model. We further show that it is possible to learn a new layer of CNN units using a generative CNN model, which is a product of experts model, and the learning algorithm admits an EM interpretation with binary latent variables.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08383",
        "title": "Efficient Discriminative Nonorthogonal Binary Subspace with its Application to Visual Tracking",
        "authors": [
            "Ang Li",
            "Feng Tang",
            "Yanwen Guo",
            "Hai Tao"
        ],
        "abstract": "One of the crucial problems in visual tracking is how the object is represented. Conventional appearance-based trackers are using increasingly more complex features in order to be robust. However, complex representations typically not only require more computation for feature extraction, but also make the state inference complicated. We show that with a careful feature selection scheme, extremely simple yet discriminative features can be used for robust object tracking. The central component of the proposed method is a succinct and discriminative representation of the object using discriminative non-orthogonal binary subspace (DNBS) which is spanned by Haar-like features. The DNBS representation inherits the merits of the original NBS in that it efficiently describes the object. It also incorporates the discriminative information to distinguish foreground from background. However, the problem of finding the DNBS bases from an over-complete dictionary is NP-hard. We propose a greedy algorithm called discriminative optimized orthogonal matching pursuit (D-OOMP) to solve this problem. An iterative formulation named iterative D-OOMP is further developed to drastically reduce the redundant computation between iterations and a hierarchical selection strategy is integrated for reducing the search space of features. The proposed DNBS representation is applied to object tracking through SSD-based template matching. We validate the effectiveness of our method through extensive experiments on challenging videos with comparisons against several state-of-the-art trackers and demonstrate its capability to track objects in clutter and moving background.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08439",
        "title": "Hyper-Fisher Vectors for Action Recognition",
        "authors": [
            "Sanath Narayan",
            "Kalpathi R. Ramakrishnan"
        ],
        "abstract": "In this paper, a novel encoding scheme combining Fisher vector and bag-of-words encodings has been proposed for recognizing action in videos. The proposed Hyper-Fisher vector encoding is sum of local Fisher vectors which are computed based on the traditional Bag-of-Words (BoW) encoding. Thus, the proposed encoding is simple and yet an effective representation over the traditional Fisher Vector encoding. By extensive evaluation on challenging action recognition datasets, viz., Youtube, Olympic Sports, UCF50 and HMDB51, we show that the proposed Hyper-Fisher Vector encoding improves the recognition performance by around 2-3% compared to the improved Fisher Vector encoding. We also perform experiments to show that the performance of the Hyper-Fisher Vector is robust to the dictionary size of the BoW encoding.\n    ",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08647",
        "title": "Long-Range Trajectories from Global and Local Motion Representations",
        "authors": [
            "Eduardo M. Pereira",
            "Jaime S. Cardoso",
            "Ricardo Morla"
        ],
        "abstract": "Motion is a fundamental cue for scene analysis and human activity understan- ding in videos. It can be encoded in trajectories for tracking objects and for action recognition, or in form of flow to address behaviour analysis in crowded scenes. Each approach can only be applied on limited scenarios. We propose a motion-based system that represents the spatial and temporal features of the flow in terms of long-range trajectories. The novelty resides on the system formulation, its generic approach to handle scene variability and motion variations, motion integration from local and global representations, and the resulting long-range trajectories that overcome trajectory-based approach problems. We report the results and conclusions that state its pertinence on different scenarios, comparing and correlating the extracted trajectories of individual pedestrians, manually annotated. We also propose an evaluation framework and stress the diverse system characteristics that can be used for human activity tasks, namely on motion segmentation.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08715",
        "title": "Retinex filtering of foggy images: generation of a bulk set with selection and ranking",
        "authors": [
            "Roberto Marazzato",
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "In this paper we are proposing the use of GIMP Retinex, a filter of the GNU Image Manipulation Program, for enhancing foggy images. This filter involves adjusting four different parameters to find the output image which has to be preferred according to some specific purposes. Aiming to obtain a processing, which is able of choosing automatically the best image from a given set, we are proposing a method for the generation a bulk set of GIMP Retinex filtered images and a preliminary approach for selecting and ranking them.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08902",
        "title": "Scalable Nonlinear Embeddings for Semantic Category-based Image Retrieval",
        "authors": [
            "Gaurav Sharma",
            "Bernt Schiele"
        ],
        "abstract": "We propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional Euclidean space. We work in the challenging setting where supervision is with constraints on similar and dissimilar pairs while training. The proposed method is derived by an approximate kernelization of a linear Mahalanobis-like distance metric learning algorithm and can also be seen as a kernel neural network. The number of model parameters and test time evaluation complexity of the proposed method are O(dD) where D is the dimensionality of the input features and d is the dimension of the projection space - this is in contrast to the usual kernelization methods as, unlike them, the complexity does not scale linearly with the number of training examples. We propose a stochastic gradient based learning algorithm which makes the method scalable (w.r.t. the number of training examples), while being nonlinear. We train the method with up to half a million training pairs of 4096 dimensional CNN features. We give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08969",
        "title": "Light Field Reconstruction Using Shearlet Transform",
        "authors": [
            "Suren Vagharshakyan",
            "Robert Bregovic",
            "Atanas Gotchev"
        ],
        "abstract": "In this article we develop an image based rendering technique based on light field reconstruction from a limited set of perspective views acquired by cameras. Our approach utilizes sparse representation of epipolar-plane images in a directionally sensitive transform domain, obtained by an adapted discrete shearlet transform. The used iterative thresholding algorithm provides high-quality reconstruction results for relatively big disparities between neighboring views. The generated densely sampled light field of a given 3D scene is thus suitable for all applications which requires light field reconstruction. The proposed algorithm is compared favorably against state of the art depth image based rendering techniques.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08970",
        "title": "Energy-Efficient Object Detection using Semantic Decomposition",
        "authors": [
            "Priyadarshini Panda",
            "Swagath Venkataramani",
            "Abhronil Sengupta",
            "Anand Raghunathan",
            "Kaushik Roy"
        ],
        "abstract": "Machine-learning algorithms offer immense possibilities in the development of several cognitive applications. In fact, large scale machine-learning classifiers now represent the state-of-the-art in a wide range of object detection/classification problems. However, the network complexities of large-scale classifiers present them as one of the most challenging and energy intensive workloads across the computing spectrum. In this paper, we present a new approach to optimize energy efficiency of object detection tasks using semantic decomposition to build a hierarchical classification framework. We observe that certain semantic information like color/texture are common across various images in real-world datasets for object detection applications. We exploit these common semantic features to distinguish the objects of interest from the remaining inputs (non-objects of interest) in a dataset at a lower computational effort. We propose a 2-stage hierarchical classification framework, with increasing levels of complexity, wherein the first stage is trained to recognize the broad representative semantic features relevant to the object of interest. The first stage rejects the input instances that do not have the representative features and passes only the relevant instances to the second stage. Our methodology thus allows us to reject certain information at lower complexity and utilize the full computational effort of a network only on a smaller fraction of inputs to perform detection. We use color and texture as distinctive traits to carry out several experiments for object detection. Our experiments on the Caltech101/CIFAR10 dataset show that the proposed method yields 1.93x/1.46x improvement in average energy, respectively, over the traditional single classifier model.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08971",
        "title": "Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition",
        "authors": [
            "Priyadarshini Panda",
            "Abhronil Sengupta",
            "Kaushik Roy"
        ],
        "abstract": "Deep learning neural networks have emerged as one of the most powerful classification tools for vision related applications. However, the computational and energy requirements associated with such deep nets can be quite high, and hence their energy-efficient implementation is of great interest. Although traditionally the entire network is utilized for the recognition of all inputs, we observe that the classification difficulty varies widely across inputs in real-world datasets; only a small fraction of inputs require the full computational effort of a network, while a large majority can be classified correctly with very low effort. In this paper, we propose Conditional Deep Learning (CDL) where the convolutional layer features are used to identify the variability in the difficulty of input instances and conditionally activate the deeper layers of the network. We achieve this by cascading a linear network of output neurons for each convolutional layer and monitoring the output of the linear network to decide whether classification can be terminated at the current stage or not. The proposed methodology thus enables the network to dynamically adjust the computational effort depending upon the difficulty of the input data while maintaining competitive classification accuracy. We evaluate our approach on the MNIST dataset. Our experiments demonstrate that our proposed CDL yields 1.91x reduction in average number of operations per input, which translates to 1.84x improvement in energy. In addition, our results show an improvement in classification accuracy from 97.5% to 98.9% as compared to the original network.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2016-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09014",
        "title": "Stats-Calculus Pose Descriptor Feeding A Discrete HMM Low-latency Detection and Recognition System For 3D Skeletal Actions",
        "authors": [
            "Rofael Emil Fayez Behnam"
        ],
        "abstract": "Recognition of human actions, under low observational latency, is a growing interest topic, nowadays. Many approaches have been represented based on a provided set of 3D Cartesian coordinates system originated at a certain specific point located on a root joint. In this paper, We will present a statistical detection and recognition system using Hidden Markov Model using 7 types of pose descriptors. * Cartesian Calculus Pose descriptor. * Angular Calculus Pose descriptor. * Mixed-mode Stats-Calculus Pose descriptor. * Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus DCT Pose descriptor. * Rela-Centro-Stats-Calculus DCT-AMDF Pose descriptor. Stats-Calculus is a feature extracting technique, that is developed on Moving Pose descriptor , but using a combination of Statistics measures and Calculus measures.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09089",
        "title": "Moving Object Detection in Video Using Saliency Map and Subspace Learning",
        "authors": [
            "Yanwei Pang",
            "Li Ye",
            "Xuelong Li",
            "Jing Pan"
        ],
        "abstract": "Moving object detection is a key to intelligent video analysis. On the one hand, what moves is not only interesting objects but also noise and cluttered background. On the other hand, moving objects without rich texture are prone not to be detected. So there are undesirable false alarms and missed alarms in many algorithms of moving object detection. To reduce the false alarms and missed alarms, in this paper, we propose to incorporate a saliency map into an incremental subspace analysis framework where the saliency map makes estimated background has less chance than foreground (i.e., moving objects) to contain salient objects. The proposed objective function systematically takes account into the properties of sparsity, low-rank, connectivity, and saliency. An alternative minimization algorithm is proposed to seek the optimal solutions. Experimental results on the Perception Test Images Sequences demonstrate that the proposed method is effective in reducing false alarms and missed alarms.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09114",
        "title": "Online Object Tracking with Proposal Selection",
        "authors": [
            "Yang Hua",
            "Karteek Alahari",
            "Cordelia Schmid"
        ],
        "abstract": "Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09243",
        "title": "A spatial compositional model (SCM) for linear unmixing and endmember uncertainty estimation",
        "authors": [
            "Yuan Zhou",
            "Anand Rangarajan",
            "Paul Gader"
        ],
        "abstract": "The normal compositional model (NCM) has been extensively used in hyperspectral unmixing. However, most of the previous research has focused on estimation of endmembers and/or their variability. Also, little work has employed spatial information in NCM. In this paper, we show that NCM can be used for calculating the uncertainty of the estimated endmembers with spatial priors incorporated for better unmixing. This results in a spatial compositional model (SCM) which features (i) spatial priors that force neighboring abundances to be similar based on their pixel similarity and (ii) a posterior that is obtained from a likelihood model which does not assume pixel independence. The resulting algorithm turns out to be easy to implement and efficient to run. We compared SCM with current state-of-the-art algorithms on synthetic and real images. The results show that SCM can in the main provide more accurate endmembers and abundances. Moreover, the estimated uncertainty can serve as a prediction of endmember error under certain conditions.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.09294",
        "title": "General Dynamic Scene Reconstruction from Multiple View Video",
        "authors": [
            "Armin Mustafa",
            "Hansung Kim",
            "Jean-Yves Guillemaut",
            "Adrian Hilton"
        ],
        "abstract": "This paper introduces a general approach to dynamic scene reconstruction from multiple moving cameras without prior knowledge or limiting constraints on the scene structure, appearance, or illumination. Existing techniques for dynamic scene reconstruction from multiple wide-baseline camera views primarily focus on accurate reconstruction in controlled environments, where the cameras are fixed and calibrated and background is known. These approaches are not robust for general dynamic scenes captured with sparse moving cameras. Previous approaches for outdoor dynamic scene reconstruction assume prior knowledge of the static background appearance and structure. The primary contributions of this paper are twofold: an automatic method for initial coarse dynamic scene segmentation and reconstruction without prior knowledge of background appearance or structure; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes from multiple wide-baseline static or moving cameras. Evaluation is performed on a variety of indoor and outdoor scenes with cluttered backgrounds and multiple dynamic non-rigid objects such as people. Comparison with state-of-the-art approaches demonstrates improved accuracy in both multiple view segmentation and dense reconstruction. The proposed approach also eliminates the requirement for prior knowledge of scene structure and appearance.\n    ",
        "submission_date": "2015-09-30T00:00:00",
        "last_modified_date": "2015-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00098",
        "title": "Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping",
        "authors": [
            "Michael Xie",
            "Neal Jean",
            "Marshall Burke",
            "David Lobell",
            "Stefano Ermon"
        ],
        "abstract": "The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00143",
        "title": "Fast Single Image Super-Resolution",
        "authors": [
            "Ningning Zhao",
            "Qi Wei",
            "Adrian Basarab",
            "Nicolas Dobigeon",
            "Denis Kouame",
            "Jean-Yves Tourneret"
        ],
        "abstract": "This paper addresses the problem of single image super-resolution (SR), which consists of recovering a high resolution image from its blurred, decimated and noisy version. The existing algorithms for single image SR use different strategies to handle the decimation and blurring operators. In addition to the traditional first-order gradient methods, recent techniques investigate splitting-based methods dividing the SR problem into up-sampling and deconvolution steps that can be easily solved. Instead of following this splitting strategy, we propose to deal with the decimation and blurring operators simultaneously by taking advantage of their particular properties in the frequency domain, leading to a new fast SR approach. Specifically, an analytical solution can be obtained and implemented efficiently for the Gaussian prior or any other regularization that can be formulated into an $\\ell_2$-regularized quadratic model, i.e., an $\\ell_2$-$\\ell_2$ optimization problem. Furthermore, the flexibility of the proposed SR scheme is shown through the use of various priors/regularizations, ranging from generic image priors to learning-based approaches. In the case of non-Gaussian priors, we show how the analytical solution derived from the Gaussian case can be embedded intotraditional splitting frameworks, allowing the computation cost of existing algorithms to be decreased significantly. Simulation results conducted on several images with different priors illustrate the effectiveness of our fast SR approach compared with the existing techniques.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00149",
        "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
        "authors": [
            "Song Han",
            "Huizi Mao",
            "William J. Dally"
        ],
        "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00203",
        "title": "Data Association for an Adaptive Multi-target Particle Filter Tracking System",
        "authors": [
            "R. Alampay",
            "K. Teknomo"
        ],
        "abstract": "This paper presents a novel approach to improve the accuracy of tracking multiple objects in a static scene using a particle filter system by introducing a data association step, a state queue for the collection of tracked objects and adaptive parameters to the system. The data association step makes use of the object detection phase and appearance model to determine if the approximated targets given by the particle filter step match the given set of detected objects. The remaining detected objects are used as information to instantiate new objects for tracking. State queues are also used for each tracked object to deal with occlusion events and occlusion recovery. Finally we present how the parameters adjust to occlusion events. The adaptive property of the system is also used for possible occlusion recovery. Results of the system are then compared to a ground truth data set for performance evaluation. Our system produced accurate results and was able to handle partially occluded objects as well as proper occlusion recovery from tracking multiple objects\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00384",
        "title": "Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier Samples",
        "authors": [
            "Greg Ongie",
            "Mathews Jacob"
        ],
        "abstract": "We introduce a method to recover a continuous domain representation of a piecewise constant two-dimensional image from few low-pass Fourier samples. Assuming the edge set of the image is localized to the zero set of a trigonometric polynomial, we show the Fourier coefficients of the partial derivatives of the image satisfy a linear annihilation relation. We present necessary and sufficient conditions for unique recovery of the image from finite low-pass Fourier samples using the annihilation relation. We also propose a practical two-stage recovery algorithm which is robust to model-mismatch and noise. In the first stage we estimate a continuous domain representation of the edge set of the image. In the second stage we perform an extrapolation in Fourier domain by a least squares two-dimensional linear prediction, which recovers the exact Fourier coefficients of the underlying image. We demonstrate our algorithm on the super-resolution recovery of MRI phantoms and real MRI data from low-pass Fourier samples, which shows benefits over standard approaches for single-image super-resolution MRI.\n    ",
        "submission_date": "2015-10-01T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00477",
        "title": "Learning a Discriminative Model for the Perception of Realism in Composite Images",
        "authors": [
            "Jun-Yan Zhu",
            "Philipp Kr\u00e4henb\u00fchl",
            "Eli Shechtman",
            "Alexei A. Efros"
        ],
        "abstract": "What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene in terms of color, lighting and texture compatibility, without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. unrealistic photos. Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00479",
        "title": "Effective Object Tracking in Unstructured Crowd Scenes",
        "authors": [
            "Ishan Jindal",
            "Shanmuganathan Raman"
        ],
        "abstract": "In this paper, we are presenting a rotation variant Oriented Texture Curve (OTC) descriptor based mean shift algorithm for tracking an object in an unstructured crowd scene. The proposed algorithm works by first obtaining the OTC features for a manually selected object target, then a visual vocabulary is created by using all the OTC features of the target. The target histogram is obtained using codebook encoding method which is then used in mean shift framework to perform similarity search. Results are obtained on different videos of challenging scenes and the comparison of the proposed approach with several state-of-the-art approaches are provided. The analysis shows the advantages and limitations of the proposed approach for tracking an object in unstructured crowd scenes.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00542",
        "title": "Local Higher-Order Statistics (LHS) describing images with statistics of local non-binarized pixel patterns",
        "authors": [
            "Gaurav Sharma",
            "Frederic Jurie"
        ],
        "abstract": "We propose a new image representation for texture categorization and facial analysis, relying on the use of higher-order local differential statistics as features. It has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute, which is in contrast to the models based on the global structure of images. Motivated by such works, we propose to use higher-order statistics of local non-binarized pixel patterns for the image description. The proposed model does not require either (i) user specified quantization of the space (of pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of the space. We propose to use a data driven soft quantization of the space, with parametric mixture models, combined with higher-order statistics, based on Fisher scores. We demonstrate that this leads to a more expressive representation which, when combined with discriminatively learned classifiers and metrics, achieves state-of-the-art performance on challenging texture and facial analysis datasets, in low complexity setup. Further, it is complementary to higher complexity features and when combined with them improves performance.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00562",
        "title": "Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks",
        "authors": [
            "Lin Sun",
            "Kui Jia",
            "Dit-Yan Yeung",
            "Bertram E. Shi"
        ],
        "abstract": "Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00745",
        "title": "WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification",
        "authors": [
            "Eric C. Orenstein",
            "Oscar Beijbom",
            "Emily E. Peacock",
            "Heidi M. Sosik"
        ],
        "abstract": "Planktonic organisms are of fundamental importance to marine ecosystems: they form the basis of the food web, provide the link between the atmosphere and the deep ocean, and influence global-scale biogeochemical cycles. Scientists are increasingly using imaging-based technologies to study these creatures in their natural habit. Images from such systems provide an unique opportunity to model and understand plankton ecosystems, but the collected datasets can be enormous. The Imaging FlowCytobot (IFCB) at Woods Hole Oceanographic Institution, for example, is an \\emph{in situ} system that has been continuously imaging plankton since 2006. To date, it has generated more than 700 million samples. Manual classification of such a vast image collection is impractical due to the size of the data set. In addition, the annotation task is challenging due to the large space of relevant classes, intra-class variability, and inter-class similarity. Methods for automated classification exist, but the accuracy is often below that of human experts. Here we introduce WHOI-Plankton: a large scale, fine-grained visual recognition dataset for plankton classification, which comprises over 3.4 million expert-labeled images across 70 classes. The labeled image set is complied from over 8 years of near continuous data collection with the IFCB at the Martha's Vineyard Coastal Observatory (MVCO). We discuss relevant metrics for evaluation of classification performance and provide results for a traditional method based on hand-engineered features and two methods based on convolutional neural networks.\n    ",
        "submission_date": "2015-10-02T00:00:00",
        "last_modified_date": "2015-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00771",
        "title": "Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor Micro Aerial Vehicles (MAVs)",
        "authors": [
            "Carlos Jaramillo"
        ],
        "abstract": "We describe the design and 3D sensing performance of an omnidirectional stereo-vision system (omnistereo) as applied to Micro Aerial Vehicles (MAVs). The proposed omnistereo model employs a monocular camera that is co-axially aligned with a pair of hyperboloidal mirrors (folded catadioptric configuration). We show that this arrangement is practical for performing stereo-vision when mounted on top of propeller-based MAVs characterized by low payloads. The theoretical single viewpoint (SVP) constraint helps us derive analytical solutions for the sensor's projective geometry and generate SVP-compliant panoramic images to compute 3D information from stereo correspondences (in a truly synchronous fashion). We perform an extensive analysis on various system characteristics such as its size, catadioptric spatial resolution, field-of-view. In addition, we pose a probabilistic model for uncertainty estimation of the depth from triangulation for skew back-projection rays. We expect to motivate the reproducibility of our solution since it can be adapted (optimally) to other catadioptric-based omnistereo vision applications.\n    ",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00857",
        "title": "Approximate Fisher Kernels of non-iid Image Models for Image Categorization",
        "authors": [
            "Ramazan Gokberk Cinbis",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods.\n    ",
        "submission_date": "2015-10-03T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00889",
        "title": "Background Image Generation Using Boolean Operations",
        "authors": [
            "Kardi Teknomo",
            "Proceso Fernandez"
        ],
        "abstract": "Tracking moving objects from a video sequence requires segmentation of these objects from the background image. However, getting the actual background image automatically without object detection and using only the video is difficult. In this paper, we describe a novel algorithm that generates background from real world images without foreground detection. The algorithm assumes that the background image is shown in the majority of the video. Given this simple assumption, the method described in this paper is able to accurately generate, with high probability, the background image from a video using only a small number of binary operations.\n    ",
        "submission_date": "2015-10-04T00:00:00",
        "last_modified_date": "2015-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00921",
        "title": "Cross-convolutional-layer Pooling for Image Recognition",
        "authors": [
            "Lingqiao Liu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN.\n    ",
        "submission_date": "2015-10-04T00:00:00",
        "last_modified_date": "2016-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.00981",
        "title": "Efficient Hand Articulations Tracking using Adaptive Hand Model and Depth map",
        "authors": [
            "Byeongkeun Kang",
            "Yeejin Lee",
            "Truong Q. Nguyen"
        ],
        "abstract": "Real-time hand articulations tracking is important for many applications such as interacting with virtual / augmented reality devices or tablets. However, most of existing algorithms highly rely on expensive and high power-consuming GPUs to achieve real-time processing. Consequently, these systems are inappropriate for mobile and wearable devices. In this paper, we propose an efficient hand tracking system which does not require high performance GPUs. In our system, we track hand articulations by minimizing discrepancy between depth map from sensor and computer-generated hand model. We also initialize hand pose at each frame using finger detection and classification. Our contributions are: (a) propose adaptive hand model to consider different hand shapes of users without generating personalized hand model; (b) improve the highly efficient frame initialization for robust tracking and automatic initialization; (c) propose hierarchical random sampling of pixels from each depth map to improve tracking accuracy while limiting required computations. To the best of our knowledge, it is the first system that achieves both automatic hand model adjustment and real-time tracking without using GPUs.\n    ",
        "submission_date": "2015-10-04T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01018",
        "title": "Single Image Dehazing through Improved Atmospheric Light Estimation",
        "authors": [
            "Huimin Lu",
            "Yujie Li",
            "Shota Nakashima",
            "Seiichi Serikawa"
        ],
        "abstract": "Image contrast enhancement for outdoor vision is important for smart car auxiliary transport systems. The video frames captured in poor weather conditions are often characterized by poor visibility. Most image dehazing algorithms consider to use a hard threshold assumptions or user input to estimate atmospheric light. However, the brightest pixels sometimes are objects such as car lights or streetlights, especially for smart car auxiliary transport systems. Simply using a hard threshold may cause a wrong estimation. In this paper, we propose a single optimized image dehazing method that estimates atmospheric light efficiently and removes haze through the estimation of a semi-globally adaptive filter. The enhanced images are characterized with little noise and good exposure in dark regions. The textures and edges of the processed images are also enhanced significantly.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01027",
        "title": "Relaxed Multiple-Instance SVM with Application to Object Discovery",
        "authors": [
            "Xinggang Wang",
            "Zhuotun Zhu",
            "Cong Yao",
            "Xiang Bai"
        ],
        "abstract": "Multiple-instance learning (MIL) has served as an important tool for a wide range of vision applications, for instance, image classification, object detection, and visual tracking. In this paper, we propose a novel method to solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM). We treat the positiveness of instance as a continuous variable, use Noisy-OR model to enforce the MIL constraints, and jointly optimize the bag label and instance label in a unified framework. The optimization problem can be efficiently solved using stochastic gradient decent. The extensive experiments demonstrate that RMI-SVM consistently achieves superior performance on various benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision task, common object discovery. The state-of-the-art results of object discovery on Pascal VOC datasets further confirm the advantages of the proposed method.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01041",
        "title": "GPU-Based Computation of 2D Least Median of Squares with Applications to Fast and Robust Line Detection",
        "authors": [
            "Gil Shapira",
            "Tal Hassner"
        ],
        "abstract": "The 2D Least Median of Squares (LMS) is a popular tool in robust regression because of its high breakdown point: up to half of the input data can be contaminated with outliers without affecting the accuracy of the LMS estimator. The complexity of 2D LMS estimation has been shown to be $\\Omega(n^2)$ where $n$ is the total number of points. This high theoretical complexity along with the availability of graphics processing units (GPU) motivates the development of a fast, parallel, GPU-based algorithm for LMS computation. We present a CUDA based algorithm for LMS computation and show it to be much faster than the optimal state of the art single threaded CPU algorithm. We begin by describing the proposed method and analyzing its performance. We then demonstrate how it can be used to modify the well-known Hough Transform (HT) in order to efficiently detect image lines in noisy images. Our method is compared with standard HT-based line detection methods and shown to overcome their shortcomings in terms of both efficiency and accuracy.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01098",
        "title": "Intensity-only optical compressive imaging using a multiply scattering material and a double phase retrieval approach",
        "authors": [
            "Boshra Rajaei",
            "Eric W. Tramel",
            "Sylvain Gigan",
            "Florent Krzakala",
            "Laurent Daudet"
        ],
        "abstract": "In this paper, the problem of compressive imaging is addressed using natural randomization by means of a multiply scattering medium. To utilize the medium in this way, its corresponding transmission matrix must be estimated. To calibrate the imager, we use a digital micromirror device (DMD) as a simple, cheap, and high-resolution binary intensity modulator. We propose a phase retrieval algorithm which is well adapted to intensity-only measurements on the camera, and to the input binary intensity patterns, both to estimate the complex transmission matrix as well as image reconstruction. We demonstrate promising experimental results for the proposed algorithm using the MNIST dataset of handwritten digits as example images.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2016-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01148",
        "title": "Visual Tracking via Nonnegative Regularization Multiple Locality Coding",
        "authors": [
            "Fanghui Liu",
            "Tao Zhou",
            "Irene Y.H. Gu",
            "Jie Yang"
        ],
        "abstract": "This paper presents a novel object tracking method based on approximated Locality-constrained Linear Coding (LLC). Rather than using a non-negativity constraint on encoding coefficients to guarantee these elements nonnegative, in this paper, the non-negativity constraint is substituted for a conventional $\\ell_2$ norm regularization term in approximated LLC to obtain the similar nonnegative effect. And we provide a detailed and adequate explanation in theoretical analysis to clarify the rationality of this replacement. Instead of specifying fixed K nearest neighbors to construct the local dictionary, a series of different dictionaries with pre-defined numbers of nearest neighbors are selected. Weights of these various dictionaries are also learned from approximated LLC in the similar framework. In order to alleviate tracking drifts, we propose a simple and efficient occlusion detection method. The occlusion detection criterion mainly depends on whether negative templates are selected to represent the severe occluded target. Both qualitative and quantitative evaluations on several challenging sequences show that the proposed tracking algorithm achieves favorable performance compared with other state-of-the-art methods.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01257",
        "title": "Efficient Object Detection for High Resolution Images",
        "authors": [
            "Yongxi Lu",
            "Tara Javidi"
        ],
        "abstract": "Efficient generation of high-quality object proposals is an essential step in state-of-the-art object detection systems based on deep convolutional neural networks (DCNN) features. Current object proposal algorithms are computationally inefficient in processing high resolution images containing small objects, which makes them the bottleneck in object detection systems. In this paper we present effective methods to detect objects for high resolution images. We combine two complementary strategies. The first approach is to predict bounding boxes based on adjacent visual features. The second approach uses high level image features to guide a two-step search process that adaptively focuses on regions that are likely to contain small objects. We extract features required for the two strategies by utilizing a pre-trained DCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm by showing its performance on a high-resolution image subset of the SUN 2012 object detection dataset.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01344",
        "title": "Within-Brain Classification for Brain Tumor Segmentation",
        "authors": [
            "Mohammad Havaei",
            "Hugo Larochelle",
            "Philippe Poulin",
            "Pierre-Marc Jodoin"
        ],
        "abstract": "Purpose: In this paper, we investigate a framework for interactive brain tumor segmentation which, at its core, treats the problem of interactive brain tumor segmentation as a machine learning problem.\n",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01401",
        "title": "On the Existence of Epipolar Matrices",
        "authors": [
            "Sameer Agarwal",
            "Hon-Leung Lee",
            "Bernd Sturmfels",
            "Rekha R. Thomas"
        ],
        "abstract": "This paper considers the foundational question of the existence of a fundamental (resp. essential) matrix given $m$ point correspondences in two views. We present a complete answer for the existence of fundamental matrices for any value of $m$. Using examples we disprove the widely held beliefs that fundamental matrices always exist whenever $m \\leq 7$. At the same time, we prove that they exist unconditionally when $m \\leq 5$. Under a mild genericity condition, we show that an essential matrix always exists when $m \\leq 4$. We also characterize the six and seven point configurations in two views for which all matrices satisfying the epipolar constraint have rank at most one.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01431",
        "title": "SentiCap: Generating Image Descriptions with Sentiments",
        "authors": [
            "Alexander Mathews",
            "Lexing Xie",
            "Xuming He"
        ],
        "abstract": "The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01440",
        "title": "Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification",
        "authors": [
            "Ruobing Wu",
            "Baoyuan Wang",
            "Wenping Wang",
            "Yizhou Yu"
        ],
        "abstract": "Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67~\\cite{MITIndoor67} and Sun397~\\cite{Sun397}\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01442",
        "title": "Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders",
        "authors": [
            "Huan Yang",
            "Baoyuan Wang",
            "Stephen Lin",
            "David Wipf",
            "Minyi Guo",
            "Baining Guo"
        ],
        "abstract": "With the growing popularity of short-form video sharing platforms such as \\em{Instagram} and \\em{Vine}, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM)~\\cite{LSTM:97} cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised settin\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01490",
        "title": "Directional Global Three-part Image Decomposition",
        "authors": [
            "Duy Hoang Thai",
            "Carsten Gottschlich"
        ],
        "abstract": "We consider the task of image decomposition and we introduce a new model coined directional global three-part decomposition (DG3PD) for solving it. As key ingredients of the DG3PD model, we introduce a discrete multi-directional total variation norm and a discrete multi-directional G-norm. Using these novel norms, the proposed discrete DG3PD model can decompose an image into two parts or into three parts. Existing models for image decomposition by Vese and Osher, by Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are included as special cases in the new model. Decomposition of an image by DG3PD results in a cartoon image, a texture image and a residual image. Advantages of the DG3PD model over existing ones lie in the properties enforced on the cartoon and texture images. The geometric objects in the cartoon image have a very smooth surface and sharp edges. The texture image yields oscillating patterns on a defined scale which is both smooth and sparse. Moreover, the DG3PD method achieves the goal of perfect reconstruction by summation of all components better than the other considered methods. Relevant applications of DG3PD are a novel way of image compression as well as feature extraction for applications such as latent fingerprint processing and optical character recognition.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01544",
        "title": "Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks",
        "authors": [
            "Efstratios Gavves",
            "Thomas Mensink",
            "Tatiana Tommasi",
            "Cees G.M. Snoek",
            "Tinne Tuytelaars"
        ],
        "abstract": "How can we reuse existing knowledge, in the form of available datasets, when solving a new and apparently unrelated target task from a set of unlabeled data? In this work we make a first contribution to answer this question in the context of image classification. We frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01553",
        "title": "Learning Deep Representations of Appearance and Motion for Anomalous Event Detection",
        "authors": [
            "Dan Xu",
            "Elisa Ricci",
            "Yan Yan",
            "Jingkuan Song",
            "Nicu Sebe"
        ],
        "abstract": "We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted appearance and motion features, we propose Appearance and Motion DeepNet (AMDN) which utilizes deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM models are used to predict the anomaly scores of each input, which are then integrated with a late fusion strategy for final anomaly detection. We evaluate the proposed method on two publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01576",
        "title": "Predicting Daily Activities From Egocentric Images Using Deep Learning",
        "authors": [
            "Daniel Castro",
            "Steven Hickson",
            "Vinay Bettadapura",
            "Edison Thomaz",
            "Gregory Abowd",
            "Henrik Christensen",
            "Irfan Essa"
        ],
        "abstract": "We present a method to analyze images taken from a passive egocentric wearable camera along with the contextual information, such as time and day of week, to learn and predict everyday activities of an individual. We collected a dataset of 40,103 egocentric images over a 6 month period with 19 activity classes and demonstrate the benefit of state-of-the-art deep learning techniques for learning and predicting daily activities. Classification is conducted using a Convolutional Neural Network (CNN) with a classification method we introduce called a late fusion ensemble. This late fusion ensemble incorporates relevant contextual information and increases our classification accuracy. Our technique achieves an overall accuracy of 83.07% in predicting a person's activity across the 19 activity classes. We also demonstrate some promising results from two additional users by fine-tuning the classifier with one day of training data.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01648",
        "title": "A Latent Source Model for Patch-Based Image Segmentation",
        "authors": [
            "George Chen",
            "Devavrat Shah",
            "Polina Golland"
        ],
        "abstract": "Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01663",
        "title": "Euclidean Auto Calibration of Camera Networks: Baseline Constraint Removes Scale Ambiguity",
        "authors": [
            "Kiran Kumar Vupparaboina",
            "Kamala Raghavan",
            "Soumya Jana"
        ],
        "abstract": "Metric auto calibration of a camera network from multiple views has been reported by several authors. Resulting 3D reconstruction recovers shape faithfully, but not scale. However, preservation of scale becomes critical in applications, such as multi-party telepresence, where multiple 3D scenes need to be fused into a single coordinate system. In this context, we propose a camera network configuration that includes a stereo pair with known baseline separation, and analytically demonstrate Euclidean auto calibration of such network under mild conditions. Further, we experimentally validate our theory using a four-camera network. Importantly, our method not only recovers scale, but also compares favorably with the well known Zhang and Pollefeys methods in terms of shape recovery.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01972",
        "title": "Event-based Camera Pose Tracking using a Generative Event Model",
        "authors": [
            "Guillermo Gallego",
            "Christian Forster",
            "Elias Mueggler",
            "Davide Scaramuzza"
        ],
        "abstract": "Event-based vision sensors mimic the operation of biological retina and they represent a major paradigm shift from traditional cameras. Instead of providing frames of intensity measurements synchronously, at artificially chosen rates, event-based cameras provide information on brightness changes asynchronously, when they occur. Such non-redundant pieces of information are called \"events\". These sensors overcome some of the limitations of traditional cameras (response time, bandwidth and dynamic range) but require new methods to deal with the data they output. We tackle the problem of event-based camera localization in a known environment, without additional sensing, using a probabilistic generative event model in a Bayesian filtering framework. Our main contribution is the design of the likelihood function used in the filter to process the observed events. Based on the physical characteristics of the sensor and on empirical evidence of the Gaussian-like distribution of spiked events with respect to the brightness change, we propose to use the contrast residual as a measure of how well the estimated pose of the event-based camera and the environment explain the observed events. The filter allows for localization in the general case of six degrees-of-freedom motions.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02055",
        "title": "Diverse Large-Scale ITS Dataset Created from Continuous Learning for Real-Time Vehicle Detection",
        "authors": [
            "Justin A. Eichel",
            "Akshaya Mishra",
            "Nicholas Miller",
            "Nicholas Jankovic",
            "Mohan A. Thomas",
            "Tyler Abbott",
            "Douglas Swanson",
            "Joel Keller"
        ],
        "abstract": "In traffic engineering, vehicle detectors are trained on limited datasets resulting in poor accuracy when deployed in real world applications. Annotating large-scale high quality datasets is challenging. Typically, these datasets have limited diversity; they do not reflect the real-world operating environment. There is a need for a large-scale, cloud based positive and negative mining (PNM) process and a large-scale learning and evaluation system for the application of traffic event detection. The proposed positive and negative mining process addresses the quality of crowd sourced ground truth data through machine learning review and human feedback mechanisms. The proposed learning and evaluation system uses a distributed cloud computing framework to handle data-scaling issues associated with large numbers of samples and a high-dimensional feature space. The system is trained using AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated video frames. The trained real-time vehicle detector achieves an accuracy of at least $95\\%$ for $1/2$ and about $78\\%$ for $19/20$ of the time when tested on approximately $7,500,000$ video frames. At the end of 2015, the dataset is expect to have over one billion annotated video frames.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02071",
        "title": "Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition",
        "authors": [
            "Vinay Bettadapura",
            "Grant Schindler",
            "Thomaz Plotz",
            "Irfan Essa"
        ],
        "abstract": "We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use of randomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02073",
        "title": "Egocentric Field-of-View Localization Using First-Person Point-of-View Devices",
        "authors": [
            "Vinay Bettadapura",
            "Irfan Essa",
            "Caroline Pantofaru"
        ],
        "abstract": "We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02078",
        "title": "Leveraging Context to Support Automated Food Recognition in Restaurants",
        "authors": [
            "Vinay Bettadapura",
            "Edison Thomaz",
            "Aman Parnami",
            "Gregory Abowd",
            "Irfan Essa"
        ],
        "abstract": "The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures reflecting what people eat. In this paper, we study how taking pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with additional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demonstrate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's online menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food images taken in 10 restaurants across 5 different types of food (American, Indian, Italian, Mexican and Thai).\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02131",
        "title": "DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer",
        "authors": [
            "Forrest N. Iandola",
            "Anting Shen",
            "Peter Gao",
            "Kurt Keutzer"
        ],
        "abstract": "Recently, there has been a flurry of industrial activity around logo recognition, such as Ditto's service for marketers to track their brands in user-generated images, and LogoGrab's mobile app platform for logo recognition. However, relatively little academic or open-source logo recognition progress has been made in the last four years. Meanwhile, deep convolutional neural networks (DCNNs) have revolutionized a broad range of object recognition applications. In this work, we apply DCNNs to logo recognition. We propose several DCNN architectures, with which we surpass published state-of-art accuracy on a popular logo recognition dataset.\n    ",
        "submission_date": "2015-10-07T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02192",
        "title": "Simultaneous Deep Transfer Across Domains and Tasks",
        "authors": [
            "Eric Tzeng",
            "Judy Hoffman",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02364",
        "title": "Texture Modelling with Nested High-order Markov-Gibbs Random Fields",
        "authors": [
            "Ralph Versteegen",
            "Georgy Gimel'farb",
            "Patricia Riddle"
        ],
        "abstract": "Currently, Markov-Gibbs random field (MGRF) image models which include high-order interactions are almost always built by modelling responses of a stack of local linear filters. Actual interaction structure is specified implicitly by the filter coefficients. In contrast, we learn an explicit high-order MGRF structure by considering the learning process in terms of general exponential family distributions nested over base models, so that potentials added later can build on previous ones. We relatively rapidly add new features by skipping over the costly optimisation of parameters.\n",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02413",
        "title": "Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition",
        "authors": [
            "Tinghui Zhou",
            "Philipp Kr\u00e4henb\u00fchl",
            "Alexei A. Efros"
        ],
        "abstract": "We propose a data-driven approach for intrinsic image decomposition, which is the process of inferring the confounding factors of reflectance and shading in an image. We pose this as a two-stage learning problem. First, we train a model to predict relative reflectance ordering between image patches (`brighter', `darker', `same') from large-scale human annotations, producing a data-driven reflectance prior. Second, we show how to naturally integrate this learned prior into existing energy minimization frameworks for intrinsic image decomposition. We compare our method to the state-of-the-art approach of Bell et al. on both decomposition and image relighting tasks, demonstrating the benefits of the simple relative reflectance prior, especially for scenes under challenging lighting conditions.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2015-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02644",
        "title": "Free-hand Sketch Synthesis with Deformable Stroke Models",
        "authors": [
            "Yi Li",
            "Yi-Zhe Song",
            "Timothy Hospedales",
            "Shaogang Gong"
        ],
        "abstract": "We present a generative model which can automatically summarize the stroke composition of free-hand sketches of a given category. When our model is fit to a collection of sketches with similar poses, it discovers and learns the structure and appearance of a set of coherent parts, with each part represented by a group of strokes. It represents both consistent (topology) as well as diverse aspects (structure and appearance variations) of each sketch category. Key to the success of our model are important insights learned from a comprehensive study performed on human stroke data. By fitting this model to images, we are able to synthesize visually similar and pleasant free-hand sketches.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02710",
        "title": "Procams-Based Cybernetics",
        "authors": [
            "Kosuke Sato",
            "Daisuke Iwai",
            "Sei Ikeda",
            "Noriko Takemura"
        ],
        "abstract": "Procams-based cybernetics is a unique, emerging research field, which aims at enhancing and supporting our activities by naturally connecting human and computers/machines as a cooperative integrated system via projector-camera systems (procams). It rests on various research domains such as virtual/augmented reality, computer vision, computer graphics, projection display, human computer interface, human robot interaction and so on. This laboratory presentation provides a brief history including recent achievements of our procams-based cybernetics project.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02774",
        "title": "Human Head Pose Estimation by Facial Features Location",
        "authors": [
            "Eugene Borovikov"
        ],
        "abstract": "We describe a method for estimating human head pose in a color image that contains enough of information to locate the head silhouette and detect non-trivial color edges of individual facial features. The method works by spotting the human head on an arbitrary background, extracting the head outline, and locating facial features necessary to describe the head orientation in the 3D space. It is robust enough to work with both color and gray-level images featuring quasi-frontal views of a human head under variable lighting conditions.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02781",
        "title": "Where Is My Puppy? Retrieving Lost Dogs by Facial Features",
        "authors": [
            "Thierry Pinheiro Moreira",
            "Mauricio Lisboa Perez",
            "Rafael de Oliveira Werneck",
            "Eduardo Valle"
        ],
        "abstract": "A pet that goes missing is among many people's worst fears: a moment of distraction is enough for a dog or a cat wandering off from home. Some measures help matching lost animals to their owners; but automated visual recognition is one that - although convenient, highly available, and low-cost - is surprisingly overlooked. In this paper, we inaugurate that promising avenue by pursuing face recognition for dogs. We contrast four ready-to-use human facial recognizers (EigenFaces, FisherFaces, LBPH, and a Sparse method) to two original solutions based upon convolutional neural networks: BARK (inspired in architecture-optimized networks employed for human facial recognition) and WOOF (based upon off-the-shelf OverFeat features). Human facial recognizers perform poorly for dogs (up to 60.5% accuracy), showing that dog facial recognition is not a trivial extension of human facial recognition. The convolutional network solutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF, 89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs of two breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02795",
        "title": "Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation",
        "authors": [
            "S\u00f8ren Hauberg",
            "Oren Freifeld",
            "Anders Boesen Lindbo Larsen",
            "John W. Fisher III",
            "Lars Kai Hansen"
        ],
        "abstract": "Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g.~new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.\n    ",
        "submission_date": "2015-10-09T00:00:00",
        "last_modified_date": "2016-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02866",
        "title": "Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and Support Prior of Frame Coefficients",
        "authors": [
            "Liangtian He",
            "Yilun Wang"
        ],
        "abstract": "The wavelet frame systems have been widely investigated and applied for image restoration and many other image processing problems over the past decades, attributing to their good capability of sparsely approximating piece-wise smooth functions such as images. Most wavelet frame based models exploit the $l_1$ norm of frame coefficients for a sparsity constraint in the past. The authors in \\cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model, where the $l_0$ norm of wavelet frame coefficients is penalized instead, and have demonstrated that significant improvements can be achieved compared to the commonly used $l_1$ minimization model. Very recently, the authors in \\cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocal prior of frame coefficients is incorporated. This model proved to outperform the single $l_0$ minimization based model in terms of better recovered image quality. In this paper, we propose a truncated $l_0$-$l_2$ minimization model which combines sparsity, nonlocal and support prior of the frame coefficients. The extensive experiments have shown that the recovery results from the proposed regularization method performs better than existing state-of-the-art wavelet frame based methods, in terms of edge enhancement and texture preserving performance.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02884",
        "title": "Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity",
        "authors": [
            "Wufeng Xue",
            "Xuanqin Mou",
            "Lei Zhang"
        ],
        "abstract": "Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is particularly challenging due to the absence of knowledge about the reference image and distortion type. Features based on natural scene statistics (NSS) have been successfully used in BIQA, while the quality relevance of the feature plays an essential role to the quality prediction performance. Motivated by the fact that the early processing stage in human visual system aims to remove the signal redundancies for efficient visual coding, we propose a simple but very effective BIQA method by computing the statistics of self-similarity (SOS) in an image. Specifically, we calculate the inter-scale similarity and intra-scale similarity of the distorted image, extract the SOS features from these similarities, and learn a regression model to map the SOS features to the subjective quality score. Extensive experiments demonstrate very competitive quality prediction performance and generalization ability of the proposed SOS based BIQA method.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02899",
        "title": "TagBook: A Semantic Video Representation without Supervision for Event Detection",
        "authors": [
            "Masoud Mazloom",
            "Xirong Li",
            "Cees G. M. Snoek"
        ],
        "abstract": "We consider the problem of event detection in video for scenarios where only few, or even zero examples are available for training. For this challenging setting, the prevailing solutions in the literature rely on a semantic video representation obtained from thousands of pre-trained concept detectors. Different from existing work, we propose a new semantic video representation that is based on freely available social tagged videos only, without the need for training any intermediate concept detectors. We introduce a simple algorithm that propagates tags from a video's nearest neighbors, similar in spirit to the ones used for image retrieval, but redesign it for video event detection by including video source set refinement and varying the video tag assignment. We call our approach TagBook and study its construction, descriptiveness and detection performance on the TRECVID 2013 and 2014 multimedia event detection datasets and the Columbia Consumer Video dataset. Despite its simple nature, the proposed TagBook video representation is remarkably effective for few-example and zero-example event detection, even outperforming very recent state-of-the-art alternatives building on supervised representations.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2016-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02906",
        "title": "Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking",
        "authors": [
            "Min Yang",
            "Yunde Jia"
        ],
        "abstract": "Robust online multi-person tracking requires the correct associations of online detection responses with existing trajectories. We address this problem by developing a novel appearance modeling approach to provide accurate appearance affinities to guide data association. In contrast to most existing algorithms that only consider the spatial structure of human appearances, we exploit the temporal dynamic characteristics within temporal appearance sequences to discriminate different persons. The temporal dynamic makes a sufficient complement to the spatial structure of varying appearances in the feature space, which significantly improves the affinity measurement between trajectories and detections. We propose a feature selection algorithm to describe the appearance variations with mid-level semantic features, and demonstrate its usefulness in terms of temporal dynamic appearance modeling. Moreover, the appearance model is learned incrementally by alternatively evaluating newly-observed appearances and adjusting the model parameters to be suitable for online tracking. Reliable tracking of multiple persons in complex scenes is achieved by incorporating the learned model into an online tracking-by-detection framework. Our experiments on the challenging benchmark MOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art multi-person tracking algorithms.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02927",
        "title": "DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations",
        "authors": [
            "Srinivas S. S. Kruthiventi",
            "Kumar Ayush",
            "R. Venkatesh Babu"
        ],
        "abstract": "Understanding and predicting the human visual attentional mechanism is an active area of research in the fields of neuroscience and computer vision. In this work, we propose DeepFix, a first-of-its-kind fully convolutional neural network for accurate saliency prediction. Unlike classical works which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant which prevents them from modeling location dependent patterns (e.g. centre-bias). Our network overcomes this limitation by incorporating a novel Location Biased Convolutional layer. We evaluate our model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that it outperforms other recent approaches by a significant margin.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02930",
        "title": "Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion",
        "authors": [
            "Wensen Feng",
            "Yunjin Chen"
        ],
        "abstract": "The degradation of the acquired signal by Poisson noise is a common problem for various imaging applications, such as medical imaging, night vision and microscopy. Up to now, many state-of-the-art Poisson denoising techniques mainly concentrate on achieving utmost performance, with little consideration for the computation efficiency. Therefore, in this study we aim to propose an efficient Poisson denoising model with both high computational efficiency and recovery quality. To this end, we exploit the newly-developed trainable nonlinear reaction diffusion model which has proven an extremely fast image restoration approach with performance surpassing recent state-of-the-arts. We retrain the model parameters, including the linear filters and influence functions by taking into account the Poisson noise statistics, and end up with an optimized nonlinear diffusion model specialized for Poisson denoising. The trained model provides strongly competitive results against state-of-the-art approaches, meanwhile bearing the properties of simple structure and high efficiency. Furthermore, our proposed model comes along with an additional advantage, that the diffusion process is well-suited for parallel computation on GPUs. For images of size $512 \\times 512$, our GPU implementation takes less than 0.1 seconds to produce state-of-the-art Poisson denoising performance.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02942",
        "title": "Evaluation of Joint Multi-Instance Multi-Label Learning For Breast Cancer Diagnosis",
        "authors": [
            "Baris Gecer",
            "Ozge Yalcinkaya",
            "Onur Tasar",
            "Selim Aksoy"
        ],
        "abstract": "Multi-instance multi-label (MIML) learning is a challenging problem in many aspects. Such learning approaches might be useful for many medical diagnosis applications including breast cancer detection and classification. In this study subset of digiPATH dataset (whole slide digital breast cancer histopathology images) are used for training and evaluation of six state-of-the-art MIML methods.\n",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02949",
        "title": "Spatial Semantic Regularisation for Large Scale Object Detection",
        "authors": [
            "Damian Mrowca",
            "Marcus Rohrbach",
            "Judy Hoffman",
            "Ronghang Hu",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections, which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label- and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering, which simultaneously optimises across all categories and all proposed locations in the image, to improve both the localisation and categorisation of selected detection proposals. Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives, while simultaneously improving the recall.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02969",
        "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?",
        "authors": [
            "Pooya Khorrami",
            "Tom Le Paine",
            "Thomas S. Huang"
        ],
        "abstract": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2017-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03125",
        "title": "Fast detection of multiple objects in traffic scenes with a common detection framework",
        "authors": [
            "Qichang Hu",
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Fatih Porikli"
        ],
        "abstract": "Traffic scene perception (TSP) aims to real-time extract accurate on-road environment information, which in- volves three phases: detection of objects of interest, recognition of detected objects, and tracking of objects in motion. Since recognition and tracking often rely on the results from detection, the ability to detect objects of interest effectively plays a crucial role in TSP. In this paper, we focus on three important classes of objects: traffic signs, cars, and cyclists. We propose to detect all the three important objects in a single learning based detection framework. The proposed framework consists of a dense feature extractor and detectors of three important classes. Once the dense features have been extracted, these features are shared with all detectors. The advantage of using one common framework is that the detection speed is much faster, since all dense features need only to be evaluated once in the testing phase. In contrast, most previous works have designed specific detectors using different features for each of these objects. To enhance the feature robustness to noises and image deformations, we introduce spatially pooled features as a part of aggregated channel features. In order to further improve the generalization performance, we propose an object subcategorization method as a means of capturing intra-class variation of objects. We experimentally demonstrate the effectiveness and efficiency of the proposed framework in three detection applications: traffic sign detection, car detection, and cyclist detection. The proposed framework achieves the competitive performance with state-of- the-art approaches on several benchmark datasets.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03199",
        "title": "Interactive multiclass segmentation using superpixel classification",
        "authors": [
            "B\u00e9reng\u00e8re Mathieu",
            "Alain Crouzil",
            "Jean-Baptiste Puel"
        ],
        "abstract": "This paper adresses the problem of interactive multiclass segmentation. We propose a fast and efficient new interactive segmentation method called Superpixel Classification-based Interactive Segmentation (SCIS). From a few strokes drawn by a human user over an image, this method extracts relevant semantic objects. To get a fast calculation and an accurate segmentation, SCIS uses superpixel over-segmentation and support vector machine classification. In this paper, we demonstrate that SCIS significantly outperfoms competing algorithms by evaluating its performances on the reference benchmarks of McGuinness and Santner.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03250",
        "title": "Using Anatomical Markers for Left Ventricular Segmentation of Long Axis Ultrasound Images",
        "authors": [
            "Yael Petrank",
            "Nahum Smirin",
            "Yossi Tsadok",
            "Zvi Friedman",
            "Peter Lysiansky",
            "Dan Adam"
        ],
        "abstract": "Left ventricular segmentation is essential for measuring left ventricular function indices. Segmentation of one or several images requires an initial guess of the contour. It is hypothesized here that creating an initial guess by first detecting anatomical markers, would lead to correct detection of the endocardium. The first step of the algorithm presented here includes automatic detection of the mitral valve. Next, the apex is detected in the same frame. The valve is then tracked throughout the cardiac cycle. Contours passing from the apex to each valve corner are then found using a dynamic programming algorithm. The resulting contour is used as an input to an active contour algorithm. The algorithm was tested on 21 long axis ultrasound clips and showed good agreement with manually traced contours. Thus, this study demonstrates that detection of anatomic markers leads to a reliable initial guess of the left ventricle border.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2015-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03283",
        "title": "Text-Attentional Convolutional Neural Networks for Scene Text Detection",
        "authors": [
            "Tong He",
            "Weilin Huang",
            "Yu Qiao",
            "Jian Yao"
        ],
        "abstract": "Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially.\n    ",
        "submission_date": "2015-10-12T00:00:00",
        "last_modified_date": "2016-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03608",
        "title": "Deep convolutional neural networks for pedestrian detection",
        "authors": [
            "Denis Tom\u00e8",
            "Federico Monti",
            "Luca Baroffio",
            "Luca Bondi",
            "Marco Tagliasacchi",
            "Stefano Tubaro"
        ],
        "abstract": "Pedestrian detection is a popular research topic due to its paramount importance for a number of applications, especially in the fields of automotive, surveillance and robotics. Despite the significant improvements, pedestrian detection is still an open challenge that calls for more and more accurate algorithms. In the last few years, deep learning and in particular convolutional neural networks emerged as the state of the art in terms of accuracy for a number of computer vision tasks such as image classification, object detection and segmentation, often outperforming the previous gold standards by a large margin. In this paper, we propose a pedestrian detection system based on deep learning, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline we propose an architecture that outperforms traditional methods, achieving a task accuracy close to that of state-of-the-art approaches, while requiring a low computational time. Finally, we tested the system on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be a forerunner computational brain of future self-driving cars.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2016-03-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03727",
        "title": "SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes",
        "authors": [
            "Stuart Golodetz",
            "Michael Sapienza",
            "Julien P. C. Valentin",
            "Vibhav Vineet",
            "Ming-Ming Cheng",
            "Anurag Arnab",
            "Victor A. Prisacariu",
            "Olaf K\u00e4hler",
            "Carl Yuheng Ren",
            "David W. Murray",
            "Shahram Izadi",
            "Philip H. S. Torr"
        ],
        "abstract": "We present an open-source, real-time implementation of SemanticPaint, a system for geometric reconstruction, object-class segmentation and learning of 3D scenes. Using our system, a user can walk into a room wearing a depth camera and a virtual reality headset, and both densely reconstruct the 3D scene and interactively segment the environment into object classes such as 'chair', 'floor' and 'table'. The user interacts physically with the real-world scene, touching objects and using voice commands to assign them appropriate labels. These user-generated labels are leveraged by an online random forest-based machine learning algorithm, which is used to predict labels for previously unseen parts of the scene. The entire pipeline runs in real time, and the user stays 'in the loop' throughout the process, receiving immediate feedback about the progress of the labelling and interacting with the scene as necessary to refine the predicted segmentation.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03743",
        "title": "Wide-Area Image Geolocalization with Aerial Reference Imagery",
        "authors": [
            "Scott Workman",
            "Richard Souvenir",
            "Nathan Jacobs"
        ],
        "abstract": "We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03909",
        "title": "Variable-state Latent Conditional Random Fields for Facial Expression Recognition and Action Unit Detection",
        "authors": [
            "Robert Walecki",
            "Ognjen Rudovic",
            "Vladimir Pavlovic",
            "Maja Pantic"
        ],
        "abstract": "Automated recognition of facial expressions of emotions, and detection of facial action units (AUs), from videos depends critically on modeling of their dynamics. These dynamics are characterized by changes in temporal phases (onset-apex-offset) and intensity of emotion expressions and AUs, the appearance of which may vary considerably among target subjects, making the recognition/detection task very challenging. The state-of-the-art Latent Conditional Random Fields (L-CRF) framework allows one to efficiently encode these dynamics through the latent states accounting for the temporal consistency in emotion expression and ordinal relationships between its intensity levels, these latent states are typically assumed to be either unordered (nominal) or fully ordered (ordinal). Yet, such an approach is often too restrictive. For instance, in the case of AU detection, the goal is to discriminate between the segments of an image sequence in which this AU is active or inactive. While the sequence segments containing activation of the target AU may better be described using ordinal latent states, the inactive segments better be described using unordered (nominal) latent states, as no assumption can be made about their underlying structure (since they can contain either neutral faces or activations of non-target AUs). To address this, we propose the variable-state L-CRF (VSL-CRF) model that automatically selects the optimal latent states for the target image sequence. To reduce the model overfitting either the nominal or ordinal latent states, we propose a novel graph-Laplacian regularization of the latent states. Our experiments on three public expression databases show that the proposed model achieves better generalization performance compared to traditional L-CRFs and other related state-of-the-art models.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03979",
        "title": "Better Exploiting OS-CNNs for Better Event Recognition in Images",
        "authors": [
            "Limin Wang",
            "Zhe Wang",
            "Sheng Guo",
            "Yu Qiao"
        ],
        "abstract": "Event recognition from still images is one of the most important problems for image understanding. However, compared with object recognition and scene recognition, event recognition has received much less research attention in computer vision community. This paper addresses the problem of cultural event recognition in still images and focuses on applying deep learning methods on this problem. In particular, we utilize the successful architecture of Object-Scene Convolutional Neural Networks (OS-CNNs) to perform event recognition. OS-CNNs are composed of object nets and scene nets, which transfer the learned representations from the pre-trained models on large-scale object and scene recognition datasets, respectively. We propose four types of scenarios to explore OS-CNNs for event recognition by treating them as either \"end-to-end event predictors\" or \"generic feature extractors\". Our experimental results demonstrate that the global and local representations of OS-CNNs are complementary to each other. Finally, based on our investigation of OS-CNNs, we come up with a solution for the cultural event recognition track at the ICCV ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third place at this challenge and our result is very close to the best performance.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04004",
        "title": "Multiresolution Search of the Rigid Motion Space for Intensity Based Registration",
        "authors": [
            "Behrooz Nasihatkon",
            "Fredrik Kahl"
        ],
        "abstract": "We study the relation between the target functions of low-resolution and high-resolution intensity-based registration for the class of rigid transformations. Our results show that low resolution target values can tightly bound the high-resolution target function in natural images. This can help with analyzing and better understanding the process of multiresolution image registration. It also gives a guideline for designing multiresolution algorithms in which the search space in higher resolution registration is restricted given the fitness values for lower resolution image pairs. To demonstrate this, we incorporate our multiresolution technique into a Lipschitz global optimization framework. We show that using the multiresolution scheme can result in large gains in the efficiency of such algorithms. The method is evaluated by applying to 2D and 3D registration problems as well as the detection of reflective symmetry in 2D and 3D images.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04074",
        "title": "Fine-Grained Product Class Recognition for Assisted Shopping",
        "authors": [
            "Marian George",
            "Dejan Mircic",
            "G\u00e1bor S\u00f6r\u00f6s",
            "Christian Floerkemeier",
            "Friedemann Mattern"
        ],
        "abstract": "Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the fine-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the large-scale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform fine-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve the recognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04238",
        "title": "Dynamical spectral unmixing of multitemporal hyperspectral images",
        "authors": [
            "Simon Henrot",
            "Jocelyn Chanussot",
            "Christian Jutten"
        ],
        "abstract": "In this paper, we consider the problem of unmixing a time series of hyperspectral images. We propose a dynamical model based on linear mixing processes at each time instant. The spectral signatures and fractional abundances of the pure materials in the scene are seen as latent variables, and assumed to follow a general dynamical structure. Based on a simplified version of this model, we derive an efficient spectral unmixing algorithm to estimate the latent variables by performing alternating minimizations. The performance of the proposed approach is demonstrated on synthetic and real multitemporal hyperspectral images.\n    ",
        "submission_date": "2015-10-14T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04373",
        "title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization",
        "authors": [
            "Muhammad Ghifary",
            "David Balduzzi",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang"
        ],
        "abstract": "This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04389",
        "title": "Sketch-based Manga Retrieval using Manga109 Dataset",
        "authors": [
            "Yusuke Matsui",
            "Kota Ito",
            "Yuji Aramaki",
            "Toshihiko Yamasaki",
            "Kiyoharu Aizawa"
        ],
        "abstract": "Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04390",
        "title": "Dual Principal Component Pursuit",
        "authors": [
            "Manolis C. Tsakiris",
            "Rene Vidal"
        ],
        "abstract": "We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2019-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04396",
        "title": "Filtrated Spectral Algebraic Subspace Clustering",
        "authors": [
            "Manolis C. Tsakiris",
            "Rene Vidal"
        ],
        "abstract": "Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04437",
        "title": "A Novel Approach for Human Action Recognition from Silhouette Images",
        "authors": [
            "Satyabrata Maity",
            "Debotosh Bhattacharjee",
            "Amlan Chakrabarti"
        ],
        "abstract": "In this paper, a novel human action recognition technique from video is presented. Any action of human is a combination of several micro action sequences performed by one or more body parts of the human. The proposed approach uses spatio-temporal body parts movement (STBPM) features extracted from foreground silhouette of the human objects. The newly proposed STBPM feature estimates the movements of different body parts for any given time segment to classify actions. We also proposed a rule based logic named rule action classifier (RAC), which uses a series of condition action rules based on prior knowledge and hence does not required training to classify any action. Since we don't require training to classify actions, the proposed approach is view independent. The experimental results on publicly available Wizeman and MuHVAi datasets are compared with that of the related research work in terms of accuracy in the human action detection, and proposed technique outperforms the others.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04445",
        "title": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers",
        "authors": [
            "Amir Ghodrati",
            "Ali Diba",
            "Marco Pedersoli",
            "Tinne Tuytelaars",
            "Luc Van Gool"
        ],
        "abstract": "In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the gen- eration of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the- art detection performance.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04493",
        "title": "Sparsity-aware Possibilistic Clustering Algorithms",
        "authors": [
            "Spyridoula D. Xenaki",
            "Konstantinos D. Koutroumbas",
            "Athanasios A. Rontogiannis"
        ],
        "abstract": "In this paper two novel possibilistic clustering algorithms are presented, which utilize the concept of sparsity. The first one, called sparse possibilistic c-means, exploits sparsity and can deal well with closely located clusters that may also be of significantly different densities. The second one, called sparse adaptive possibilistic c-means, is an extension of the first, where now the involved parameters are dynamically adapted. The latter can deal well with even more challenging cases, where, in addition to the above, clusters may be of significantly different variances. More specifically, it provides improved estimates of the cluster representatives, while, in addition, it has the ability to estimate the actual number of clusters, given an overestimate of it. Extensive experimental results on both synthetic and real data sets support the previous statements.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04563",
        "title": "Elasticity-based Matching by Minimizing the Symmetric Difference of Shapes",
        "authors": [
            "Konrad Simon",
            "Ronen Basri"
        ],
        "abstract": "We consider the problem of matching two shapes assuming these shapes are related by an elastic deformation. Using linearized elasticity theory and the finite element method we seek an elastic deformation that is caused by simple external boundary forces and accounts for the difference between the two shapes. Our main contribution is in proposing a cost function and an optimization procedure to minimize the symmetric difference between the deformed and the target shapes as an alternative to point matches that guide the matching in other techniques. We show how to approximate the nonlinear optimization problem by a sequence of convex problems. We demonstrate the utility of our method in experiments and compare it to an ICP-like matching algorithm.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04565",
        "title": "Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for Action Recognition",
        "authors": [
            "Zhenzhong Lan",
            "Alexander G. Hauptmann"
        ],
        "abstract": "We address the problem of generating video features for action recognition. The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance. Although it seems straightforward to extend spatial pyramid to the temporal domain (spatio-temporal pyramid), the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing. This paper introduces the space-time extended descriptor, a simple but efficient alternative way to include the spatio-temporal location into the video features. Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage, location information is used as part of the encoding step. This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low. Experimental results on several benchmark datasets show that, despite its simplicity, this method achieves comparable or better results than spatio-temporal pyramid.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04585",
        "title": "A Brief Survey of Image Processing Algorithms in Electrical Capacitance Tomography",
        "authors": [
            "Kezhi Li"
        ],
        "abstract": "To study the fundamental physics of complex multiphase flow systems using advanced measurement techniques, especially the electrical capacitance tomography (ECT) approach, this article carries out an initial literature review of the ECT method from a point of view of signal processing and algorithm design. After introducing the physical laws governing the ECT system, we will focus on various reconstruction techniques that are capable to recover the image of the internal characteristics of a specified region based on the measuring capacitances of multi-electrode sensors surrounding the region. Each technique has its own advantages and limitations, and many algorithms have been examined by simulations or experiments. Future researches in 3D reconstruction and other potential improvements of the system are discussed in the end.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04601",
        "title": "A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from Dense Binary Pixels",
        "authors": [
            "Tal Remez",
            "Or Litany",
            "Alex Bronstein"
        ],
        "abstract": "The pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form-factor requirements of sensors and optics in the cellular phone market. Recently, Eric Fossum proposed a novel concept of an image sensor with dense sub-diffraction limit one-bit pixels jots, which can be considered a digital emulation of silver halide photographic film. This idea has been recently embodied as the EPFL Gigavision camera. A major bottleneck in the design of such sensors is the image reconstruction process, producing a continuous high dynamic range image from oversampled binary measurements. The extreme quantization of the Poisson statistics is incompatible with the assumptions of most standard image processing and enhancement frameworks. The recently proposed maximum-likelihood (ML) approach addresses this difficulty, but suffers from image artifacts and has impractically high computational complexity. In this work, we study a variant of a sensor with binary threshold pixels and propose a reconstruction algorithm combining an ML data fitting term with a sparse synthesis prior. We also show an efficient hardware-friendly real-time approximation of this inverse ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04609",
        "title": "Layer-Specific Adaptive Learning Rates for Deep Networks",
        "authors": [
            "Bharat Singh",
            "Soham De",
            "Yangmuzi Zhang",
            "Thomas Goldstein",
            "Gavin Taylor"
        ],
        "abstract": "The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04706",
        "title": "Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling Problems",
        "authors": [
            "John S.H. Baxter",
            "Jing Yuan",
            "Terry M. Peters"
        ],
        "abstract": "Although topological considerations amongst multiple labels have been previously investigated in the context of continuous max-flow image segmentation, similar investigations have yet to be made about shape considerations in a general and extendable manner. This paper presents shape complexes for segmentation, which capture more complex shapes by combining multiple labels and super-labels constrained by geodesic star convexity. Shape complexes combine geodesic star convexity constraints with hierarchical label organization, which together allow for more complex shapes to be represented. This framework avoids the use of co-ordinate system warping techniques to convert shape constraints into topological constraints, which may be ambiguous or ill-defined for certain segmentation problems.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04842",
        "title": "Multiresolution hierarchy co-clustering for semantic segmentation in sequences with small variations",
        "authors": [
            "David Varas",
            "M\u00f3nica Alfaro",
            "Ferran Marques"
        ],
        "abstract": "This paper presents a co-clustering technique that, given a collection of images and their hierarchies, clusters nodes from these hierarchies to obtain a coherent multiresolution representation of the image collection. We formalize the co-clustering as a Quadratic Semi-Assignment Problem and solve it with a linear programming relaxation approach that makes effective use of information from hierarchies. Initially, we address the problem of generating an optimal, coherent partition per image and, afterwards, we extend this method to a multiresolution framework. Finally, we particularize this framework to an iterative multiresolution video segmentation algorithm in sequences with small variations. We evaluate the algorithm on the Video Occlusion/Object Boundary Detection Dataset, showing that it produces state-of-the-art results in these scenarios.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04860",
        "title": "Measurement of Road Traffic Parameters Based on Multi-Vehicle Tracking",
        "authors": [
            "Kristian Kova\u010di\u0107",
            "Edouard Ivanjko",
            "Niko Jelu\u0161i\u0107"
        ],
        "abstract": "Development of computing power and cheap video cameras enabled today's traffic management systems to include more cameras and computer vision applications for transportation system monitoring and control. Combined with image processing algorithms cameras are used as sensors to measure road traffic parameters like flow volume, origin-destination matrices, classify vehicles, etc. In this paper we propose a system for measurement of road traffic parameters (basic motion model parameters and macro-scopic traffic parameters). The system is based on Local Binary Pattern (LBP) image features classification with a cascade of Gentle Adaboost (GAB) classifiers to determine vehicle existence and its location in an image. Additionally, vehicle tracking and counting in a road traffic video is performed by using Extended Kalman Filter (EKF) and virtual markers. The newly proposed system is compared with a system based on background subtraction. Comparison is performed by the means of execution time and accuracy.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04861",
        "title": "Towards Reversible De-Identification in Video Sequences Using 3D Avatars and Steganography",
        "authors": [
            "Martin Bla\u017eevi\u0107",
            "Karla Brki\u0107",
            "Tomislav Hrka\u0107"
        ],
        "abstract": "We propose a de-identification pipeline that protects the privacy of humans in video sequences by replacing them with rendered 3D human models, hence concealing their identity while retaining the naturalness of the scene. The original images of humans are steganographically encoded in the carrier image, i.e. the image containing the original scene and the rendered 3D human models. We qualitatively explore the feasibility of our approach, utilizing the Kinect sensor and its libraries to detect and localize human joints. A 3D avatar is rendered into the scene using the obtained joint positions, and the original human image is steganographically encoded in the new scene. Our qualitative evaluation shows reasonably good results that merit further exploration.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04862",
        "title": "You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards Video-Based Guidance",
        "authors": [
            "Dima Damen",
            "Teesid Leelasawassuk",
            "Walterio Mayol-Cuevas"
        ],
        "abstract": "This paper presents an unsupervised approach towards automatically extracting video-based guidance on object usage, from egocentric video and wearable gaze tracking, collected from multiple users while performing tasks. The approach i) discovers task relevant objects, ii) builds a model for each, iii) distinguishes different ways in which each discovered object has been used and iv) discovers the dependencies between object interactions. The work investigates using appearance, position, motion and attention, and presents results using each and a combination of relevant features. Moreover, an online scalable approach is presented and is compared to offline results. The paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user's gaze. The potential assistive mode can also recommend an object to be used next based on the learnt sequence of object interactions. The approach was tested on a variety of daily tasks such as initialising a printer, preparing a coffee and setting up a gym machine.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04863",
        "title": "An Extension to Hough Transform Based on Gradient Orientation",
        "authors": [
            "Tomislav Petkovi\u0107",
            "Sven Lon\u010dari\u0107"
        ],
        "abstract": "The Hough transform is one of the most common methods for line detection. In this paper we propose a novel extension of the regular Hough transform. The proposed extension combines the extension of the accumulator space and the local gradient orientation resulting in clutter reduction and yielding more prominent peaks, thus enabling better line identification. We demonstrate benefits in applications such as visual quality inspection and rectangle detection.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04908",
        "title": "No Spare Parts: Sharing Part Detectors for Image Categorization",
        "authors": [
            "Pascal Mettes",
            "Jan C. van Gemert",
            "Cees G. M. Snoek"
        ],
        "abstract": "This work aims for image categorization using a representation of distinctive parts. Different from existing part-based work, we argue that parts are naturally shared between image categories and should be modeled as such. We motivate our approach with a quantitative and qualitative analysis by backtracking where selected parts come from. Our analysis shows that in addition to the category parts defining the class, the parts coming from the background context and parts from other image categories improve categorization performance. Part selection should not be done separately for each category, but instead be shared and optimized over all categories. To incorporate part sharing between categories, we present an algorithm based on AdaBoost to jointly optimize part sharing and selection, as well as fusion with the global image representation. We achieve results competitive to the state-of-the-art on object, scene, and action categories, further improving over deep convolutional neural networks.\n    ",
        "submission_date": "2015-10-16T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05138",
        "title": "Integral Images: Efficient Algorithms for Their Computation and Storage in Resource-Constrained Embedded Vision Systems",
        "authors": [
            "Shoaib Ehsan",
            "Adrian F. Clark",
            "Naveed ur Rehman",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "The integral image, an intermediate image representation, has found extensive use in multi-scale local feature detection algorithms, such as Speeded-Up Robust Features (SURF), allowing fast computation of rectangular features at constant speed, independent of filter size. For resource-constrained real-time embedded vision systems, computation and storage of integral image presents several design challenges due to strict timing and hardware limitations. Although calculation of the integral image only consists of simple addition operations, the total number of operations is large owing to the generally large size of image data. Recursive equations allow substantial decrease in the number of operations but require calculation in a serial fashion. This paper presents two new hardware algorithms that are based on the decomposition of these recursive equations, allowing calculation of up to four integral image values in a row-parallel way without significantly increasing the number of operations. An efficient design strategy is also proposed for a parallel integral image computation unit to reduce the size of the required internal memory (nearly 35% for common HD video). Addressing the storage problem of integral image in embedded vision systems, the paper presents two algorithms which allow substantial decrease (at least 44.44%) in the memory requirements. Finally, the paper provides a case study that highlights the utility of the proposed architectures in embedded vision systems.\n    ",
        "submission_date": "2015-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05142",
        "title": "Memory-Efficient Design Strategy for a Parallel Embedded Integral Image Computation Engine",
        "authors": [
            "Shoaib Ehsan",
            "Adrian F. Clark",
            "Wah M. Cheung",
            "Arjunsingh M. Bais",
            "Bayar I. Menzat",
            "Nadia Kanwal",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "In embedded vision systems, parallel computation of the integral image presents several design challenges in terms of hardware resources, speed and power consumption. Although recursive equations significantly reduce the number of operations for computing the integral image, the required internal memory becomes prohibitively large for an embedded integral image computation engine for increasing image sizes. With the objective of achieving high-throughput with minimum hardware resources, this paper proposes a memory-efficient design strategy for a parallel embedded integral image computation engine. Results show that the design achieves nearly 35% reduction in memory for common HD video.\n    ",
        "submission_date": "2015-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05145",
        "title": "Rapid Online Analysis of Local Feature Detectors and Their Complementarity",
        "authors": [
            "Shoaib Ehsan",
            "Adrian F. Clark",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "A vision system that can assess its own performance and take appropriate actions online to maximize its effectiveness would be a step towards achieving the long-cherished goal of imitating humans. This paper proposes a method for performing an online performance analysis of local feature detectors, the primary stage of many practical vision systems. It advocates the spatial distribution of local image features as a good performance indicator and presents a metric that can be calculated rapidly, concurs with human visual assessments and is complementary to existing offline measures such as repeatability. The metric is shown to provide a measure of complementarity for combinations of detectors, correctly reflecting the underlying principles of individual detectors. Qualitative results on well-established datasets for several state-of-the-art detectors are presented based on the proposed measure. Using a hypothesis testing approach and a newly-acquired, larger image database, statistically-significant performance differences are identified. Different detector pairs and triplets are examined quantitatively and the results provide a useful guideline for combining detectors in applications that require a reasonable spatial distribution of image features. A principled framework for combining feature detectors in these applications is also presented. Timing results reveal the potential of the metric for online applications.\n    ",
        "submission_date": "2015-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05156",
        "title": "Assessing The Performance Bounds Of Local Feature Detectors: Taking Inspiration From Electronics Design Practices",
        "authors": [
            "Shoaib Ehsan",
            "Adrian F. Clark",
            "Bruno Ferrarini",
            "Naveed Ur Rehman",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "Since local feature detection has been one of the most active research areas in computer vision, a large number of detectors have been proposed. This has rendered the task of characterizing the performance of various feature detection methods an important issue in vision research. Inspired by the good practices of electronic system design, a generic framework based on the improved repeatability measure is presented in this paper that allows assessment of the upper and lower bounds of detector performance in an effort to design more reliable and effective vision systems. This framework is then employed to establish operating and guarantee regions for several state-of-the art detectors for JPEG compression and uniform light changes. The results are obtained using a newly acquired, large image database (15092 images) with 539 different scenes. These results provide new insights into the behavior of detectors and are also useful from the vision systems design perspective.\n    ",
        "submission_date": "2015-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05157",
        "title": "Performance Characterization of Image Feature Detectors in Relation to the Scene Content Utilizing a Large Image Database",
        "authors": [
            "Bruno Ferrarini",
            "Shoaib Ehsan",
            "Naveed Ur Rehman",
            "Klaus D. McDonald-Maier"
        ],
        "abstract": "Selecting the most suitable local invariant feature detector for a particular application has rendered the task of evaluating feature detectors a critical issue in vision research. No state-of-the-art image feature detector works satisfactorily under all types of image transformations. Although the literature offers a variety of comparison works focusing on performance evaluation of image feature detectors under several types of image transformation, the influence of the scene content on the performance of local feature detectors has received little attention so far. This paper aims to bridge this gap with a new framework for determining the type of scenes, which maximize and minimize the performance of detectors in terms of repeatability rate. Several state-of-the-art feature detectors have been assessed utilizing a large database of 12936 images generated by applying uniform light and blur changes to 539 scenes captured from the real world. The results obtained provide new insights into the behaviour of feature detectors.\n    ",
        "submission_date": "2015-10-17T00:00:00",
        "last_modified_date": "2015-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05275",
        "title": "Real-time Tracking Based on Neuromrophic Vision",
        "authors": [
            "Hongmin Li",
            "Pei Jing",
            "Guoqi Li"
        ],
        "abstract": "Real-time tracking is an important problem in computer vision in which most methods are based on the conventional cameras. Neuromorphic vision is a concept defined by incorporating neuromorphic vision sensors such as silicon retinas in vision processing system. With the development of the silicon technology, asynchronous event-based silicon retinas that mimic neuro-biological architectures has been developed in recent years. In this work, we combine the vision tracking algorithm of computer vision with the information encoding mechanism of event-based sensors which is inspired from the neural rate coding mechanism. The real-time tracking of single object with the advantage of high speed of 100 time bins per second is successfully realized. Our method demonstrates that the computer vision methods could be used for the neuromorphic vision processing and we can realize fast real-time tracking using neuromorphic vision sensors compare to the conventional camera.\n    ",
        "submission_date": "2015-10-18T00:00:00",
        "last_modified_date": "2015-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05436",
        "title": "Color graph based wavelet transform with perceptual information",
        "authors": [
            "Mohamed Malek",
            "David Helbert",
            "Philippe Carre"
        ],
        "abstract": "In this paper, we propose a numerical strategy to define a multiscale analysis for color and multicomponent images based on the representation of data on a graph. Our approach consists in computing the graph of an image using the psychovisual information and analysing it by using the spectral graph wavelet transform. We suggest introducing color dimension into the computation of the weights of the graph and using the geodesic distance as a means of distance measurement. We thus have defined a wavelet transform based on a graph with perceptual information by using the CIELab color distance. This new representation is illustrated with denoising and inpainting applications. Overall, by introducing psychovisual information in the graph computation for the graph wavelet transform we obtain very promising results. Therefore results in image restoration highlight the interest of the appropriate use of color information.\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05484",
        "title": "DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection",
        "authors": [
            "Xi Li",
            "Liming Zhao",
            "Lina Wei",
            "Ming-Hsuan Yang",
            "Fei Wu",
            "Yueting Zhuang",
            "Haibin Ling",
            "Jingdong Wang"
        ],
        "abstract": "A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network (FCNN) with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with great feature redundancy reduction. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05559",
        "title": "Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel Matrix for Impulse Noise Removal",
        "authors": [
            "Kyong Hwan Jin",
            "Jong Chul Ye"
        ],
        "abstract": "Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA) approach was proposed as a powerful image inpainting method. Based on the observation that smoothness or textures within an image patch corresponds to sparse spectral components in the frequency domain, ALOHA exploits the existence of annihilating filters and the associated rank-deficient Hankel matrices in the image domain to estimate the missing pixels. By extending this idea, here we propose a novel impulse noise removal algorithm using sparse + low rank decomposition of an annihilating filter-based Hankel matrix. The new approach, what we call the robust ALOHA, is motivated by the observation that an image corrupted with impulse noises has intact pixels; so the impulse noises can be modeled as sparse components, whereas the underlying image can be still modeled using a low-rank Hankel structured matrix. To solve the sparse + low rank decomposition problem, we propose an alternating direction method of multiplier (ADMM) method with initial factorized matrices coming from low rank matrix fitting (LMaFit) algorithm. To adapt the local image statistics that have distinct spectral distributions, the robust ALOHA is applied patch by patch. Experimental results from two types of impulse noises - random valued impulse noises and salt/pepper noises - for both single channel and multi-channel color images demonstrate that the robust ALOHA outperforms the existing algorithms up to 8dB in terms of the peak signal to noise ratio (PSNR).\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05613",
        "title": "PERCH: Perception via Search for Multi-Object Recognition and Localization",
        "authors": [
            "Venkatraman Narayanan",
            "Maxim Likhachev"
        ],
        "abstract": "In many robotic domains such as flexible automated manufacturing or personal assistance, a fundamental perception task is that of identifying and localizing objects whose 3D models are known. Canonical approaches to this problem include discriminative methods that find correspondences between feature descriptors computed over the model and observed data. While these methods have been employed successfully, they can be unreliable when the feature descriptors fail to capture variations in observed data; a classic cause being occlusion. As a step towards deliberative reasoning, we present PERCH: PErception via SeaRCH, an algorithm that seeks to find the best explanation of the observed sensor data by hypothesizing possible scenes in a generative fashion. Our contributions are: i) formulating the multi-object recognition and localization task as an optimization problem over the space of hypothesized scenes, ii) exploiting structure in the optimization to cast it as a combinatorial search problem on what we call the Monotone Scene Generation Tree, and iii) leveraging parallelization and recent advances in multi-heuristic search in making combinatorial search tractable. We prove that our system can guaranteedly produce the best explanation of the scene under the chosen cost function, and validate our claims on real world RGB-D test data. Our experimental results show that we can identify and localize objects under heavy occlusion--cases where state-of-the-art methods struggle.\n    ",
        "submission_date": "2015-10-19T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05822",
        "title": "Sequential Score Adaptation with Extreme Value Theory for Robust Railway Track Inspection",
        "authors": [
            "Xavier Gibert",
            "Vishal M. Patel",
            "Rama Chellappa"
        ],
        "abstract": "Periodic inspections are necessary to keep railroad tracks in state of good repair and prevent train accidents. Automatic track inspection using machine vision technology has become a very effective inspection tool. Because of its non-contact nature, this technology can be deployed on virtually any railway vehicle to continuously survey the tracks and send exception reports to track maintenance personnel. However, as appearance and imaging conditions vary, false alarm rates can dramatically change, making it difficult to select a good operating point. In this paper, we use extreme value theory (EVT) within a Bayesian framework to optimally adjust the sensitivity of anomaly detectors. We show that by approximating the lower tail of the probability density function (PDF) of the scores with an Exponential distribution (a special case of the Generalized Pareto distribution), and using the Gamma conjugate prior learned from the training data, it is possible to reduce the variability in false alarm rate and improve the overall performance. This method has shown an increase in the defect detection rate of rail fasteners in the presence of clutter (at PFA 0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013 concrete tie dataset.\n    ",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05970",
        "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
        "authors": [
            "Jure \u017dbontar",
            "Yann LeCun"
        ],
        "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.\n    ",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2016-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06093",
        "title": "Content adaptive screen image scaling",
        "authors": [
            "Yao Zhai",
            "Qifei Wang",
            "Yan Lu",
            "Shipeng Li"
        ],
        "abstract": "This paper proposes an efficient content adaptive screen image scaling scheme for the real-time screen applications like remote desktop and screen sharing. In the proposed screen scaling scheme, a screen content classification step is first introduced to classify the screen image into text and pictorial regions. Afterward, we propose an adaptive shift linear interpolation algorithm to predict the new pixel values with the shift offset adapted to the content type of each pixel. The shift offset for each screen content type is offline optimized by minimizing the theoretical interpolation error based on the training samples respectively. The proposed content adaptive screen image scaling scheme can achieve good visual quality and also keep the low complexity for real-time applications.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06375",
        "title": "Towards Direct Medical Image Analysis without Segmentation",
        "authors": [
            "Xiantong Zhen",
            "Shuo Li"
        ],
        "abstract": "Direct methods have recently emerged as an effective and efficient tool in automated medical image analysis and become a trend to solve diverse challenging tasks in clinical practise. Compared to traditional methods, direct methods are of much more clinical significance by straightly targeting to the final clinical goal rather than relying on any intermediate steps. These intermediate steps, e.g., segmentation, registration and tracking, are actually not necessary and only limited to very constrained tasks far from being used in practical clinical applications; moreover they are computationally expensive and time-consuming, which causes a high waste of research resources. The advantages of direct methods stem from \\textbf{1)} removal of intermediate steps, e.g., segmentation, tracking and registration; \\textbf{2)} avoidance of user inputs and initialization; \\textbf{3)} reformulation of conventional challenging problems, e.g., inversion problem, with efficient solutions.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06503",
        "title": "Personalized Age Progression with Aging Dictionary",
        "authors": [
            "Xiangbo Shu",
            "Jinhui Tang",
            "Hanjiang Lai",
            "Luoqi Liu",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2015-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06507",
        "title": "Modelling, Measuring and Compensating Color Weak Vision",
        "authors": [
            "Satoshi Oshima",
            "Rica Mochizuki",
            "Reiner Lenz",
            "Jinhui Chao"
        ],
        "abstract": "We use methods from Riemann geometry to investigate transformations between the color spaces of color-normal and color weak observers. The two main applications are the simulation of the perception of a color weak observer for a color normal observer and the compensation of color images in a way that a color weak observer has approximately the same perception as a color normal observer. The metrics in the color spaces of interest are characterized with the help of ellipsoids defined by the just-noticable-differences between color which are measured with the help of color-matching experiments. The constructed mappings are isometries of Riemann spaces that preserve the perceived color-differences for both observers. Among the two approaches to build such an isometry, we introduce normal coordinates in Riemann spaces as a tool to construct a global color-weak compensation map. Compared to previously used methods this method is free from approximation errors due to local linearizations and it avoids the problem of shifting locations of the origin of the local coordinate system. We analyse the variations of the Riemann metrics for different observers obtained from new color matching experiments and describe three variations of the basic method. The performance of the methods is evaluated with the help of semantic differential (SD) tests.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2015-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06595",
        "title": "Efficient Unsupervised Temporal Segmentation of Motion Data",
        "authors": [
            "Bj\u00f6rn Kr\u00fcger",
            "Anna V\u00f6gele",
            "Tobias Willig",
            "Angela Yao",
            "Reinhard Klein",
            "Andreas Weber"
        ],
        "abstract": "We introduce a method for automated temporal segmentation of human motion data into distinct actions and compositing motion primitives based on self-similar structures in the motion sequence. We use neighbourhood graphs for the partitioning and the similarity information in the graph is further exploited to cluster the motion primitives into larger entities of semantic significance. The method requires no assumptions about the motion sequences at hand and no user interaction is required for the segmentation or clustering. In addition, we introduce a feature bundling preprocessing technique to make the segmentation more robust to noise, as well as a notion of motion symmetry for more refined primitive detection. We test our method on several sensor modalities, including markered and markerless motion capture as well as on electromyograph and accelerometer recordings. The results highlight our system's capabilities for both segmentation and for analysis of the finer structures of motion data, all in a completely unsupervised manner.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2015-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06767",
        "title": "Order-Fractal transition in abstract paintings",
        "authors": [
            "E. M. De la Calleja",
            "F. Cervantes",
            "J. De la Calleja"
        ],
        "abstract": "We report the degree of order of twenty-two Jackson Pollock's paintings using \\emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value of each multi-fractal spectrum, the artworks are classify by the year in which they were painted. It has been reported that Pollock's paintings are fractal and it increased on his latest works. However our results show that fractal dimension of the paintings are on a range of fractal dimension with values close to two. We identify this behavior as a fractal-order transition. Based on the study of disorder-order transition in physical systems, we interpreted the fractal-order transition through its dark paint strokes in Pollocks' paintings, as structured lines following a power law measured by fractal dimension. We obtain self-similarity in some specific Pollock's paintings, that reveal an important dependence on the scale of observation. We also characterize by its fractal spectrum, the called \\emph{Teri's Find}. We obtained similar spectrums between \\emph{Teri's Find} and \\emph{Number 5} from Pollock, suggesting that fractal dimension cannot be completely rejected as a quantitative parameter to authenticate this kind of artworks.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06915",
        "title": "Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys using Random Forests",
        "authors": [
            "Kanishka Sharma",
            "Loic Peter",
            "Christian Rupprecht",
            "Anna Caroli",
            "Lichao Wang",
            "Andrea Remuzzi",
            "Maximilian Baust",
            "Nassir Navab"
        ],
        "abstract": "This paper presents a method for 3D segmentation of kidneys from patients with autosomal dominant polycystic kidney disease (ADPKD) and severe renal insufficiency, using computed tomography (CT) data. ADPKD severely alters the shape of the kidneys due to non-uniform formation of cysts. As a consequence, fully automatic segmentation of such kidneys is very challenging. We present a segmentation method with minimal user interaction based on a random forest classifier. One of the major novelties of the proposed approach is the usage of geodesic distance volumes as additional source of information. These volumes contain the intensity weighted distance to a manual outline of the respective kidney in only one slice (for each kidney) of the CT volume. We evaluate our method qualitatively and quantitatively on 55 CT acquisitions using ground truth annotations from clinical experts.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06925",
        "title": "Confusing Deep Convolution Networks by Relabelling",
        "authors": [
            "Leigh Robinson",
            "Benjamin Graham"
        ],
        "abstract": "Deep convolutional neural networks have become the gold standard for image recognition tasks, demonstrating many current state-of-the-art results and even achieving near-human level performance on some tasks. Despite this fact it has been shown that their strong generalisation qualities can be fooled to misclassify previously correctly classified natural images and give erroneous high confidence classifications to nonsense synthetic images. In this paper we extend that work, by presenting a straightforward way to perturb an image in such a way as to cause it to acquire any other label from within the dataset while leaving this perturbed image visually indistinguishable from the original.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06939",
        "title": "Objects2action: Classifying and localizing actions without any video example",
        "authors": [
            "Mihir Jain",
            "Jan C. van Gemert",
            "Thomas Mensink",
            "Cees G. M. Snoek"
        ],
        "abstract": "The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07112",
        "title": "Predicting Performance of a Face Recognition System Based on Image Quality",
        "authors": [
            "Abhishek Dutta"
        ],
        "abstract": "In this dissertation, we present a generative model to capture the relation between facial image quality features (like pose, illumination direction, etc) and face recognition performance. Such a model can be used to predict the performance of a face recognition system. Since the model is based solely on image quality features, performance predictions can be done even before the actual recognition has taken place thereby facilitating many preemptive action. A practical limitation of such a data driven generative model is the limited nature of training data set. To address this limitation, we have developed a Bayesian approach to model the distribution of recognition performance measure based on the number of match and non-match scores in small regions of the image quality space. Random samples drawn from these models provide the initial data essential for training the generative model. Experiment results based on six face recognition systems operating on three independent data sets show that the proposed performance prediction model can accurately predict face recognition performance using an accurate and unbiased Image Quality Assessor (IQA). Furthermore, our results show that variability in the unaccounted quality space -- the image quality features not considered by the IQA -- is the major factor causing inaccuracies in predicted performance.\n    ",
        "submission_date": "2015-10-24T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07119",
        "title": "Predicting Face Recognition Performance Using Image Quality",
        "authors": [
            "Abhishek Dutta",
            "Raymond Veldhuis",
            "Luuk Spreeuwers"
        ],
        "abstract": "This paper proposes a data driven model to predict the performance of a face recognition system based on image quality features. We model the relationship between image quality features (e.g. pose, illumination, etc.) and recognition performance measures using a probability density function. To address the issue of limited nature of practical training data inherent in most data driven models, we have developed a Bayesian approach to model the distribution of recognition performance measures in small regions of the quality space. Since the model is based solely on image quality features, it can predict performance even before the actual recognition has taken place. We evaluate the performance predictive capabilities of the proposed model for six face recognition systems (two commercial and four open source) operating on three independent data sets: MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can accurately predict performance using an accurate and unbiased Image Quality Assessor (IQA). Furthermore, our experiments highlight the impact of the unaccounted quality space -- the image quality features not considered by IQA -- in contributing to performance prediction errors.\n    ",
        "submission_date": "2015-10-24T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07136",
        "title": "Image Parsing with a Wide Range of Classes and Scene-Level Context",
        "authors": [
            "Marian George"
        ],
        "abstract": "This paper presents a nonparametric scene parsing approach that improves the overall accuracy, as well as the coverage of foreground classes in scene images. We first improve the label likelihood estimates at superpixels by merging likelihood scores from different probabilistic classifiers. This boosts the classification performance and enriches the representation of less-represented classes. Our second contribution consists of incorporating semantic context in the parsing process through global label costs. Our method does not rely on image retrieval sets but rather assigns a global likelihood estimate to each label, which is plugged into the overall energy function. We evaluate our system on two large-scale datasets, SIFTflow and LMSun. We achieve state-of-the-art performance on the SIFTflow dataset and near-record results on LMSun.\n    ",
        "submission_date": "2015-10-24T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07182",
        "title": "Computational models of attention",
        "authors": [
            "Laurent Itti",
            "Ali Borji"
        ],
        "abstract": "This chapter reviews recent computational models of visual attention. We begin with models for the bottom-up or stimulus-driven guidance of attention to salient visual items, which we examine in seven different broad categories. We then examine more complex models which address the top-down or goal-oriented guidance of attention towards items that are more relevant to the task at hand.\n    ",
        "submission_date": "2015-10-24T00:00:00",
        "last_modified_date": "2015-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07234",
        "title": "Seam Puckering Objective Evaluation Method for Sewing Process",
        "authors": [
            "Raluca Brad",
            "Eugen H\u0102loiu",
            "Remus Brad"
        ],
        "abstract": "The paper presents an automated method for the assessment and classification of puckering defects detected during the preproduction control stage of the sewing machine or product inspection. In this respect, we have presented the possible causes and remedies of the wrinkle nonconformities. Subjective factors related to the control environment and operators during the seams evaluation can be reduced using an automated system whose operation is based on image processing. Our implementation involves spectral image analysis using Fourier transform and an unsupervised neural network, the Kohonen Map, employed to classify material specimens, the input images, into five discrete degrees of quality, from grade 5 (best) to grade 1 (the worst).\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07317",
        "title": "Depth Extraction from Videos Using Geometric Context and Occlusion Boundaries",
        "authors": [
            "S. Hussain Raza",
            "Omar Javed",
            "Aveek Das",
            "Harpreet Sawhney",
            "Hui Cheng",
            "Irfan Essa"
        ],
        "abstract": "We present an algorithm to estimate depth in dynamic video scenes. We propose to learn and infer depth in videos from appearance, motion, occlusion boundaries, and geometric context of the scene. Using our method, depth can be estimated from unconstrained videos with no requirement of camera pose estimation, and with significant background/foreground motions. We start by decomposing a video into spatio-temporal regions. For each spatio-temporal region, we learn the relationship of depth to visual appearance, motion, and geometric classes. Then we infer the depth information of new scenes using piecewise planar parametrization estimated within a Markov random field (MRF) framework by combining appearance to depth learned mappings and occlusion boundary guided smoothness constraints. Subsequently, we perform temporal smoothing to obtain temporally consistent depth maps. To evaluate our depth estimation algorithm, we provide a novel dataset with ground truth depth for outdoor video scenes. We present a thorough evaluation of our algorithm on our new dataset and the publicly available Make3d static image dataset.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07320",
        "title": "Geometric Context from Videos",
        "authors": [
            "S. Hussain Raza",
            "Matthias Grundmann",
            "Irfan Essa"
        ],
        "abstract": "We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation, we decompose a dynamic scene captured by a video into geometric classes, based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction, we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori. We built a novel, extensive dataset on geometric context of video to evaluate our method, consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames. To further scale beyond this dataset, we propose a semi-supervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07323",
        "title": "Finding Temporally Consistent Occlusion Boundaries in Videos using Geometric Context",
        "authors": [
            "S. Hussain Raza",
            "Ahmad Humayun",
            "Matthias Grundmann",
            "David Anderson",
            "Irfan Essa"
        ],
        "abstract": "We present an algorithm for finding temporally consistent occlusion boundaries in videos to support segmentation of dynamic scenes. We learn occlusion boundaries in a pairwise Markov random field (MRF) framework. We first estimate the probability of an spatio-temporal edge being an occlusion boundary by using appearance, flow, and geometric features. Next, we enforce occlusion boundary continuity in a MRF model by learning pairwise occlusion probabilities using a random forest. Then, we temporally smooth boundaries to remove temporal inconsistencies in occlusion boundary estimation. Our proposed framework provides an efficient approach for finding temporally consistent occlusion boundaries in video by utilizing causality, redundancy in videos, and semantic layout of the scene. We have developed a dataset with fully annotated ground-truth occlusion boundaries of over 30 videos ($5000 frames). This dataset is used to evaluate temporal occlusion boundaries and provides a much needed baseline for future studies. We perform experiments to demonstrate the role of scene layout, and temporal information for occlusion reasoning in dynamic scenes.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07391",
        "title": "Vehicle Color Recognition using Convolutional Neural Network",
        "authors": [
            "Reza Fuad Rachmadi",
            "I Ketut Eddy Purnama"
        ],
        "abstract": "Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2018-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07474",
        "title": "A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns",
        "authors": [
            "Alexander G\u00f3mez",
            "German D\u00edez",
            "Jhony Giraldo",
            "Augusto Salazar",
            "Juan M. Daza"
        ],
        "abstract": "Non-intrusive biometrics of animals using images allows to analyze phenotypic populations and individuals with patterns like stripes and spots without affecting the studied subjects. However, non-intrusive biometrics demand a well trained subject or the development of computer vision algorithms that ease the identification task. In this work, an analysis of classic segmentation approaches that require a supervised tuning of their parameters such as threshold, adaptive threshold, histogram equalization, and saturation correction is presented. In contrast, a general unsupervised algorithm using Markov Random Fields (MRF) for segmentation of spots patterns is proposed. Active contours are used to boost results using MRF output as seeds. As study subject the Diploglossus millepunctatus lizard is used. The proposed method achieved a maximum efficiency of $91.11\\%$.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07493",
        "title": "Aggregating Deep Convolutional Features for Image Retrieval",
        "authors": [
            "Artem Babenko",
            "Victor Lempitsky"
        ],
        "abstract": "Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregation approaches developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptors.\n",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07712",
        "title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks",
        "authors": [
            "Haonan Yu",
            "Jiang Wang",
            "Zhiheng Huang",
            "Yi Yang",
            "Wei Xu"
        ],
        "abstract": "We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2016-04-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07748",
        "title": "Computational models: Bottom-up and top-down aspects",
        "authors": [
            "Laurent Itti",
            "Ali Borji"
        ],
        "abstract": "Computational models of visual attention have become popular over the past decade, we believe primarily for two reasons: First, models make testable predictions that can be explored by experimentalists as well as theoreticians, second, models have practical and technological applications of interest to the applied science and engineering communities. In this chapter, we take a critical look at recent attention modeling efforts. We focus on {\\em computational models of attention} as defined by Tsotsos \\& Rothenstein \\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus (typically, an image or video clip), which can possibly also be given some task definition, and which make predictions that can be compared to human or animal behavioral or physiological responses elicited by the same stimulus and task. Thus, we here place less emphasis on abstract models, phenomenological models, purely data-driven fitting or extrapolation models, or models specifically designed for a single task or for a restricted class of stimuli. For theoretical models, we refer the reader to a number of previous reviews that address attention theories and models more generally \\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07867",
        "title": "Some like it hot - visual guidance for preference prediction",
        "authors": [
            "Rasmus Rothe",
            "Radu Timofte",
            "Luc Van Gool"
        ],
        "abstract": "For people first impressions of someone are of determining importance. They are hard to alter through further information. This begs the question if a computer can reach the same judgement. Earlier research has already pointed out that age, gender, and average attractiveness can be estimated with reasonable precision. We improve the state-of-the-art, but also predict - based on someone's known preferences - how much that particular person is attracted to a novel face. Our computational pipeline comprises a face detector, convolutional neural networks for the extraction of deep features, standard support vector regression for gender, age and facial beauty, and - as the main novelties - visual regularized collaborative filtering to infer inter-person preferences as well as a novel regression technique for handling visual queries without rating history. We validate the method using a very large dataset from a dating site as well as images from celebrities. Our experiments yield convincing results, i.e. we predict 76% of the ratings correctly solely based on an image, and reveal some sociologically relevant conclusions. We also validate our collaborative filtering solution on the standard MovieLens rating dataset, augmented with movie posters, to predict an individual's movie rating. We demonstrate our algorithms on ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2016-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07905",
        "title": "Defect Detection Techniques for Airbag Production Sewing Stages",
        "authors": [
            "Raluca Brad",
            "Lavinia Barac",
            "Remus Brad"
        ],
        "abstract": "Airbags are subject to strict quality control in order to ensure passengers safety. The quality of fabric and sewing thread influence the final product and therefore, sewing defects must be early and accurately detected, in order to remove the item from production. Airbag seams assembly can take various forms, using linear and circle primitives, with threads of different colors and length densities, creating lockstitch or double threads chainstitch. The paper presents a framework for the automatic detection of defects occurring during the airbag sewing stage. Types of defects as skipped stitch, missed stitch or superimposed seam for lockstitch and two threads chainstitch are detected and marked. Using image processing methods, the proposed framework follows the seams path and determines if a color pattern of the considered stitches is valid.\n    ",
        "submission_date": "2015-10-25T00:00:00",
        "last_modified_date": "2015-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07945",
        "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking",
        "authors": [
            "Hyeonseob Nam",
            "Bohyung Han"
        ],
        "abstract": "We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify the target in each domain. We train the network with respect to each domain iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance compared with state-of-the-art methods in existing tracking benchmarks.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08012",
        "title": "ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion",
        "authors": [
            "Guofeng Zhang",
            "Haomin Liu",
            "Zilong Dong",
            "Jiaya Jia",
            "Tien-Tsin Wong",
            "Hujun Bao"
        ],
        "abstract": "Structure-from-motion (SfM) largely relies on feature tracking. In image sequences, if disjointed tracks caused by objects moving in and out of the field of view, occasional occlusion, or image noise, are not handled well, corresponding SfM could be affected. This problem becomes severer for large-scale scenes, which typically requires to capture multiple sequences to cover the whole scene. In this paper, we propose an efficient non-consecutive feature tracking (ENFT) framework to match interrupted tracks distributed in different subsequences or even in different videos. Our framework consists of steps of solving the feature `dropout' problem when indistinctive structures, noise or large image distortion exists, and of rapidly recognizing and joining common features located in different subsequences. In addition, we contribute an effective segment-based coarse-to-fine SfM algorithm for robustly handling large datasets. Experimental results on challenging video data demonstrate the effectiveness of the proposed system.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08039",
        "title": "Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties",
        "authors": [
            "Georg Poier",
            "Konstantinos Roditakis",
            "Samuel Schulter",
            "Damien Michel",
            "Horst Bischof",
            "Antonis A. Argyros"
        ],
        "abstract": "Model-based approaches to 3D hand tracking have been shown to perform well in a wide range of scenarios. However, they require initialisation and cannot recover easily from tracking failures that occur due to fast hand motions. Data-driven approaches, on the other hand, can quickly deliver a solution, but the results often suffer from lower accuracy or missing anatomical validity compared to those obtained from model-based approaches. In this work we propose a hybrid approach for hand pose estimation from a single depth image. First, a learned regressor is employed to deliver multiple initial hypotheses for the 3D position of each hand joint. Subsequently, the kinematic parameters of a 3D hand model are found by deliberately exploiting the inherent uncertainty of the inferred joint proposals. This way, the method provides anatomically valid and accurate solutions without requiring manual initialisation or suffering from track losses. Quantitative results on several standard datasets demonstrate that the proposed method outperforms state-of-the-art representatives of the model-based, data-driven and hybrid paradigms.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08160",
        "title": "Scale-aware Fast R-CNN for Pedestrian Detection",
        "authors": [
            "Jianan Li",
            "Xiaodan Liang",
            "ShengMei Shen",
            "Tingfa Xu",
            "Jiashi Feng",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in sub-networks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2016-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08291",
        "title": "Linear Shape Deformation Models with Local Support Using Graph-based Structured Matrix Factorisation",
        "authors": [
            "Florian Bernard",
            "Peter Gemmar",
            "Frank Hertel",
            "Jorge Goncalves",
            "Johan Thunberg"
        ],
        "abstract": "Representing 3D shape deformations by linear models in high-dimensional space has many applications in computer vision and medical imaging, such as shape-based interpolation or segmentation. Commonly, using Principal Components Analysis a low-dimensional (affine) subspace of the high-dimensional shape space is determined. However, the resulting factors (the most dominant eigenvectors of the covariance matrix) have global support, i.e. changing the coefficient of a single factor deforms the entire shape. In this paper, a method to obtain deformation factors with local support is presented. The benefits of such models include better flexibility and interpretability as well as the possibility of interactively deforming shapes locally. For that, based on a well-grounded theoretical motivation, we formulate a matrix factorisation problem employing sparsity and graph-based regularisation terms. We demonstrate that for brain shapes our method outperforms the state of the art in local support models with respect to generalisation ability and sparse shape reconstruction, whereas for human body shapes our method gives more realistic deformations.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08470",
        "title": "Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera Arrays",
        "authors": [
            "Jason Holloway",
            "M. Salman Asif",
            "Manoj Kumar Sharma",
            "Nathan Matsuda",
            "Roarke Horstmeyer",
            "Oliver Cossairt",
            "Ashok Veeraraghavan"
        ],
        "abstract": "In this work, we propose using camera arrays coupled with coherent illumination as an effective method of improving spatial resolution in long distance images by a factor of ten and beyond. Recent advances in ptychography have demonstrated that one can image beyond the diffraction limit of the objective lens in a microscope. We demonstrate a similar imaging system to image beyond the diffraction limit in long range imaging. We emulate a camera array with a single camera attached to an X-Y translation stage. We show that an appropriate phase retrieval based reconstruction algorithm can be used to effectively recover the lost high resolution details from the multiple low resolution acquired images. We analyze the effects of noise, required degree of image overlap, and the effect of increasing synthetic aperture size on the reconstructed image quality. We show that coherent camera arrays have the potential to greatly improve imaging performance. Our simulations show resolution gains of 10x and more are achievable. Furthermore, experimental results from our proof-of-concept systems show resolution gains of 4x-7x for real scenes. Finally, we introduce and analyze in simulation a new strategy to capture macroscopic Fourier Ptychography images in a single snapshot, albeit using a camera array.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08583",
        "title": "Privacy Prediction of Images Shared on Social Media Sites Using Deep Features",
        "authors": [
            "Ashwini Tonge",
            "Cornelia Caragea"
        ],
        "abstract": "Online image sharing in social media sites such as Facebook, Flickr, and Instagram can lead to unwanted disclosure and privacy violations, when privacy settings are used inappropriately. With the exponential increase in the number of images that are shared online every day, the development of effective and efficient prediction methods for image privacy settings are highly needed. The performance of models critically depends on the choice of the feature representation. In this paper, we present an approach to image privacy prediction that uses deep features and deep image tags as feature representations. Specifically, we explore deep features at various neural network layers and use the top layer (probability) as an auto-annotation mechanism. The results of our experiments show that models trained on the proposed deep features and deep image tags substantially outperform baselines such as those based on SIFT and GIST as well as those that use \"bag of tags\" as features.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08893",
        "title": "A Deep Siamese Network for Scene Detection in Broadcast Videos",
        "authors": [
            "Lorenzo Baraldi",
            "Costantino Grana",
            "Rita Cucchiara"
        ],
        "abstract": "We present a model that automatically divides broadcast videos into coherent scenes by learning a distance measure between shots. Experiments are performed to demonstrate the effectiveness of our approach by comparing our algorithm against recent proposals for automatic scene segmentation. We also propose an improved performance measure that aims to reduce the gap between numerical evaluation and expected results, and propose and release a new benchmark dataset.\n    ",
        "submission_date": "2015-10-29T00:00:00",
        "last_modified_date": "2015-10-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08971",
        "title": "Robust Subspace Clustering via Tighter Rank Approximation",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Qiang Cheng"
        ],
        "abstract": "Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctangent function is used as a tighter approximation to the rank function. We use it on the challenging subspace clustering problem. For this nonconvex minimization problem, we develop an effective optimization procedure based on a type of augmented Lagrange multipliers (ALM) method. Extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08973",
        "title": "VISALOGY: Answering Visual Analogy Questions",
        "authors": [
            "Fereshteh Sadeghi",
            "C. Lawrence Zitnick",
            "Ali Farhadi"
        ],
        "abstract": "In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture. We introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09041",
        "title": "Postprocessing of Compressed Images via Sequential Denoising",
        "authors": [
            "Yehuda Dar",
            "Alfred M. Bruckstein",
            "Michael Elad",
            "Raja Giryes"
        ],
        "abstract": "In this work we propose a novel postprocessing technique for compression-artifact reduction. Our approach is based on posing this task as an inverse problem, with a regularization that leverages on existing state-of-the-art image denoising algorithms. We rely on the recently proposed Plug-and-Play Prior framework, suggesting the solution of general inverse problems via Alternating Direction Method of Multipliers (ADMM), leading to a sequence of Gaussian denoising steps. A key feature in our scheme is a linearization of the compression-decompression process, so as to get a formulation that can be optimized. In addition, we supply a thorough analysis of this linear approximation for several basic compression procedures. The proposed method is suitable for diverse compression techniques that rely on transform coding. Specifically, we demonstrate impressive gains in image quality for several leading compression methods - JPEG, JPEG2000, and HEVC.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09083",
        "title": "Deep Recurrent Regression for Facial Landmark Detection",
        "authors": [
            "Hanjiang Lai",
            "Shengtao Xiao",
            "Yan Pan",
            "Zhen Cui",
            "Jiashi Feng",
            "Chunyan Xu",
            "Jian Yin",
            "Shuicheng Yan"
        ],
        "abstract": "We propose a novel end-to-end deep architecture for face landmark detection, based on a deep convolutional and deconvolutional network followed by carefully designed recurrent network structures. The pipeline of this architecture consists of three parts. Through the first part, we encode an input face image to resolution-preserved deconvolutional feature maps via a deep network with stacked convolutional and deconvolutional layers. Then, in the second part, we estimate the initial coordinates of the facial key points by an additional convolutional layer on top of these deconvolutional feature maps. In the last part, by using the deconvolutional feature maps and the initial facial key points as input, we refine the coordinates of the facial key points by a recurrent network that consists of multiple Long-Short Term Memory (LSTM) components. Extensive evaluations on several benchmark datasets show that the proposed deep architecture has superior performance against the state-of-the-art methods.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2016-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09184",
        "title": "Estimating Target Signatures with Diverse Density",
        "authors": [
            "Taylor Glenn",
            "Alina Zare"
        ],
        "abstract": "Hyperspectral target detection algorithms rely on knowing the desired target signature in advance. However, obtaining an effective target signature can be difficult; signatures obtained from laboratory measurements or hand-spectrometers in the field may not transfer to airborne imagery effectively. One approach to dealing with this difficulty is to learn an effective target signature from training data. An approach for learning target signatures from training data is presented. The proposed approach addresses uncertainty and imprecision in groundtruth in the training data using a multiple instance learning, diverse density (DD) based objective function. After learning the target signature given data with uncertain and imprecise groundtruth, target detection can be applied on test data. Results are shown on simulated and real data.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00096",
        "title": "Bioinspired Visual Motion Estimation",
        "authors": [
            "Garrick Orchard",
            "Ralph Etienne-Cummings"
        ],
        "abstract": "Visual motion estimation is a computationally intensive, but important task for sighted animals. Replicating the robustness and efficiency of biological visual motion estimation in artificial systems would significantly enhance the capabilities of future robotic agents. 25 years ago, in this very journal, Carver Mead outlined his argument for replicating biological processing in silicon circuits. His vision served as the foundation for the field of neuromorphic engineering, which has experienced a rapid growth in interest over recent years as the ideas and technologies mature. Replicating biological visual sensing was one of the first tasks attempted in the neuromorphic field. In this paper we focus specifically on the task of visual motion estimation. We describe the task itself, present the progression of works from the early first attempts through to the modern day state-of-the-art, and provide an outlook for future directions in the field.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00098",
        "title": "Semantic Cross-View Matching",
        "authors": [
            "Francesco Castaldo",
            "Amir Zamir",
            "Roland Angst",
            "Francesco Palmieri",
            "Silvio Savarese"
        ],
        "abstract": "Matching cross-view images is challenging because the appearance and viewpoints are significantly different. While low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint, semantic information of images however shows an invariant characteristic in this respect. Consequently, semantically labeled regions can be used for performing cross-view matching. In this paper, we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an RGB image with the goal of performing cross-view matching with a (non-RGB) geographic information system (GIS). A segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to robustly capture both, the presence of semantic concepts and the spatial layout of those segments. Pairwise distances between the descriptors extracted from the GIS map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout. An experimental evaluation with challenging query images and a large urban area shows promising results.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00099",
        "title": "Sketch-based Image Retrieval from Millions of Images under Rotation, Translation and Scale Variations",
        "authors": [
            "Sarthak Parui",
            "Anurag Mittal"
        ],
        "abstract": "Proliferation of touch-based devices has made sketch-based image retrieval practical. While many methods exist for sketch-based object detection/image retrieval on small datasets, relatively less work has been done on large (web)-scale image retrieval. In this paper, we present an efficient approach for image retrieval from millions of images based on user-drawn sketches. Unlike existing methods for this problem which are sensitive to even translation or scale variations, our method handles rotation, translation, scale (i.e. a similarity transformation) and small deformations. The object boundaries are represented as chains of connected segments and the database images are pre-processed to obtain such chains that have a high chance of containing the object. This is accomplished using two approaches in this work: a) extracting long chains in contour segment networks and b) extracting boundaries of segmented object proposals. These chains are then represented by similarity-invariant variable length descriptors. Descriptor similarities are computed by a fast Dynamic Programming-based partial matching algorithm. This matching mechanism is used to generate a hierarchical k-medoids based indexing structure for the extracted chains of all database images in an offline process which is used to efficiently retrieve a small set of possible matched images for query chains. Finally, a geometric verification step is employed to test geometric consistency of multiple chain matches to improve results. Qualitative and quantitative results clearly demonstrate superiority of the approach over existing methods.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00100",
        "title": "Fast Neuromimetic Object Recognition using FPGA Outperforms GPU Implementations",
        "authors": [
            "Garrick Orchard",
            "Jacob G. Martin",
            "R. Jacob Vogelstein",
            "Ralph Etienne-Cummings"
        ],
        "abstract": "Recognition of objects in still images has traditionally been regarded as a difficult computational problem. Although modern automated methods for visual object recognition have achieved steadily increasing recognition accuracy, even the most advanced computational vision approaches are unable to obtain performance equal to that of humans. This has led to the creation of many biologically-inspired models of visual object recognition, among them the HMAX model. HMAX is traditionally known to achieve high accuracy in visual object recognition tasks at the expense of significant computational complexity. Increasing complexity, in turn, increases computation time, reducing the number of images that can be processed per unit time. In this paper we describe how the computationally intensive, biologically inspired HMAX model for visual object recognition can be modified for implementation on a commercial Field Programmable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluation board with XC6VLX240T FPGA. We show that with minor modifications to the traditional HMAX model we can perform recognition on images of size 128x128 pixels at a rate of 190 images per second with a less than 1% loss in recognition accuracy in both binary and multi-class visual object recognition tasks.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00111",
        "title": "Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation",
        "authors": [
            "M. Abdelsamea"
        ],
        "abstract": "Image segmentation is the problem of partitioning an image into different subsets, where each subset may have a different characterization in terms of color, intensity, texture, and/or other features. Segmentation is a fundamental component of image processing, and plays a significant role in computer vision, object recognition, and object tracking. Active Contour Models (ACMs) constitute a powerful energy-based minimization framework for image segmentation, which relies on the concept of contour evolution. Starting from an initial guess, the contour is evolved with the aim of approximating better and better the actual object boundary. Handling complex images in an efficient, effective, and robust way is a real challenge, especially in the presence of intensity inhomogeneity, overlap between the foreground/background intensity distributions, objects characterized by many different intensities, and/or additive noise. In this thesis, to deal with these challenges, we propose a number of image segmentation models relying on variational level set methods and specific kinds of neural networks, to handle complex images in both supervised and unsupervised ways. Experimental results demonstrate the high accuracy of the segmentation results, obtained by the proposed models on various benchmark synthetic and real images compared with state-of-the-art active contour models.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-10-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00175",
        "title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters",
        "authors": [
            "Forrest N. Iandola",
            "Khalid Ashraf",
            "Matthew W. Moskewicz",
            "Kurt Keutzer"
        ],
        "abstract": "Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs.\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00423",
        "title": "Towards Reading Hidden Emotions: A comparative Study of Spontaneous Micro-expression Spotting and Recognition Methods",
        "authors": [
            "Xiaobai Li",
            "Xiaopeng Hong",
            "Antti Moilanen",
            "Xiaohua Huang",
            "Tomas Pfister",
            "Guoying Zhao",
            "Matti Pietik\u00e4inen"
        ],
        "abstract": "Micro-expressions (MEs) are rapid, involuntary facial expressions which reveal emotions that people do not intend to show. Studying MEs is valuable as recognizing them has many important applications, particularly in forensic science and psychotherapy. However, analyzing spontaneous MEs is very challenging due to their short duration and low intensity. Automatic ME analysis includes two tasks: ME spotting and ME recognition. For ME spotting, previous studies have focused on posed rather than spontaneous videos. For ME recognition, the performance of previous studies is low. To address these challenges, we make the following contributions: (i)We propose the first method for spotting spontaneous MEs in long videos (by exploiting feature difference contrast). This method is training free and works on arbitrary unseen videos. (ii)We present an advanced ME recognition framework, which outperforms previous work by a large margin on two challenging spontaneous ME databases (SMIC and CASMEII). (iii)We propose the first automatic ME analysis system (MESR), which can spot and recognize MEs from spontaneous video data. Finally, we show our method outperforms humans in the ME recognition task by a large margin, and achieves comparable performance to humans at the very challenging task of spotting and then recognizing spontaneous MEs.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2017-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00438",
        "title": "Semantic Summarization of Egocentric Photo Stream Events",
        "authors": [
            "Aniol Lidon",
            "Marc Bola\u00f1os",
            "Mariella Dimiccoli",
            "Petia Radeva",
            "Maite Garolera",
            "Xavier Gir\u00f3-i-Nieto"
        ],
        "abstract": "With the rapid increase of users of wearable cameras in recent years and of the amount of data they produce, there is a strong need for automatic retrieval and summarization techniques. This work addresses the problem of automatically summarizing egocentric photo streams captured through a wearable camera by taking an image retrieval perspective. After removing non-informative images by a new CNN-based filter, images are ranked by relevance to ensure semantic diversity and finally re-ranked by a novelty criterion to reduce redundancy. To assess the results, a new evaluation metric is proposed which takes into account the non-uniqueness of the solution. Experimental results applied on a database of 7,110 images from 6 different subjects and evaluated by experts gave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of 5.0. Source code is available at ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2017-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00461",
        "title": "Circle detection using isosceles triangles sampling",
        "authors": [
            "Hanqing Zhang",
            "Krister Wiklund",
            "Magnus Andersson"
        ],
        "abstract": "Detection of circular objects in digital images is an important problem in several vision applications. Circle detection using randomized sampling has been developed in recent years to reduce the computational intensity. Randomized sampling, however, is sensitive to noise that can lead to reduced accuracy and false-positive candidates. This paper presents a new circle detection method based upon randomized isosceles triangles sampling to improve the robustness of randomized circle detection in noisy conditions. It is shown that the geometrical property of isosceles triangles provide a robust criterion to find relevant edge pixels and thereby efficiently provide an estimation of the circle center and radii. The estimated results given by the isosceles triangles sampling from each connected component of edge map were analyzed using a simple clustering approach for efficiency. To further improve on the accuracy we applied a two-step refinement process using chords and linear error compensation with gradient information of the edge pixels. Extensive experiments using both synthetic and real images were presented and results were compared to leading state-of-the-art algorithms and showed that the proposed algorithm: are efficient in finding circles with a low number of iterations; has high rejection rate of false-positive circle candidates; and has high robustness against noise, making it adaptive and useful in many vision applications.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00472",
        "title": "Water Detection through Spatio-Temporal Invariant Descriptors",
        "authors": [
            "Pascal Mettes",
            "Robby T. Tan",
            "Remco C. Veltkamp"
        ],
        "abstract": "In this work, we aim to segment and detect water in videos. Water detection is beneficial for appllications such as video search, outdoor surveillance, and systems such as unmanned ground vehicles and unmanned aerial vehicles. The specific problem, however, is less discussed compared to general texture recognition. Here, we analyze several motion properties of water. First, we describe a video pre-processing step, to increase invariance against water reflections and water colours. Second, we investigate the temporal and spatial properties of water and derive corresponding local descriptors. The descriptors are used to locally classify the presence of water and a binary water detection mask is generated through spatio-temporal Markov Random Field regularization of the local classifications. Third, we introduce the Video Water Database, containing several hours of water and non-water videos, to validate our algorithm. Experimental evaluation on the Video Water Database and the DynTex database indicates the effectiveness of the proposed algorithm, outperforming multiple algorithms for dynamic texture recognition and material recognition by ca. 5% and 15% respectively.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00513",
        "title": "Pixel-wise Segmentation of Street with Neural Networks",
        "authors": [
            "Sebastian Bittel",
            "Vitali Kaiser",
            "Marvin Teichmann",
            "Martin Thoma"
        ],
        "abstract": "Pixel-wise street segmentation of photographs taken from a drivers perspective is important for self-driving cars and can also support other object recognition tasks. A framework called SST was developed to examine the accuracy and execution time of different neural networks. The best neural network achieved an $F_1$-score of 89.5% with a simple feedforward neural network which trained to solve a regression task.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2015-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00561",
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
        "authors": [
            "Vijay Badrinarayanan",
            "Alex Kendall",
            "Roberto Cipolla"
        ],
        "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.\n",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00871",
        "title": "Properties of the Sample Mean in Graph Spaces and the Majorize-Minimize-Mean Algorithm",
        "authors": [
            "Brijnesh J. Jain"
        ],
        "abstract": "One of the most fundamental concepts in statistics is the concept of sample mean. Properties of the sample mean that are well-defined in Euclidean spaces become unwieldy or even unclear in graph spaces. Open problems related to the sample mean of graphs include: non-existence, non-uniqueness, statistical inconsistency, lack of convergence results of mean algorithms, non-existence of midpoints, and disparity to midpoints. We present conditions to resolve all six problems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on graph datasets representing images and molecules show that the MMM-Algorithm best approximates a sample mean of graphs compared to six other mean algorithms.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01064",
        "title": "Color Space Transformation Network",
        "authors": [
            "Alexandros Karargyris"
        ],
        "abstract": "Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict knowledge in a very easy and efficient way. Convolutional neural networks and auto-encoders have become the normal in the area of imaging and computer vision achieving unprecedented accuracy levels in many applications. The most common strategy is to build and train networks with many layers by tuning their hyper-parameters. While this approach has proven to be a successful way to build robust deep learning schemes it suffers from high complexity. In this paper we introduce a module that learns color space transformations within a network. Given a large dataset of colored images the color space transformation module tries to learn color space transformations that increase overall classification accuracy. This module has shown to increase overall accuracy for the same network design and to achieve faster convergence. It is part of a broader family of image transformations (e.g. spatial transformer network).\n    ",
        "submission_date": "2015-10-31T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01154",
        "title": "Robust Registration of Calcium Images by Learned Contrast Synthesis",
        "authors": [
            "John A. Bogovic",
            "Philipp Hanslovsky",
            "Allan Wong",
            "Stephan Saalfeld"
        ],
        "abstract": "Multi-modal image registration is a challenging task that is vital to fuse complementary signals for subsequent analyses. Despite much research into cost functions addressing this challenge, there exist cases in which these are ineffective. In this work, we show that (1) this is true for the registration of in-vivo Drosophila brain volumes visualizing genetically encoded calcium indicators to an nc82 atlas and (2) that machine learning based contrast synthesis can yield improvements. More specifically, the number of subjects for which the registration outright failed was greatly reduced (from 40% to 15%) by using a synthesized image.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01156",
        "title": "Robust Large-Scale Localization in 3D Point Clouds Revisited",
        "authors": [
            "Fabian Tschopp",
            "Marco Zorzi"
        ],
        "abstract": "We tackle the problem of getting a full 6-DOF pose estimation of a query image inside a given point cloud. This technical report re-evaluates the algorithms proposed by Y. Li et al. \"Worldwide Pose Estimation using 3D Point Cloud\". Our code computes poses from 3 or 4 points, with both known and unknown focal length. The results can easily be displayed and analyzed with Meshlab. We found both advantages and shortcomings of the methods proposed. Furthermore, additional priors and parameters for point selection, RANSAC and pose quality estimate (inlier test) are proposed and applied.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01161",
        "title": "Image-Based Correction of Continuous and Discontinuous Non-Planar Axial Distortion in Serial Section Microscopy",
        "authors": [
            "Philipp Hanslovsky",
            "John A. Bogovic",
            "Stephan Saalfeld"
        ],
        "abstract": "Motivation: Serial section microscopy is an established method for detailed anatomy reconstruction of biological specimen. During the last decade, high resolution electron microscopy (EM) of serial sections has become the de-facto standard for reconstruction of neural connectivity at ever increasing scales (EM connectomics). In serial section microscopy, the axial dimension of the volume is sampled by physically removing thin sections from the embedded specimen and subsequently imaging either the block-face or the section series. This process has limited precision leading to inhomogeneous non-planar sampling of the axial dimension of the volume which, in turn, results in distorted image volumes. This includes that section series may be collected and imaged in unknown order.\n",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01168",
        "title": "Cell identification in whole-brain multiview images of neural activation",
        "authors": [
            "Marco Paciscopi",
            "Ludovico Silvestri",
            "Francesco Saverio Pavone",
            "Paolo Frasconi"
        ],
        "abstract": "We present a scalable method for brain cell identification in multiview confocal light sheet microscopy images. Our algorithmic pipeline includes a hierarchical registration approach and a novel multiview version of semantic deconvolution that simultaneously enhance visibility of fluorescent cell bodies, equalize their contrast, and fuses adjacent views into a single 3D images on which cell identification is performed with mean shift.\n",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01186",
        "title": "Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse Representation",
        "authors": [
            "Hongyu Yang",
            "Di Huang",
            "Yunhong Wang",
            "Heng Wang",
            "Yuanyan Tang"
        ],
        "abstract": "Face aging simulation has received rising investigations nowadays, whereas it still remains a challenge to generate convincing and natural age-progressed face images. In this paper, we present a novel approach to such an issue by using hidden factor analysis joint sparse representation. In contrast to the majority of tasks in the literature that handle the facial texture integrally, the proposed aging approach separately models the person-specific facial properties that tend to be stable in a relatively long period and the age-specific clues that change gradually over time. It then merely transforms the age component to a target age group via sparse reconstruction, yielding aging effects, which is finally combined with the identity component to achieve the aged face. Experiments are carried out on three aging databases, and the results achieved clearly demonstrate the effectiveness and robustness of the proposed method in rendering a face with aging effects. Additionally, a series of evaluations prove its validity with respect to identity preservation and aging effect generation.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01245",
        "title": "Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset",
        "authors": [
            "Thierry Bouwmans",
            "Andrews Sobral",
            "Sajid Javed",
            "Soon Ki Jung",
            "El-Hadi Zahzah"
        ],
        "abstract": "Recent research on problem formulations based on decomposition into low-rank plus sparse matrices shows a suitable framework to separate moving objects from the background. The most representative problem formulation is the Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit (PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix. However, similar robust implicit or explicit decompositions can be made in the following problem formulations: Robust Non-negative Matrix Factorization (RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust Subspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal of these similar problem formulations is to obtain explicitly or implicitly a decomposition into low-rank matrix plus additive matrices. In this context, this work aims to initiate a rigorous and comprehensive review of the similar problem formulations in robust subspace learning and tracking based on decomposition into low-rank plus additive matrices for testing and ranking existing algorithms for background/foreground separation. For this, we first provide a preliminary review of the recent developments in the different problem formulations which allows us to define a unified view that we called Decomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine carefully each method in each robust subspace learning/tracking frameworks with their decomposition, their loss functions, their optimization problem and their solvers. Furthermore, we investigate if incremental algorithms and real-time implementations can be achieved for background/foreground separation. Finally, experimental results on a large-scale dataset called Background Models Challenge (BMC 2012) show the comparative performance of 32 different robust subspace learning/tracking methods.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2016-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01293",
        "title": "Towards a tracking algorithm based on the clustering of spatio-temporal clouds of points",
        "authors": [
            "Andrea Cavagna",
            "Chiara Creato",
            "Lorenzo Del Castello",
            "Stefania Melillo",
            "Leonardo Parisi",
            "Massimiliano Viale"
        ],
        "abstract": "The interest in 3D dynamical tracking is growing in fields such as robotics, biology and fluid dynamics. Recently, a major source of progress in 3D tracking has been the study of collective behaviour in biological systems, where the trajectories of individual animals moving within large and dense groups need to be reconstructed to understand the behavioural interaction rules. Experimental data in this field are generally noisy and at low spatial resolution, so that individuals appear as small featureless objects and trajectories must be retrieved by making use of epipolar information only. Moreover, optical occlusions often occur: in a multi-camera system one or more objects become indistinguishable in one view, potentially jeopardizing the conservation of identity over long-time trajectories. The most advanced 3D tracking algorithms overcome optical occlusions making use of set-cover techniques, which however have to solve NP-hard optimization problems. Moreover, current methods are not able to cope with occlusions arising from actual physical proximity of objects in 3D space. Here, we present a new method designed to work directly in 3D space and time, creating (3D+1) clouds of points representing the full spatio-temporal evolution of the moving targets. We can then use a simple connected components labeling routine, which is linear in time, to solve optical occlusions, hence lowering from NP to P the complexity of the problem. Finally, we use normalized cut spectral clustering to tackle 3D physical proximity.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01508",
        "title": "Enhancing Feature Tracking With Gyro Regularization",
        "authors": [
            "Bryan Poling",
            "Gilad Lerman"
        ],
        "abstract": "We present a deeply integrated method of exploiting low-cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast, we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor-quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template-based tracking methods, and is in fact competitive with more complex and computationally expensive state-of-the-art trackers, but at a fraction of the computational cost. Additionally, we show that the practice of initializing template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using gyro-predicted optical flow offers no advantage over using a careful optical-only initialization method, suggesting that some deeper level of integration, like the method we propose, is needed in order to realize a genuine improvement in tracking performance from these inertial sensors.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01619",
        "title": "Coherent Motion Segmentation in Moving Camera Videos using Optical Flow Orientations",
        "authors": [
            "Manjunath Narayana",
            "Allen Hanson",
            "Erik Learned-Miller"
        ],
        "abstract": "In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01627",
        "title": "Background subtraction - separating the modeling and the inference",
        "authors": [
            "Manjunath Narayana",
            "Allen Hanson",
            "Erik Learned-Miller"
        ],
        "abstract": "In its early implementations, background modeling was a process of building a model for the background of a video with a stationary camera, and identifying pixels that did not conform well to this model. The pixels that were not well-described by the background model were assumed to be moving objects. Many systems today maintain models for the foreground as well as the background, and these models compete to explain the pixels in a video. In this paper, we argue that the logical endpoint of this evolution is to simply use Bayes' rule to classify pixels. In particular, it is essential to have a background likelihood, a foreground likelihood, and a prior at each pixel. A simple application of Bayes' rule then gives a posterior probability over the label. The only remaining question is the quality of the component models: the background likelihood, the foreground likelihood, and the prior. We describe a model for the likelihoods that is built by using not only the past observations at a given pixel location, but by also including observations in a spatial neighborhood around the location. This enables us to model the influence between neighboring pixels and is an improvement over earlier pixelwise models that do not allow for such influence. Although similar in spirit to the joint domain-range model, we show that our model overcomes certain deficiencies in that model. We use a spatially dependent prior for the background and foreground. The background and foreground labels from the previous frame, after spatial smoothing to account for movement of objects,are used to build the prior for the current frame.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01631",
        "title": "Background Modeling Using Adaptive Pixelwise Kernel Variances in a Hybrid Feature Space",
        "authors": [
            "Manjunath Narayana",
            "Allen Hanson",
            "Erik Learned-Miller"
        ],
        "abstract": "Recent work on background subtraction has shown developments on two major fronts. In one, there has been increasing sophistication of probabilistic models, from mixtures of Gaussians at each pixel [7], to kernel density estimates at each pixel [1], and more recently to joint domainrange density estimates that incorporate spatial information [6]. Another line of work has shown the benefits of increasingly complex feature representations, including the use of texture information, local binary patterns, and recently scale-invariant local ternary patterns [4]. In this work, we use joint domain-range based estimates for background and foreground scores and show that dynamically choosing kernel variances in our kernel estimates at each individual pixel can significantly improve results. We give a heuristic method for selectively applying the adaptive kernel calculations which is nearly as accurate as the full procedure but runs much faster. We combine these modeling improvements with recently developed complex features [4] and show significant improvements on a standard backgrounding benchmark.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01706",
        "title": "Image classification based on support vector machine and the fusion of complementary features",
        "authors": [
            "Huilin Gao",
            "Wenjie Chen",
            "Lihua Dou"
        ],
        "abstract": "Image Classification based on BOW (Bag-of-words) has broad application prospect in pattern recognition field but the shortcomings are existed because of single feature and low classification accuracy. To this end we combine three ingredients: (i) Three features with functions of mutual complementation are adopted to describe the images, including PHOW (Pyramid Histogram of Words), PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated Gradients). (ii) The improvement of traditional BOW model is presented by using dense sample and an improved K-means clustering method for constructing the visual dictionary. (iii) An adaptive feature-weight adjusted image categorization algorithm based on the SVM and the fusion of multiple features is adopted. Experiments carried out on Caltech 101 database confirm the validity of the proposed approach. From the experimental results can be seen that the classification accuracy rate of the proposed method is improved by 7%-17% higher than that of the traditional BOW methods. This algorithm makes full use of global, local and spatial information and has significant improvements to the classification accuracy.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01726",
        "title": "Multi-Target Tracking and Occlusion Handling with Learned Variational Bayesian Clusters and a Social Force Model",
        "authors": [
            "Ata-ur-Rehman",
            "Syed Mohsen Naqvi",
            "Lyudmila Mihaylova",
            "Jonathon Chambers"
        ],
        "abstract": "This paper considers the problem of multiple human target tracking in a sequence of video data. A solution is proposed which is able to deal with the challenges of a varying number of targets, interactions and when every target gives rise to multiple measurements. The developed novel algorithm comprises variational Bayesian clustering combined with a social force model, integrated within a particle filter with an enhanced prediction step. It performs measurement-to-target association by automatically detecting the measurement relevance. The performance of the developed algorithm is evaluated over several sequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, which demonstrates that the proposed algorithm successfully initializes and tracks a variable number of targets in the presence of complex occlusions. A comparison with state-of-the-art techniques due to Khan et al., Laet et al. and Czyz et al. shows improved tracking performance.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01804",
        "title": "Wood Species Recognition Based on SIFT Keypoint Histogram",
        "authors": [
            "Shuaiqi Hu",
            "Ke Li",
            "Xudong Bao"
        ],
        "abstract": "Traditionally, only experts who are equipped with professional knowledge and rich experience are able to recognize different species of wood. Applying image processing techniques for wood species recognition can not only reduce the expense to train qualified identifiers, but also increase the recognition accuracy. In this paper, a wood species recognition technique base on Scale Invariant Feature Transformation (SIFT) keypoint histogram is proposed. We use first the SIFT algorithm to extract keypoints from wood cross section images, and then k-means and k-means++ algorithms are used for clustering. Using the clustering results, an SIFT keypoints histogram is calculated for each wood image. Furthermore, several classification models, including Artificial Neural Networks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) are used to verify the performance of the method. Finally, through comparing with other prevalent wood recognition methods such as GLCM and LBP, results show that our scheme achieves higher accuracy.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01887",
        "title": "Radon-Nikodym approximation in application to image analysis",
        "authors": [
            "Vladislav Gennadievich Malyshkin"
        ],
        "abstract": "For an image pixel information can be converted to the moments of some basis $Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number of moments pixel information can be completely recovered, for insufficient number of moments only partial information can be recovered and the image reconstruction is, at best, of interpolatory type. Standard approach is to present interpolated value as a linear combination of basis functions, what is equivalent to least squares expansion. However, recent progress in numerical stability of moments estimation allows image information to be recovered from moments in a completely different manner, applying Radon-Nikodym type of expansion, what gives the result as a ratio of two quadratic forms of basis functions. In contrast with least squares the Radon-Nikodym approach has oscillation near the boundaries very much suppressed and does not diverge outside of basis support. While least squares theory operate with vectors $<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what make the approach much more suitable to image transforms and statistical property estimation.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01954",
        "title": "Recovering hard-to-find object instances by sampling context-based object proposals",
        "authors": [
            "Jose Oramas M.",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper we focus on improving object detection performance in terms of recall. We propose a post-detection stage during which we explore the image with the objective of recovering missed detections. This exploration is performed by sampling object proposals in the image. We analyze four different strategies to perform this sampling, giving special attention to strategies that exploit spatial relations between objects. In addition, we propose a novel method to discover higher-order relations between groups of objects. Experiments on the challenging KITTI dataset show that our proposed relations-based proposal generation strategies can help improving recall at the cost of a relatively low amount of object proposals.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2016-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01966",
        "title": "Enhanced Low-Rank Matrix Approximation",
        "authors": [
            "Ankit Parekh",
            "Ivan W. Selesnick"
        ],
        "abstract": "This letter proposes to estimate low-rank matrices by formulating a convex optimization problem with non-convex regularization. We employ parameterized non-convex penalty functions to estimate the non-zero singular values more accurately than the nuclear norm. A closed-form solution for the global optimum of the proposed objective function (sum of data fidelity and the non-convex regularizer) is also derived. The solution reduces to singular value thresholding method as a special case. The proposed method is demonstrated for image denoising.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01994",
        "title": "Next Generation Multicuts for Semi-Planar Graphs",
        "authors": [
            "Julian Yarkony"
        ],
        "abstract": "We study the problem of multicut segmentation. We introduce modified versions of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work to natural image segmentation.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02023",
        "title": "Facial Expression Recognition Using Sparse Gaussian Conditional Random Field",
        "authors": [
            "Mohammadamin Abbasnejad",
            "Mohammad Ali Masnadi-Shirazi"
        ],
        "abstract": "The analysis of expression and facial Action Units (AUs) detection are very important tasks in fields of computer vision and Human Computer Interaction (HCI) due to the wide range of applications in human life. Many works has been done during the past few years which has their own advantages and disadvantages. In this work we present a new model based on Gaussian Conditional Random Field. We solve our objective problem using ADMM and we show how well the proposed model works. We train and test our work on two facial expression datasets, CK+ and RU-FACS. Experimental evaluation shows that our proposed approach outperform state of the art expression recognition.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02126",
        "title": "Pooling the Convolutional Layers in Deep ConvNets for Action Recognition",
        "authors": [
            "Shichao Zhao",
            "Yanbin Liu",
            "Yahong Han",
            "Richang Hong"
        ],
        "abstract": "Deep ConvNets have shown its good performance in image classification tasks. However it still remains as a problem in deep video representation for action recognition. The problem comes from two aspects: on one hand, current video ConvNets are relatively shallow compared with image ConvNets, which limits its capability of capturing the complex video action information; on the other hand, temporal information of videos is not properly utilized to pool and encode the video sequences. Towards these issues, in this paper, we utilize two state-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and the temporal net from Two-Stream ConvNets, for action representation. The convolutional layers and the proposed new layer, called frame-diff layer, are extracted and pooled with two temporal pooling strategy: Trajectory pooling and line pooling. The pooled local descriptors are then encoded with VLAD to form the video representations. In order to verify the effectiveness of the proposed framework, we conduct experiments on UCF101 and HMDB51 datasets. It achieves the accuracy of 93.78\\% on UCF101 which is the state-of-the-art and the accuracy of 65.62\\% on HMDB51 which is comparable to the state-of-the-art.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02228",
        "title": "Seven ways to improve example-based single image super resolution",
        "authors": [
            "Radu Timofte",
            "Rasmus Rothe",
            "Luc Van Gool"
        ],
        "abstract": "In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning. We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02251",
        "title": "Learning Visual Features from Large Weakly Supervised Data",
        "authors": [
            "Armand Joulin",
            "Laurens van der Maaten",
            "Allan Jabri",
            "Nicolas Vasilache"
        ],
        "abstract": "Convolutional networks trained on large supervised dataset produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and captions, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity, and learn correspondences between different languages.\n    ",
        "submission_date": "2015-11-06T00:00:00",
        "last_modified_date": "2015-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02282",
        "title": "Fingertip in the Eye: A cascaded CNN pipeline for the real-time fingertip detection in egocentric videos",
        "authors": [
            "Xiaorui Liu",
            "Yichao Huang",
            "Xin Zhang",
            "Lianwen Jin"
        ],
        "abstract": "We introduce a new pipeline for hand localization and fingertip detection. For RGB images captured from an egocentric vision mobile camera, hand and fingertip detection remains a challenging problem due to factors like background complexity and hand shape variety. To address these issues accurately and robustly, we build a large scale dataset named Ego-Fingertip and propose a bi-level cascaded pipeline of convolutional neural networks, namely, Attention-based Hand Detector as well as Multi-point Fingertip Detector. The proposed method significantly tackles challenges and achieves satisfactorily accurate prediction and real-time performance compared to previous hand and fingertip detection methods.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02283",
        "title": "Generation and Comprehension of Unambiguous Object Descriptions",
        "authors": [
            "Junhua Mao",
            "Jonathan Huang",
            "Alexander Toshev",
            "Oana Camburu",
            "Alan Yuille",
            "Kevin Murphy"
        ],
        "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02300",
        "title": "Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images",
        "authors": [
            "Shuran Song",
            "Jianxiong Xiao"
        ],
        "abstract": "We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200x faster than the original Sliding Shapes. All source code and pre-trained models will be available at GitHub.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02319",
        "title": "Review of Person Re-identification Techniques",
        "authors": [
            "Mohammad Ali Saghafi",
            "Aini Hussain",
            "Halimah Badioze Zaman",
            "Mohamad Hanif Md Saad"
        ],
        "abstract": "Person re-identification across different surveillance cameras with disjoint fields of view has become one of the most interesting and challenging subjects in the area of intelligent video surveillance. Although several methods have been developed and proposed, certain limitations and unresolved issues remain. In all of the existing re-identification approaches, feature vectors are extracted from segmented still images or video frames. Different similarity or dissimilarity measures have been applied to these vectors. Some methods have used simple constant metrics, whereas others have utilised models to obtain optimised metrics. Some have created models based on local colour or texture information, and others have built models based on the gait of people. In general, the main objective of all these approaches is to achieve a higher-accuracy rate and lowercomputational costs. This study summarises several developments in recent literature and discusses the various available methods used in person re-identification. Specifically, their advantages and disadvantages are mentioned and compared.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02407",
        "title": "A Survey of the Trends in Facial and Expression Recognition Databases and Methods",
        "authors": [
            "Sohini Roychowdhury",
            "Michelle Emmons"
        ],
        "abstract": "Automated facial identification and facial expression recognition have been topics of active research over the past few decades. Facial and expression recognition find applications in human-computer interfaces, subject tracking, real-time security surveillance systems and social networking. Several holistic and geometric methods have been developed to identify faces and expressions using public and local facial image databases. In this work we present the evolution in facial image data sets and the methodologies for facial identification and recognition of expressions such as anger, sadness, happiness, disgust, fear and surprise. We observe that most of the earlier methods for facial and expression recognition aimed at improving the recognition rates for facial feature-based methods using static images. However, the recent methodologies have shifted focus towards robust implementation of facial/expression recognition from large image databases that vary with space (gathered from the internet) and time (video recordings). The evolution trends in databases and methodologies for facial and expression recognition can be useful for assessing the next-generation topics that may have applications in security systems or personal identification systems that involve \"Quantitative face\" assessments.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02459",
        "title": "SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception",
        "authors": [
            "Duorui Xie",
            "Lingyu Liang",
            "Lianwen Jin",
            "Jie Xu",
            "Mengru Li"
        ],
        "abstract": "In this paper, a novel face dataset with attractiveness ratings, namely, the SCUT-FBP dataset, is developed for automatic facial beauty perception. This dataset provides a benchmark to evaluate the performance of different methods for facial attractiveness prediction, including the state-of-the-art deep learning method. The SCUT-FBP dataset contains face portraits of 500 Asian female subjects with attractiveness ratings, all of which have been verified in terms of rating distribution, standard deviation, consistency, and self-consistency. Benchmark evaluations for facial attractiveness prediction were performed with different combinations of facial geometrical features and texture features using classical statistical learning methods and the deep learning method. The best Pearson correlation (0.8187) was achieved by the CNN model. Thus, the results of our experiments indicate that the SCUT-FBP dataset provides a reliable benchmark for facial beauty perception.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02462",
        "title": "LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks",
        "authors": [
            "Steven C.H. Hoi",
            "Xiongwei Wu",
            "Hantang Liu",
            "Yue Wu",
            "Huiqiong Wang",
            "Hui Xue",
            "Qiang Wu"
        ],
        "abstract": "Logo detection from images has many applications, particularly for brand recognition and intellectual property protection. Most existing studies for logo recognition and detection are based on small-scale datasets which are not comprehensive enough when exploring emerging deep learning techniques. In this paper, we introduce \"LOGO-Net\", a large-scale logo image database for logo detection and brand recognition from real-world product images. To facilitate research, LOGO-Net has two datasets: (i)\"logos-18\" consists of 18 logo classes, 10 brands, and 16,043 logo objects, and (ii) \"logos-160\" consists of 160 logo classes, 100 brands, and 130,608 logo objects. We describe the ideas and challenges for constructing such a large-scale database. Another key contribution of this work is to apply emerging deep learning techniques for logo detection and brand recognition tasks, and conduct extensive experiments by exploring several state-of-the-art deep region-based convolutional networks techniques for object detection tasks. The LOGO-net will be released at ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02465",
        "title": "A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model",
        "authors": [
            "Jie Xu",
            "Lianwen Jin",
            "Lingyu Liang",
            "Ziyong Feng",
            "Duorui Xie"
        ],
        "abstract": "This paper proposes a deep leaning method to address the challenging facial attractiveness prediction problem. The method constructs a convolutional neural network of facial beauty prediction using a new deep cascaded fine-turning scheme with various face inputting channels, such as the original RGB face image, the detail layer image, and the lighting layer image. With a carefully designed CNN model of deep structure, large input size and small convolutional kernels, we have achieved a high prediction correlation of 0.88. This result convinces us that the problem of facial attractiveness prediction can be solved by deep learning approach, and it also shows the important roles of the facial smoothness, lightness, and color information that were involved in facial beauty perception, which is consistent with the result of recent psychology studies. Furthermore, we analyze the high-level features learnt by CNN through visualization of its hidden layers, and some interesting phenomena were observed. It is found that the contours and appearance of facial features, especially eyes and moth, are the most significant facial attributes for facial attractiveness prediction, which is also consistent with the visual perception intuition of human.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02492",
        "title": "VideoStory Embeddings Recognize Events when Examples are Scarce",
        "authors": [
            "Amirhossein Habibian",
            "Thomas Mensink",
            "Cees G.M. Snoek"
        ],
        "abstract": "This paper aims for event recognition when video examples are scarce or even completely absent. The key in such a challenging setting is a semantic video representation. Rather than building the representation from individual attribute detectors and their annotations, we propose to learn the entire representation from freely available web videos and their descriptions using an embedding between video features and term vectors. In our proposed embedding, which we call VideoStory, the correlations between the terms are utilized to learn a more effective representation by optimizing a joint objective balancing descriptiveness and ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02500",
        "title": "Poisson Inverse Problems by the Plug-and-Play scheme",
        "authors": [
            "Arie Rond",
            "Raja Giryes",
            "Michael Elad"
        ],
        "abstract": "The Anscombe transform offers an approximate conversion of a Poisson random variable into a Gaussian one. This transform is important and appealing, as it is easy to compute, and becomes handy in various inverse problems with Poisson noise contamination. Solution to such problems can be done by first applying the Anscombe transform, then applying a Gaussian-noise-oriented restoration algorithm of choice, and finally applying an inverse Anscombe transform. The appeal in this approach is due to the abundance of high-performance restoration algorithms designed for white additive Gaussian noise (we will refer to these hereafter as \"Gaussian-solvers\"). This process is known to work well for high SNR images, where the Anscombe transform provides a rather accurate approximation. When the noise level is high, the above path loses much of its effectiveness, and the common practice is to replace it with a direct treatment of the Poisson distribution. Naturally, with this we lose the ability to leverage on vastly available Gaussian-solvers.\n",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2015-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02503",
        "title": "Bearing fault diagnosis based on spectrum images of vibration signals",
        "authors": [
            "Wei Li",
            "Mingquan Qiu",
            "Zhencai Zhu",
            "Bo Wu",
            "Gongbo Zhou"
        ],
        "abstract": "Bearing fault diagnosis has been a challenge in the monitoring activities of rotating machinery, and it's receiving more and more attention. The conventional fault diagnosis methods usually extract features from the waveforms or spectrums of vibration signals in order to realize fault classification. In this paper, a novel feature in the form of images is presented, namely the spectrum images of vibration signals. The spectrum images are simply obtained by doing fast Fourier transformation. Such images are processed with two-dimensional principal component analysis (2DPCA) to reduce the dimensions, and then a minimum distance method is applied to classify the faults of bearings. The effectiveness of the proposed method is verified with experimental data.\n    ",
        "submission_date": "2015-11-08T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02570",
        "title": "Explicit Knowledge-based Reasoning for Visual Question Answering",
        "authors": [
            "Peng Wang",
            "Qi Wu",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Anthony Dick"
        ],
        "abstract": "We describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can provide an explanation of the reasoning by which it developed its answer. The method is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in the testing. We also provide a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual ques- tion answering.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02575",
        "title": "A Century of Portraits: A Visual Historical Record of American High School Yearbooks",
        "authors": [
            "Shiry Ginosar",
            "Kate Rakelly",
            "Sarah Sachs",
            "Brian Yin",
            "Crystal Lee",
            "Philipp Krahenbuhl",
            "Alexei A. Efros"
        ],
        "abstract": "Imagery offers a rich description of our world and communicates a volume and type of information that cannot be captured by text alone. Since the invention of the camera, an ever-increasing number of photographs document our \"visual culture\" complementing historical texts. But currently, this treasure trove of knowledge can only be analyzed manually by historians, and only at small scale. In this paper we perform automated analysis on a large-scale historical image dataset. Our main contributions are: 1) A publicly-available dataset of 168,055 (37,921 frontal-facing) American high school yearbook portraits. 2) Weakly-supervised data-driven techniques to discover historical visual trends in fashion and identify date-specific visual patterns. 3) A classifier to predict when a portrait was taken, with median error of 4 years for women and 6 for men. 4) A new method for discovering and displaying the visual elements used by the CNN-based date-prediction model to date portraits, finding that they correspond to the tell-tale fashions of each era. Project page can be found at: ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2019-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02583",
        "title": "Batch-normalized Maxout Network in Network",
        "authors": [
            "Jia-Ren Chang",
            "Yong-Sheng Chen"
        ],
        "abstract": "This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02667",
        "title": "An Efficient Multilinear Optimization Framework for Hypergraph Matching",
        "authors": [
            "Quynh Nguyen",
            "Francesco Tudisco",
            "Antoine Gautier",
            "Matthias Hein"
        ],
        "abstract": "Hypergraph matching has recently become a popular approach for solving correspondence problems in computer vision as it allows to integrate higher-order geometric information. Hypergraph matching can be formulated as a third-order optimization problem subject to the assignment constraints which turns out to be NP-hard. In recent work, we have proposed an algorithm for hypergraph matching which first lifts the third-order problem to a fourth-order problem and then solves the fourth-order problem via optimization of the corresponding multilinear form. This leads to a tensor block coordinate ascent scheme which has the guarantee of providing monotonic ascent in the original matching score function and leads to state-of-the-art performance both in terms of achieved matching score and accuracy. In this paper we show that the lifting step to a fourth-order problem can be avoided yielding a third-order scheme with the same guarantees and performance but being two times faster. Moreover, we introduce a homotopy type method which further improves the performance.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02674",
        "title": "Semantic Segmentation with Boundary Neural Fields",
        "authors": [
            "Gedas Bertasius",
            "Jianbo Shi",
            "Lorenzo Torresani"
        ],
        "abstract": "The state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs). However, FCNs use large receptive fields and many pooling layers, both of which cause blurring and low spatial resolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object boundaries. Prior work has attempted to address this issue in post-processing steps, for example using a color-based CRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difficult to tune and integrate into the original network architecture. Additionally, most CRFs use color-based pixel affinities, which are not well suited for semantic segmentation and lead to spatially disjoint predictions.\n",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02680",
        "title": "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding",
        "authors": [
            "Alex Kendall",
            "Vijay Badrinarayanan",
            "Roberto Cipolla"
        ],
        "abstract": "We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02682",
        "title": "Exploiting Egocentric Object Prior for 3D Saliency Detection",
        "authors": [
            "Gedas Bertasius",
            "Hyun Soo Park",
            "Jianbo Shi"
        ],
        "abstract": "On a minute-to-minute basis people undergo numerous fluid interactions with objects that barely register on a conscious level. Recent neuroscientific research demonstrates that humans have a fixed size prior for salient objects. This suggests that a salient object in 3D undergoes a consistent transformation such that people's visual system perceives it with an approximately fixed size. This finding indicates that there exists a consistent egocentric object prior that can be characterized by shape, size, depth, and location in the first person view.\n",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02683",
        "title": "A Light CNN for Deep Face Representation with Noisy Labels",
        "authors": [
            "Xiang Wu",
            "Ran He",
            "Zhenan Sun",
            "Tieniu Tan"
        ],
        "abstract": "The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2018-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02705",
        "title": "Biologically Inspired Dynamic Textures for Probing Motion Perception",
        "authors": [
            "Jonathan Vacher",
            "Andrew Meso",
            "Laurent U Perrinet",
            "Gabriel Peyr\u00e9"
        ],
        "abstract": "Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02799",
        "title": "Neural Module Networks",
        "authors": [
            "Jacob Andreas",
            "Marcus Rohrbach",
            "Trevor Darrell",
            "Dan Klein"
        ],
        "abstract": "Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2017-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02825",
        "title": "Multiple Instance Dictionary Learning using Functions of Multiple Instances",
        "authors": [
            "Changzhe Jiao",
            "Alina Zare"
        ],
        "abstract": "A multiple instance dictionary learning method using functions of multiple instances (DL-FUMI) is proposed to address target detection and two-class classification problems with inaccurate training labels. Given inaccurate training labels, DL-FUMI learns a set of target dictionary atoms that describe the most distinctive and representative features of the true positive class as well as a set of nontarget dictionary atoms that account for the shared information found in both the positive and negative instances. Experimental results show that the estimated target dictionary atoms found by DL-FUMI are more representative prototypes and identify better discriminative features of the true positive class than existing methods in the literature. DL-FUMI is shown to have significantly better performance on several target detection and classification problems as compared to other multiple instance learning (MIL) dictionary learning algorithms on a variety of MIL problems.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-08-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02841",
        "title": "Symmetries and control in generative neural nets",
        "authors": [
            "Galin Georgiev"
        ],
        "abstract": "We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers in specialized attention nets. In field-theoretical terms, these learned symmetry statistics form the gauge group of the data set. Plugging them in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class, ii) describe the low-dimensional manifold encoding the \"essence\" of the data, after superfluous attributes are factored out, and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE, with added spatial and color symmetry statistics, on the distorted MNIST and CIFAR10 datasets.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02853",
        "title": "Weakly Supervised Deep Detection Networks",
        "authors": [
            "Hakan Bilen",
            "Andrea Vedaldi"
        ],
        "abstract": "Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02872",
        "title": "Visual Language Modeling on CNN Image Representations",
        "authors": [
            "Hiroharu Kato",
            "Tatsuya Harada"
        ],
        "abstract": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02911",
        "title": "Spatially Coherent Random Forests",
        "authors": [
            "Tal Remez",
            "Shai Avidan"
        ],
        "abstract": "Spatially Coherent Random Forest (SCRF) extends Random Forest to create spatially coherent labeling. Each split function in SCRF is evaluated based on a traditional information gain measure that is regularized by a spatial coherency term. This way, SCRF is encouraged to choose split functions that cluster pixels both in appearance space and in image space. In particular, we use SCRF to detect contours in images, where contours are taken to be the boundaries between different regions. Each tree in the forest produces a segmentation of the image plane and the boundaries of the segmentations of all trees are aggregated to produce a final hierarchical contour map. We show that this modification improves the performance of regular Random Forest by about 10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can be used in other settings as well.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02916",
        "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders",
        "authors": [
            "Zhouhan Lin",
            "Yushi Chen",
            "Xing Zhao",
            "Gang Wang"
        ],
        "abstract": "Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02917",
        "title": "Detecting events and key actors in multi-person videos",
        "authors": [
            "Vignesh Ramanathan",
            "Jonathan Huang",
            "Sami Abu-El-Haija",
            "Alexander Gorban",
            "Kevin Murphy",
            "Li Fei-Fei"
        ],
        "abstract": "Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically \"attending\" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02919",
        "title": "Massive Online Crowdsourced Study of Subjective and Objective Picture Quality",
        "authors": [
            "Deepti Ghadiyaram",
            "Alan C. Bovik"
        ],
        "abstract": "Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Towards overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment subjective study. Our database consists of over 350000 opinion scores on 1162 images evaluated by over 7000 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective dataset. We also evaluate several top-performing blind Image Quality Assessment algorithms on it and present insights on how mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02928",
        "title": "Hyperspectral Image Recovery via Hybrid Regularization",
        "authors": [
            "Reza Arablouei",
            "Frank de Hoog"
        ],
        "abstract": "Natural images tend to mostly consist of smooth regions with individual pixels having highly correlated spectra. This information can be exploited to recover hyperspectral images of natural scenes from their incomplete and noisy measurements. To perform the recovery while taking full advantage of the prior knowledge, we formulate a composite cost function containing a square-error data-fitting term and two distinct regularization terms pertaining to spatial and spectral domains. The regularization for the spatial domain is the sum of total-variation of the image frames corresponding to all spectral bands. The regularization for the spectral domain is the l1-norm of the coefficient matrix obtained by applying a suitable sparsifying transform to the spectra of the pixels. We use an accelerated proximal-subgradient method to minimize the formulated cost function. We analyze the performance of the proposed algorithm and prove its convergence. Numerical simulations using real hyperspectral images exhibit that the proposed algorithm offers an excellent recovery performance with a number of measurements that is only a small fraction of the hyperspectral image data size. Simulation results also show that the proposed algorithm significantly outperforms an accelerated proximal-gradient algorithm that solves the classical basis-pursuit denoising problem to recover the hyperspectral image.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02992",
        "title": "Traffic Sign Classification Using Deep Inception Based Convolutional Networks",
        "authors": [
            "Mrinal Haloi"
        ],
        "abstract": "In this work, we propose a novel deep network for traffic sign classification that achieves outstanding performance on GTSRB surpassing all previous methods. Our deep network consists of spatial transformer layers and a modified version of inception module specifically designed for capturing local and global features together. This features adoption allows our network to classify precisely intraclass samples even under deformations. Use of spatial transformer layer makes this network more robust to deformations such as translation, rotation, scaling of input images. Unlike existing approaches that are developed with hand-crafted features, multiple deep networks with huge parameters and data augmentations, our method addresses the concern of exploding parameters and augmentations. We have achieved the state-of-the-art performance of 99.81\\% on GTSRB dataset.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02999",
        "title": "Improvised Salient Object Detection and Manipulation",
        "authors": [
            "Abhishek Maity"
        ],
        "abstract": "In case of salient subject recognition, computer algorithms have been heavily relied on scanning of images from top-left to bottom-right systematically and apply brute-force when attempting to locate objects of interest. Thus, the process turns out to be quite time consuming. Here a novel approach and a simple solution to the above problem is discussed. In this paper, we implement an approach to object manipulation and detection through segmentation map, which would help to desaturate or, in other words, wash out the background of the image. Evaluation for the performance is carried out using the Jaccard index against the well-known Ground-truth target box technique.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03015",
        "title": "Deep Representation of Facial Geometric and Photometric Attributes for Automatic 3D Facial Expression Recognition",
        "authors": [
            "Huibin Li",
            "Jian Sun",
            "Dong Wang",
            "Zongben Xu",
            "Liming Chen"
        ],
        "abstract": "In this paper, we present a novel approach to automatic 3D Facial Expression Recognition (FER) based on deep representation of facial 3D geometric and 2D photometric attributes. A 3D face is firstly represented by its geometric and photometric attributes, including the geometry map, normal maps, normalized curvature map and texture map. These maps are then fed into a pre-trained deep convolutional neural network to generate the deep representation. Then the facial expression prediction is simplyachieved by training linear SVMs over the deep representation for different maps and fusing these SVM scores. The visualizations show that the deep representation provides a complete and highly discriminative coding scheme for 3D faces. Comprehensive experiments on the BU-3DFE database demonstrate that the proposed deep representation can outperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG, Gabor) and the state-of-art approaches under the same experimental protocols.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03019",
        "title": "3D Time-lapse Reconstruction from Internet Photos",
        "authors": [
            "Ricardo Martin-Brualla",
            "David Gallup",
            "Steven M. Seitz"
        ],
        "abstract": "Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2020-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03028",
        "title": "Online Action Recognition based on Incremental Learning of Weighted Covariance Descriptors",
        "authors": [
            "Chang Tang",
            "Pichao Wang",
            "Wanqing Li"
        ],
        "abstract": "Different from traditional action recognition based on video segments, online action recognition aims to recognize actions from unsegmented streams of data in a continuous manner. One way for online recognition is based on the evidence accumulation over time to make predictions from stream videos. This paper presents a fast yet effective method to recognize actions from stream of noisy skeleton data, and a novel weighted covariance descriptor is adopted to accumulate evidence. In particular, a fast incremental updating method for the weighted covariance descriptor is developed for accumulation of temporal information and online prediction. The weighted covariance descriptor takes the following principles into consideration: past frames have less contribution for recognition and recent and informative frames such as key frames contribute more to the recognition. The online recognition is achieved using a simple nearest neighbor search against a set of offline trained action models. Experimental results on MSC-12 Kinect Gesture dataset and our newly constructed online action recognition dataset have demonstrated the efficacy of the proposed method.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2017-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03042",
        "title": "Analyzing Stability of Convolutional Neural Networks in the Frequency Domain",
        "authors": [
            "Elnaz J. Heravi",
            "Hamed H. Aghdam",
            "Domenec Puig"
        ],
        "abstract": "Understanding the internal process of ConvNets is commonly done using visualization techniques. However, these techniques do not usually provide a tool for estimating the stability of a ConvNet against noise. In this paper, we show how to analyze a ConvNet in the frequency domain using a 4-dimensional visualization technique. Using the frequency domain analysis, we show the reason that a ConvNet might be sensitive to a very low magnitude additive noise. Our experiments on a few ConvNets trained on different datasets revealed that convolution kernels of a trained ConvNet usually pass most of the frequencies and they are not able to effectively eliminate the effect of high frequencies. Our next experiments shows that a convolution kernel which has a more concentrated frequency response could be more stable. Finally, we show that fine-tuning a ConvNet using a training set augmented with noisy images can produce more stable ConvNets.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03183",
        "title": "Dynamic Belief Fusion for Object Detection",
        "authors": [
            "Hyungtae Lee",
            "Heesung Kwon",
            "Ryan M. Robinson",
            "William d. Nothwang",
            "Amar M. Marathe"
        ],
        "abstract": "A novel approach for the fusion of heterogeneous object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score is estimated using the precision/recall relationship of the corresponding detector. The main contribution of the proposed work is a novel fusion method, called Dynamic Belief Fusion (DBF), which dynamically assigns probabilities to hypotheses (target, non-target, intermediate state (target or non-target)) based on confidence levels in the detection results conditioned on the prior performance of individual detectors. In DBF, a joint basic probability assignment, optimally fusing information from all detectors, is determined by the Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as individual detectors used for the fusion.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03206",
        "title": "The Radon cumulative distribution transform and its application to image classification",
        "authors": [
            "Soheil Kolouri",
            "Se Rim Park",
            "Gustavo K. Rohde"
        ],
        "abstract": "Invertible image representation methods (transforms) are routinely employed as low-level image processing operations based on which feature extraction and recognition algorithms are developed. Most transforms in current use (e.g. Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable to substantially simplify the representation of image classes for classification. Here we describe a nonlinear, invertible, low-level image processing transform based on combining the well known Radon transform for image data, and the 1D Cumulative Distribution Transform proposed earlier. We describe a few of the properties of this new transform, and with both theoretical and experimental results show that it can often render certain problems linearly separable in transform space.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03240",
        "title": "Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer",
        "authors": [
            "Jun Xie",
            "Martin Kiefel",
            "Ming-Ting Sun",
            "Andreas Geiger"
        ],
        "abstract": "Semantic annotations are vital for training models for object recognition, semantic segmentation or scene understanding. Unfortunately, pixelwise annotation of images at very large scale is labor-intensive and only little labeled data is available, particularly at instance level and for street scenes. In this paper, we propose to tackle this problem by lifting the semantic instance labeling task from 2D into 3D. Given reconstructions from stereo or laser data, we annotate static 3D scene elements with rough bounding primitives and develop a model which transfers this information into the image domain. We leverage our method to obtain 2D labels for a novel suburban video dataset which we have collected, resulting in 400k semantic and instance image annotations. A comparison of our method to state-of-the-art label transfer baselines reveals that 3D information enables more efficient annotation while at the same time resulting in improved accuracy and time-coherent labels.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03244",
        "title": "TemplateNet for Depth-Based Object Instance Recognition",
        "authors": [
            "Ujwal Bonde",
            "Vijay Badrinarayanan",
            "Roberto Cipolla",
            "Minh-Tri Pham"
        ],
        "abstract": "We present a novel deep architecture termed templateNet for depth based object instance recognition. Using an intermediate template layer we exploit prior knowledge of an object's shape to sparsify the feature maps. This has three advantages: (i) the network is better regularised resulting in structured filters; (ii) the sparse feature maps results in intuitive features been learnt which can be visualized as the output of the template layer and (iii) the resulting network achieves state-of-the-art performance. The network benefits from this without any additional parametrization from the template layer. We derive the weight updates needed to efficiently train this network in an end-to-end manner. We benchmark the templateNet for depth based object instance recognition using two publicly available datasets. The datasets present multiple challenges of clutter, large pose variations and similar looking distractors. Through our experiments we show that with the addition of a template layer, a depth based CNN is able to outperform existing state-of-the-art methods in the field.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03257",
        "title": "Online Supervised Hashing for Ever-Growing Datasets",
        "authors": [
            "Fatih Cakir",
            "Sarah Adel Bargal",
            "Stan Sclaroff"
        ],
        "abstract": "Supervised hashing methods are widely-used for nearest neighbor search in computer vision applications. Most state-of-the-art supervised hashing approaches employ batch-learners. Unfortunately, batch-learning strategies can be inefficient when confronted with large training datasets. Moreover, with batch-learners, it is unclear how to adapt the hash functions as a dataset continues to grow and diversify over time. Yet, in many practical scenarios the dataset grows and diversifies; thus, both the hash functions and the indexing must swiftly accommodate these changes. To address these issues, we propose an online hashing method that is amenable to changes and expansions of the datasets. Since it is an online algorithm, our approach offers linear complexity with the dataset size. Our solution is supervised, in that we incorporate available label information to preserve the semantic neighborhood. Such an adaptive hashing method is attractive; but it requires recomputing the hash table as the hash functions are updated. If the frequency of update is high, then recomputing the hash table entries may cause inefficiencies in the system, especially for large indexes. Thus, we also propose a framework to reduce hash table updates. We compare our method to state-of-the-art solutions on two benchmarks and demonstrate significant improvements over previous work.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03292",
        "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge",
        "authors": [
            "Somak Aditya",
            "Yezhou Yang",
            "Chitta Baral",
            "Cornelia Fermuller",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03296",
        "title": "The Fast Bilateral Solver",
        "authors": [
            "Jonathan T. Barron",
            "Ben Poole"
        ],
        "abstract": "We present the bilateral solver, a novel algorithm for edge-aware smoothing that combines the flexibility and speed of simple filtering approaches with the accuracy of domain-specific optimization algorithms. Our technique is capable of matching or improving upon state-of-the-art results on several different computer vision tasks (stereo, depth superresolution, colorization, and semantic segmentation) while being 10-1000 times faster than competing approaches. The bilateral solver is fast, robust, straightforward to generalize to new domains, and simple to integrate into deep learning pipelines.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03328",
        "title": "Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform",
        "authors": [
            "Liang-Chieh Chen",
            "Jonathan T. Barron",
            "George Papandreou",
            "Kevin Murphy",
            "Alan L. Yuille"
        ],
        "abstract": "Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03339",
        "title": "Attention to Scale: Scale-aware Semantic Image Segmentation",
        "authors": [
            "Liang-Chieh Chen",
            "Yi Yang",
            "Jiang Wang",
            "Wei Xu",
            "Alan L. Yuille"
        ],
        "abstract": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2016-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03361",
        "title": "Discovery Radiomics via StochasticNet Sequencers for Cancer Detection",
        "authors": [
            "Mohammad Javad Shafiee",
            "Audrey G. Chung",
            "Devinder Kumar",
            "Farzad Khalvati",
            "Masoom Haider",
            "Alexander Wong"
        ],
        "abstract": "Radiomics has proven to be a powerful prognostic tool for cancer detection, and has previously been applied in lung, breast, prostate, and head-and-neck cancer studies with great success. However, these radiomics-driven methods rely on pre-defined, hand-crafted radiomic feature sets that can limit their ability to characterize unique cancer traits. In this study, we introduce a novel discovery radiomics framework where we directly discover custom radiomic features from the wealth of available medical imaging data. In particular, we leverage novel StochasticNet radiomic sequencers for extracting custom radiomic features tailored for characterizing unique cancer tissue phenotype. Using StochasticNet radiomic sequencers discovered using a wealth of lung CT data, we perform binary classification on 42,340 lung lesions obtained from the CT scans of 93 patients in the LIDC-IDRI dataset. Preliminary results show significant improvement over previous state-of-the-art methods, indicating the potential of the proposed discovery radiomics framework for improving cancer screening and diagnosis.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03363",
        "title": "Facial Expression Detection using Patch-based Eigen-face Isomap Networks",
        "authors": [
            "Sohini Roychowdhury"
        ],
        "abstract": "Automated facial expression detection problem pose two primary challenges that include variations in expression and facial occlusions (glasses, beard, mustache or face covers). In this paper we introduce a novel automated patch creation technique that masks a particular region of interest in the face, followed by Eigen-value decomposition of the patched faces and generation of Isomaps to detect underlying clustering patterns among faces. The proposed masked Eigen-face based Isomap clustering technique achieves 75% sensitivity and 66-73% accuracy in classification of faces with occlusions and smiling faces in around 1 second per image. Also, betweenness centrality, Eigen centrality and maximum information flow can be used as network-based measures to identify the most significant training faces for expression classification tasks. The proposed method can be used in combination with feature-based expression classification methods in large data sets for improving expression classification accuracies.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03369",
        "title": "Multimodal MRI Neuroimaging with Motion Compensation Based on Particle Filtering",
        "authors": [
            "Yu-Hui Chen",
            "Roni Mittelman",
            "Boklye Kim",
            "Charles Meyer",
            "Alfred Hero"
        ],
        "abstract": "Head movement during scanning impedes activation detection in fMRI studies. Head motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can be estimated and compensated by aligning the images onto a reference volume through image registration. However, registering EPI images volume to volume fails to consider head motion between slices, which may lead to severely biased head motion estimates. Slice-to-volume registration can be used to estimate motion parameters for each slice by more accurately representing the image acquisition sequence. However, accurate slice to volume mapping is dependent on the information content of the slices: middle slices are information rich, while edge slides are information poor and more prone to distortion. In this work, we propose a Gaussian particle filter based head motion tracking algorithm to reduce the image misregistration errors. The algorithm uses a dynamic state space model of head motion with an observation equation that models continuous slice acquisition of the scanner. Under this model the particle filter provides more accurate motion estimates and voxel position estimates. We demonstrate significant performance improvement of the proposed approach as compared to registration-only methods of head motion estimation and brain activation detection.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03416",
        "title": "Visual7W: Grounded Question Answering in Images",
        "authors": [
            "Yuke Zhu",
            "Oliver Groth",
            "Michael Bernstein",
            "Li Fei-Fei"
        ],
        "abstract": "We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2016-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03464",
        "title": "A Directional Diffusion Algorithm for Inpainting",
        "authors": [
            "Jan Deriu",
            "Rolf Jagerman",
            "Kai-En Tsay"
        ],
        "abstract": "The problem of inpainting involves reconstructing the missing areas of an image. Inpainting has many applications, such as reconstructing old damaged photographs or removing obfuscations from images. In this paper we present the directional diffusion algorithm for inpainting. Typical diffusion algorithms are bad at propagating edges from the image into the unknown masked regions. The directional diffusion algorithm improves on the regular diffusion algorithm by reconstructing edges more accurately. It scores better than regular diffusion when reconstructing images that are obfuscated by a text mask.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03466",
        "title": "God(s) Know(s): Developmental and Cross-Cultural Patterns in Children Drawings",
        "authors": [
            "Ksenia Konyushkova",
            "Nikolaos Arvanitopoulos",
            "Zhargalma Dandarova Robert",
            "Pierre-Yves Brandt",
            "Sabine S\u00fcsstrunk"
        ],
        "abstract": "This paper introduces a novel approach to data analysis designed for the needs of specialists in psychology of religion. We detect developmental and cross-cultural patterns in children's drawings of God(s) and other supernatural agents. We develop methods to objectively evaluate our empirical observations of the drawings with respect to: (1) the gravity center, (2) the average intensities of the colors \\emph{green} and \\emph{yellow}, (3) the use of different colors (palette) and (4) the visual complexity of the drawings. We find statistically significant differences across ages and countries in the gravity centers and in the average intensities of colors. These findings support the hypotheses of the experts and raise new questions for further investigation.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2016-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03476",
        "title": "Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning",
        "authors": [
            "Pingbo Pan",
            "Zhongwen Xu",
            "Yi Yang",
            "Fei Wu",
            "Yueting Zhuang"
        ],
        "abstract": "Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e., it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks. Notably, even using a single network with only RGB stream as input, HRNE beats all the recent systems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03629",
        "title": "A Continuous Max-Flow Approach to Cyclic Field Reconstruction",
        "authors": [
            "John S.H. Baxter",
            "Jonathan McLeod",
            "Terry M. Peters"
        ],
        "abstract": "Reconstruction of an image from noisy data using Markov Random Field theory has been explored by both the graph-cuts and continuous max-flow community in the form of the Potts and Ishikawa models. However, neither model takes into account the particular cyclic topology of specific intensity types such as the hue in natural colour images, or the phase in complex valued MRI. This paper presents \\textit{cyclic continuous max-flow} image reconstruction which models the intensity being reconstructed as having a fundamentally cyclic topology. This model complements the Ishikawa model in that it is designed with image reconstruction in mind, having the topology of the intensity space inherent in the model while being readily extendable to an arbitrary intensity resolution.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03650",
        "title": "Piecewise Linear Activation Functions For More Efficient Deep Networks",
        "authors": [
            "Cheng-Yang Fu",
            "Alexander C. Berg"
        ],
        "abstract": "This submission has been withdrawn by arXiv administrators because it is intentionally incomplete, which is in violation of our policies.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03690",
        "title": "Deep Multimodal Semantic Embeddings for Speech and Images",
        "authors": [
            "David Harwath",
            "James Glass"
        ],
        "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03745",
        "title": "Grounding of Textual Phrases in Images by Reconstruction",
        "authors": [
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Ronghang Hu",
            "Trevor Darrell",
            "Bernt Schiele"
        ],
        "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2017-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03748",
        "title": "Automatic Content-Aware Color and Tone Stylization",
        "authors": [
            "Joon-Young Lee",
            "Kalyan Sunkavalli",
            "Zhe Lin",
            "Xiaohui Shen",
            "In So Kweon"
        ],
        "abstract": "We introduce a new technique that automatically generates diverse, visually compelling stylizations for a photograph in an unsupervised manner. We achieve this by learning style ranking for a given input using a large photo collection and selecting a diverse subset of matching styles for final style transfer. We also propose a novel technique that transfers the global color and tone of the chosen exemplars to the input photograph while avoiding the common visual artifacts produced by the existing style transfer methods. Together, our style selection and transfer techniques produce compelling, artifact-free results on a wide range of input photographs, and a user study shows that our results are preferred over other techniques.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03753",
        "title": "Shearlet-Based Detection of Flame Fronts",
        "authors": [
            "Rafael Reisenhofer",
            "Johannes Kiefer",
            "Emily J. King"
        ],
        "abstract": "Identifying and characterizing flame fronts is the most common task in the computer-assisted analysis of data obtained from imaging techniques such as planar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), or particle imaging velocimetry (PIV). We present a novel edge and ridge (line) detection algorithm based on complex-valued wavelet-like analyzing functions -- so-called complex shearlets -- displaying several traits useful for the extraction of flame fronts. In addition to providing a unified approach to the detection of edges and ridges, our method inherently yields estimates of local tangent orientations and local curvatures. To examine the applicability for high-frequency recordings of combustion processes, the algorithm is applied to mock images distorted with varying degrees of noise and real-world PLIF images of both OH and CH radicals. Furthermore, we compare the performance of the newly proposed complex shearlet-based measure to well-established edge and ridge detection techniques such as the Canny edge detector, another shearlet-based edge detector, and the phase congruency measure.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03776",
        "title": "ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks",
        "authors": [
            "Chen Sun",
            "Manohar Paluri",
            "Ronan Collobert",
            "Ram Nevatia",
            "Lubomir Bourdev"
        ],
        "abstract": "This paper aims to classify and locate objects accurately and efficiently, without using bounding box annotations. It is challenging as objects in the wild could appear at arbitrary locations and in different scales. In this paper, we propose a novel classification architecture ProNet based on convolutional neural networks. It uses computationally efficient neural networks to propose image regions that are likely to contain objects, and applies more powerful but slower networks on the proposed regions. The basic building block is a multi-scale fully-convolutional network which assigns object confidence scores to boxes at different locations and scales. We show that such networks can be trained effectively using image-level annotations, and can be connected into cascades or trees for efficient object classification. ProNet outperforms previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point-based localization.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03814",
        "title": "Hand-Object Interaction and Precise Localization in Transitive Action Recognition",
        "authors": [
            "Amir Rosenfeld",
            "Shimon Ullman"
        ],
        "abstract": "Action recognition in still images has seen major improvement in recent years due to advances in human pose estimation, object recognition and stronger feature representations produced by deep neural networks. However, there are still many cases in which performance remains far from that of humans. A major difficulty arises in distinguishing between transitive actions in which the overall actor pose is similar, and recognition therefore depends on details of the grasp and the object, which may be largely occluded. In this paper we demonstrate how recognition is improved by obtaining precise localization of the action-object and consequently extracting details of the object shape together with the actor-object interaction. To obtain exact localization of the action object and its interaction with the actor, we employ a coarse-to-fine approach which combines semantic segmentation and contextual features, in successive stages. We focus on (but are not limited) to face-related actions, a set of actions that includes several currently challenging categories. We present an average relative improvement of 35% over state-of-the art and validate through experimentation the effectiveness of our approach.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03853",
        "title": "When Na\u00efve Bayes Nearest Neighbours Meet Convolutional Neural Networks",
        "authors": [
            "Ilja Kuzborskij",
            "Fabio Maria Carlucci",
            "Barbara Caputo"
        ],
        "abstract": "Since Convolutional Neural Networks (CNNs) have become the leading learning paradigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-based classifiers have lost momentum in the community. This is because (1) such algorithms cannot use CNN activations as input features; (2) they cannot be used as final layer of CNN architectures for end-to-end training , and (3) they are generally not scalable and hence cannot handle big data. This paper proposes a framework that addresses all these issues, thus bringing back NBNNs on the map. We solve the first by extracting CNN activations from local patches at multiple scale levels, similarly to [1]. We address simultaneously the second and third by proposing a scalable version of Naive Bayes Non-linear Learning (NBNL, [2]). Results obtained using pre-trained CNNs on standard scene and domain adaptation databases show the strength of our approach, opening a new season for NBNNs.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03995",
        "title": "LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement",
        "authors": [
            "Kin Gwn Lore",
            "Adedotun Akintayo",
            "Soumik Sarkar"
        ],
        "abstract": "In surveillance, monitoring and tactical reconnaissance, gathering the right visual information from a dynamic environment and accurately processing such data are essential ingredients to making informed decisions which determines the success of an operation. Camera sensors are often cost-limited in ability to clearly capture objects without defects from images or videos taken in a poorly-lit environment. The goal in many applications is to enhance the brightness, contrast and reduce noise content of such images in an on-board real-time manner. We propose a deep autoencoder-based approach to identify signal features from low-light images handcrafting and adaptively brighten images without over-amplifying the lighter parts in images (i.e., without saturation of image pixels) in high dynamic range. We show that a variant of the recently proposed stacked-sparse denoising autoencoder can learn to adaptively enhance and denoise from synthetically darkened and noisy training examples. The network can then be successfully applied to naturally low-light environment and/or hardware degraded images. Results show significant credibility of deep learning based approaches both visually and by quantitative comparison with various popular enhancing, state-of-the-art denoising and hybrid enhancing-denoising techniques.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04003",
        "title": "Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest",
        "authors": [
            "Dmitry Kislyuk",
            "Yuchen Liu",
            "David Liu",
            "Eric Tzeng",
            "Yushi Jing"
        ],
        "abstract": "This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04031",
        "title": "Facial Landmark Detection with Tweaked Convolutional Neural Networks",
        "authors": [
            "Yue Wu",
            "Tal Hassner",
            "KangGeon Kim",
            "Gerard Medioni",
            "Prem Natarajan"
        ],
        "abstract": "We present a novel convolutional neural network (CNN) design for facial landmark coordinate regression. We examine the intermediate features of a standard CNN trained for landmark detection and show that features extracted from later, more specialized layers capture rough landmark locations. This provides a natural means of applying differential treatment midway through the network, tweaking processing based on facial alignment. The resulting Tweaked CNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in an appearance-sensitive manner without training multi-part or multi-scale models. Our results on standard face landmark detection and face verification benchmarks show TCNN to surpasses previously published performances by wide margins.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-03-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04048",
        "title": "Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images",
        "authors": [
            "Roozbeh Mottaghi",
            "Hessam Bagherinezhad",
            "Mohammad Rastegari",
            "Ali Farhadi"
        ],
        "abstract": "In this paper, we study the challenging problem of predicting the dynamics of objects in static images. Given a query object in an image, our goal is to provide a physical understanding of the object in terms of the forces acting upon it and its long term motion as response to those forces. Direct and explicit estimation of the forces and the motion of objects from a single image is extremely challenging. We define intermediate physical abstractions called Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns to map a single image to a state in a Newtonian scenario. Our experimental evaluations show that our method can reliably predict dynamics of a query object from a single image. In addition, our approach can provide physical reasoning that supports the predicted dynamics in terms of velocity and force vectors. To spur research in this direction we compiled Visual Newtonian Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian scenarios represented using game engines, and 4516 still images with their ground truth dynamics.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04067",
        "title": "Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising",
        "authors": [
            "Raviteja Vemulapalli",
            "Oncel Tuzel",
            "Ming-Yu Liu"
        ],
        "abstract": "We propose a novel deep network architecture for image\\\\ denoising based on a Gaussian Conditional Random Field (GCRF) model. In contrast to the existing discriminative denoising methods that train a separate model for each noise level, the proposed deep network explicitly models the input noise variance and hence is capable of handling a range of noise levels. Our deep network, which we refer to as deep GCRF network, consists of two sub-networks: (i) a parameter generation network that generates the pairwise potential parameters based on the noisy input image, and (ii) an inference network whose layers perform the computations involved in an iterative GCRF inference procedure.\\ We train the entire deep GCRF network (both parameter generation and inference networks) discriminatively in an end-to-end fashion by maximizing the peak signal-to-noise ratio measure. Experiments on Berkeley segmentation and PASCALVOC datasets show that the proposed deep GCRF network outperforms state-of-the-art image denoising approaches for several noise levels.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04103",
        "title": "Basic Level Categorization Facilitates Visual Object Recognition",
        "authors": [
            "Panqu Wang",
            "Garrison W. Cottrell"
        ],
        "abstract": "Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children's visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04136",
        "title": "UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking",
        "authors": [
            "Longyin Wen",
            "Dawei Du",
            "Zhaowei Cai",
            "Zhen Lei",
            "Ming-Ching Chang",
            "Honggang Qi",
            "Jongwoo Lim",
            "Ming-Hsuan Yang",
            "Siwei Lyu"
        ],
        "abstract": "In recent years, numerous effective multi-object tracking (MOT) methods are developed because of the wide range of applications. Existing performance evaluations of MOT methods usually separate the object tracking step from the object detection step by using the same fixed object detection results for comparisons. In this work, we perform a comprehensive quantitative study on the effects of object detection accuracy to the overall MOT performance, using the new large-scale University at Albany DETection and tRACking (UA-DETRAC) benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and object tracking methods. Our analysis shows the complex effects of object detection accuracy on MOT system performance. Based on these observations, we propose new evaluation tools and metrics for MOT systems that consider both object detection and object tracking for comprehensive analysis.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2020-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04153",
        "title": "Adaptive Affinity Matrix for Unsupervised Metric Learning",
        "authors": [
            "Yaoyi Li",
            "Junxuan Chen",
            "Hongtao Lu"
        ],
        "abstract": "Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Most spectral clustering methods provide a nonlinear map from the data manifold to a subspace. Only a little work focuses on the explicit linear map which can be viewed as the unsupervised distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. While much success of affinity learning has been achieved in recent years, some issues such as noise reduction remain to be addressed. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric from the affinity. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. We yield the affinity from both the original data distribution and the widely-used heat kernel. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of real-world data sets show the effectiveness and efficiency of AdaAM.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04164",
        "title": "Natural Language Object Retrieval",
        "authors": [
            "Ronghang Hu",
            "Huazhe Xu",
            "Marcus Rohrbach",
            "Jiashi Feng",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04166",
        "title": "Unsupervised Learning of Edges",
        "authors": [
            "Yin Li",
            "Manohar Paluri",
            "James M. Rehg",
            "Piotr Doll\u00e1r"
        ],
        "abstract": "Data-driven approaches for edge detection have proven effective and achieve top results on modern benchmarks. However, all current data-driven edge detectors require manual supervision for training in the form of hand-labeled region segments or object boundaries. Specifically, human annotators mark semantically meaningful edges which are subsequently used for training. Is this form of strong, high-level supervision actually necessary to learn to accurately detect edges? In this work we present a simple yet effective approach for training edge detectors without human supervision. To this end we utilize motion, and more specifically, the only input to our method is noisy semi-dense matches between frames. We begin with only a rudimentary knowledge of edges (in the form of image gradients), and alternate between improving motion estimation and edge detection in turn. Using a large corpus of video data, we show that edge detectors trained using our unsupervised scheme approach the performance of the same methods trained with full supervision (within 3-5%). Finally, we show that when using a deep network for the edge detector, our approach provides a novel pre-training scheme for object detection.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04176",
        "title": "Sequence to Sequence Learning for Optical Character Recognition",
        "authors": [
            "Devendra Kumar Sahu",
            "Mohak Sukhwani"
        ],
        "abstract": "We propose an end-to-end recurrent encoder-decoder based sequence learning approach for printed text Optical Character Recognition (OCR). In contrast to present day existing state-of-art OCR solution which uses connectionist temporal classification (CTC) output layer, our approach makes minimalistic assumptions on the structure and length of the sequence. We use a two step encoder-decoder approach -- (a) A recurrent encoder reads a variable length printed text word image and encodes it to a fixed dimensional embedding. (b) This fixed dimensional embedding is subsequently comprehended by decoder structure which converts it into a variable length text output. Our architecture gives competitive performance relative to connectionist temporal classification (CTC) output layer while being executed in more natural settings. The learnt deep word image embedding from encoder can be used for printed text based retrieval systems. The expressive fixed dimensional embedding for any variable length input expedites the task of retrieval and makes it more efficient which is not possible with other recurrent neural network architectures. We empirically investigate the expressiveness and the learnability of long short term memory (LSTMs) in the sequence to sequence learning regime by training our network for prediction tasks in segmentation free printed text OCR. The utility of the proposed architecture for printed text is demonstrated by quantitative and qualitative evaluation of two tasks -- word prediction and retrieval.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04192",
        "title": "DISC: Deep Image Saliency Computing via Progressive Representation Learning",
        "authors": [
            "Tianshui Chen",
            "Liang Lin",
            "Lingbo Liu",
            "Xiaonan Luo",
            "Xuelong Li"
        ],
        "abstract": "Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04196",
        "title": "Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition",
        "authors": [
            "Zhiwei Deng",
            "Arash Vahdat",
            "Hexiang Hu",
            "Greg Mori"
        ],
        "abstract": "Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04240",
        "title": "An Adaptive Data Representation for Robust Point-Set Registration and Merging",
        "authors": [
            "Dylan Campbell",
            "Lars Petersson"
        ],
        "abstract": "This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm that minimises the L2 distance between our support vector--parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04242",
        "title": "Volume-based Semantic Labeling with Signed Distance Functions",
        "authors": [
            "Tommaso Cavallari",
            "Luigi Di Stefano"
        ],
        "abstract": "Research works on the two topics of Semantic Segmentation and SLAM (Simultaneous Localization and Mapping) have been following separate tracks. Here, we link them quite tightly by delineating a category label fusion technique that allows for embedding semantic information into the dense map created by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our approach is the first to provide a semantically labeled dense reconstruction of the environment from a stream of RGB-D images. We validate our proposal using a publicly available semantically annotated RGB-D dataset and a) employing ground truth labels, b) corrupting such annotations with synthetic noise, c) deploying a state of the art semantic segmentation algorithm based on Convolutional Neural Networks.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04273",
        "title": "Learning to Assign Orientations to Feature Points",
        "authors": [
            "Kwang Moo Yi",
            "Yannick Verdie",
            "Pascal Fua",
            "Vincent Lepetit"
        ],
        "abstract": "We show how to train a Convolutional Neural Network to assign a canonical orientation to feature points given an image patch centered on the feature point. Our method improves feature point matching upon the state-of-the art and can be used in conjunction with any existing rotation sensitive descriptors. To avoid the tedious and almost impossible task of finding a target orientation to learn, we propose to use Siamese networks which implicitly find the optimal orientations during training. We also propose a new type of activation function for Neural Networks that generalizes the popular ReLU, maxout, and PReLU activation functions. This novel activation performs better for our task. We validate the effectiveness of our method extensively with four existing datasets, including two non-planar datasets, as well as our own dataset. We show that we outperform the state-of-the-art without the need of retraining for each dataset.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04320",
        "title": "Standard methods for inexpensive pollen loads authentication by means of computer vision and machine learning",
        "authors": [
            "Manuel Chica",
            "Pascual Campoy"
        ],
        "abstract": "We present a complete methodology for authenticating local bee pollen against fraudulent samples using image processing and machine learning techniques. The proposed standard methods do not need expensive equipment such as advanced microscopes and can be used for a preliminary fast rejection of unknown pollen types. The system is able to rapidly reject the non-local pollen samples with inexpensive hardware and without the need to send the product to the laboratory. Methods are based on the color properties of bee pollen loads images and the use of one-class classifiers which are appropriate to reject unknown pollen samples when there is limited data about them. The validation of the method is carried out by authenticating Spanish bee pollen types. Experimentation shows that the proposed methods can obtain an overall authentication accuracy of 94%. We finally illustrate the user interaction with the software in some practical cases by showing the developed application prototype.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04377",
        "title": "Learning Dense Convolutional Embeddings for Semantic Segmentation",
        "authors": [
            "Adam W. Harley",
            "Konstantinos G. Derpanis",
            "Iasonas Kokkinos"
        ],
        "abstract": "This paper proposes a new deep convolutional neural network (DCNN) architecture that learns pixel embeddings, such that pairwise distances between the embeddings can be used to infer whether or not the pixels lie on the same region. That is, for any two pixels on the same object, the embeddings are trained to be similar; for any pair that straddles an object boundary, the embeddings are trained to be dissimilar. Experimental results show that when this embedding network is used in conjunction with a DCNN trained on semantic segmentation, there is a systematic improvement in per-pixel classification accuracy. Our contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines. As such, they can be exploited for any task involving convolution layers.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04384",
        "title": "Deep Reflectance Maps",
        "authors": [
            "Konstantinos Rematas",
            "Tobias Ritschel",
            "Mario Fritz",
            "Efstratios Gavves",
            "Tinne Tuytelaars"
        ],
        "abstract": "Undoing the image formation process and therefore decomposing appearance into its intrinsic properties is a challenging task due to the under-constraint nature of this inverse problem. While significant progress has been made on inferring shape, materials and illumination from images only, progress in an unconstrained setting is still limited. We propose a convolutional neural architecture to estimate reflectance maps of specular materials in natural lighting conditions. We achieve this in an end-to-end learning formulation that directly predicts a reflectance map from the image itself. We show how to improve estimates by facilitating additional supervision in an indirect scheme that first predicts surface orientation and afterwards predicts the reflectance map by a learning-based sparse data interpolation.\n",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04397",
        "title": "Similarity-based Text Recognition by Deeply Supervised Siamese Network",
        "authors": [
            "Ehsan Hosseini-Asl",
            "Angshuman Guha"
        ],
        "abstract": "In this paper, we propose a new text recognition model based on measuring the visual similarity of text and predicting the content of unlabeled texts. First a Siamese convolutional network is trained with deep supervision on a labeled training dataset. This network projects texts into a similarity manifold. The Deeply Supervised Siamese network learns visual similarity of texts. Then a K-nearest neighbor classifier is used to predict unlabeled text based on similarity distance to labeled texts. The performance of the model is evaluated on three datasets of machine-print and hand-written text combined. We demonstrate that the model reduces the cost of human estimation by $50\\%-85\\%$. The error of the system is less than $0.5\\%$. The proposed model outperform conventional Siamese network by finding visually-similar barely-readable and readable text, e.g. machine-printed, handwritten, due to deep supervision. The results also demonstrate that the predicted labels are sometimes better than human labels e.g. spelling correction.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04401",
        "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements",
        "authors": [
            "Federico Raue",
            "Andreas Dengel",
            "Thomas M. Breuel",
            "Marcus Liwicki"
        ],
        "abstract": "In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2017-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04404",
        "title": "Robust Face Alignment Using a Mixture of Invariant Experts",
        "authors": [
            "Oncel Tuzel",
            "Tim K. Marks",
            "Salil Tambe"
        ],
        "abstract": "Face alignment, which is the task of finding the locations of a set of facial landmark points in an image of a face, is useful in widespread application areas. Face alignment is particularly challenging when there are large variations in pose (in-plane and out-of-plane rotations) and facial expression. To address this issue, we propose a cascade in which each stage consists of a mixture of regression experts. Each expert learns a customized regression model that is specialized to a different subset of the joint space of pose and expressions. The system is invariant to a predefined class of transformations (e.g., affine), because the input is transformed to match each expert's prototype shape before the regression is applied. We also present a method to include deformation constraints within the discriminative alignment framework, which makes our algorithm more robust. Our algorithm significantly outperforms previous methods on publicly available face alignment datasets.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04458",
        "title": "Transductive Zero-Shot Action Recognition by Word-Vector Embedding",
        "authors": [
            "Xun Xu",
            "Timothy Hospedales",
            "Shaogang Gong"
        ],
        "abstract": "The number of categories for action recognition is growing rapidly and it has become increasingly hard to label sufficient training data for learning conventional models for all categories. Instead of collecting ever more data and labelling them exhaustively for all categories, an attractive alternative approach is zero-shot learning\" (ZSL). To that end, in this study we construct a mapping between visual features and a semantic descriptor of each action category, allowing new categories to be recognised in the absence of any visual training data. Existing ZSL studies focus primarily on still images, and attribute-based semantic representations. In this work, we explore word-vectors as the shared semantic space to embed videos and category labels for ZSL action recognition. This is a more challenging problem than existing ZSL of still images and/or attributes, because the mapping between video spacetime features of actions and the semantic space is more complex and harder to learn for the purpose of generalising over any cross-category domain shift. To solve this generalisation problem in ZSL action recognition, we investigate a series of synergistic strategies to improve upon the standard ZSL pipeline. Most of these strategies are transductive in nature which means access to testing data in the training phase.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2016-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04472",
        "title": "Solving Jigsaw Puzzles with Linear Programming",
        "authors": [
            "Rui Yu",
            "Chris Russell",
            "Lourdes Agapito"
        ],
        "abstract": "We propose a novel Linear Program (LP) based formula- tion for solving jigsaw puzzles. We formulate jigsaw solving as a set of successive global convex relaxations of the stan- dard NP-hard formulation, that can describe both jigsaws with pieces of unknown position and puzzles of unknown po- sition and orientation. The main contribution and strength of our approach comes from the LP assembly strategy. In contrast to existing greedy methods, our LP solver exploits all the pairwise matches simultaneously, and computes the position of each piece/component globally. The main ad- vantages of our LP approach include: (i) a reduced sensi- tivity to local minima compared to greedy approaches, since our successive approximations are global and convex and (ii) an increased robustness to the presence of mismatches in the pairwise matches due to the use of a weighted L1 penalty. To demonstrate the effectiveness of our approach, we test our algorithm on public jigsaw datasets and show that it outperforms state-of-the-art methods.\n    ",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04491",
        "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
        "authors": [
            "Jiwon Kim",
            "Jung Kwon Lee",
            "Kyoung Mu Lee"
        ],
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04510",
        "title": "Semantic Object Parsing with Local-Global Long Short-Term Memory",
        "authors": [
            "Xiaodan Liang",
            "Xiaohui Shen",
            "Donglai Xiang",
            "Jiashi Feng",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "Semantic object parsing is a fundamental task for understanding objects in detail in computer vision community, where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition. Prior methods often leverage the contextual information through post-processing predicted confidence maps. In this work, we propose a novel deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions. In each LG-LSTM layer, local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information. Individual LSTMs for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images, yielding distinct hidden and memory cells of each position for each dimension. In our parsing approach, several LG-LSTM layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features, allowing network parameters to be learned in an end-to-end way. The long chains of sequential computation by stacked LG-LSTM layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions. Comprehensive evaluations on three public datasets well demonstrate the significant superiority of our LG-LSTM over other state-of-the-art methods.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2015-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04511",
        "title": "Sequential Optimization for Efficient High-Quality Object Proposal Generation",
        "authors": [
            "Ziming Zhang",
            "Yun Liu",
            "Xi Chen",
            "Yanjun Zhu",
            "Ming-Ming Cheng",
            "Venkatesh Saligrama",
            "Philip H.S. Torr"
        ],
        "abstract": "We are motivated by the need for a generic object proposal generation algorithm which achieves good balance between object detection recall, proposal localization quality and computational efficiency. We propose a novel object proposal algorithm, BING++, which inherits the virtue of good computational efficiency of BING but significantly improves its proposal localization quality. At high level we formulate the problem of object proposal generation from a novel probabilistic perspective, based on which our BING++ manages to improve the localization quality by employing edges and segments to estimate object boundaries and update the proposals sequentially. We propose learning the parameters efficiently by searching for approximate solutions in a quantized parameter space for complexity reduction. We demonstrate the generalization of BING++ with the same fixed parameters across different object classes and datasets. Empirically our BING++ can run at half speed of BING on CPU, but significantly improve the localization quality by 18.5% and 16.7% on both VOC2007 and Microhsoft COCO datasets, respectively. Compared with other state-of-the-art approaches, BING++ can achieve comparable performance, but run significantly faster.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2017-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04512",
        "title": "Zero-Shot Learning via Joint Latent Similarity Embedding",
        "authors": [
            "Ziming Zhang",
            "Venkatesh Saligrama"
        ],
        "abstract": "Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90\\% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45\\% improvement accordingly in mean average precision (mAP).\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04517",
        "title": "Reversible Recursive Instance-level Object Segmentation",
        "authors": [
            "Xiaodan Liang",
            "Yunchao Wei",
            "Xiaohui Shen",
            "Zequn Jie",
            "Jiashi Feng",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two sub-networks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the $\\text{AP}^r$ over $20$ classes at $0.5$ IoU achieves $66.7\\%$, which significantly outperforms the results of $58.7\\%$ by PFN~\\cite{PFN} and $46.3\\%$ by~\\cite{liu2015multi}.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04524",
        "title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing",
        "authors": [
            "Ziming Zhang",
            "Yuting Chen",
            "Venkatesh Saligrama"
        ],
        "abstract": "In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \"shallow\" networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04534",
        "title": "Learning Fine-grained Features via a CNN Tree for Large-scale Classification",
        "authors": [
            "Zhenhua Wang",
            "Xingxing Wang",
            "Gang Wang"
        ],
        "abstract": "We propose a novel approach to enhance the discriminability of Convolutional Neural Networks (CNN). The key idea is to build a tree structure that could progressively learn fine-grained features to distinguish a subset of classes, by learning features only among these classes. Such features are expected to be more discriminative, compared to features learned for all the classes. We develop a new algorithm to effectively learn the tree structure from a large number of classes. Experiments on large-scale image classification tasks demonstrate that our method could boost the performance of a given basic CNN model. Our method is quite general, hence it can potentially be used in combination with many other deep learning models.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2017-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04587",
        "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks",
        "authors": [
            "Jiwon Kim",
            "Jung Kwon Lee",
            "Kyoung Mu Lee"
        ],
        "abstract": "We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification \\cite{simonyan2015very}. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates ($10^4$ times higher than SRCNN \\cite{dong2015image}) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04590",
        "title": "Oracle performance for visual captioning",
        "authors": [
            "Li Yao",
            "Nicolas Ballas",
            "Kyunghyun Cho",
            "John R. Smith",
            "Yoshua Bengio"
        ],
        "abstract": "The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04601",
        "title": "Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification",
        "authors": [
            "Weiyang Liu",
            "Zhiding Yu",
            "Yandong Wen",
            "Rongmei Lin",
            "Meng Yang"
        ],
        "abstract": "Sparse coding with dictionary learning (DL) has shown excellent classification performance. Despite the considerable number of existing works, how to obtain features on top of which dictionaries can be better learned remains an open and interesting question. Many current prevailing DL methods directly adopt well-performing crafted features. While such strategy may empirically work well, it ignores certain intrinsic relationship between dictionaries and features. We propose a framework where features and dictionaries are jointly learned and optimized. The framework, named joint non-negative projection and dictionary learning (JNPDL), enables interaction between the input features and the dictionaries. The non-negative projection leads to discriminative parts-based object features while DL seeks a more suitable representation. Discriminative graph constraints are further imposed to simultaneously maximize intra-class compactness and inter-class separability. Experiments on both image and image set classification show the excellent performance of JNPDL by outperforming several state-of-the-art approaches.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04659",
        "title": "Implementation and comparative quantitative assessment of different multispectral image pansharpening approches",
        "authors": [
            "Shailesh Panchal",
            "Rajesh Thakker"
        ],
        "abstract": "In remote sensing, images acquired by various earth observation satellites tend to have either a high spatial and low spectral resolution or vice versa. Pansharpening is a technique which aims to improve spatial resolution of multispectral image. The challenges involve in the pansharpening are not only to improve the spatial resolution but also to preserve spectral quality of the multispectral image. In this paper, various pansharpening algorithms are discussed and classified based on approaches they have adopted. Using MATLAB image processing toolbox, several state-of-art pan-sharpening algorithms are implemented. Quality of pansharpened images are assessed visually and quantitatively. Correlation coefficient (CC), Root mean square error (RMSE), Relative average spectral error (RASE) and Universal quality index (Q) indices are used to easure spectral quality while to spatial-CC (SCC) quantitative parameter is used for spatial quality measurement. Finally, the paper is concluded with useful remarks.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04668",
        "title": "Deep Neural Network for Real-Time Autonomous Indoor Navigation",
        "authors": [
            "Dong Ki Kim",
            "Tsuhan Chen"
        ],
        "abstract": "Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses many challenges. One main reason is that GPS has limited precision in indoor environments. The additional fact that MAVs are not able to carry heavy weight or power consuming sensors, such as range finders, makes indoor autonomous navigation a challenging task. In this paper, we propose a practical system in which a quadcopter autonomously navigates indoors and finds a specific target, i.e., a book bag, by using a single camera. A deep learning model, Convolutional Neural Network (ConvNet), is used to learn a controller strategy that mimics an expert pilot's choice of action. We show our system's performance through real-time experiments in diverse indoor locations. To understand more about our trained network, we use several visualization techniques.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04670",
        "title": "Uncovering Temporal Context for Video Question and Answering",
        "authors": [
            "Linchao Zhu",
            "Zhongwen Xu",
            "Yi Yang",
            "Alexander G. Hauptmann"
        ],
        "abstract": "In this work, we introduce Video Question Answering in temporal domain to infer the past, describe the present and predict the future. We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions. We explore approaches for finer understanding of video content using question form of \"fill-in-the-blank\", and managed to collect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD, MEDTest 14 datasets, while the corresponding 390,744 questions are generated from annotations. Extensive experiments demonstrate that our approach significantly outperforms the compared baselines.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04687",
        "title": "Separation Surfaces in the Spectral TV Domain for Texture Decomposition",
        "authors": [
            "Dikla Horesh",
            "Guy Gilboa"
        ],
        "abstract": "In this paper we introduce a novel notion of separation surfaces for image decomposition. A surface is embedded in the spectral total-variation (TV) three dimensional domain and encodes a spatially-varying separation scale. The method allows good separation of textures with gradually varying pattern-size, pattern-contrast or illumination. The recently proposed total variation spectral framework is used to decompose the image into a continuum of textural scales. A desired texture, within a scale range, is found by fitting a surface to the local maximal responses in the spectral domain. A band above and below the surface, referred to as the \\textit{Texture Stratum}, defines for each pixel the adaptive scale-range of the texture. Based on the decomposition an application is proposed which can attenuate or enhance textures in the image in a very natural and visually convincing manner.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04798",
        "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization",
        "authors": [
            "Baohan Xu",
            "Yanwei Fu",
            "Yu-Gang Jiang",
            "Boyang Li",
            "Leonid Sigal"
        ],
        "abstract": "Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2018-02-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04808",
        "title": "Learning Mid-level Words on Riemannian Manifold for Action Recognition",
        "authors": [
            "Mengyi Liu",
            "Ruiping Wang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "Human action recognition remains a challenging task due to the various sources of video data and large intra-class variations. It thus becomes one of the key issues in recent research to explore effective and robust representation to handle such challenges. In this paper, we propose a novel representation approach by constructing mid-level words in videos and encoding them on Riemannian manifold. Specifically, we first conduct a global alignment on the densely extracted low-level features to build a bank of corresponding feature groups, each of which can be statistically modeled as a mid-level word lying on some specific Riemannian manifold. Based on these mid-level words, we construct intrinsic Riemannian codebooks by employing K-Karcher-means clustering and Riemannian Gaussian Mixture Model, and consequently extend the Riemannian manifold version of three well studied encoding methods in Euclidean space, i.e. Bag of Visual Words (BoVW), Vector of Locally Aggregated Descriptors (VLAD), and Fisher Vector (FV), to obtain the final action video representations. Our method is evaluated in two tasks on four popular realistic datasets: action recognition on YouTube, UCF50, HMDB51 databases, and action similarity labeling on ASLAN database. In all cases, the reported results achieve very competitive performance with those most recent state-of-the-art works.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04891",
        "title": "Sherlock: Scalable Fact Learning in Images",
        "authors": [
            "Mohamed Elhoseiny",
            "Scott Cohen",
            "Walter Chang",
            "Brian Price",
            "Ahmed Elgammal"
        ],
        "abstract": "We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way. The training data comes as structured facts in images, including (1) objects (e.g., $<$boy$>$), (2) attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). We show that learning visual facts in a structured way enables not only a uniform but also generalizable visual understanding. We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04901",
        "title": "Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression",
        "authors": [
            "Zhiao Huang",
            "Erjin Zhou",
            "Zhimin Cao"
        ],
        "abstract": "Facial landmark localization plays an important role in face recognition and analysis applications. In this paper, we give a brief introduction to a coarse-to-fine pipeline with neural networks and sequential regression. First, a global convolutional network is applied to the holistic facial image to give an initial landmark prediction. A pyramid of multi-scale local image patches is then cropped to feed to a new network for each landmark to refine the prediction. As the refinement network outputs a more accurate position estimation than the input, such procedure could be repeated several times until the estimation converges. We evaluate our system on the 300-W dataset [11] and it outperforms the recent state-of-the-arts.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04902",
        "title": "Graph-based denoising for time-varying point clouds",
        "authors": [
            "Yann Schoenenberger",
            "Johan Paratte",
            "Pierre Vandergheynst"
        ],
        "abstract": "Noisy 3D point clouds arise in many applications. They may be due to errors when constructing a 3D model from images or simply to imprecise depth sensors. Point clouds can be given geometrical structure using graphs created from the similarity information between points. This paper introduces a technique that uses this graph structure and convex optimization methods to denoise 3D point clouds. A short discussion presents how those methods naturally generalize to time-varying inputs such as 3D point cloud time series.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04934",
        "title": "Identification and Counting White Blood Cells and Red Blood Cells using Image Processing Case Study of Leukemia",
        "authors": [
            "Esti Suryani",
            "Wiharto Wiharto",
            "Nizomjon Polvonov"
        ],
        "abstract": "Leukemia is diagnosed with complete blood counts which is by calculating all blood cells and compare the number of white blood cells (White Blood Cells / WBC) and red blood cells (Red Blood Cells / RBC). Information obtained from a complete blood count, has become a cornerstone in the hematology laboratory for diagnostic purposes and monitoring of hematological disorders. However, the traditional procedure for counting blood cells manually requires effort and a long time, therefore this method is one of the most expensive routine tests in laboratory hematology clinic. Solution for such kind of time consuming task and necessity of data tracability can be found in image processing techniques based on blood cell morphology . This study aims to identify Acute Lymphocytic Leukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy Rule Based System based on morphology of white blood cells. Characteristic parameters witch extractedare WBC Area, Nucleus and Granule Ratio of white blood cells. Image processing algorithms such as thresholding, Canny edge detection and color identification filters are ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04960",
        "title": "Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering",
        "authors": [
            "Mohammad Najafi",
            "Sarah Taghavi Namin",
            "Mathieu Salzmann",
            "Lars Petersson"
        ],
        "abstract": "Scene parsing has attracted a lot of attention in computer vision. While parametric models have proven effective for this task, they cannot easily incorporate new training data. By contrast, nonparametric approaches, which bypass any learning phase and directly transfer the labels from the training data to the query images, can readily exploit new labeled samples as they become available. Unfortunately, because of the computational cost of their label transfer procedures, state-of-the-art nonparametric methods typically filter out most training images to only keep a few relevant ones to label the query. As such, these methods throw away many images that still contain valuable information and generally obtain an unbalanced set of labeled samples. In this paper, we introduce a nonparametric approach to scene parsing that follows a sample-and-filter strategy. More specifically, we propose to sample labeled superpixels according to an image similarity score, which allows us to obtain a balanced set of samples. We then formulate label transfer as an efficient filtering procedure, which lets us exploit more labeled samples than existing techniques. Our experiments evidence the benefits of our approach over state-of-the-art nonparametric methods on two benchmark datasets.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05045",
        "title": "Handcrafted Local Features are Convolutional Neural Networks",
        "authors": [
            "Zhenzhong Lan",
            "Shoou-I Yu",
            "Ming Lin",
            "Bhiksha Raj",
            "Alexander G. Hauptmann"
        ],
        "abstract": "Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition.\n",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05049",
        "title": "An Empirical Study of Recent Face Alignment Methods",
        "authors": [
            "Heng Yang",
            "Xuhui Jia",
            "Chen Change Loy",
            "Peter Robinson"
        ],
        "abstract": "The problem of face alignment has been intensively studied in the past years. A large number of novel methods have been proposed and reported very good performance on benchmark dataset such as 300W. However, the differences in the experimental setting and evaluation metric, missing details in the description of the methods make it hard to reproduce the results reported and evaluate the relative merits. For instance, most recent face alignment methods are built on top of face detection but from different face detectors. In this paper, we carry out a rigorous evaluation of these methods by making the following contributions: 1) we proposes a new evaluation metric for face alignment on a set of images, i.e., area under error distribution curve within a threshold, AUC$_\\alpha$, given the fact that the traditional evaluation measure (mean error) is very sensitive to big alignment error. 2) we extend the 300W database with more practical face detections to make fair comparison possible. 3) we carry out face alignment sensitivity analysis w.r.t. face detection, on both synthetic and real data, using both off-the-shelf and re-retrained models. 4) we study factors that are particularly important to achieve good performance and provide suggestions for practical applications. Most of the conclusions drawn from our comparative analysis cannot be inferred from the original publications.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05065",
        "title": "Proposal Flow",
        "authors": [
            "Bumsub Ham",
            "Minsu Cho",
            "Cordelia Schmid",
            "Jean Ponce"
        ],
        "abstract": "Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout.~Semantic flow methods are designed to handle images depicting different instances of the same object or scene category. We introduce a novel approach to semantic flow, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing semantic flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that proposal flow can effectively be transformed into a conventional dense flow field. We introduce a new dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. We use this benchmark to compare different matching algorithms, object proposals, and region features within proposal flow, to the state of the art in semantic flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing semantic flow methods in various settings.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05067",
        "title": "Joint Training of Generic CNN-CRF Models with Stochastic Optimization",
        "authors": [
            "Alexander Kirillov",
            "Dmitrij Schlesinger",
            "Shuai Zheng",
            "Bogdan Savchynskyy",
            "Philip H.S. Torr",
            "Carsten Rother"
        ],
        "abstract": "We propose a new CNN-CRF end-to-end learning framework, which is based on joint stochastic optimization with respect to both Convolutional Neural Network (CNN) and Conditional Random Field (CRF) parameters. While stochastic gradient descent is a standard technique for CNN training, it was not used for joint models so far. We show that our learning method is (i) general, i.e. it applies to arbitrary CNN and CRF architectures and potential functions; (ii) scalable, i.e. it has a low memory footprint and straightforwardly parallelizes on GPUs; (iii) easy in implementation. Additionally, the unified CNN-CRF optimization approach simplifies a potential hardware implementation. We empirically evaluate our method on the task of semantic labeling of body parts in depth images and show that it compares favorably to competing techniques.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05084",
        "title": "Understanding learned CNN features through Filter Decoding with Substitution",
        "authors": [
            "Ivet Rafegas",
            "Maria Vanrell"
        ],
        "abstract": "In parallel with the success of CNNs to solve vision problems, there is a growing interest in developing methodologies to understand and visualize the internal representations of these networks. How the responses of a trained CNN encode the visual information is a fundamental question both for computer and human vision research. Image representations provided by the first convolutional layer as well as the resolution change provided by the max-polling operation are easy to understand, however, as soon as a second and further convolutional layers are added in the representation, any intuition is lost. A usual way to deal with this problem has been to define deconvolutional networks that somehow allow to explore the internal representations of the most important activations towards the image space, where deconvolution is assumed as a convolution with the transposed filter. However, this assumption is not the best approximation of an inverse convolution. In this paper we propose a new assumption based on filter substitution to reverse the encoding of a convolutional layer. This provides us with a new tool to directly visualize any CNN single neuron as a filter in the first layer, this is in terms of the image space.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05122",
        "title": "Adversarial Manipulation of Deep Representations",
        "authors": [
            "Sara Sabour",
            "Yanshuai Cao",
            "Fartash Faghri",
            "David J. Fleet"
        ],
        "abstract": "We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05169",
        "title": "Nonlinear Local Metric Learning for Person Re-identification",
        "authors": [
            "Siyuan Huang",
            "Jiwen Lu",
            "Jie Zhou",
            "Anil K. Jain"
        ],
        "abstract": "Person re-identification aims at matching pedestrians observed from non-overlapping camera views. Feature descriptor and metric learning are two significant problems in person re-identification. A discriminative metric learning method should be capable of exploiting complex nonlinear transformations due to the large variations in feature space. In this paper, we propose a nonlinear local metric learning (NLML) method to improve the state-of-the-art performance of person re-identification on public datasets. Motivated by the fact that local metric learning has been introduced to handle the data which varies locally and deep neural network has presented outstanding capability in exploiting the nonlinearity of samples, we utilize the merits of both local metric learning and deep neural network to learn multiple sets of nonlinear transformations. By enforcing a margin between the distances of positive pedestrian image pairs and distances of negative pairs in the transformed feature subspace, discriminative information can be effectively exploited in the developed neural networks. Our experiments show that the proposed NLML method achieves the state-of-the-art results on the widely used VIPeR, GRID, and CUHK 01 datasets.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05174",
        "title": "Cross-scale predictive dictionaries",
        "authors": [
            "Vishwanath Saragadam",
            "Xin Li",
            "Aswin Sankaranarayanan"
        ],
        "abstract": "Sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. However, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. In this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. The specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. We show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2018-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05175",
        "title": "Convolutional Models for Joint Object Categorization and Pose Estimation",
        "authors": [
            "Mohamed Elhoseiny",
            "Tarek El-Gaaly",
            "Amr Bakry",
            "Ahmed Elgammal"
        ],
        "abstract": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05197",
        "title": "Visualizing and Understanding Deep Texture Representations",
        "authors": [
            "Tsung-Yu Lin",
            "Subhransu Maji"
        ],
        "abstract": "A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent general-purpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05204",
        "title": "Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition",
        "authors": [
            "Mengyi Liu",
            "Shiguang Shan",
            "Ruiping Wang",
            "Xilin Chen"
        ],
        "abstract": "Facial expression is temporally dynamic event which can be decomposed into a set of muscle motions occurring in different facial regions over various time intervals. For dynamic expression recognition, two key issues, temporal alignment and semantics-aware dynamic representation, must be taken into account. In this paper, we attempt to solve both problems via manifold modeling of videos based on a novel mid-level representation, i.e. \\textbf{expressionlet}. Specifically, our method contains three key stages: 1) each expression video clip is characterized as a spatial-temporal manifold (STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM) is learned over all low-level features and represented as a set of local modes to statistically unify all the STMs. 3) the local modes on each STM can be instantiated by fitting to UMM, and the corresponding expressionlet is constructed by modeling the variations in each local mode. With above strategy, expression videos are naturally aligned both spatially and temporally. To enhance the discriminative power, the expressionlet-based STM representation is further processed with discriminant embedding. Our method is evaluated on four public expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, our method outperforms the known state-of-the-art by a large margin.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05234",
        "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",
        "authors": [
            "Huijuan Xu",
            "Kate Saenko"
        ],
        "abstract": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05261",
        "title": "Robust PCA via Nonconvex Rank Approximation",
        "authors": [
            "Zhao Kang",
            "Chong Peng",
            "Qiang Cheng"
        ],
        "abstract": "Numerous applications in data mining and machine learning require recovering a matrix of minimal rank. Robust principal component analysis (RPCA) is a general framework for handling this kind of problems. Nuclear norm based convex surrogate of the rank function in RPCA is widely investigated. Under certain assumptions, it can recover the underlying true low rank matrix with high probability. However, those assumptions may not hold in real-world applications. Since the nuclear norm approximates the rank by adding all singular values together, which is essentially a $\\ell_1$-norm of the singular values, the resulting approximation error is not trivial and thus the resulting matrix estimator can be significantly biased. To seek a closer approximation and to alleviate the above-mentioned limitations of the nuclear norm, we propose a nonconvex rank approximation. This approximation to the matrix rank is tighter than the nuclear norm. To solve the associated nonconvex minimization problem, we develop an efficient augmented Lagrange multiplier based optimization algorithm. Experimental results demonstrate that our method outperforms current state-of-the-art algorithms in both accuracy and efficiency.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05284",
        "title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
        "authors": [
            "Lisa Anne Hendricks",
            "Subhashini Venugopalan",
            "Marcus Rohrbach",
            "Raymond Mooney",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05286",
        "title": "Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning",
        "authors": [
            "Oren Z. Kraus",
            "Lei Jimmy Ba",
            "Brendan Frey"
        ],
        "abstract": "Convolutional neural networks (CNN) have achieved state of the art performance on both classification and segmentation tasks. Applying CNNs to microscopy images is challenging due to the lack of datasets labeled at the single cell level. We extend the application of CNNs to microscopy image classification and segmentation using multiple instance learning (MIL). We present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using full resolution microscopy images with global labels. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. We show that training MIL CNNs end-to-end outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05292",
        "title": "Hierarchical Spatial Sum-Product Networks for Action Recognition in Still Images",
        "authors": [
            "Jinghua Wang",
            "Gang Wang"
        ],
        "abstract": "Recognizing actions from still images is popularly studied recently. In this paper, we model an action class as a flexible number of spatial configurations of body parts by proposing a new spatial SPN (Sum-Product Networks). First, we discover a set of parts in image collections via unsupervised learning. Then, our new spatial SPN is applied to model the spatial relationship and also the high-order correlations of parts. To learn robust networks, we further develop a hierarchical spatial SPN method, which models pairwise spatial relationship between parts inside sub-images and models the correlation of sub-images via extra layers of SPN. Our method is shown to be effective on two benchmark datasets.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05296",
        "title": "Towards Predicting the Likeability of Fashion Images",
        "authors": [
            "Jinghua Wang",
            "Abrar Abdul Nabi",
            "Gang Wang",
            "Chengde Wan",
            "Tian-Tsong Ng"
        ],
        "abstract": "In this paper, we propose a method for ranking fashion images to find the ones which might be liked by more people. We collect two new datasets from image sharing websites (Pinterest and Polyvore). We represent fashion images based on attributes: semantic attributes and data-driven attributes. To learn semantic attributes from limited training data, we use an algorithm on multi-task convolutional neural networks to share visual knowledge among different semantic attribute categories. To discover data-driven attributes unsupervisedly, we propose an algorithm to simultaneously discover visual clusters and learn fashion-specific feature representations. Given attributes as representations, we propose to learn a ranking SPN (sum product networks) to rank pairs of fashion images. The proposed ranking SPN can capture the high-order correlations of the attributes. We show the effectiveness of our method on our two newly collected datasets.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05298",
        "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs",
        "authors": [
            "Ashesh Jain",
            "Amir R. Zamir",
            "Silvio Savarese",
            "Ashutosh Saxena"
        ],
        "abstract": "Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05512",
        "title": "Moral Lineage Tracing",
        "authors": [
            "Florian Jug",
            "Evgeny Levinkov",
            "Corinna Blasse",
            "Eugene W. Myers",
            "Bjoern Andres"
        ],
        "abstract": "Lineage tracing, the tracking of living cells as they move and divide, is a central problem in biological image analysis. Solutions, called lineage forests, are key to understanding how the structure of multicellular organisms emerges. We propose an integer linear program (ILP) whose feasible solutions define a decomposition of each image in a sequence into cells (segmentation), and a lineage forest of cells across images (tracing). Unlike previous formulations, we do not constrain the set of decompositions, except by contracting pixels to superpixels. The main challenge, as we show, is to enforce the morality of lineages, i.e., the constraint that cells do not merge. To enforce morality, we introduce path-cut inequalities. To find feasible solutions of the NP-hard ILP, with certified bounds to the global optimum, we define efficient separation procedures and apply these as part of a branch-and-cut algorithm. We show the effectiveness of this approach by analyzing feasible solutions for real microscopy data in terms of bounds and run-time, and by their weighted edit distance to ground truth lineage forests traced by humans.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05547",
        "title": "Return of Frustratingly Easy Domain Adaptation",
        "authors": [
            "Baochen Sun",
            "Jiashi Feng",
            "Kate Saenko"
        ],
        "abstract": "Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being \"frustratingly easy\" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05607",
        "title": "Identifying the Absorption Bump with Deep Learning",
        "authors": [
            "Min Li",
            "Sudeep Gaddam",
            "Xiaolin Li",
            "Yinan Zhao",
            "Jingzhe Ma",
            "Jian Ge"
        ],
        "abstract": "The pervasive interstellar dust grains provide significant insights to understand the formation and evolution of the stars, planetary systems, and the galaxies, and may harbor the building blocks of life. One of the most effective way to analyze the dust is via their interaction with the light from background sources. The observed extinction curves and spectral features carry the size and composition information of dust. The broad absorption bump at 2175 Angstrom is the most prominent feature in the extinction curves. Traditionally, statistical methods are applied to detect the existence of the absorption bump. These methods require heavy preprocessing and the co-existence of other reference features to alleviate the influence from the noises. In this paper, we apply Deep Learning techniques to detect the broad absorption bump. We demonstrate the key steps for training the selected models and their results. The success of Deep Learning based method inspires us to generalize a common methodology for broader science discovery problems. We present our on-going work to build the DeepDis system for such kind of applications.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05616",
        "title": "Learning Structured Inference Neural Networks with Label Relations",
        "authors": [
            "Hexiang Hu",
            "Guang-Tong Zhou",
            "Zhiwei Deng",
            "Zicheng Liao",
            "Greg Mori"
        ],
        "abstract": "Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with fine-grained labels that describe major components, coarse-grained labels that depict high level abstraction or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05635",
        "title": "Competitive Multi-scale Convolution",
        "authors": [
            "Zhibin Liao",
            "Gustavo Carneiro"
        ],
        "abstract": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05643",
        "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation",
        "authors": [
            "Md Kamrul Hasan",
            "Christopher J. Pal"
        ],
        "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05666",
        "title": "Super-Resolution with Deep Convolutional Sufficient Statistics",
        "authors": [
            "Joan Bruna",
            "Pablo Sprechmann",
            "Yann LeCun"
        ],
        "abstract": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05676",
        "title": "Compositional Memory for Visual Question Answering",
        "authors": [
            "Aiwen Jiang",
            "Fang Wang",
            "Fatih Porikli",
            "Yi Li"
        ],
        "abstract": "Visual Question Answering (VQA) emerges as one of the most fascinating topics in computer vision recently. Many state of the art methods naively use holistic visual features with language features into a Long Short-Term Memory (LSTM) module, neglecting the sophisticated interaction between them. This coarse modeling also blocks the possibilities of exploring finer-grained local features that contribute to the question answering dynamically over time.\n",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05756",
        "title": "Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction",
        "authors": [
            "Hyeonwoo Noh",
            "Paul Hongsuck Seo",
            "Bohyung Han"
        ],
        "abstract": "We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05768",
        "title": "Labeled pupils in the wild: A dataset for studying pupil detection in unconstrained environments",
        "authors": [
            "Marc Tonsen",
            "Xucong Zhang",
            "Yusuke Sugano",
            "Andreas Bulling"
        ],
        "abstract": "We present labelled pupils in the wild (LPW), a novel dataset of 66 high-quality, high-speed eye region videos for the development and evaluation of pupil detection algorithms. The videos in our dataset were recorded from 22 participants in everyday locations at about 95 FPS using a state-of-the-art dark-pupil head-mounted eye tracker. They cover people with different ethnicities, a diverse set of everyday indoor and outdoor illumination environments, as well as natural gaze direction distributions. The dataset also includes participants wearing glasses, contact lenses, as well as make-up. We benchmark five state-of-the-art pupil detection algorithms on our dataset with respect to robustness and accuracy. We further study the influence of image resolution, vision aids, as well as recording location (indoor, outdoor) on pupil detection performance. Our evaluations provide valuable insights into the general pupil detection problem and allow us to identify key challenges for robust pupil detection on head-mounted eye trackers.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05788",
        "title": "From Pose to Activity: Surveying Datasets and Introducing CONVERSE",
        "authors": [
            "Michael Edwards",
            "Jingjing Deng",
            "Xianghua Xie"
        ],
        "abstract": "We present a review on the current state of publicly available datasets within the human action recognition community; highlighting the revival of pose based methods and recent progress of understanding person-person interaction modeling. We categorize datasets regarding several key properties for usage as a benchmark dataset; including the number of class labels, ground truths provided, and application domain they occupy. We also consider the level of abstraction of each dataset; grouping those that present actions, interactions and higher level semantic activities. The survey identifies key appearance and pose based datasets, noting a tendency for simplistic, emphasized, or scripted action classes that are often readily definable by a stable collection of sub-action gestures. There is a clear lack of datasets that provide closely related actions, those that are not implicitly identified via a series of poses and gestures, but rather a dynamic set of interactions. We therefore propose a novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors. The intention is to provide events that are constructed from numerous primitive actions, interactions and motions, over a period of time; providing a set of subtle action classes that are more representative of the real world, and a challenge to currently developed recognition methodologies. We believe this is among one of the first datasets devoted to conversational interaction classification using 3D pose features and the attributed papers show this task is indeed possible. The full dataset is made publicly available to the research community at ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05879",
        "title": "Particular object retrieval with integral max-pooling of CNN activations",
        "authors": [
            "Giorgos Tolias",
            "Ronan Sicre",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05904",
        "title": "Dense Human Body Correspondences Using Convolutional Networks",
        "authors": [
            "Lingyu Wei",
            "Qixing Huang",
            "Duygu Ceylan",
            "Etienne Vouga",
            "Hao Li"
        ],
        "abstract": "We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods that require full watertight 3D geometry.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05914",
        "title": "Collecting and Annotating the Large Continuous Action Dataset",
        "authors": [
            "Daniel Paul Barrett",
            "Ran Xu",
            "Haonan Yu",
            "Jeffrey Mark Siskind"
        ],
        "abstract": "We make available to the community a new dataset to support action-recognition research. This dataset is different from prior datasets in several key ways. It is significantly larger. It contains streaming video with long segments containing multiple action occurrences that often overlap in space and/or time. All actions were filmed in the same collection of backgrounds so that background gives little clue as to action class. We had five humans replicate the annotation of temporal extent of action occurrences labeled with their class and measured a surprisingly low level of intercoder agreement. A baseline experiment shows that recent state-of-the-art methods perform poorly on this dataset. This suggests that this will be a challenging dataset to foster advances in action-recognition research. This manuscript serves to describe the novel content and characteristics of the LCA dataset, present the design decisions made when filming the dataset, and document the novel methods employed to annotate the dataset.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05960",
        "title": "ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering",
        "authors": [
            "Kan Chen",
            "Jiang Wang",
            "Liang-Chieh Chen",
            "Haoyuan Gao",
            "Wei Xu",
            "Ram Nevatia"
        ],
        "abstract": "We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image related natural language question, VQA generates the natural language answer for the question. Generating the correct answers requires the model's attention to focus on the regions corresponding to the question, because different questions inquire about the attributes of different image regions. We introduce an attention based configurable convolutional neural network (ABC-CNN) to learn such question-guided attention. ABC-CNN determines an attention map for an image-question pair by convolving the image feature map with configurable convolutional kernels derived from the question's semantics. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods on these datasets. The question-guided attention generated by ABC-CNN is also shown to reflect the regions that are highly relevant to the questions.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06015",
        "title": "Active Object Localization with Deep Reinforcement Learning",
        "authors": [
            "Juan C. Caicedo",
            "Svetlana Lazebnik"
        ],
        "abstract": "We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06040",
        "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition",
        "authors": [
            "Moustafa Ibrahim",
            "Srikanth Muralidharan",
            "Zhiwei Deng",
            "Arash Vahdat",
            "Greg Mori"
        ],
        "abstract": "In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06062",
        "title": "Compact Bilinear Pooling",
        "authors": [
            "Yang Gao",
            "Oscar Beijbom",
            "Ning Zhang",
            "Trevor Darrell"
        ],
        "abstract": "Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06070",
        "title": "Structured Depth Prediction in Challenging Monocular Video Sequences",
        "authors": [
            "Miaomiao Liu",
            "Mathieu Salzmann",
            "Xuming He"
        ],
        "abstract": "In this paper, we tackle the problem of estimating the depth of a scene from a monocular video sequence. In particular, we handle challenging scenarios, such as non-translational camera motion and dynamic scenes, where traditional structure from motion and motion stereo methods do not apply. To this end, we first study the problem of depth estimation from a single image. In this context, we exploit the availability of a pool of images for which the depth is known, and formulate monocular depth estimation as a discrete-continuous optimization problem, where the continuous variables encode the depth of the superpixels in the input image, and the discrete ones represent relationships between neighboring superpixels. The solution to this discrete-continuous optimization problem is obtained by performing inference in a graphical model using particle belief propagation. To handle video sequences, we then extend our single image model to a two-frame one that naturally encodes short-range temporal consistency and inherently handles dynamic objects. Based on the prediction of this model, we then introduce a fully-connected pairwise CRF that accounts for longer range spatio-temporal interactions throughout a video. We demonstrate the effectiveness of our model in both the indoor and outdoor scenarios.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06078",
        "title": "Learning Deep Structure-Preserving Image-Text Embeddings",
        "authors": [
            "Liwei Wang",
            "Yin Li",
            "Svetlana Lazebnik"
        ],
        "abstract": "This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06085",
        "title": "Variable Rate Image Compression with Recurrent Neural Networks",
        "authors": [
            "George Toderici",
            "Sean M. O'Malley",
            "Sung Jin Hwang",
            "Damien Vincent",
            "David Minnen",
            "Shumeet Baluja",
            "Michele Covell",
            "Rahul Sukthankar"
        ],
        "abstract": "A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06103",
        "title": "Principled Parallel Mean-Field Inference for Discrete Random Fields",
        "authors": [
            "Pierre Baqu\u00e9",
            "Timur Bagautdinov",
            "Fran\u00e7ois Fleuret",
            "Pascal Fua"
        ],
        "abstract": "Mean-field variational inference is one of the most popular approaches to inference in discrete random fields. Standard mean-field optimization is based on coordinate descent and in many situations can be impractical. Thus, in practice, various parallel techniques are used, which either rely on ad-hoc smoothing with heuristically set parameters, or put strong constraints on the type of models. In this paper, we propose a novel proximal gradient-based approach to optimizing the variational objective. It is naturally parallelizable and easy to implement. We prove its convergence, and then demonstrate that, in practice, it yields faster convergence and often finds better optima than more traditional mean-field optimization techniques. Moreover, our method is less sensitive to the choice of parameters.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06106",
        "title": "Quantitative Analysis of Particles Segregation",
        "authors": [
            "Ting Peng",
            "Aiping Qu",
            "Xiaoling Wang"
        ],
        "abstract": "Segregation is a popular phenomenon. It has considerable effects on material performance. To the author's knowledge, there is still no automated objective quantitative indicator for segregation. In order to full fill this task, segregation of particles is analyzed. Edges of the particles are extracted from the digital picture. Then, the whole picture of particles is splintered to small rectangles with the same shape. Statistical index of the edges in each rectangle is calculated. Accordingly, segregation between the indexes corresponding to the rectangles is evaluated. The results show coincident with subjective evaluated results. Further more, it can be implemented as an automated system, which would facilitate the materials quality control mechanism during production process.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06147",
        "title": "Coreset-Based Adaptive Tracking",
        "authors": [
            "Abhimanyu Dubey",
            "Nikhil Naik",
            "Dan Raviv",
            "Rahul Sukthankar",
            "Ramesh Raskar"
        ],
        "abstract": "We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a 'coreset' representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06181",
        "title": "What Players do with the Ball: A Physically Constrained Interaction Modeling",
        "authors": [
            "Andrii Maksai",
            "Xinchao Wang",
            "Pascal Fua"
        ],
        "abstract": "Tracking the ball is critical for video-based analysis of team sports. However, it is difficult, especially in low-resolution images, due to the small size of the ball, its speed that creates motion blur, and its often being occluded by players. In this paper, we propose a generic and principled approach to modeling the interaction between the ball and the players while also imposing appropriate physical constraints on the ball's trajectory. We show that our approach, formulated in terms of a Mixed Integer Program, is more robust and more accurate than several state-of-the-art approaches on real-life volleyball, basketball, and soccer sequences.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06214",
        "title": "Automatically selecting inference algorithms for discrete energy minimisation",
        "authors": [
            "Paul Henderson",
            "Vittorio Ferrari"
        ],
        "abstract": "Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms perform better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark, containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96% of variables the same as the best available algorithm.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06233",
        "title": "Towards Open Set Deep Networks",
        "authors": [
            "Abhijit Bendale",
            "Terrance Boult"
        ],
        "abstract": "Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of \"fooling\" and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06276",
        "title": "Faster method for Deep Belief Network based Object classification using DWT",
        "authors": [
            "Saurabh Sihag",
            "Pranab Kumar Dutta"
        ],
        "abstract": "A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06314",
        "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks",
        "authors": [
            "Stefan Lee",
            "Senthil Purushwalkam",
            "Michael Cogswell",
            "David Crandall",
            "Dhruv Batra"
        ],
        "abstract": "Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher \"oracle\" accuracies than classical ensembles.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06316",
        "title": "face anti-spoofing based on color texture analysis",
        "authors": [
            "Zinelabidine Boulkenafet",
            "Jukka Komulainen",
            "Abdenour Hadid"
        ],
        "abstract": "Research on face spoofing detection has mainly been focused on analyzing the luminance of the face images, hence discarding the chrominance information which can be useful for discriminating fake faces from genuine ones. In this work, we propose a new face anti-spoofing method based on color texture analysis. We analyze the joint color-texture information from the luminance and the chrominance channels using a color local binary pattern descriptor. More specifically, the feature histograms are extracted from each image band separately. Extensive experiments on two benchmark datasets, namely CASIA face anti-spoofing and Replay-Attack databases, showed excellent results compared to the state-of-the-art. Most importantly, our inter-database evaluation depicts that the proposed approach showed very promising generalization capabilities.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06394",
        "title": "Geodesics of learned representations",
        "authors": [
            "Olivier J. H\u00e9naff",
            "Eero P. Simoncelli"
        ],
        "abstract": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06408",
        "title": "Feature-based Attention in Convolutional Neural Networks",
        "authors": [
            "Grace W. Lindsay"
        ],
        "abstract": "Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the \"feature similarity gain model\") is effective in increasing performance.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06425",
        "title": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks",
        "authors": [
            "Quan Gan",
            "Qipeng Guo",
            "Zheng Zhang",
            "Kyunghyun Cho"
        ],
        "abstract": "In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06432",
        "title": "Delving Deeper into Convolutional Networks for Learning Video Representations",
        "authors": [
            "Nicolas Ballas",
            "Li Yao",
            "Chris Pal",
            "Aaron Courville"
        ],
        "abstract": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \"percepts\" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations.\n",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06437",
        "title": "A convnet for non-maximum suppression",
        "authors": [
            "Jan Hosang",
            "Rodrigo Benenson",
            "Bernt Schiele"
        ],
        "abstract": "Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06449",
        "title": "Learning to decompose for object detection and instance segmentation",
        "authors": [
            "Eunbyung Park",
            "Alexander C. Berg"
        ],
        "abstract": "Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in the KITTI benchmark dataset. The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06452",
        "title": "Deep Metric Learning via Lifted Structured Feature Embedding",
        "authors": [
            "Hyun Oh Song",
            "Yu Xiang",
            "Stefanie Jegelka",
            "Silvio Savarese"
        ],
        "abstract": "Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06457",
        "title": "DOC: Deep OCclusion Estimation From a Single Image",
        "authors": [
            "Peng Wang",
            "Alan Yuille"
        ],
        "abstract": "Recovering the occlusion relationships between objects is a fundamental human visual ability which yields important information about the 3D world. In this paper we propose a deep network architecture, called DOC, which acts on a single image, detects object boundaries and estimates the border ownership (i.e. which side of the boundary is foreground and which is background). We represent occlusion relations by a binary edge map, to indicate the object boundary, and an occlusion orientation variable which is tangential to the boundary and whose direction specifies border ownership by a left-hand rule. We train two related deep convolutional neural networks, called DOC, which exploit local and non-local image cues to estimate this representation and hence recover occlusion relations. In order to train and test DOC we construct a large-scale instance occlusion boundary dataset using PASCAL VOC images, which we call the PASCAL instance occlusion dataset (PIOD). This contains 10,000 images and hence is two orders of magnitude larger than existing occlusion datasets for outdoor images. We test two variants of DOC on PIOD and on the BSDS occlusion dataset and show they outperform state-of-the-art methods. Finally, we perform numerous experiments investigating multiple settings of DOC and transfer between BSDS and PIOD, which provides more insights for further study of occlusion estimation.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06489",
        "title": "A Simple Hierarchical Pooling Data Structure for Loop Closure",
        "authors": [
            "Xiaohan Fei",
            "Konstantine Tsotsos",
            "Stefano Soatto"
        ],
        "abstract": "We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2018-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06494",
        "title": "Bidirectional Warping of Active Appearance Model",
        "authors": [
            "Ali Mollahosseini",
            "Mohammad H. Mahoor"
        ],
        "abstract": "Active Appearance Model (AAM) is a commonly used method for facial image analysis with applications in face identification and facial expression recognition. This paper proposes a new approach based on image alignment for AAM fitting called bidirectional warping. Previous approaches warp either the input image or the appearance template. We propose to warp both the input image, using incremental update by an affine transformation, and the appearance template, using an inverse compositional approach. Our experimental results on Multi-PIE face database show that the bidirectional approach outperforms state-of-the-art inverse compositional fitting approaches in extracting landmark points of faces with shape and pose variations.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06522",
        "title": "Integrating Deep Features for Material Recognition",
        "authors": [
            "Yan Zhang",
            "Mete Ozay",
            "Xing Liu",
            "Takayuki Okatani"
        ],
        "abstract": "We propose a method for integration of features extracted using deep representations of Convolutional Neural Networks (CNNs) each of which is learned using a different image dataset of objects and materials for material recognition. Given a set of representations of multiple pre-trained CNNs, we first compute activations of features using the representations on the images to select a set of samples which are best represented by the features. Then, we measure the uncertainty of the features by computing the entropy of class distributions for each sample set. Finally, we compute the contribution of each feature to representation of classes for feature selection and integration. We examine the proposed method on three benchmark datasets for material recognition. Experimental results show that the proposed method achieves state-of-the-art performance by integrating deep features. Additionally, we introduce a new material dataset called EFMD by extending Flickr Material Database (FMD). By the employment of the EFMD with transfer learning for updating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD dataset which is close to human performance that is 84.9%.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06523",
        "title": "WIDER FACE: A Face Detection Benchmark",
        "authors": [
            "Shuo Yang",
            "Ping Luo",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated. Dataset can be downloaded at: ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06530",
        "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications",
        "authors": [
            "Yong-Deok Kim",
            "Eunhyeok Park",
            "Sungjoo Yoo",
            "Taelim Choi",
            "Lu Yang",
            "Dongjun Shin"
        ],
        "abstract": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-02-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06545",
        "title": "A dense subgraph based algorithm for compact salient image region detection",
        "authors": [
            "Souradeep Chakraborty",
            "Pabitra Mitra"
        ],
        "abstract": "We present an algorithm for graph based saliency computation that utilizes the underlying dense subgraphs in finding visually salient regions in an image. To compute the salient regions, the model first obtains a saliency map using random walks on a Markov chain. Next, k-dense subgraphs are detected to further enhance the salient regions in the image. Dense subgraphs convey more information about local graph structure than simple centrality measures. To generate the Markov chain, intensity and color features of an image in addition to region compactness is used. For evaluating the proposed model, we do extensive experiments on benchmark image data sets. The proposed method performs comparable to well-known algorithms in salient region detection.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06575",
        "title": "ElSe: Ellipse Selection for Robust Pupil Detection in Real-World Environments",
        "authors": [
            "Wolfgang Fuhl",
            "Thiago C. Santini",
            "Thomas Kuebler",
            "Enkelejda Kasneci"
        ],
        "abstract": "Fast and robust pupil detection is an essential prerequisite for video-based eye-tracking in real-world settings. Several algorithms for image-based pupil detection have been proposed, their applicability is mostly limited to laboratory conditions. In realworld scenarios, automated pupil detection has to face various challenges, such as illumination changes, reflections (on glasses), make-up, non-centered eye recording, and physiological eye characteristics. We propose ElSe, a novel algorithm based on ellipse evaluation of a filtered edge image. We aim at a robust, resource-saving approach that can be integrated in embedded architectures e.g. driving. The proposed algorithm was evaluated against four state-of-the-art methods on over 93,000 hand-labeled images from which 55,000 are new images contributed by this work. On average, the proposed method achieved a 14.53% improvement on the detection rate relative to the best state-of-the-art performer. download:",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06586",
        "title": "Crowd Behavior Analysis: A Review where Physics meets Biology",
        "authors": [
            "Ven Jyn Kok",
            "Mei Kuan Lim",
            "Chee Seng Chan"
        ],
        "abstract": "Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06627",
        "title": "Towards Arbitrary-View Face Alignment by Recommendation Trees",
        "authors": [
            "Shizhan Zhu",
            "Cheng Li",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Learning to simultaneously handle face alignment of arbitrary views, e.g. frontal and profile views, appears to be more challenging than we thought. The difficulties lay in i) accommodating the complex appearance-shape relations exhibited in different views, and ii) encompassing the varying landmark point sets due to self-occlusion and different landmark protocols. Most existing studies approach this problem via training multiple viewpoint-specific models, and conduct head pose estimation for model selection. This solution is intuitive but the performance is highly susceptible to inaccurate head pose estimation. In this study, we address this shortcoming through learning an Ensemble of Model Recommendation Trees (EMRT), which is capable of selecting optimal model configuration without prior head pose estimation. The unified framework seamlessly handles different viewpoints and landmark protocols, and it is trained by optimising directly on landmark locations, thus yielding superior results on arbitrary-view face alignment. This is the first study that performs face alignment on the full AFLWdataset with faces of different views including profile view. State-of-the-art performances are also reported on MultiPIE and AFW datasets containing both frontaland profile-view faces.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06645",
        "title": "DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation",
        "authors": [
            "Leonid Pishchulin",
            "Eldar Insafutdinov",
            "Siyu Tang",
            "Bjoern Andres",
            "Mykhaylo Andriluka",
            "Peter Gehler",
            "Bernt Schiele"
        ],
        "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06653",
        "title": "Recurrent Semi-supervised Classification and Constrained Adversarial Generation with Motion Capture Data",
        "authors": [
            "F\u00e9lix G. Harvey",
            "Julien Roy",
            "David Kanaa",
            "Christopher Pal"
        ],
        "abstract": "We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2018-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06654",
        "title": "Tracklet Association by Online Target-Specific Metric Learning and Coherent Dynamics Estimation",
        "authors": [
            "Bing Wang",
            "Gang Wang",
            "Kap Luk Chan",
            "Li Wang"
        ],
        "abstract": "In this paper, we present a novel method based on online target-specific metric learning and coherent dynamics estimation for tracklet (track fragment) association by network flow optimization in long-term multi-person tracking. Our proposed framework aims to exploit appearance and motion cues to prevent identity switches during tracking and to recover missed detections. Furthermore, target-specific metrics (appearance cue) and motion dynamics (motion cue) are proposed to be learned and estimated online, i.e. during the tracking process. Our approach is effective even when such cues fail to identify or follow the target due to occlusions or object-to-object interactions. We also propose to learn the weights of these two tracking cues to handle the difficult situations, such as severe occlusions and object-to-object interactions effectively. Our method has been validated on several public datasets and the experimental results show that it outperforms several state-of-the-art tracking methods.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06674",
        "title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation",
        "authors": [
            "Anirudh Goyal",
            "Marius Leordeanu"
        ],
        "abstract": "Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06676",
        "title": "Personalizing Human Video Pose Estimation",
        "authors": [
            "James Charles",
            "Tomas Pfister",
            "Derek Magee",
            "David Hogg",
            "Andrew Zisserman"
        ],
        "abstract": "We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos. We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet. Our method outperforms the state of the art (including top ConvNet methods) by a large margin on two standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-06-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06681",
        "title": "Deep End2End Voxel2Voxel Prediction",
        "authors": [
            "Du Tran",
            "Lubomir Bourdev",
            "Rob Fergus",
            "Lorenzo Torresani",
            "Manohar Paluri"
        ],
        "abstract": "Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods.\n",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06692",
        "title": "Direct Prediction of 3D Body Poses from Motion Compensated Sequences",
        "authors": [
            "Bugra Tekin",
            "Artem Rozantsev",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people. Previous approaches typically compute candidate poses in individual frames and then link them in a post-processing step to resolve ambiguities. By contrast, we directly regress from a spatio-temporal volume of bounding boxes to a 3D pose in the central frame.\n",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06702",
        "title": "Multi-view 3D Models from Single Images with a Convolutional Network",
        "authors": [
            "Maxim Tatarchenko",
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "abstract": "We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06704",
        "title": "Semantic Diversity versus Visual Diversity in Visual Dictionaries",
        "authors": [
            "Ot\u00e1vio A. B. Penatti",
            "Sandra Avila",
            "Eduardo Valle",
            "Ricardo da S. Torres"
        ],
        "abstract": "Visual dictionaries are a critical component for image classification/retrieval systems based on the bag-of-visual-words (BoVW) model. Dictionaries are usually learned without supervision from a training set of images sampled from the collection of interest. However, for large, general-purpose, dynamic image collections (e.g., the Web), obtaining a representative sample in terms of semantic concepts is not straightforward. In this paper, we evaluate the impact of semantics in the dictionary quality, aiming at verifying the importance of semantic diversity in relation visual diversity for visual dictionaries. In the experiments, we vary the amount of classes used for creating the dictionary and then compute different BoVW descriptors, using multiple codebook sizes and different coding and pooling methods (standard BoVW and Fisher Vectors). Results for image classification show that as visual dictionaries are based on low-level visual appearances, visual diversity is more important than semantic diversity. Our conclusions open the opportunity to alleviate the burden in generating visual dictionaries as we need only a visually diverse set of images instead of the whole collection to create a good dictionary.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06728",
        "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning",
        "authors": [
            "Natalia Neverova",
            "Christian Wolf",
            "Florian Nebout",
            "Graham Taylor"
        ],
        "abstract": "We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2017-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06739",
        "title": "Superpixel Convolutional Networks using Bilateral Inceptions",
        "authors": [
            "Raghudeep Gadde",
            "Varun Jampani",
            "Martin Kiefel",
            "Daniel Kappler",
            "Peter V. Gehler"
        ],
        "abstract": "In this paper we propose a CNN architecture for semantic image segmentation. We introduce a new 'bilateral inception' module that can be inserted in existing CNN architectures and performs bilateral filtering, at multiple feature-scales, between superpixels in an image. The feature spaces for bilateral filtering and other parameters of the module are learned end-to-end using standard backpropagation techniques. The bilateral inception module addresses two issues that arise with general CNN segmentation architectures. First, this module propagates information between (super) pixels while respecting image edges, thus using the structured information of the problem for improved results. Second, the layer recovers a full resolution segmentation result from the lower resolution solution of a CNN. In the experiments, we modify several existing CNN architectures by inserting our inception module between the last CNN (1x1 convolution) layers. Empirical results on three different datasets show reliable improvements not only in comparison to the baseline networks, but also in comparison to several dense-pixel prediction techniques such as CRFs, while being competitive in time.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06744",
        "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification",
        "authors": [
            "Yani Ioannou",
            "Duncan Robertson",
            "Jamie Shotton",
            "Roberto Cipolla",
            "Antonio Criminisi"
        ],
        "abstract": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06746",
        "title": "Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank",
        "authors": [
            "Corey Lynch",
            "Kamelia Aryafar",
            "Josh Attenberg"
        ],
        "abstract": "Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item's text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06783",
        "title": "Recognizing Activities of Daily Living with a Wrist-mounted Camera",
        "authors": [
            "Katsunori Ohnishi",
            "Atsushi Kanehira",
            "Asako Kanezaki",
            "Tatsuya Harada"
        ],
        "abstract": "We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person wearable camera. Handled objects are crucially important for egocentric ADL recognition. For specific examination of objects related to users' actions separately from other objects in an environment, many previous works have addressed the detection of handled objects in images captured from head-mounted and chest-mounted cameras. Nevertheless, detecting handled objects is not always easy because they tend to appear small in images. They can be occluded by a user's body. As described herein, we mount a camera on a user's wrist. A wrist-mounted camera can capture handled objects at a large scale, and thus it enables us to skip object detection process. To compare a wrist-mounted camera and a head-mounted camera, we also develop a novel and publicly available dataset that includes videos and annotations of daily activities captured simultaneously by both cameras. Additionally, we propose a discriminative video representation that retains spatial and temporal information after encoding frame descriptors extracted by Convolutional Neural Networks (CNN).\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06789",
        "title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition",
        "authors": [
            "Jonathan Krause",
            "Benjamin Sapp",
            "Andrew Howard",
            "Howard Zhou",
            "Alexander Toshev",
            "Tom Duerig",
            "James Philbin",
            "Li Fei-Fei"
        ],
        "abstract": "Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-10-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06815",
        "title": "An Immersive Telepresence System using RGB-D Sensors and Head Mounted Display",
        "authors": [
            "Xinzhong Lu",
            "Ju Shen",
            "Saverio Perugini",
            "Jianjun Yang"
        ],
        "abstract": "We present a tele-immersive system that enables people to interact with each other in a virtual world using body gestures in addition to verbal communication. Beyond the obvious applications, including general online conversations and gaming, we hypothesize that our proposed system would be particularly beneficial to education by offering rich visual contents and interactivity. One distinct feature is the integration of egocentric pose recognition that allows participants to use their gestures to demonstrate and manipulate virtual objects simultaneously. This functionality enables the instructor to ef- fectively and efficiently explain and illustrate complex concepts or sophisticated problems in an intuitive manner. The highly interactive and flexible environment can capture and sustain more student attention than the traditional classroom setting and, thus, delivers a compelling experience to the students. Our main focus here is to investigate possible solutions for the system design and implementation and devise strategies for fast, efficient computation suitable for visual data processing and network transmission. We describe the technique and experiments in details and provide quantitative performance results, demonstrating our system can be run comfortably and reliably for different application scenarios. Our preliminary results are promising and demonstrate the potential for more compelling directions in cyberlearning.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06830",
        "title": "Ground-truth dataset and baseline evaluations for image base-detail separation algorithms",
        "authors": [
            "Xuan Dong",
            "Boyan Bonev",
            "Weixin Li",
            "Weichao Qiu",
            "Xianjie Chen",
            "Alan Yuille"
        ],
        "abstract": "Base-detail separation is a fundamental computer vision problem consisting of modeling a smooth base layer with the coarse structures, and a detail layer containing the texture-like structures. One of the challenges of estimating the base is to preserve sharp boundaries between objects or parts to avoid halo artifacts. Many methods have been proposed to address this problem, but there is no ground-truth dataset of real images for quantitative evaluation. We proposed a procedure to construct such a dataset, and provide two datasets: Pascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images, respectively. Our assumption is that the base is piecewise smooth and we label the appearance of each piece by a polynomial model. The pieces are objects and parts of objects, obtained from human annotations. Finally, we proposed a way to evaluate methods with our base-detail ground-truth and we compared the performances of seven state-of-the-art algorithms.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06834",
        "title": "Fidelity-Naturalness Evaluation of Single Image Super Resolution",
        "authors": [
            "Xuan Dong",
            "Yu Zhu",
            "Weixin Li",
            "Lingxi Xie",
            "Alex Wong",
            "Alan Yuille"
        ],
        "abstract": "We study the problem of evaluating super resolution methods. Traditional evaluation methods usually judge the quality of super resolved images based on a single measure of their difference with the original high resolution images. In this paper, we proposed to use both fidelity (the difference with original images) and naturalness (human visual perception of super resolved images) for evaluation. For fidelity evaluation, a new metric is proposed to solve the bias problem of traditional evaluation. For naturalness evaluation, we let humans label preference of super resolution results using pair-wise comparison, and test the correlation between human labeling results and image quality assessment metrics' outputs. Experimental results show that our fidelity-naturalness method is better than the traditional evaluation method for super resolution methods, which could help future research on single-image super resolution.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06838",
        "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets",
        "authors": [
            "Takuya Narihira",
            "Damian Borth",
            "Stella X. Yu",
            "Karl Ni",
            "Trevor Darrell"
        ],
        "abstract": "We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as \"cute baby\". To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06853",
        "title": "TransCut: Transparent Object Segmentation from a Light-Field Image",
        "authors": [
            "Yichao Xu",
            "Hajime Nagahara",
            "Atsushi Shimada",
            "Rin-ichiro Taniguchi"
        ],
        "abstract": "The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06856",
        "title": "Data-dependent Initializations of Convolutional Neural Networks",
        "authors": [
            "Philipp Kr\u00e4henb\u00fchl",
            "Carl Doersch",
            "Jeff Donahue",
            "Trevor Darrell"
        ],
        "abstract": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06860",
        "title": "Convex Sparse Spectral Clustering: Single-view to Multi-view",
        "authors": [
            "Canyi Lu",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $U^\\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $UU^\\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $UU^\\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \\canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2018-05-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06881",
        "title": "Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net",
        "authors": [
            "Fangting Xia",
            "Peng Wang",
            "Liang-Chieh Chen",
            "Alan L. Yuille"
        ],
        "abstract": "Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two \"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively \"zoom\" (resize) predicted image regions into their proper scales to refine the parsing.\n",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06911",
        "title": "Screen Content Image Segmentation Using Sparse-Smooth Decomposition",
        "authors": [
            "Shervin Minaee",
            "Amirali Abdolrashidi",
            "Yao Wang"
        ],
        "abstract": "Sparse decomposition has been extensively used for different applications including signal compression and denoising and document analysis. In this paper, sparse decomposition is used for image segmentation. The proposed algorithm separates the background and foreground using a sparse-smooth decomposition technique such that the smooth and sparse components correspond to the background and foreground respectively. This algorithm is tested on several test images from HEVC test sequences and is shown to have superior performance over other methods, such as the hierarchical k-means clustering in DjVu. This segmentation algorithm can also be used for text extraction, video compression and medical image segmentation.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06919",
        "title": "Semantic Segmentation of Colon Glands with Deep Convolutional Neural Networks and Total Variation Segmentation",
        "authors": [
            "Philipp Kainz",
            "Michael Pfeiffer",
            "Martin Urschler"
        ],
        "abstract": "Segmentation of histopathology sections is an ubiquitous requirement in digital pathology and due to the large variability of biological tissue, machine learning techniques have shown superior performance over standard image processing methods. As part of the GlaS@MICCAI2015 colon gland segmentation challenge, we present a learning-based algorithm to segment glands in tissue of benign and malignant colorectal cancer. Images are preprocessed according to the Hematoxylin-Eosin staining protocol and two deep convolutional neural networks (CNN) are trained as pixel classifiers. The CNN predictions are then regularized using a figure-ground segmentation based on weighted total variation to produce the final segmentation result. On two test sets, our approach achieves a tissue classification accuracy of 98% and 94%, making use of the inherent capability of our system to distinguish between benign and malignant tissue.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2017-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06936",
        "title": "Real-Time Anomaly Detection and Localization in Crowded Scenes",
        "authors": [
            "Mohammad Sabokrou",
            "Mahmood Fathy",
            "Mojtaba Hosseini",
            "Reinhard Klette"
        ],
        "abstract": "In this paper, we propose a method for real-time anomaly detection and localization in crowded scenes. Each video is defined as a set of non-overlapping cubic patches, and is described using two local and global descriptors. These descriptors capture the video properties from different aspects. By incorporating simple and cost-effective Gaussian classifiers, we can distinguish normal activities and anomalies in videos. The local and global features are based on structure similarity between adjacent patches and the features learned in an unsupervised way, using a sparse auto- encoder. Experimental results show that our algorithm is comparable to a state-of-the-art procedure on UCSD ped2 and UMN benchmarks, but even more time-efficient. The experiments confirm that our system can reliably detect and localize anomalies as soon as they happen in a video.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06973",
        "title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources",
        "authors": [
            "Qi Wu",
            "Peng Wang",
            "Chunhua Shen",
            "Anthony Dick",
            "Anton van den Hengel"
        ],
        "abstract": "We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA and MS COCO-VQA and show that it produces the best reported results in both cases.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06984",
        "title": "End-to-end Learning of Action Detection from Frame Glimpses in Videos",
        "authors": [
            "Serena Yeung",
            "Olga Russakovsky",
            "Greg Mori",
            "Li Fei-Fei"
        ],
        "abstract": "In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and when to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2017-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06988",
        "title": "Learning High-level Prior with Convolutional Neural Networks for Semantic Segmentation",
        "authors": [
            "Haitian Zheng",
            "Yebin Liu",
            "Mengqi Ji",
            "Feng Wu",
            "Lu Fang"
        ],
        "abstract": "This paper proposes a convolutional neural network that can fuse high-level prior for semantic image segmentation. Motivated by humans' vision recognition system, our key design is a three-layer generative structure consisting of high-level coding, middle-level segmentation and low-level image to introduce global prior for semantic segmentation. Based on this structure, we proposed a generative model called conditional variational auto-encoder (CVAE) that can build up the links behind these three layers. These important links include an image encoder that extracts high level info from image, a segmentation encoder that extracts high level info from segmentation, and a hybrid decoder that outputs semantic segmentation from the high level prior and input image. We theoretically derive the semantic segmentation as an optimization problem parameterized by these links. Finally, the optimization problem enables us to take advantage of state-of-the-art fully convolutional network structure for the implementation of the above encoders and decoder. Experimental results on several representative datasets demonstrate our supreme performance for semantic segmentation.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07041",
        "title": "SceneNet: Understanding Real World Indoor Scenes With Synthetic Data",
        "authors": [
            "Ankur Handa",
            "Viorica Patraucean",
            "Vijay Badrinarayanan",
            "Simon Stent",
            "Roberto Cipolla"
        ],
        "abstract": "Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07053",
        "title": "ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation",
        "authors": [
            "Francesco Visin",
            "Marco Ciccone",
            "Adriana Romero",
            "Kyle Kastner",
            "Kyunghyun Cho",
            "Yoshua Bengio",
            "Matteo Matteucci",
            "Aaron Courville"
        ],
        "abstract": "We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07063",
        "title": "Fine-grained pose prediction, normalization, and recognition",
        "authors": [
            "Ning Zhang",
            "Evan Shelhamer",
            "Yang Gao",
            "Trevor Darrell"
        ],
        "abstract": "Pose variation and subtle differences in appearance are key challenges to fine-grained classification. While deep networks have markedly improved general recognition, many approaches to fine-grained recognition rely on anchoring networks to parts for better accuracy. Identifying parts to find correspondence discounts pose variation so that features can be tuned to appearance. To this end previous methods have examined how to find parts and extract pose-normalized features. These methods have generally separated fine-grained recognition into stages which first localize parts using hand-engineered and coarsely-localized proposal features, and then separately learn deep descriptors centered on inferred part positions. We unify these steps in an end-to-end trainable network supervised by keypoint locations and class labels that localizes parts by a fully convolutional network to focus the learning of feature representations for the fine-grained classification task. Experiments on the popular CUB200 dataset show that our method is state-of-the-art and suggest a continuing role for strong supervision.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07067",
        "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes",
        "authors": [
            "Satwik Kottur",
            "Ramakrishna Vedantam",
            "Jos\u00e9 M. F. Moura",
            "Devi Parikh"
        ],
        "abstract": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although \"eats\" and \"stares at\" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like \"eats\" and \"stares at\" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2016-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07069",
        "title": "Auxiliary Image Regularization for Deep CNNs with Noisy Labels",
        "authors": [
            "Samaneh Azadi",
            "Jiashi Feng",
            "Stefanie Jegelka",
            "Trevor Darrell"
        ],
        "abstract": "Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07111",
        "title": "Adapting Deep Visuomotor Representations with Weak Pairwise Constraints",
        "authors": [
            "Eric Tzeng",
            "Coline Devin",
            "Judy Hoffman",
            "Chelsea Finn",
            "Pieter Abbeel",
            "Sergey Levine",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "Real-world robotics problems often occur in domains that differ significantly from the robot's prior training environment. For many robotic control tasks, real world experience is expensive to obtain, but data is easy to collect in either an instrumented environment or in simulation. We propose a novel domain adaptation approach for robot perception that adapts visual representations learned on a large easy-to-obtain source dataset (e.g. synthetic images) to a target real-world domain, without requiring expensive manual data annotation of real world data before policy search. Supervised domain adaptation methods minimize cross-domain differences using pairs of aligned images that contain the same object or scene in both the source and target domains, thus learning a domain-invariant representation. However, they require manual alignment of such image pairs. Fully unsupervised adaptation methods rely on minimizing the discrepancy between the feature distributions across domains. We propose a novel, more powerful combination of both distribution and pairwise image alignment, and remove the requirement for expensive annotation by using weakly aligned pairs of images in the source and target domains. Focusing on adapting from simulation to real world data using a PR2 robot, we evaluate our approach on a manipulation task and show that by using weakly paired images, our method compensates for domain shift more effectively than previous techniques, enabling better robot performance in the real world.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2017-05-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07122",
        "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
        "authors": [
            "Fisher Yu",
            "Vladlen Koltun"
        ],
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07131",
        "title": "DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization",
        "authors": [
            "Jun Zhu",
            "Xianjie Chen",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper, we propose a deep part-based model (DeePM) for symbiotic object detection and semantic part localization. For this purpose, we annotate semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset, which provides information on object pose, occlusion, viewpoint and functionality. DeePM is a latent graphical model based on the state-of-the-art R-CNN framework, which learns an explicit representation of the object-part configuration with flexible type sharing (e.g., a sideview horse head can be shared by a fully-visible sideview horse and a highly truncated sideview horse with head and neck only). For comparison, we also present an end-to-end Object-Part (OP) R-CNN which learns an implicit feature representation for jointly mapping an image ROI to the object and part bounding boxes. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in detecting objects and parts. In addition, it obtains superior performance to Fast and Faster R-CNNs in object detection.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07212",
        "title": "Face Alignment Across Large Poses: A 3D Solution",
        "authors": [
            "Xiangyu Zhu",
            "Zhen Lei",
            "Xiaoming Liu",
            "Hailin Shi",
            "Stan Z. Li"
        ],
        "abstract": "Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07247",
        "title": "NetVLAD: CNN architecture for weakly supervised place recognition",
        "authors": [
            "Relja Arandjelovi\u0107",
            "Petr Gronat",
            "Akihiko Torii",
            "Tomas Pajdla",
            "Josef Sivic"
        ],
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07299",
        "title": "Rendering refraction and reflection of eyeglasses for synthetic eye tracker images",
        "authors": [
            "Thomas C. K\u00fcbler",
            "Tobias Rittig",
            "Judith Ungewiss",
            "Christina Krauss",
            "Enkelejda Kasneci"
        ],
        "abstract": "While for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential, there are many applications where simulated, synthetic eye images are of advantage. They can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended. We extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses. On the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions, on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses. We show how a polynomial function fitting calibration performs equally well with and without eyeglasses, and how a geometrical eye model behaves when exposed to glasses.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07347",
        "title": "Node Specificity in Convolutional Deep Nets Depends on Receptive Field Position and Size",
        "authors": [
            "Karl Zipser"
        ],
        "abstract": "In convolutional deep neural networks, receptive field (RF) size increases with hierarchical depth. When RF size approaches full coverage of the input image, different RF positions result in RFs with different specificity, as portions of the RF fall out of the input space. This leads to a departure from the convolutional concept of positional invariance and opens the possibility for complex forms of context specificity.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07356",
        "title": "Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation",
        "authors": [
            "Sina Honari",
            "Jason Yosinski",
            "Pascal Vincent",
            "Christopher Pal"
        ],
        "abstract": "Deep neural networks with alternating convolutional, max-pooling and decimation layers are widely used in state of the art architectures for computer vision. Max-pooling purposefully discards precise spatial information in order to create features that are more robust, and typically organized as lower resolution spatial feature maps. On some tasks, such as whole-image classification, max-pooling derived features are well suited; however, for tasks requiring precise localization, such as pixel level prediction and segmentation, max-pooling destroys exactly the information required to perform well. Precise localization may be preserved by shallow convnets without pooling but at the expense of robustness. Can we have our max-pooled multi-layered cake and eat it too? Several papers have proposed summation and concatenation based methods for combining upsampled coarse, abstract features with finer features to produce robust pixel level predictions. Here we introduce another model --- dubbed Recombinator Networks --- where coarse features inform finer features early in their formation such that finer features can make use of several layers of computation in deciding how to use coarse features. The model is trained once, end-to-end and performs better than summation-based architectures, reducing the error from the previous state of the art on two facial keypoint datasets, AFW and AFLW, by 30\\% and beating the current state-of-the-art on 300W without using extra data. We improve performance even further by adding a denoising prediction model based on a novel convnet formulation.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07386",
        "title": "Pushing the Boundaries of Boundary Detection using Deep Learning",
        "authors": [
            "Iasonas Kokkinos"
        ],
        "abstract": "In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection.\n",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07394",
        "title": "Where To Look: Focus Regions for Visual Question Answering",
        "authors": [
            "Kevin J. Shih",
            "Saurabh Singh",
            "Derek Hoiem"
        ],
        "abstract": "We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method exhibits significant improvements in answering questions such as \"what color,\" where it is necessary to evaluate a specific location, and \"what room,\" where it selectively identifies informative image regions. Our model is tested on the VQA dataset which is the largest human-annotated visual question answering dataset to our knowledge.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07404",
        "title": "Learning Visual Predictive Models of Physics for Playing Billiards",
        "authors": [
            "Katerina Fragkiadaki",
            "Pulkit Agrawal",
            "Sergey Levine",
            "Jitendra Malik"
        ],
        "abstract": "The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07409",
        "title": "Top-Down Learning for Structured Labeling with Convolutional Pseudoprior",
        "authors": [
            "Saining Xie",
            "Xun Huang",
            "Zhuowen Tu"
        ],
        "abstract": "Current practice in convolutional neural networks (CNN) remains largely bottom-up and the role of top-down process in CNN for pattern analysis and visual inference is not very clear. In this paper, we propose a new method for structured labeling by developing convolutional pseudo-prior (ConvPP) on the ground-truth labels. Our method has several interesting properties: (1) compared with classical machine learning algorithms like CRFs and Structural SVM, ConvPP automatically learns rich convolutional kernels to capture both short- and long- range contexts; (2) compared with cascade classifiers like Auto-Context, ConvPP avoids the iterative steps of learning a series of discriminative classifiers and automatically learns contextual configurations; (3) compared with recent efforts combing CNN models with CRFs and RNNs, ConvPP learns convolution in the labeling space with much improved modeling capability and less manual specification; (4) compared with Bayesian models like MRFs, ConvPP capitalizes on the rich representation power of convolution by automatically learning priors built on convolutional filters. We accomplish our task using pseudo-likelihood approximation to the prior under a novel fixed-point network structure that facilitates an end-to-end learning process. We show state-of-the-art results on sequential labeling and image labeling benchmarks.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07425",
        "title": "Real-Time Anomalous Behavior Detection and Localization in Crowded Scenes",
        "authors": [
            "Mohammad Sabokrou",
            "Mahmood Fathy",
            "Mojtaba Hosseini"
        ],
        "abstract": "In this paper, we propose an accurate and real-time anomaly detection and localization in crowded scenes, and two descriptors for representing anomalous behavior in video are proposed. We consider a video as being a set of cubic patches. Based on the low likelihood of an anomaly occurrence, and the redundancy of structures in normal patches in videos, two (global and local) views are considered for modeling the video. Our algorithm has two components, for (1) representing the patches using local and global descriptors, and for (2) modeling the training patches using a new representation. We have two Gaussian models for all training patches respect to global and local descriptors. The local and global features are based on structure similarity between adjacent patches and the features that are learned in an unsupervised way. We propose a fusion strategy to combine the two descriptors as the output of our system. Experimental results show that our algorithm performs like a state-of-the-art method on several standard datasets, but even is more time-efficient.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07497",
        "title": "Constrained Structured Regression with Convolutional Neural Networks",
        "authors": [
            "Deepak Pathak",
            "Philipp Kr\u00e4henb\u00fchl",
            "Stella X. Yu",
            "Trevor Darrell"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) have recently emerged as the dominant model in computer vision. If provided with enough training data, they predict almost any visual quantity. In a discrete setting, such as classification, CNNs are not only able to predict a label but often predict a confidence in the form of a probability distribution over the output space. In continuous regression tasks, such a probability estimate is often lacking. We present a regression framework which models the output distribution of neural networks. This output distribution allows us to infer the most likely labeling following a set of physical or modeling constraints. These constraints capture the intricate interplay between different input and output variables, and complement the output of a CNN. However, they may not hold everywhere. Our setup further allows to learn a confidence with which a constraint holds, in the form of a distribution of the constrain satisfaction. We evaluate our approach on the problem of intrinsic image decomposition, and show that constrained structured regression significantly increases the state-of-the-art.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07545",
        "title": "Constrained Deep Metric Learning for Person Re-identification",
        "authors": [
            "Hailin Shi",
            "Xiangyu Zhu",
            "Shengcai Liao",
            "Zhen Lei",
            "Yang Yang",
            "Stan Z. Li"
        ],
        "abstract": "Person re-identification aims to re-identify the probe image from a given set of images under different camera views. It is challenging due to large variations of pose, illumination, occlusion and camera view. Since the convolutional neural networks (CNN) have excellent capability of feature extraction, certain deep learning methods have been recently applied in person re-identification. However, in person re-identification, the deep networks often suffer from the over-fitting problem. In this paper, we propose a novel CNN-based method to learn a discriminative metric with good robustness to the over-fitting problem in person re-identification. Firstly, a novel deep architecture is built where the Mahalanobis metric is learned with a weight constraint. This weight constraint is used to regularize the learning, so that the learned metric has a better generalization ability. Secondly, we find that the selection of intra-class sample pairs is crucial for learning but has received little attention. To cope with the large intra-class variations in pedestrian images, we propose a novel training strategy named moderate positive mining to prevent the training process from over-fitting to the extreme samples in intra-class pairs. Experiments show that our approach significantly outperforms state-of-the-art methods on several benchmarks of person re-identification.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07571",
        "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
        "authors": [
            "Justin Johnson",
            "Andrej Karpathy",
            "Li Fei-Fei"
        ],
        "abstract": "We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07611",
        "title": "Mouse Pose Estimation From Depth Images",
        "authors": [
            "Ashwin Nanjappa",
            "Li Cheng",
            "Wei Gao",
            "Chi Xu",
            "Adam Claridge-Chang",
            "Zoe Bichler"
        ],
        "abstract": "We focus on the challenging problem of efficient mouse 3D pose estimation based on static images, and especially single depth images. We introduce an approach to discriminatively train the split nodes of trees in random forest to improve their performance on estimation of 3D joint positions of mouse. Our algorithm is capable of working with different types of rodents and with different types of depth cameras and imaging setups. In particular, it is demonstrated in this paper that when a top-mounted depth camera is combined with a bottom-mounted color camera, the final system is capable of delivering full-body pose estimation including four limbs and the paws. Empirical examinations on synthesized and real-world depth images confirm the applicability of our approach on mouse pose estimation, as well as the closely related task of part-based labeling of mouse.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07710",
        "title": "Searching for Objects using Structure in Indoor Scenes",
        "authors": [
            "Varun K. Nagaraja",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "To identify the location of objects of a particular class, a passive computer vision system generally processes all the regions in an image to finally output few regions. However, we can use structure in the scene to search for objects without processing the entire image. We propose a search technique that sequentially processes image regions such that the regions that are more likely to correspond to the query class object are explored earlier. We frame the problem as a Markov decision process and use an imitation learning algorithm to learn a search strategy. Since structure in the scene is essential for search, we work with indoor scene images as they contain both unary scene context information and object-object context in the scene. We perform experiments on the NYU-depth v2 dataset and show that the unary scene context features alone can achieve a significantly high average precision while processing only 20-25\\% of the regions for classes like bed and sofa. By considering object-object context along with the scene context features, the performance is further improved for classes like counter, lamp, pillow and sofa.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07732",
        "title": "Bayesian Identification of Fixations, Saccades, and Smooth Pursuits",
        "authors": [
            "Thiago Santini",
            "Wolfgang Fuhl",
            "Thomas K\u00fcbler",
            "Enkelejda Kasneci"
        ],
        "abstract": "Smooth pursuit eye movements provide meaningful insights and information on subject's behavior and health and may, in particular situations, disturb the performance of typical fixation/saccade classification algorithms. Thus, an automatic and efficient algorithm to identify these eye movements is paramount for eye-tracking research involving dynamic stimuli. In this paper, we propose the Bayesian Decision Theory Identification (I-BDT) algorithm, a novel algorithm for ternary classification of eye movements that is able to reliably separate fixations, saccades, and smooth pursuits in an online fashion, even for low-resolution eye trackers. The proposed algorithm is evaluated on four datasets with distinct mixtures of eye movements, including fixations, saccades, as well as straight and circular smooth pursuits; data was collected with a sample rate of 30 Hz from six subjects, totaling 24 evaluation datasets. The algorithm exhibits high and consistent performance across all datasets and movements relative to a manual annotation by a domain expert (recall: \\mu = 91.42%, \\sigma = 9.52%; precision: \\mu = 95.60%, \\sigma = 5.29%; specificity \\mu = 95.41%, \\sigma = 7.02%) and displays a significant improvement when compared to I-VDT, an state-of-the-art algorithm (recall: \\mu = 87.67%, \\sigma = 14.73%; precision: \\mu = 89.57%, \\sigma = 8.05%; specificity \\mu = 92.10%, \\sigma = 11.21%). For algorithm implementation and annotated datasets, please contact the first author.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07763",
        "title": "LocNet: Improving Localization Accuracy for Object Detection",
        "authors": [
            "Spyros Gidaris",
            "Nikos Komodakis"
        ],
        "abstract": "We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework.\n",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2016-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07803",
        "title": "Weakly Supervised Object Boundaries",
        "authors": [
            "Anna Khoreva",
            "Rodrigo Benenson",
            "Mohamed Omran",
            "Matthias Hein",
            "Bernt Schiele"
        ],
        "abstract": "State-of-the-art learning based boundary detection methods require extensive training data. Since labelling object boundaries is one of the most expensive types of annotations, there is a need to relax the requirement to carefully annotate images to make both the training more affordable and to extend the amount of training data. In this paper we propose a technique to generate weakly supervised annotations and show that bounding box annotations alone suffice to reach high-quality object boundaries without using any object-specific boundary annotations. With the proposed weak supervision techniques we achieve the top performance on the object boundary detection task, outperforming by a large margin the current fully supervised state-of-the-art methods.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07845",
        "title": "Shape and Symmetry Induction for 3D Objects",
        "authors": [
            "Shubham Tulsiani",
            "Abhishek Kar",
            "Qixing Huang",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "Actions as simple as grasping an object or navigating around it require a rich understanding of that object's 3D shape from a given viewpoint. In this paper we repurpose powerful learning machinery, originally developed for object classification, to discover image cues relevant for recovering the 3D shape of potentially unfamiliar objects. We cast the problem as one of local prediction of surface normals and global detection of 3D reflection symmetry planes, which open the door for extrapolating occluded surfaces from visible ones. We demonstrate that our method is able to recover accurate 3D shape information for classes of objects it was not trained on, in both synthetic and real images.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07917",
        "title": "Context-aware CNNs for person head detection",
        "authors": [
            "Tuan-Hung Vu",
            "Anton Osokin",
            "Ivan Laptev"
        ],
        "abstract": "Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under a full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent local R-CNN object detector, we extend it with two types of contextual cues. First, we leverage person-scene relations and propose a Global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among objects and train a Pairwise CNN model using a structured-output surrogate loss. The Local, Global and Pairwise models are combined into a joint CNN framework. To train and test our full model, we introduce a large dataset composed of 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection against several recent baselines in three datasets. We also show improvements of the detection speed provided by our model.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07927",
        "title": "Principal Basis Analysis in Sparse Representation",
        "authors": [
            "Hong Sun",
            "Cheng-Wei Sang",
            "Chen-Guang Liu"
        ],
        "abstract": "This article introduces a new signal analysis method, which can be interpreted as a principal component analysis in sparse decomposition of the signal. The method, called principal basis analysis, is based on a novel criterion: reproducibility of component which is an intrinsic characteristic of regularity in natural signals. We show how to measure reproducibility. Then we present the principal basis analysis method, which chooses, in a sparse representation of the signal, the components optimizing the reproducibility degree to build the so-called principal basis. With this principal basis, we show that the underlying signal pattern could be effectively extracted from corrupted data. As illustration, we apply the principal basis analysis to image denoising corrupted by Gaussian and non-Gaussian noises, showing better performances than some reference methods at suppressing strong noise and at preserving signal details.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07940",
        "title": "Video Tracking Using Learned Hierarchical Features",
        "authors": [
            "Li Wang",
            "Ting Liu",
            "Gang Wang",
            "Kap Luk Chan",
            "Qingxiong Yang"
        ],
        "abstract": "In this paper, we propose an approach to learn hierarchical features for visual object tracking. First, we offline learn features robust to diverse motion patterns from auxiliary video sequences. The hierarchical features are learned via a two-layer convolutional neural network. Embedding the temporal slowness constraint in the stacked architecture makes the learned features robust to complicated motion transformations, which is important for visual object tracking. Then, given a target video sequence, we propose a domain adaptation module to online adapt the pre-learned features according to the specific target object. The adaptation is conducted in both layers of the deep feature learning module so as to include appearance information of the specific target object. As a result, the learned hierarchical features can be robust to both complicated motion transformations and appearance changes of target objects. We integrate our feature learning algorithm into three tracking methods. Experimental results demonstrate that significant improvement can be achieved using our learned hierarchical features, especially on video sequences with complicated motion transformations.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07951",
        "title": "PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset",
        "authors": [
            "Vittal Premachandran",
            "Boyan Bonev",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper, we address the boundary detection task motivated by the ambiguities in current definition of edge detection. To this end, we generate a large database consisting of more than 10k images (which is 20x bigger than existing edge detection databases) along with ground truth boundaries between 459 semantic classes including both foreground objects and different types of background, and call it the PASCAL Boundaries dataset, which will be released to the community. In addition, we propose a novel deep network-based multi-scale semantic boundary detector and name it Multi-scale Deep Semantic Boundary Detector (M-DSBD). We provide baselines using models that were trained on edge detection and show that they transfer reasonably to the task of boundary detection. Finally, we point to various important research problems that this dataset can be used for.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07963",
        "title": "Calculate distance to object in the area where car, using video analysis",
        "authors": [
            "Elena Legchekova",
            "Oleg Titov"
        ],
        "abstract": "The method of using video cameras installed on the car, to calculate the distance to the object in its area of movement.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08058",
        "title": "Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry",
        "authors": [
            "Jiale Cao",
            "Yanwei Pang",
            "Xuelong Li"
        ],
        "abstract": "The discrimination and simplicity of features are very important for effective and efficient pedestrian detection. However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency. Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF). SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part. SSF can capture the symmetrical similarity of pedestrian shape. However, it's difficult for neighboring features to have such above characterization abilities. Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection. It's found that non-neighboring features can further decrease the average miss rate by 4.44%. Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method. Compared to the state-of-the-art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08119",
        "title": "Higher Order Conditional Random Fields in Deep Neural Networks",
        "authors": [
            "Anurag Arnab",
            "Sadeep Jayasumana",
            "Shuai Zheng",
            "Philip Torr"
        ],
        "abstract": "We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08131",
        "title": "Unsupervised Deep Feature Extraction for Remote Sensing Image Classification",
        "authors": [
            "Adriana Romero",
            "Carlo Gatta",
            "Gustau Camps-Valls"
        ],
        "abstract": "This paper introduces the use of single layer and deep convolutional networks for remote sensing data analysis. Direct application to multi- and hyper-spectral imagery of supervised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layer-wise unsupervised pre-training coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution (VHR), or land-cover classification from multi- and hyper-spectral images. The proposed algorithm clearly outperforms standard Principal Component Analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single layer convolutional networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels, and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single layers variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08166",
        "title": "Tracking Motion and Proxemics using Thermal-sensor Array",
        "authors": [
            "Chandrayee Basu",
            "Anthony Rowe"
        ],
        "abstract": "Indoor tracking has all-pervasive applications beyond mere surveillance, for example in education, health monitoring, marketing, energy management and so on. Image and video based tracking systems are intrusive. Thermal array sensors on the other hand can provide coarse-grained tracking while preserving privacy of the subjects. The goal of the project is to facilitate motion detection and group proxemics modeling using an 8 x 8 infrared sensor array. Each of the 8 x 8 pixels is a temperature reading in Fahrenheit. We refer to each 8 x 8 matrix as a scene. We collected approximately 902 scenes with different configurations of human groups and different walking directions. We infer direction of motion of a subject across a set of scenes as left-to-right, right-to-left, up-to-down and down-to-up using cross-correlation analysis. We used features from connected component analysis of each background subtracted scene and performed Support Vector Machine classification to estimate number of instances of human subjects in the scene.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08177",
        "title": "Exploring Person Context and Local Scene Context for Object Detection",
        "authors": [
            "Saurabh Gupta",
            "Bharath Hariharan",
            "Jitendra Malik"
        ],
        "abstract": "In this paper we explore two ways of using context for object detection. The first model focusses on people and the objects they commonly interact with, such as fashion and sports accessories. The second model considers more general object detection and uses the spatial relationships between objects and between objects and scenes. Our models are able to capture precise spatial relationships between the context and the object of interest, and make effective use of the appearance of the contextual region. On the newly released COCO dataset, our models provide relative improvements of up to 5% over CNN-based state-of-the-art detectors, with the gains concentrated on hard cases such as small objects (10% relative improvement).\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08250",
        "title": "Recurrent Instance Segmentation",
        "authors": [
            "Bernardino Romera-Paredes",
            "Philip H. S. Torr"
        ],
        "abstract": "Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2016-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08418",
        "title": "A Computational Model for Amodal Completion",
        "authors": [
            "Maria Oliver",
            "Gloria Haro",
            "Mariella Dimiccoli",
            "Baptiste Mazin",
            "Coloma Ballester"
        ],
        "abstract": "This paper presents a computational model to recover the most likely interpretation of the 3D scene structure from a planar image, where some objects may occlude others. The estimated scene interpretation is obtained by integrating some global and local cues and provides both the complete disoccluded objects that form the scene and their ordering according to depth. Our method first computes several distal scenes which are compatible with the proximal planar image. To compute these different hypothesized scenes, we propose a perceptually inspired object disocclusion method, which works by minimizing the Euler's elastica as well as by incorporating the relatability of partially occluded contours and the convexity of the disoccluded objects. Then, to estimate the preferred scene we rely on a Bayesian model and define probabilities taking into account the global complexity of the objects in the hypothesized scenes as well as the effort of bringing these objects in their relative position in the planar image, which is also measured by an Euler's elastica-based quantity. The model is illustrated with numerical experiments on, both, synthetic and real images showing the ability of our model to reconstruct the occluded objects and the preferred perceptual order among them. We also present results on images of the Berkeley dataset with provided figure-ground ground-truth labeling.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2016-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08446",
        "title": "Towards Automatic Image Editing: Learning to See another You",
        "authors": [
            "Amir Ghodrati",
            "Xu Jia",
            "Marco Pedersoli",
            "Tinne Tuytelaars"
        ],
        "abstract": "Learning the distribution of images in order to generate new samples is a challenging task due to the high dimensionality of the data and the highly non-linear relations that are involved. Nevertheless, some promising results have been reported in the literature recently,building on deep network architectures. In this work, we zoom in on a specific type of image generation: given an image and knowing the category of objects it belongs to (e.g. faces), our goal is to generate a similar and plausible image, but with some altered attributes. This is particularly challenging, as the model needs to learn to disentangle the effect of each attribute and to apply a desired attribute change to a given input image, while keeping the other attributes and overall object appearance intact. To this end, we learn a convolutional network, where the desired attribute information is encoded then merged with the encoded image at feature map level. We show promising results, both qualitatively as well as quantitatively, in the context of a retrieval experiment, on two face datasets (MultiPie and CAS-PEAL-R1).\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08478",
        "title": "An analysis of the factors affecting keypoint stability in scale-space",
        "authors": [
            "Ives Rey-Otero",
            "Jean-Michel Morel",
            "Mauricio Delbracio"
        ],
        "abstract": "The most popular image matching algorithm SIFT, introduced by D. Lowe a decade ago, has proven to be sufficiently scale invariant to be used in numerous applications. In practice, however, scale invariance may be weakened by various sources of error inherent to the SIFT implementation affecting the stability and accuracy of keypoint detection. The density of the sampling of the Gaussian scale-space and the level of blur in the input image are two of these sources. This article presents a numerical analysis of their impact on the extracted keypoints stability. Such an analysis has both methodological and practical implications, on how to compare feature detectors and on how to improve SIFT. We show that even with a significantly oversampled scale-space numerical errors prevent from achieving perfect stability. Usual strategies to filter out unstable detections are shown to be inefficient. We also prove that the effect of the error in the assumption on the initial blur is asymmetric and that the method is strongly degraded in presence of aliasing or without a correct assumption on the camera blur.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08498",
        "title": "Iterative Instance Segmentation",
        "authors": [
            "Ke Li",
            "Bharath Hariharan",
            "Jitendra Malik"
        ],
        "abstract": "Existing methods for pixel-wise labelling tasks generally disregard the underlying structure of labellings, often leading to predictions that are visually implausible. While incorporating structure into the model should improve prediction quality, doing so is challenging - manually specifying the form of structural constraints may be impractical and inference often becomes intractable even if structural constraints are given. We sidestep this problem by reducing structured prediction to a sequence of unconstrained prediction problems and demonstrate that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification. On the instance segmentation task, this method outperforms the state-of-the-art, achieving a mean $\\mathrm{AP}^{r}$ of 63.6% at 50% overlap and 43.3% at 70% overlap.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2016-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08522",
        "title": "TennisVid2Text: Fine-grained Descriptions for Domain Specific Videos",
        "authors": [
            "Mohak Sukhwani",
            "C.V. Jawahar"
        ],
        "abstract": "Automatically describing videos has ever been fascinating. In this work, we attempt to describe videos from a specific domain - broadcast videos of lawn tennis matches. Given a video shot from a tennis match, we intend to generate a textual commentary similar to what a human expert would write on a sports website. Unlike many recent works that focus on generating short captions, we are interested in generating semantically richer descriptions. This demands a detailed low-level analysis of the video content, specially the actions and interactions among subjects. We address this by limiting our domain to the game of lawn tennis. Rich descriptions are generated by leveraging a large corpus of human created descriptions harvested from Internet. We evaluate our method on a newly created tennis video data set. Extensive analysis demonstrate that our approach addresses both semantic correctness as well as readability aspects involved in the task.\n    ",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08531",
        "title": "Structured learning of metric ensembles with application to person re-identification",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Lin Wu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Matching individuals across non-overlapping camera networks, known as person re-identification, is a fundamentally challenging problem due to the large visual appearance changes caused by variations of viewpoints, lighting, and occlusion. Approaches in literature can be categoried into two streams: The first stream is to develop reliable features against realistic conditions by combining several visual features in a pre-defined way; the second stream is to learn a metric from training data to ensure strong inter-class differences and intra-class similarities. However, seeking an optimal combination of visual features which is generic yet adaptive to different benchmarks is a unsoved problem, and metric learning models easily get over-fitted due to the scarcity of training data in person re-identification. In this paper, we propose two effective structured learning based approaches which explore the adaptive effects of visual features in recognizing persons in different benchmark data sets. Our framework is built on the basis of multiple low-level visual features with an optimal ensemble of their metrics. We formulate two optimization algorithms, CMCtriplet and CMCstruct, which directly optimize evaluation measures commonly used in person re-identification, also known as the Cumulative Matching Characteristic (CMC) curve.\n    ",
        "submission_date": "2015-11-27T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08861",
        "title": "Loss Functions for Neural Networks for Image Processing",
        "authors": [
            "Hang Zhao",
            "Orazio Gallo",
            "Iuri Frosio",
            "Jan Kautz"
        ],
        "abstract": "Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2018-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08886",
        "title": "Real-Time Depth Refinement for Specular Objects",
        "authors": [
            "Roy Or - El",
            "Rom Hershkovitz",
            "Aaron Wetzler",
            "Guy Rosman",
            "Alfred M. Bruckstein",
            "Ron Kimmel"
        ],
        "abstract": "The introduction of consumer RGB-D scanners set off a major boost in 3D computer vision research. Yet, the precision of existing depth scanners is not accurate enough to recover fine details of a scanned object. While modern shading based depth refinement methods have been proven to work well with Lambertian objects, they break down in the presence of specularities. We present a novel shape from shading framework that addresses this issue and enhances both diffuse and specular objects' depth profiles. We take advantage of the built-in monochromatic IR projector and IR images of the RGB-D scanners and present a lighting model that accounts for the specular regions in the input image. Using this model, we reconstruct the depth map in real-time. Both quantitative tests and visual evaluations prove that the proposed method produces state of the art depth reconstruction results.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2016-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08899",
        "title": "Applying deep learning to classify pornographic images and videos",
        "authors": [
            "Mohamed Moustafa"
        ],
        "abstract": "It is no secret that pornographic material is now a one-click-away from everyone, including children and minors. General social media networks are striving to isolate adult images and videos from normal ones. Intelligent image analysis methods can help to automatically detect and isolate questionable images in media. Unfortunately, these methods require vast experience to design the classifier including one or more of the popular computer vision feature descriptors. We propose to build a classifier based on one of the recently flourishing deep learning techniques. Convolutional neural networks contain many layers for both automatic features extraction and classification. The benefit is an easier system to build (no need for hand-crafting features and classifiers). Additionally, our experiments show that it is even more accurate than the state of the art methods on the most recent benchmark dataset.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2015-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08913",
        "title": "Sliding-Window Optimization on an Ambiguity-Clearness Graph for Multi-object Tracking",
        "authors": [
            "Qi Guo",
            "Le Dan",
            "Dong Yin",
            "Xiangyang Ji"
        ],
        "abstract": "Multi-object tracking remains challenging due to frequent occurrence of occlusions and outliers. In order to handle this problem, we propose an Approximation-Shrink Scheme for sequential optimization. This scheme is realized by introducing an Ambiguity-Clearness Graph to avoid conflicts and maintain sequence independent, as well as a sliding window optimization framework to constrain the size of state space and guarantee convergence. Based on this window-wise framework, the states of targets are clustered in a self-organizing manner. Moreover, we show that the traditional online and batch tracking methods can be embraced by the window-wise framework. Experiments indicate that with only a small window, the optimization performance can be much better than online methods and approach to batch methods.\n    ",
        "submission_date": "2015-11-28T00:00:00",
        "last_modified_date": "2015-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08951",
        "title": "MidRank: Learning to rank based on subsequences",
        "authors": [
            "Basura Fernando",
            "Efstratios Gavves",
            "Damien Muselet",
            "Tinne Tuytelaars"
        ],
        "abstract": "We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences. Most often in the supervised learning to rank literature, ranking is approached either by analyzing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image ranking applications and datasets.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08956",
        "title": "Sparseness helps: Sparsity Augmented Collaborative Representation for Classification",
        "authors": [
            "Naveed Akhtar",
            "Faisal Shafait",
            "Ajmal Mian"
        ],
        "abstract": "Many classification approaches first represent a test sample using the training samples of all the classes. This collaborative representation is then used to label the test sample. It was a common belief that sparseness of the representation is the key to success for this classification scheme. However, more recently, it has been claimed that it is the collaboration and not the sparseness that makes the scheme effective. This claim is attractive as it allows to relinquish the computationally expensive sparsity constraint over the representation. In this paper, we first extend the analysis supporting this claim and then show that sparseness explicitly contributes to improved classification, hence it should not be completely ignored for computational gains. Inspired by this result, we augment a dense collaborative representation with a sparse representation and propose an efficient classification method that capitalizes on the resulting representation. The augmented representation and the classification method work together meticulously to achieve higher accuracy and lower computational time compared to state-of-the-art collaborative representation based classification approaches. Experiments on benchmark face, object and action databases show the efficacy of our approach.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09030",
        "title": "On-line Recognition of Handwritten Mathematical Symbols",
        "authors": [
            "Martin Thoma"
        ],
        "abstract": "Finding the name of an unknown symbol is often hard, but writing the symbol is easy. This bachelor's thesis presents multiple systems that use the pen trajectory to classify handwritten symbols. Five preprocessing steps, one data augmentation algorithm, five features and five variants for multilayer Perceptron training were evaluated using 166898 recordings which were collected with two crowdsourcing projects. The evaluation results of these 21 experiments were used to create an optimized recognizer which has a TOP1 error of less than 17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1 error and 29.7% for the TOP3 error.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09033",
        "title": "The Multiverse Loss for Robust Transfer Learning",
        "authors": [
            "Etai Littwin",
            "Lior Wolf"
        ],
        "abstract": "Deep learning techniques are renowned for supporting effective transfer learning. However, as we demonstrate, the transferred representations support only a few modes of separation and much of its dimensionality is unutilized. In this work, we suggest to learn, in the source domain, multiple orthogonal classifiers. We prove that this leads to a reduced rank representation, which, however, supports more discriminative directions. Interestingly, the softmax probabilities produced by the multiple classifiers are likely to be identical. Experimental results, on CIFAR-100 and LFW, further demonstrate the effectiveness of our method.\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09067",
        "title": "Sparse Coral Classification Using Deep Convolutional Neural Networks",
        "authors": [
            "Mohamed Elawady"
        ],
        "abstract": "Autonomous repair of deep-sea coral reefs is a recent proposed idea to support the oceans ecosystem in which is vital for commercial fishing, tourism and other species. This idea can be operated through using many small autonomous underwater vehicles (AUVs) and swarm intelligence techniques to locate and replace chunks of coral which have been broken off, thus enabling re-growth and maintaining the habitat. The aim of this project is developing machine vision algorithms to enable an underwater robot to locate a coral reef and a chunk of coral on the seabed and prompt the robot to pick it up. Although there is no literature on this particular problem, related work on fish counting may give some insight into the problem. The technical challenges are principally due to the potential lack of clarity of the water and platform stabilization as well as spurious artifacts (rocks, fish, and crabs). We present an efficient sparse classification for coral species using supervised deep learning method called Convolutional Neural Networks (CNNs). We compute Weber Local Descriptor (WLD), Phase Congruency (PC), and Zero Component Analysis (ZCA) Whitening to extract shape and texture feature descriptors, which are employed to be supplementary channels (feature-based maps) besides basic spatial color channels (spatial-based maps) of coral input image, we also experiment state-of-art preprocessing underwater algorithms for image enhancement and color normalization and color conversion adjustment. Our proposed coral classification method is developed under MATLAB platform, and evaluated by two different coral datasets (University of California San Diego's Moorea Labeled Corals, and Heriot-Watt University's Atlantic Deep Sea).\n    ",
        "submission_date": "2015-11-29T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09150",
        "title": "Hierarchical Invariant Feature Learning with Marginalization for Person Re-Identification",
        "authors": [
            "Rahul Rama Varior",
            "Gang Wang"
        ],
        "abstract": "This paper addresses the problem of matching pedestrians across multiple camera views, known as person re-identification. Variations in lighting conditions, environment and pose changes across camera views make re-identification a challenging problem. Previous methods address these challenges by designing specific features or by learning a distance function. We propose a hierarchical feature learning framework that learns invariant representations from labeled image pairs. A mapping is learned such that the extracted features are invariant for images belonging to same individual across views. To learn robust representations and to achieve better generalization to unseen data, the system has to be trained with a large amount of data. Critically, most of the person re-identification datasets are small. Manually augmenting the dataset by partial corruption of input data introduces additional computational burden as it requires several training epochs to converge. We propose a hierarchical network which incorporates a marginalization technique that can reap the benefits of training on large datasets without explicit augmentation. We compare our approach with several baseline algorithms as well as popular linear and non-linear metric learning algorithms and demonstrate improved performance on challenging publicly available datasets, VIPeR, CUHK01, CAVIAR4REID and iLIDS. Our approach also achieves the stateof-the-art results on these datasets.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09207",
        "title": "Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015 Robust Reading Competition Challenge 4",
        "authors": [
            "Cong Yao",
            "Jianan Wu",
            "Xinyu Zhou",
            "Chi Zhang",
            "Shuchang Zhou",
            "Zhimin Cao",
            "Qi Yin"
        ],
        "abstract": "Different from focused texts present in natural images, which are captured with user's intention and intervention, incidental texts usually exhibit much more diversity, variability and complexity, thus posing significant difficulties and challenges for scene text detection and recognition algorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launched to assess the performance of existing scene text detection and recognition methods on incidental texts as well as to stimulate novel ideas and solutions. This report is dedicated to briefly introduce our strategies for this challenging problem and compare them with prior arts in this field.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2016-02-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09209",
        "title": "Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks",
        "authors": [
            "ZongYuan Ge",
            "Alex Bewley",
            "Christopher McCool",
            "Ben Upcroft",
            "Peter Corke",
            "Conrad Sanderson"
        ],
        "abstract": "We present a novel deep convolutional neural network (DCNN) system for fine-grained image classification, called a mixture of DCNNs (MixDCNN). The fine-grained image classification problem is characterised by large intra-class variations and small inter-class variations. To overcome these problems our proposed MixDCNN system partitions images into K subsets of similar images and learns an expert DCNN for each subset. The output from each of the K DCNNs is combined to form a single classification decision. In contrast to previous techniques, we provide a formulation to perform joint end-to-end training of the K DCNNs simultaneously. Extensive experiments, on three datasets using two network structures (AlexNet and GoogLeNet), show that the proposed MixDCNN system consistently outperforms other methods. It provides a relative improvement of 12.7% and achieves state-of-the-art results on two datasets.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2015-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09231",
        "title": "Design of Kernels in Convolutional Neural Networks for Image Classification",
        "authors": [
            "Zhun Sun",
            "Mete Ozay",
            "Takayuki Okatani"
        ],
        "abstract": "Despite the effectiveness of Convolutional Neural Networks (CNNs) for image classification, our understanding of the relationship between shape of convolution kernels and learned representations is limited. In this work, we explore and employ the relationship between shape of kernels which define Receptive Fields (RFs) in CNNs for learning of feature representations and image classification. For this purpose, we first propose a feature visualization method for visualization of pixel-wise classification score maps of learned features. Motivated by our experimental results, and observations reported in the literature for modeling of visual systems, we propose a novel design of shape of kernels for learning of representations in CNNs. In the experimental results, we achieved a state-of-the-art classification performance compared to a base CNN model [28] by reducing the number of parameters and computational time of the model using the ILSVRC-2012 dataset [24]. The proposed models also outperform the state-of-the-art models employed on the CIFAR-10/100 datasets [12] for image classification. Additionally, we analyzed the robustness of the proposed method to occlusion for classification of partially occluded images compared with the state-of-the-art methods. Our results indicate the effectiveness of the proposed approach. The code is available in ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2016-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09319",
        "title": "Behavior Discovery and Alignment of Articulated Object Classes from Unstructured Video",
        "authors": [
            "Luca Del Pero",
            "Susanna Ricco",
            "Rahul Sukthankar",
            "Vittorio Ferrari"
        ],
        "abstract": "We propose an automatic system for organizing the content of a collection of unstructured videos of an articulated object class (e.g. tiger, horse). By exploiting the recurring motion patterns of the class across videos, our system: 1) identifies its characteristic behaviors; and 2) recovers pixel-to-pixel alignments across different instances. Our system can be useful for organizing video collections for indexing and retrieval. Moreover, it can be a platform for learning the appearance or behaviors of object classes from Internet video. Traditional supervised techniques cannot exploit this wealth of data directly, as they require a large amount of time-consuming manual annotations.\n",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2016-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09439",
        "title": "Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video",
        "authors": [
            "Xiaowei Zhou",
            "Menglong Zhu",
            "Spyridon Leonardos",
            "Kosta Derpanis",
            "Kostas Daniilidis"
        ],
        "abstract": "This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2016-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00130",
        "title": "Implicit Sparse Code Hashing",
        "authors": [
            "Tsung-Yu Lin",
            "Tsung-Wei Ke",
            "Tyng-Luh Liu"
        ],
        "abstract": "We address the problem of converting large-scale high-dimensional image data into binary codes so that approximate nearest-neighbor search over them can be efficiently performed. Different from most of the existing unsupervised approaches for yielding binary codes, our method is based on a dimensionality-reduction criterion that its resulting mapping is designed to preserve the image relationships entailed by the inner products of sparse codes, rather than those implied by the Euclidean distances in the ambient space. While the proposed formulation does not require computing any sparse codes, the underlying computation model still inevitably involves solving an unmanageable eigenproblem when extremely high-dimensional descriptors are used. To overcome the difficulty, we consider the column-sampling technique and presume a special form of rotation matrix to facilitate subproblem decomposition. We test our method on several challenging image datasets and demonstrate its effectiveness by comparing with state-of-the-art binary coding techniques.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00172",
        "title": "Analyzing Classifiers: Fisher Vectors and Deep Neural Networks",
        "authors": [
            "Sebastian Bach",
            "Alexander Binder",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ],
        "abstract": "Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher Vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 data set.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00237",
        "title": "Fast and High Quality Highlight Removal from A Single Image",
        "authors": [
            "Dongsheng An",
            "Jinli Suo",
            "Xiangyang Ji",
            "Haoqian Wang",
            "Qionghai Dai"
        ],
        "abstract": "Specular reflection exists widely in photography and causes the recorded color deviating from its true value, so fast and high quality highlight removal from a single nature image is of great importance. In spite of the progress in the past decades in highlight removal, achieving wide applicability to the large diversity of nature scenes is quite challenging. To handle this problem, we propose an analytic solution to highlight removal based on an L2 chromaticity definition and corresponding dichromatic model. Specifically, this paper derives a normalized dichromatic model for the pixels with identical diffuse color: a unit circle equation of projection coefficients in two subspaces that are orthogonal to and parallel with the illumination, respectively. In the former illumination orthogonal subspace, which is specular-free, we can conduct robust clustering with an explicit criterion to determine the cluster number adaptively. In the latter illumination parallel subspace, a property called pure diffuse pixels distribution rule (PDDR) helps map each specular-influenced pixel to its diffuse component. In terms of efficiency, the proposed approach involves few complex calculation, and thus can remove highlight from high resolution images fast. Experiments show that this method is of superior performance in various challenging cases.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00389",
        "title": "Accelerated graph-based nonlinear denoising filters",
        "authors": [
            "Andrew Knyazev",
            "Alexander Malyshev"
        ],
        "abstract": "Denoising filters, such as bilateral, guided, and total variation filters, applied to images on general graphs may require repeated application if noise is not small enough. We formulate two acceleration techniques of the resulted iterations: conjugate gradient method and Nesterov's acceleration. We numerically show efficiency of the accelerated nonlinear filters for image denoising and demonstrate 2-12 times speed-up, i.e., the acceleration techniques reduce the number of iterations required to reach a given peak signal-to-noise ratio (PSNR) by the above indicated factor of 2-12.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00517",
        "title": "Labeling the Features Not the Samples: Efficient Video Classification with Minimal Supervision",
        "authors": [
            "Marius Leordeanu",
            "Alexandra Radu",
            "Shumeet Baluja",
            "Rahul Sukthankar"
        ],
        "abstract": "Feature selection is essential for effective visual recognition. We propose an efficient joint classifier learning and feature selection method that discovers sparse, compact representations of input features from a vast sea of candidates, with an almost unsupervised formulation. Our method requires only the following knowledge, which we call the \\emph{feature sign}---whether or not a particular feature has on average stronger values over positive samples than over negatives. We show how this can be estimated using as few as a single labeled training sample per class. Then, using these feature signs, we extend an initial supervised learning problem into an (almost) unsupervised clustering formulation that can incorporate new data without requiring ground truth labels. Our method works both as a feature selection mechanism and as a fully competitive classifier. It has important properties, low computational cost and excellent accuracy, especially in difficult cases of very limited training data. We experiment on large-scale recognition in video and show superior speed and performance to established feature selection approaches such as AdaBoost, Lasso, greedy forward-backward selection, and powerful classifiers such as SVM.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00567",
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "authors": [
            "Christian Szegedy",
            "Vincent Vanhoucke",
            "Sergey Ioffe",
            "Jonathon Shlens",
            "Zbigniew Wojna"
        ],
        "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00596",
        "title": "The MegaFace Benchmark: 1 Million Faces for Recognition at Scale",
        "authors": [
            "Ira Kemelmacher-Shlizerman",
            "Steve Seitz",
            "Daniel Miller",
            "Evan Brossard"
        ],
        "abstract": "Recent face recognition experiments on a major benchmark LFW show stunning performance--a number of algorithms achieve near to perfect score, surpassing human recognition rates. In this paper, we advocate evaluations at the million scale (LFW includes only 13K photos of 5K people). To this end, we have assembled the MegaFace dataset and created the first MegaFace challenge. Our dataset includes One Million photos that capture more than 690K different individuals. The challenge evaluates performance of algorithms with increasing numbers of distractors (going from 10 to 1M) in the gallery set. We present both identification and verification performance, evaluate performance with respect to pose and a person's age, and compare as a function of training data size (number of photos and people). We report results of state of the art and baseline algorithms. Our key observations are that testing at the million scale reveals big performance differences (of algorithms that perform similarly well on smaller scale) and that age invariant recognition as well as pose are still challenging for most. The MegaFace dataset, baseline code, and evaluation scripts, are all publicly released for further experimentations at: ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00607",
        "title": "Double Sparse Multi-Frame Image Super Resolution",
        "authors": [
            "Toshiyuki Kato",
            "Hideitsu Hino",
            "Noboru Murata"
        ],
        "abstract": "A large number of image super resolution algorithms based on the sparse coding are proposed, and some algorithms realize the multi-frame super resolution. In multi-frame super resolution based on the sparse coding, both accurate image registration and sparse coding are required. Previous study on multi-frame super resolution based on sparse coding firstly apply block matching for image registration, followed by sparse coding to enhance the image resolution. In this paper, these two problems are solved by optimizing a single objective function. The results of numerical experiments support the effectiveness of the proposed approch.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00717",
        "title": "MMSE Estimation for Poisson Noise Removal in Images",
        "authors": [
            "Stanislav Pyatykh",
            "J\u00fcrgen Hesser"
        ],
        "abstract": "Poisson noise suppression is an important preprocessing step in several applications, such as medical imaging, microscopy, and astronomical imaging. In this work, we propose a novel patch-wise Poisson noise removal strategy, in which the MMSE estimator is utilized in order to produce the denoising result for each image patch. Fast and accurate computation of the MMSE estimator is carried out using k-d tree search followed by search in the K-nearest neighbor graph. Our experiments show that the proposed method is the preferable choice for low signal-to-noise ratios.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00747",
        "title": "Active Learning for Delineation of Curvilinear Structures",
        "authors": [
            "Agata Mosinska",
            "Raphael Sznitman",
            "Przemys\u0142aw G\u0142owacki",
            "Pascal Fua"
        ],
        "abstract": "Many recent delineation techniques owe much of their increased effectiveness to path classification algorithms that make it possible to distinguish promising paths from others. The downside of this development is that they require annotated training data, which is tedious to produce.\n",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00795",
        "title": "Actions ~ Transformations",
        "authors": [
            "Xiaolong Wang",
            "Ali Farhadi",
            "Abhinav Gupta"
        ],
        "abstract": "What defines an action like \"kicking ball\"? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00818",
        "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos",
        "authors": [
            "Mohamed Elhoseiny",
            "Jingen Liu",
            "Hui Cheng",
            "Harpreet Sawhney",
            "Ahmed Elgammal"
        ],
        "abstract": "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., \"changing a vehicle tire\") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00901",
        "title": "Compressive hyperspectral imaging via adaptive sampling and dictionary learning",
        "authors": [
            "Mingrui Yang",
            "Frank de Hoog",
            "Yuqi Fan",
            "Wen Hu"
        ],
        "abstract": "In this paper, we propose a new sampling strategy for hyperspectral signals that is based on dictionary learning and singular value decomposition (SVD). Specifically, we first learn a sparsifying dictionary from training spectral data using dictionary learning. We then perform an SVD on the dictionary and use the first few left singular vectors as the rows of the measurement matrix to obtain the compressive measurements for reconstruction. The proposed method provides significant improvement over the conventional compressive sensing approaches. The reconstruction performance is further improved by reconditioning the sensing matrix using matrix balancing. We also demonstrate that the combination of dictionary learning and SVD is robust by applying them to different datasets.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00907",
        "title": "Innovation Pursuit: A New Approach to Subspace Clustering",
        "authors": [
            "Mostafa Rahmani",
            "George Atia"
        ],
        "abstract": "In subspace clustering, a group of data points belonging to a union of subspaces are assigned membership to their respective subspaces. This paper presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of subspace clustering using a new geometrical idea whereby subspaces are identified based on their relative novelties. We present two frameworks in which the idea of innovation pursuit is used to distinguish the subspaces. Underlying the first framework is an iterative method that finds the subspaces consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data potentially orthogonal to all subspaces except for the one to be identified in one step of the algorithm. A detailed mathematical analysis is provided establishing sufficient conditions for iPursuit to correctly cluster the data. The proposed approach can provably yield exact clustering even when the subspaces have significant intersections. It is shown that the complexity of the iterative approach scales only linearly in the number of data points and subspaces, and quadratically in the dimension of the subspaces. The second framework integrates iPursuit with spectral clustering to yield a new variant of spectral-clustering-based algorithms. The numerical simulations with both real and synthetic data demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms, more so for subspaces with significant intersections, and that it significantly improves the state-of-the-art result for subspace-segmentation-based face clustering.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2017-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00932",
        "title": "The Indian Spontaneous Expression Database for Emotion Recognition",
        "authors": [
            "S L Happy",
            "Priyadarshi Patnaik",
            "Aurobinda Routray",
            "Rajlakshmi Guha"
        ],
        "abstract": "Automatic recognition of spontaneous facial expressions is a major challenge in the field of affective computing. Head rotation, face pose, illumination variation, occlusion etc. are the attributes that increase the complexity of recognition of spontaneous expressions in practical applications. Effective recognition of expressions depends significantly on the quality of the database used. Most well-known facial expression databases consist of posed expressions. However, currently there is a huge demand for spontaneous expression databases for the pragmatic implementation of the facial expression recognition algorithms. In this paper, we propose and establish a new facial expression database containing spontaneous expressions of both male and female participants of Indian origin. The database consists of 428 segmented video clips of the spontaneous facial expressions of 50 participants. In our experiment, emotions were induced among the participants by using emotional videos and simultaneously their self-ratings were collected for each experienced emotion. Facial expression clips were annotated carefully by four trained decoders, which were further validated by the nature of stimuli used and self-report of emotions. An extensive analysis was carried out on the database using several machine learning algorithms and the results are provided for future reference. Such a spontaneous database will help in the development and validation of algorithms for recognition of spontaneous expressions.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00939",
        "title": "A Literature Survey of various Fingerprint De-noising Techniques to justify the need of a new De-noising model based upon Pixel Component Analysis",
        "authors": [
            "Siddharth Choubey",
            "Deepika Banchhor"
        ],
        "abstract": "Image Preprocessing is a vital step in the field of image processing for biometric pattern recognition. This paper studies and reviews various classical and modern fingerprint image de-noising models. The various model used for de-noising ranges widely from transform matrix using frequency, histogram model de-noising, de-noising by introducing Gabor filter and its types to enhance fingerprint images.\n",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01003",
        "title": "Weighted Schatten $p$-Norm Minimization for Image Denoising and Background Subtraction",
        "authors": [
            "Yuan Xie",
            "Shuhang Gu",
            "Yan Liu",
            "Wangmeng Zuo",
            "Wensheng Zhang",
            "Lei Zhang"
        ],
        "abstract": "Low rank matrix approximation (LRMA), which aims to recover the underlying low rank matrix from its degraded observation, has a wide range of applications in computer vision. The latest LRMA methods resort to using the nuclear norm minimization (NNM) as a convex relaxation of the nonconvex rank minimization. However, NNM tends to over-shrink the rank components and treats the different rank components equally, limiting its flexibility in practical applications. We propose a more flexible model, namely the Weighted Schatten $p$-Norm Minimization (WSNM), to generalize the NNM to the Schatten $p$-norm minimization with weights assigned to different singular values. The proposed WSNM not only gives better approximation to the original low-rank assumption, but also considers the importance of different rank components. We analyze the solution of WSNM and prove that, under certain weights permutation, WSNM can be equivalently transformed into independent non-convex $l_p$-norm subproblems, whose global optimum can be efficiently solved by generalized iterated shrinkage algorithm. We apply WSNM to typical low-level vision problems, e.g., image denoising and background subtraction. Extensive experimental results show, both qualitatively and quantitatively, that the proposed WSNM can more effectively remove noise, and model complex and dynamic scenes compared with state-of-the-art methods.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01030",
        "title": "Simulations for Validation of Vision Systems",
        "authors": [
            "V S R Veeravasarapu",
            "Rudra Narayan Hota",
            "Constantin Rothkopf",
            "Ramesh Visvanathan"
        ],
        "abstract": "As the computer vision matures into a systems science and engineering discipline, there is a trend in leveraging latest advances in computer graphics simulations for performance evaluation, learning, and inference. However, there is an open question on the utility of graphics simulations for vision with apparently contradicting views in the literature. In this paper, we place the results from the recent literature in the context of performance characterization methodology outlined in the 90's and note that insights derived from simulations can be qualitative or quantitative depending on the degree of fidelity of models used in simulation and the nature of the question posed by the experimenter. We describe a simulation platform that incorporates latest graphics advances and use it for systematic performance characterization and trade-off analysis for vision system design. We verify the utility of the platform in a case study of validating a generative model inspired vision hypothesis, Rank-Order consistency model, in the contexts of global and local illumination changes, and bad weather, and high-frequency noise. Our approach establishes the link between alternative viewpoints, involving models with physics based semantics and signal and perturbation semantics and confirms insights in literature on robust change detection.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01055",
        "title": "Occlusion-Aware Human Pose Estimation with Mixtures of Sub-Trees",
        "authors": [
            "Ibrahim Radwan",
            "Abhinav Dhall",
            "Roland Goecke"
        ],
        "abstract": "In this paper, we study the problem of learning a model for human pose estimation as mixtures of compositional sub-trees in two layers of prediction. This involves estimating the pose of a sub-tree followed by identifying the relationships between sub-trees that are used to handle occlusions between different parts. The mixtures of the sub-trees are learnt utilising both geometric and appearance distances. The Chow-Liu (CL) algorithm is recursively applied to determine the inter-relations between the nodes and to build the structure of the sub-trees. These structures are used to learn the latent parameters of the sub-trees and the inference is done using a standard belief propagation technique. The proposed method handles occlusions during the inference process by identifying overlapping regions between different sub-trees and introducing a penalty term for overlapping parts. Experiments are performed on three different datasets: the Leeds Sports, Image Parse and UIUC People datasets. The results show the robustness of the proposed method to occlusions over the state-of-the-art approaches.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2015-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01192",
        "title": "Prototypical Priors: From Improving Classification to Zero-Shot Learning",
        "authors": [
            "Saumya Jetley",
            "Bernardino Romera-Paredes",
            "Sadeep Jayasumana",
            "Philip Torr"
        ],
        "abstract": "Recent works on zero-shot learning make use of side information such as visual attributes or natural language semantics to define the relations between output visual classes and then use these relationships to draw inference on new unseen classes at test time. In a novel extension to this idea, we propose the use of visual prototypical concepts as side information. For most real-world visual object categories, it may be difficult to establish a unique prototype. However, in cases such as traffic signs, brand logos, flags, and even natural language characters, these prototypical templates are available and can be leveraged for an improved recognition performance. The present work proposes a way to incorporate this prototypical information in a deep learning framework. Using prototypes as prior information, the deepnet pipeline learns the input image projections into the prototypical embedding space subject to minimization of the final classification loss. Based on our experiments with two different datasets of traffic signs and brand logos, prototypical embeddings incorporated in a conventional convolutional neural network improve the recognition performance. Recognition accuracy on the Belga logo dataset is especially noteworthy and establishes a new state-of-the-art. In zero-shot learning scenarios, the same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information for these new classes at test time. Thus, unlike earlier approaches, testing on seen and unseen classes is handled using the same pipeline, and the system can be tuned for a trade-off of seen and unseen class performance as per task requirement. Comparison with one of the latest works in the zero-shot learning domain yields top results on the two datasets mentioned above.\n    ",
        "submission_date": "2015-12-03T00:00:00",
        "last_modified_date": "2018-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01289",
        "title": "Predicting and visualizing psychological attributions with a deep neural network",
        "authors": [
            "Edward Grant",
            "Stephan Sahm",
            "Mariam Zabihi",
            "Marcel van Gerven"
        ],
        "abstract": "Judgments about personality based on facial appearance are strong effectors in social decision making, and are known to have impact on areas from presidential elections to jury decisions. Recent work has shown that it is possible to predict perception of memorability, trustworthiness, intelligence and other attributes in human face images. The most successful of these approaches require face images expertly annotated with key facial landmarks. We demonstrate a Convolutional Neural Network (CNN) model that is able to perform the same task without the need for landmark features, thereby greatly increasing efficiency. The model has high accuracy, surpassing human-level performance in some cases. Furthermore, we use a deconvolutional approach to visualize important features for perception of 22 attributes and demonstrate a new method for separately visualizing positive and negative features.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01320",
        "title": "What can we learn about CNNs from a large scale controlled object dataset?",
        "authors": [
            "Ali Borji",
            "Saeed Izadi",
            "Laurent Itti"
        ],
        "abstract": "Tolerance to image variations (e.g. translation, scale, pose, illumination) is an important desired property of any object recognition system, be it human or machine. Moving towards increasingly bigger datasets has been trending in computer vision specially with the emergence of highly popular deep learning models. While being very useful for learning invariance to object inter- and intra-class shape variability, these large-scale wild datasets are not very useful for learning invariance to other parameters forcing researchers to resort to other tricks for training a model. In this work, we introduce a large-scale synthetic dataset, which is freely and publicly available, and use it to answer several fundamental questions regarding invariance and selectivity properties of convolutional neural networks. Our dataset contains two parts: a) objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a semicircular arch, 5 lighting conditions, 3 focus levels, variety of backgrounds (23.4 per instance) generating 1320 images per instance (over 20 million images in total), and b) scenes: in which a robot arm takes pictures of objects on a 1:160 scale scene. We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs. We also explore how our analyses can lead the field to develop more efficient CNNs.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01325",
        "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images",
        "authors": [
            "Babak Saleh",
            "Ahmed Elgammal",
            "Jacob Feldman",
            "Ali Farhadi"
        ],
        "abstract": "The human visual system can spot an abnormal image, and reason about what makes it strange. This task has not received enough attention in computer vision. In this paper we study various types of atypicalities in images in a more comprehensive way than has been done before. We propose a new dataset of abnormal images showing a wide range of atypicalities. We design human subject experiments to discover a coarse taxonomy of the reasons for abnormality. Our experiments reveal three major categories of abnormality: object-centric, scene-centric, and contextual. Based on this taxonomy, we propose a comprehensive computational model that can predict all different types of abnormality in images and outperform prior arts in abnormality recognition.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01355",
        "title": "Staple: Complementary Learners for Real-Time Tracking",
        "authors": [
            "Luca Bertinetto",
            "Jack Valmadre",
            "Stuart Golodetz",
            "Ondrej Miksik",
            "Philip Torr"
        ],
        "abstract": "Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01383",
        "title": "Sublabel-Accurate Relaxation of Nonconvex Energies",
        "authors": [
            "Thomas M\u00f6llenhoff",
            "Emanuel Laude",
            "Michael Moeller",
            "Jan Lellmann",
            "Daniel Cremers"
        ],
        "abstract": "We propose a novel spatially continuous framework for convex relaxations based on functional lifting. Our method can be interpreted as a sublabel-accurate solution to multilabel problems. We show that previously proposed functional lifting methods optimize an energy which is linear between two labels and hence require (often infinitely) many labels for a faithful approximation. In contrast, the proposed formulation is based on a piecewise convex approximation and therefore needs far fewer labels. In comparison to recent MRF-based approaches, our method is formulated in a spatially continuous setting and shows less grid bias. Moreover, in a local sense, our formulation is the tightest possible convex relaxation. It is easy to implement and allows an efficient primal-dual optimization on GPUs. We show the effectiveness of our approach on several computer vision problems.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01401",
        "title": "Model Validation for Vision Systems via Graphics Simulation",
        "authors": [
            "V S R Veeravasarapu",
            "Rudra Narayan Hota",
            "Constantin Rothkopf",
            "Ramesh Visvanathan"
        ],
        "abstract": "Rapid advances in computation, combined with latest advances in computer graphics simulations have facilitated the development of vision systems and training them in virtual environments. One major stumbling block is in certification of the designs and tuned parameters of these systems to work in real world. In this paper, we begin to explore the fundamental question: Which type of information transfer is more analogous to real world? Inspired from the performance characterization methodology outlined in the 90's, we note that insights derived from simulations can be qualitative or quantitative depending on the degree of the fidelity of models used in simulations and the nature of the questions posed by the experimenter. We adapt the methodology in the context of current graphics simulation tools for modeling data generation processes and, for systematic performance characterization and trade-off analysis for vision system design leading to qualitative and quantitative insights. In concrete, we examine invariance assumptions used in vision algorithms for video surveillance settings as a case study and assess the degree to which those invariance assumptions deviate as a function of contextual variables on both graphics simulations and in real data. As computer graphics rendering quality improves, we believe teasing apart the degree to which model assumptions are valid via systematic graphics simulation can be a significant aid to assisting more principled ways of approaching vision system design and performance modeling.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01515",
        "title": "ASIST: Automatic Semantically Invariant Scene Transformation",
        "authors": [
            "Or Litany",
            "Tal Remez",
            "Daniel Freedman",
            "Lior Shapira",
            "Alex Bronstein",
            "Ran Gal"
        ],
        "abstract": "We present ASIST, a technique for transforming point clouds by replacing objects with their semantically equivalent counterparts. Transformations of this kind have applications in virtual reality, repair of fused scans, and robotics. ASIST is based on a unified formulation of semantic labeling and object replacement; both result from minimizing a single objective. We present numerical tools for the efficient solution of this optimization problem. The method is experimentally assessed on new datasets of both synthetic and real point clouds, and is additionally compared to two recent works on object replacement on data from the corresponding papers.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01533",
        "title": "Motion trails from time-lapse video",
        "authors": [
            "Camille Goudeseune"
        ],
        "abstract": "From an image sequence captured by a stationary camera, background subtraction can detect moving foreground objects in the scene. Distinguishing foreground from background is further improved by various heuristics. Then each object's motion can be emphasized by duplicating its positions as a motion trail. These trails clarify the objects' spatial relationships. Also, adding motion trails to a video before previewing it at high speed reduces the risk of overlooking transient events.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01642",
        "title": "A Deep Structured Model with Radius-Margin Bound for 3D Human Activity Recognition",
        "authors": [
            "Liang Lin",
            "Keze Wang",
            "Wangmeng Zuo",
            "Meng Wang",
            "Jiebo Luo",
            "Lei Zhang"
        ],
        "abstract": "Understanding human activity is very challenging even with the recently developed 3D/depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks (CNNs). Our model advances the traditional deep learning approaches in two aspects. First, { we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture}. Second, we incorporate a radius-margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) { updates the classifiers} based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01655",
        "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics",
        "authors": [
            "Nicola Pezzotti",
            "Boudewijn P.F. Lelieveldt",
            "Laurens van der Maaten",
            "Thomas H\u00f6llt",
            "Elmar Eisemann",
            "Anna Vilanova"
        ],
        "abstract": "Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of several high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2016-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01680",
        "title": "A Shapley Value Solution to Game Theoretic-based Feature Reduction in False Alarm Detection",
        "authors": [
            "Fatemeh Afghah",
            "Abolfazl Razi",
            "Kayvan Najarian"
        ],
        "abstract": "False alarm is one of the main concerns in intensive care units and can result in care disruption, sleep deprivation, and insensitivity of care-givers to alarms. Several methods have been proposed to suppress the false alarm rate through improving the quality of physiological signals by filtering, and developing more accurate sensors. However, significant intrinsic correlation among the extracted features limits the performance of most currently available data mining techniques, as they often discard the predictors with low individual impact that may potentially have strong discriminatory power when grouped with others. We propose a model based on coalition game theory that considers the inter-features dependencies in determining the salient predictors in respect to false alarm, which results in improved classification accuracy. The superior performance of this method compared to current methods is shown in simulation results using PhysionNet's MIMIC II database.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01691",
        "title": "Maximum Entropy Binary Encoding for Face Template Protection",
        "authors": [
            "Rohit Kumar Pandey",
            "Yingbo Zhou",
            "Bhargava Urala Kota",
            "Venu Govindaraju"
        ],
        "abstract": "In this paper we present a framework for secure identification using deep neural networks, and apply it to the task of template protection for face authentication. We use deep convolutional neural networks (CNNs) to learn a mapping from face images to maximum entropy binary (MEB) codes. The mapping is robust enough to tackle the problem of exact matching, yielding the same code for new samples of a user as the code assigned during training. These codes are then hashed using any hash function that follows the random oracle model (like SHA-512) to generate protected face templates (similar to text based password protection). The algorithm makes no unrealistic assumptions and offers high template security, cancelability, and state-of-the-art matching performance. The efficacy of the approach is shown on CMU-PIE, Extended Yale B, and Multi-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) at zero false accept rate (FAR) with up to 1024 bits of template security.\n    ",
        "submission_date": "2015-12-05T00:00:00",
        "last_modified_date": "2015-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01715",
        "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding",
        "authors": [
            "Hang Qi",
            "Tianfu Wu",
            "Mun-Wai Lee",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \"true/false\" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01722",
        "title": "Vanishing point attracts gaze in free-viewing and visual search tasks",
        "authors": [
            "Ali Borji",
            "Mengyang Feng"
        ],
        "abstract": "To investigate whether the vanishing point (VP) plays a significant role in gaze guidance, we ran two experiments. In the first one, we recorded fixations of 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out of which 319 had VP (shuffled presentation; each image for 4 secs). We found that the average number of fixations at a local region (80x80 pixels) centered at the VP is significantly higher than the average fixations at random locations (t-test; n=319; p=1.8e-35). To address the confounding factor of saliency, we learned a combined model of bottom-up saliency and VP. AUC score of our model (0.85; SD=0.01) is significantly higher than the original saliency model (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p= 3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second experiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search for a target character (T or L) placed randomly on a 3x3 imaginary grid overlaid on top of an image. Subjects reported their answers by pressing one of two keys. Stimuli consisted of 270 color images (180 with a single VP, 90 without). The target happened with equal probability inside each cell (15 times L, 15 times T). We found that subjects were significantly faster (and more accurate) when target happened inside the cell containing the VP compared to cells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon rank-sum test; p = 0.0014). Response time at VP cells were also significantly lower than response time on images without VP (median 2.37; p= 4.77e-05). These findings support the hypothesis that vanishing point, similar to face and text (Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts attention in free-viewing and visual search.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2016-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01774",
        "title": "Image reconstruction from dense binary pixels",
        "authors": [
            "Or Litany",
            "Tal Remez",
            "Alex Bronstein"
        ],
        "abstract": "Recently, the dense binary pixel Gigavision camera had been introduced, emulating a digital version of the photographic film. While seems to be a promising solution for HDR imaging, its output is not directly usable and requires an image reconstruction process. In this work, we formulate this problem as the minimization of a convex objective combining a maximum-likelihood term with a sparse synthesis prior. We present MLNet - a novel feed-forward neural network, producing acceptable output quality at a fixed complexity and is two orders of magnitude faster than iterative algorithms. We present state of the art results in the abstract.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01789",
        "title": "The Next Best Underwater View",
        "authors": [
            "Mark Sheinin",
            "Yoav Y. Schechner"
        ],
        "abstract": "To image in high resolution large and occlusion-prone scenes, a camera must move above and around. Degradation of visibility due to geometric occlusions and distances is exacerbated by scattering, when the scene is in a participating medium. Moreover, underwater and in other media, artificial lighting is needed. Overall, data quality depends on the observed surface, medium and the time-varying poses of the camera and light source. This work proposes to optimize camera/light poses as they move, so that the surface is scanned efficiently and the descattered recovery has the highest quality. The work generalizes the next best view concept of robot vision to scattering media and cooperative movable lighting. It also extends descattering to platforms that move optimally. The optimization criterion is information gain, taken from information theory. We exploit the existence of a prior rough 3D model, since underwater such a model is routinely obtained using sonar. We demonstrate this principle in a scaled-down setup.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01815",
        "title": "PatchBatch: a Batch Augmented Loss for Optical Flow",
        "authors": [
            "David Gadot",
            "Lior Wolf"
        ],
        "abstract": "We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2016-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01848",
        "title": "Rank Pooling for Action Recognition",
        "authors": [
            "Basura Fernando",
            "Efstratios Gavves",
            "Jose Oramas",
            "Amir Ghodrati",
            "Tinne Tuytelaars"
        ],
        "abstract": "We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g. how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01858",
        "title": "Fixation prediction with a combined model of bottom-up saliency and vanishing point",
        "authors": [
            "Mengyang Feng",
            "Ali Borji",
            "Huchuan Lu"
        ],
        "abstract": "By predicting where humans look in natural scenes, we can understand how they perceive complex natural scenes and prioritize information for further high-level visual processing. Several models have been proposed for this purpose, yet there is a gap between best existing saliency models and human performance. While many researchers have developed purely computational models for fixation prediction, less attempts have been made to discover cognitive factors that guide gaze. Here, we study the effect of a particular type of scene structural information, known as the vanishing point, and show that human gaze is attracted to the vanishing point regions. We record eye movements of 10 observers over 532 images, out of which 319 have vanishing points. We then construct a combined model of traditional saliency and a vanishing point channel and show that our model outperforms state of the art saliency models using three scores on our dataset.\n    ",
        "submission_date": "2015-12-06T00:00:00",
        "last_modified_date": "2015-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01881",
        "title": "Recognition from Hand Cameras",
        "authors": [
            "Cheng-Sheng Chan",
            "Shou-Zhong Chen",
            "Pei-Xuan Xie",
            "Chiung-Chih Chang",
            "Min Sun"
        ],
        "abstract": "We revisit the study of a wrist-mounted camera system (referred to as HandCam) for recognizing activities of hands. HandCam has two unique properties as compared to egocentric systems (referred to as HeadCam): (1) it avoids the need to detect hands; (2) it more consistently observes the activities of hands. By taking advantage of these properties, we propose a deep-learning-based method to recognize hand states (free v.s. active hands, hand gestures, object categories), and discover object categories. Moreover, we propose a novel two-streams deep network to further take advantage of both HandCam and HeadCam. We have collected a new synchronized HandCam and HeadCam dataset with 20 videos captured in three scenes for hand states recognition. Experiments show that our HandCam system consistently outperforms a deep-learning-based HeadCam method (with estimated manipulation regions) and a dense-trajectory-based HeadCam method in all tasks. We also show that HandCam videos captured by different users can be easily aligned to improve free v.s. active recognition accuracy (3.3% improvement) in across-scenes use case. Moreover, we observe that finetuning Convolutional Neural Network consistently improves accuracy. Finally, our novel two-streams deep network combining HandCam and HeadCam features achieves the best performance in four out of five tasks. With more data, we believe a joint HandCam and HeadCam system can robustly log hand states in daily life.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01891",
        "title": "Sparsifying Neural Network Connections for Face Recognition",
        "authors": [
            "Yi Sun",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "This paper proposes to learn high-performance deep ConvNets with sparse neural connections, referred to as sparse ConvNets, for face recognition. The sparse ConvNets are learned in an iterative way, each time one additional layer is sparsified and the entire model is re-trained given the initial weights learned in previous iterations. One important finding is that directly training the sparse ConvNet from scratch failed to find good solutions for face recognition, while using a previously learned denser model to properly initialize a sparser model is critical to continue learning effective features for face recognition. This paper also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration. When taking a moderately sparse structure (26%-76% of weights in the dense model), the proposed sparse ConvNet model significantly improves the face recognition performance of the previous state-of-the-art DeepID2+ models given the same training data, while it keeps the performance of the baseline model with only 12% of the original parameters.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02013",
        "title": "Scalable domain adaptation of convolutional neural networks",
        "authors": [
            "Adrian Popescu",
            "Etienne Gadeski",
            "Herv\u00e9 Le Borgne"
        ],
        "abstract": "Convolutional neural networks (CNNs) tend to become a standard approach to solve a wide array of computer vision problems. Besides important theoretical and practical advances in their design, their success is built on the existence of manually labeled visual resources, such as ImageNet. The creation of such datasets is cumbersome and here we focus on alternatives to manual labeling. We hypothesize that new resources are of uttermost importance in domains which are not or weakly covered by ImageNet, such as tourism photographs. We first collect noisy Flickr images for tourist points of interest and apply automatic or weakly-supervised reranking techniques to reduce noise. Then, we learn domain adapted models with a standard CNN architecture and compare them to a generic model obtained from ImageNet. Experimental validation is conducted with publicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred. Results show that low-cost domain adaptation improves results compared to the use of generic models but also compared to strong non-CNN baselines such as triangulation embedding.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02017",
        "title": "Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images",
        "authors": [
            "Aravindh Mahendran",
            "Andrea Vedaldi"
        ],
        "abstract": "Image representations, from SIFT and bag of visual words to Convolutional Neural Networks (CNNs) are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of \"natural pre-image\", namely a natural-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose these as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02072",
        "title": "On The Continuous Steering of the Scale of Tight Wavelet Frames",
        "authors": [
            "Zsuzsanna P\u00fcsp\u00f6ki",
            "John Paul Ward",
            "Daniel Sage",
            "Michael Unser"
        ],
        "abstract": "In analogy with steerable wavelets, we present a general construction of adaptable tight wavelet frames, with an emphasis on scaling operations. In particular, the derived wavelets can be \"dilated\" by a procedure comparable to the operation of steering steerable wavelets. The fundamental aspects of the construction are the same: an admissible collection of Fourier multipliers is used to extend a tight wavelet frame, and the \"scale\" of the wavelets is adapted by scaling the multipliers. As an application, the proposed wavelets can be used to improve the frequency localization. Importantly, the localized frequency bands specified by this construction can be scaled efficiently using matrix multiplication.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02110",
        "title": "In-situ multi-scattering tomography",
        "authors": [
            "Vadim Holodovsky",
            "Yoav Y. Schechner",
            "Anat Levin",
            "Aviad Levis",
            "Amit Aides"
        ],
        "abstract": "To recover the three dimensional (3D) volumetric distribution of matter in an object, images of the object are captured from multiple directions and locations. Using these images tomographic computations extract the distribution. In highly scattering media and constrained, natural irradiance, tomography must explicitly account for off-axis scattering. Furthermore, the tomographic model and recovery must function when imaging is done in-situ, as occurs in medical imaging and ground-based atmospheric sensing. We formulate tomography that handles arbitrary orders of scattering, using a monte-carlo model. Moreover, the model is highly parallelizable in our formulation. This enables large scale rendering and recovery of volumetric scenes having a large number of variables. We solve stability and conditioning problems that stem from radiative transfer (RT) modeling in-situ.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02134",
        "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",
        "authors": [
            "Nikolaus Mayer",
            "Eddy Ilg",
            "Philip H\u00e4usser",
            "Philipp Fischer",
            "Daniel Cremers",
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "abstract": "Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02167",
        "title": "Simple Baseline for Visual Question Answering",
        "authors": [
            "Bolei Zhou",
            "Yuandong Tian",
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Rob Fergus"
        ],
        "abstract": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. .\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02188",
        "title": "Pseudo-Bayesian Robust PCA: Algorithms and Analyses",
        "authors": [
            "Tae-Hyun Oh",
            "Yasuyuki Matsushita",
            "In So Kweon",
            "David Wipf"
        ],
        "abstract": "Commonly used in computer vision and other applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting optimization problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation. Surprisingly, our algorithm can even outperform convex matrix completion despite the fact that the latter is provided with perfect knowledge of which entries are not corrupted.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2016-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02311",
        "title": "Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression",
        "authors": [
            "Takuya Narihira",
            "Michael Maire",
            "Stella X. Yu"
        ],
        "abstract": "We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components. Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch. Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms.\n",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02325",
        "title": "SSD: Single Shot MultiBox Detector",
        "authors": [
            "Wei Liu",
            "Dragomir Anguelov",
            "Dumitru Erhan",
            "Christian Szegedy",
            "Scott Reed",
            "Cheng-Yang Fu",
            "Alexander C. Berg"
        ],
        "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2016-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02326",
        "title": "Learning to Point and Count",
        "authors": [
            "Jie Shao",
            "Dequan Wang",
            "Xiangyang Xue",
            "Zheng Zhang"
        ],
        "abstract": "This paper proposes the problem of point-and-count as a test case to break the what-and-where deadlock. Different from the traditional detection problem, the goal is to discover key salient points as a way to localize and count the number of objects simultaneously. We propose two alternatives, one that counts first and then point, and another that works the other way around. Fundamentally, they pivot around whether we solve \"what\" or \"where\" first. We evaluate their performance on dataset that contains multiple instances of the same class, demonstrating the potentials and their synergies. The experiences derive a few important insights that explains why this is a much harder problem than classification, including strong data bias and the inability to deal with object scales robustly in state-of-art convolutional neural networks.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02329",
        "title": "Computational Models for Multiview Dense Depth Maps of Dynamic Scene",
        "authors": [
            "Qifei Wang"
        ],
        "abstract": "This paper reviews the recent progresses of the depth map generation for dynamic scene and its corresponding computational models. This paper mainly covers the homogeneous ambiguity models in depth sensing, resolution models in depth processing, and consistency models in depth optimization. We also summarize the future work in the depth map generation.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02355",
        "title": "Is Hamming distance the only way for matching binary image feature descriptors?",
        "authors": [
            "Erkan Bostanci"
        ],
        "abstract": "Brute force matching of binary image feature descriptors is conventionally performed using the Hamming distance. This paper assesses the use of alternative metrics in order to see whether they can produce feature correspondences that yield more accurate homography matrices. Two statistical tests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for evaluation. Results show that Jackard-Needham and Dice metrics can display better performance for some descriptors. Yet, these performance differences were not found to be statistically significant.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02357",
        "title": "Towards the Application of Linear Programming Methods For Multi-Camera Pose Estimation",
        "authors": [
            "Masoud Aghamohamadian-Sharbaf",
            "Ahmadreza Heravi",
            "Hamidreza Pourreza"
        ],
        "abstract": "We presented a separation based optimization algorithm which, rather than optimization the entire variables altogether, This would allow us to employ: 1) a class of nonlinear functions with three variables and 2) a convex quadratic multivariable polynomial, for minimization of reprojection error. Neglecting the inversion required to minimize the nonlinear functions, in this paper we demonstrate how separation allows eradication of matrix inversion.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02413",
        "title": "Tracking Objects with Higher Order Interactions using Delayed Column Generation",
        "authors": [
            "Shaofei Wang",
            "Steffen Wolf",
            "Charless Fowlkes",
            "Julian Yarkony"
        ],
        "abstract": "We study the problem of multi-target tracking and data association in video. We formulate this in terms of selecting a subset of high-quality tracks subject to the constraint that no pair of selected tracks is associated with a common detection (of an object). This objective is equivalent to the classic NP-hard problem of finding a maximum-weight set packing (MWSP) where tracks correspond to sets and is made further difficult since the number of candidate tracks grows exponentially in the number of detections. We present a relaxation of this combinatorial problem that uses a column generation formulation where the pricing problem is solved via dynamic programming to efficiently explore the space of tracks. We employ row generation to tighten the bound in such a way as to preserve efficient inference in the pricing problem. We show the practical utility of this algorithm for tracking problems in natural and biological video datasets.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2016-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02497",
        "title": "Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views",
        "authors": [
            "Francisco Massa",
            "Bryan Russell",
            "Mathieu Aubry"
        ],
        "abstract": "This paper presents an end-to-end convolutional neural network (CNN) for 2D-3D exemplar detection. We demonstrate that the ability to adapt the features of natural images to better align with those of CAD rendered views is critical to the success of our technique. We show that the adaptation can be learned by compositing rendered views of textured object models on natural images. Our approach can be naturally incorporated into a CNN detection pipeline and extends the accuracy and speed benefits from recent advances in deep learning to 2D-3D exemplar detection. We applied our method to two tasks: instance detection, where we evaluated on the IKEA dataset, and object category detection, where we out-perform Aubry et al. for \"chair\" detection on a subset of the Pascal VOC dataset.\n    ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02665",
        "title": "Fine-grained Image Classification by Exploring Bipartite-Graph Labels",
        "authors": [
            "Feng Zhou",
            "Yuanqing Lin"
        ],
        "abstract": "Given a food image, can a fine-grained object recognition engine tell \"which restaurant which dish\" the food belongs to? Such ultra-fine grained image recognition is the key for many applications like search by images, but it is very challenging because it needs to discern subtle difference between classes while dealing with the scarcity of training data. Fortunately, the ultra-fine granularity naturally brings rich relationships among object classes. This paper proposes a novel approach to exploit the rich relationships through bipartite-graph labels (BGL). We show how to model BGL in an overall convolutional neural networks and the resulting system can be optimized through back-propagation. We also show that it is computationally efficient in inference thanks to the bipartite structure. To facilitate the study, we construct a new food benchmark dataset, which consists of 37,885 food images collected from 6 restaurants and totally 975 menus. Experimental results on this new food and three other datasets demonstrates BGL advances previous works in fine-grained object recognition. An online demo is available at ",
        "submission_date": "2015-12-08T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02736",
        "title": "Window-Object Relationship Guided Representation Learning for Generic Object Detections",
        "authors": [
            "Xingyu Zeng",
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "abstract": "In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02767",
        "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding",
        "authors": [
            "Michael Maire",
            "Takuya Narihira",
            "Stella X. Yu"
        ],
        "abstract": "Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02895",
        "title": "Embedding Label Structures for Fine-Grained Feature Representation",
        "authors": [
            "Xiaofan Zhang",
            "Feng Zhou",
            "Yuanqing Lin",
            "Shaoting Zhang"
        ],
        "abstract": "Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate similar images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2016-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02902",
        "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
        "authors": [
            "Makarand Tapaswi",
            "Yukun Zhu",
            "Rainer Stiefelhagen",
            "Antonio Torralba",
            "Raquel Urtasun",
            "Sanja Fidler"
        ],
        "abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02949",
        "title": "Video captioning with recurrent networks based on frame- and video-level features and visual content classification",
        "authors": [
            "Rakshith Shetty",
            "Jorma Laaksonen"
        ],
        "abstract": "In this paper, we describe the system for generating textual descriptions of short video clips using recurrent neural networks (RNN), which we used while participating in the Large Scale Movie Description Challenge 2015 in ICCV 2015. Our work builds on static image captioning systems with RNN based language models and extends this framework to videos utilizing both static image features and video-specific features. In addition, we study the usefulness of visual content classifiers as a source of additional information for caption generation. With experimental results we show that utilizing keyframe based features, dense trajectory video features and content classifier outputs together gives better performance than any one of them individually.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02972",
        "title": "Get More With Less: Near Real-Time Image Clustering on Mobile Phones",
        "authors": [
            "Jorge Ortiz",
            "Chien-Chin Huang",
            "Supriyo Chakraborty"
        ],
        "abstract": "Machine learning algorithms, in conjunction with user data, hold the promise of revolutionizing the way we interact with our phones, and indeed their widespread adoption in the design of apps bear testimony to this promise. However, currently, the computationally expensive segments of the learning pipeline, such as feature extraction and model training, are offloaded to the cloud, resulting in an over-reliance on the network and under-utilization of computing resources available on mobile platforms. In this paper, we show that by combining the computing power distributed over a number of phones, judicious optimization choices, and contextual information it is possible to execute the end-to-end pipeline entirely on the phones at the edge of the network, efficiently. We also show that by harnessing the power of this combination, it is possible to execute a computationally expensive pipeline at near real-time.\n",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03019",
        "title": "Minimally Supervised Feature Selection for Classification (Master's Thesis, University Politehnica of Bucharest)",
        "authors": [
            "Alexandra Maria Radu"
        ],
        "abstract": "In the context of the highly increasing number of features that are available nowadays we design a robust and fast method for feature selection. The method tries to select the most representative features that are independent from each other, but are strong together. We propose an algorithm that requires very limited labeled data (as few as one labeled frame per class) and can accommodate as many unlabeled samples. We also present here the supervised approach from which we started. We compare our two formulations with established methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and show that our method is much faster and it has constant training time. Moreover, the unsupervised approach outperforms all the methods with which we compared and the difference might be quite prominent. The supervised approach is in most cases better than the other methods, especially when the number of training shots is very limited. All that the algorithm needs is to choose from a pool of positively correlated features. The methods are evaluated on the Youtube-Objects dataset of videos and on MNIST digits dataset, while at training time we also used features obtained on CIFAR10 dataset and others pre-trained on ImageNet dataset. Thereby, we also proved that transfer learning is useful, even though the datasets differ very much: from low-resolution centered images from 10 classes, to high-resolution images with objects from 1000 classes occurring in different regions of the images or to very difficult videos with very high intraclass variance. 7\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03131",
        "title": "Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey",
        "authors": [
            "Li Wang",
            "Dennis Sng"
        ],
        "abstract": "Deep learning has recently achieved very promising results in a wide range of areas such as computer vision, speech recognition and natural language processing. It aims to learn hierarchical representations of data by using deep architecture models. In a smart city, a lot of data (e.g. videos captured from many distributed sensors) need to be automatically processed and analyzed. In this paper, we review the deep learning algorithms applied to video analytics of smart city in terms of different research topics: object detection, object tracking, face recognition, image classification and scene labeling.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03155",
        "title": "Enhanced image feature coverage: Key-point selection using genetic algorithms",
        "authors": [
            "Erkan Bostanci"
        ],
        "abstract": "Coverage of image features play an important role in many vision algorithms since their distribution affect the estimated homography. This paper presents a Genetic Algorithm (GA) in order to select the optimal set of features yielding maximum coverage of the image which is measured by a robust method based on spatial statistics. It is shown with statistical tests on two datasets that the metric yields better coverage and this is also confirmed by an accuracy test on the computed homography for the original set and the newly selected set of features. Results have demonstrated that the new set has similar performance in terms of the accuracy of the computed homography with the original one with an extra benefit of using fewer number of features ultimately reducing the time required for descriptor calculation and matching.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03156",
        "title": "3D Reconstruction of Crime Scenes and Design Considerations for an Interactive Investigation Tool",
        "authors": [
            "Erkan Bostanci"
        ],
        "abstract": "Crime Scene Investigation (CSI) is a carefully planned systematic process with the purpose of acquiring physical evidences to shed light upon the physical reality of the crime and eventually detect the identity of the criminal. Capturing images and videos of the crime scene is an important part of this process in order to conduct a deeper analysis on the digital evidence for possible hints. This work brings this idea further to use the acquired footage for generating a 3D model of the crime scene. Results show that realistic reconstructions can be obtained using sophisticated computer vision techniques. The paper also discusses a number of important design considerations describing key features that should be present in a powerful interactive CSI analysis tool.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03384",
        "title": "VRFP: On-the-fly Video Retrieval using Web Images and Fast Fisher Vector Products",
        "authors": [
            "Xintong Han",
            "Bharat Singh",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "VRFP is a real-time video retrieval framework based on short text input queries, which obtains weakly labeled training images from the web after the query is known. The retrieved web images representing the query and each database video are treated as unordered collections of images, and each collection is represented using a single Fisher Vector built on CNN features. Our experiments show that a Fisher Vector is robust to noise present in web images and compares favorably in terms of accuracy to other standard representations. While a Fisher Vector can be constructed efficiently for a new query, matching against the test set is slow due to its high dimensionality. To perform matching in real-time, we present a lossless algorithm that accelerates the inner product computation between high dimensional Fisher Vectors. We prove that the expected number of multiplications required decreases quadratically with the sparsity of Fisher Vectors. We are not only able to construct and apply query models in real-time, but with the help of a simple re-ranking scheme, we also outperform state-of-the-art automatic retrieval methods by a significant margin on TRECVID MED13 (3.5%), MED14 (1.3%) and CCV datasets (5.2%). We also provide a direct comparison on standard datasets between two different paradigms for automatic video retrieval - zero-shot learning and on-the-fly retrieval.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2017-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03385",
        "title": "Deep Residual Learning for Image Recognition",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03424",
        "title": "Evaluation of Object Detection Proposals Under Condition Variations",
        "authors": [
            "Fahimeh Rezazadegan",
            "Sareh Shirazi",
            "Michael Milford",
            "Ben Upcroft"
        ],
        "abstract": "Object detection is a fundamental task in many computer vision applications, therefore the importance of evaluating the quality of object detection is well acknowledged in this domain. This process gives insight into the capabilities of methods in handling environmental changes. In this paper, a new method for object detection is introduced that combines the Selective Search and EdgeBoxes. We tested these three methods under environmental variations. Our experiments demonstrate the outperformance of the combination method under illumination and view point variations.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03460",
        "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering",
        "authors": [
            "Yezhou Yang",
            "Yi Li",
            "Cornelia Fermuller",
            "Yiannis Aloimonos"
        ],
        "abstract": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.\n    ",
        "submission_date": "2015-12-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03526",
        "title": "Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection",
        "authors": [
            "N. Benjamin Erichson",
            "Carl Donovan"
        ],
        "abstract": "This paper introduces a fast algorithm for randomized computation of a low-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this matrix to represent the development of a spatial grid through time e.g. data from a static video source. DMD was originally introduced in the fluid mechanics community, but is also suitable for motion detection in video streams and its use for background subtraction has received little previous investigation. In this study we present a comprehensive evaluation of background subtraction, using the randomized DMD and compare the results with leading robust principal component analysis algorithms. The results are convincing and show the random DMD is an efficient and powerful approach for background modeling, allowing processing of high resolution videos in real-time. Supplementary materials include implementations of the algorithms in Python.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03617",
        "title": "Robust Dictionary based Data Representation",
        "authors": [
            "Wei-Ya Ren"
        ],
        "abstract": "The robustness to noise and outliers is an important issue in linear representation in real applications. We focus on the problem that samples are grossly corrupted, which is also the 'sample specific' corruptions problem. A reasonable assumption is that corrupted samples cannot be represented by the dictionary while clean samples can be well represented. This assumption is enforced in this paper by investigating the coefficients of corrupted samples. Concretely, we require the coefficients of corrupted samples be zero. In this way, the representation quality of clean data can be assured without the effect of corrupted data. At last, a robust dictionary based data representation approach and its sparse representation version are proposed, which have directive significance for future applications.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03622",
        "title": "Deep Feature Learning with Relative Distance Comparison for Person Re-identification",
        "authors": [
            "Shengyong Ding",
            "Liang Lin",
            "Guangrun Wang",
            "Hongyang Chao"
        ],
        "abstract": "Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the $L2$ distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depends on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03706",
        "title": "A New Approach of Gray Images Binarization with Threshold Methods",
        "authors": [
            "Andrei Hossu",
            "Daniela Andone"
        ],
        "abstract": "The paper presents some aspects of the (gray level) image binarization methods used in artificial vision systems. It is introduced a new approach of gray level image binarization for artificial vision systems dedicated to industrial automation temporal thresholding. In the first part of the paper are extracted some limitations of using the global optimum thresholding in gray level image binarization. In the second part of this paper are presented some aspects of the dynamic optimum thresholding method for gray level image binarization. Starting from classic methods of global and dynamic optimal thresholding of the gray level images in the next section are introduced the concepts of temporal histogram and temporal thresholding. In the final section are presented some practical aspects of the temporal thresholding method in artificial vision applications form the moving scene in robotic automation class; pointing out the influence of the acquisition frequency on the methods results.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03740",
        "title": "Improving Human Activity Recognition Through Ranking and Re-ranking",
        "authors": [
            "Zhenzhong Lan",
            "Shoou-I Yu",
            "Alexander G. Hauptmann"
        ],
        "abstract": "We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.\n    ",
        "submission_date": "2015-12-11T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03958",
        "title": "RNN Fisher Vectors for Action Recognition and Image Annotation",
        "authors": [
            "Guy Lev",
            "Gil Sadeh",
            "Benjamin Klein",
            "Lior Wolf"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition.\n    ",
        "submission_date": "2015-12-12T00:00:00",
        "last_modified_date": "2015-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03980",
        "title": "Action Recognition with Image Based CNN Features",
        "authors": [
            "Mahdyar Ravanbakhsh",
            "Hossein Mousavi",
            "Mohammad Rastegari",
            "Vittorio Murino",
            "Larry S. Davis"
        ],
        "abstract": "Most of human actions consist of complex temporal compositions of more simple actions. Action recognition tasks usually relies on complex handcrafted structures as features to represent the human action model. Convolutional Neural Nets (CNN) have shown to be a powerful tool that eliminate the need for designing handcrafted features. Usually, the output of the last layer in CNN (a layer before the classification layer -known as fc7) is used as a generic feature for images. In this paper, we show that fc7 features, per se, can not get a good performance for the task of action recognition, when the network is trained only on images. We present a feature structure on top of fc7 features, which can capture the temporal variation in a video. To represent the temporal components, which is needed to capture motion information, we introduced a hierarchical structure. The hierarchical model enables to capture sub-actions from a complex action. At the higher levels of the hierarchy, it represents a coarse capture of action sequence and lower levels represent fine action elements. Furthermore, we introduce a method for extracting key-frames using binary coding of each frame in a video, which helps to improve the performance of our hierarchical model. We experimented our method on several action datasets and show that our method achieves superior results compared to other state-of-the-arts methods.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03993",
        "title": "Deep Tracking: Visual Tracking Using Deep Convolutional Networks",
        "authors": [
            "Meera Hahn",
            "Si Chen",
            "Afshin Dehghan"
        ],
        "abstract": "In this paper, we study a discriminatively trained deep convolutional network for the task of visual tracking. Our tracker utilizes both motion and appearance features that are extracted from a pre-trained dual stream deep convolution network. We show that the features extracted from our dual-stream network can provide rich information about the target and this leads to competitive performance against state of the art tracking methods on a visual tracking benchmark.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04065",
        "title": "Cross-dimensional Weighting for Aggregated Deep Convolutional Features",
        "authors": [
            "Yannis Kalantidis",
            "Clayton Mellina",
            "Simon Osindero"
        ],
        "abstract": "We propose a simple and straightforward way of creating powerful image representations via cross-dimensional weighting and aggregation of deep convolutional neural network layer outputs. We first present a generalized framework that encompasses a broad family of approaches and includes cross-dimensional pooling and weighting steps. We then propose specific non-parametric schemes for both spatial- and channel-wise weighting that boost the effect of highly active spatial responses and at the same time regulate burstiness effects. We experiment on different public datasets for image search and show that our approach outperforms the current state-of-the-art for approaches based on pre-trained networks. We also provide an easy-to-use, open source implementation that reproduces our results.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2016-07-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04077",
        "title": "Learning the Correction for Multi-Path Deviations in Time-of-Flight Cameras",
        "authors": [
            "Mojmir Mutny",
            "Rahul Nair",
            "Jens-Malte Gottfried"
        ],
        "abstract": "The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a challenging problem that hinders further processing of 3D data information. Based on the evidence from previous literature, we explored the possibility of using machine learning techniques to correct this effect. Firstly, we created two new datasets of of ToF images rendered via ToF simulator of LuxRender. These two datasets contain corners in multiple orientations and with different material properties. We chose scenes with corners as multipath effects are most pronounced in corners. Secondly, we used this dataset to construct a learning model to predict real valued corrections to the ToF data using Random Forests. We found out that in our smaller dataset we were able to predict real valued correction and improve the quality of depth images significantly by removing multipath bias. With our algorithm, we improved relative per-pixel error from average value of 19% to 3%. Additionally, variance of the error was lowered by an order of magnitude.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2016-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04086",
        "title": "Deep Learning-Based Image Kernel for Inductive Transfer",
        "authors": [
            "Neeraj Kumar",
            "Animesh Karmakar",
            "Ranti Dev Sharma",
            "Abhinav Mittal",
            "Amit Sethi"
        ],
        "abstract": "We propose a method to classify images from target classes with a small number of training examples based on transfer learning from non-target classes. Without using any more information than class labels for samples from non-target classes, we train a Siamese net to estimate the probability of two images to belong to the same class. With some post-processing, output of the Siamese net can be used to form a gram matrix of a Mercer kernel. Coupled with a support vector machine (SVM), such a kernel gave reasonable classification accuracy on target classes without any fine-tuning. When the Siamese net was only partially fine-tuned using a small number of samples from the target classes, the resulting classifier outperformed the state-of-the-art and other alternatives. We share class separation capabilities and insights into the learning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10 datasets.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04103",
        "title": "Deep Relative Attributes",
        "authors": [
            "Yaser Souri",
            "Erfan Noury",
            "Ehsan Adeli"
        ],
        "abstract": "Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2016-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04115",
        "title": "Unsupervised Temporal Segmentation of Repetitive Human Actions Based on Kinematic Modeling and Frequency Analysis",
        "authors": [
            "Qifei Wang",
            "Gregorij Kurillo",
            "Ferda Ofli",
            "Ruzena Bajcsy"
        ],
        "abstract": "In this paper, we propose a method for temporal segmentation of human repetitive actions based on frequency analysis of kinematic parameters, zero-velocity crossing detection, and adaptive k-means clustering. Since the human motion data may be captured with different modalities which have different temporal sampling rate and accuracy (e.g., optical motion capture systems vs. Microsoft Kinect), we first apply a generic full-body kinematic model with an unscented Kalman filter to convert the motion data into a unified representation that is robust to noise. Furthermore, we extract the most representative kinematic parameters via the primary frequency analysis. The sequences are segmented based on zero-velocity crossing of the selected parameters followed by an adaptive k-means clustering to identify the repetition segments. Experimental results demonstrate that for the motion data captured by both the motion capture system and the Microsoft Kinect, our proposed algorithm obtains robust segmentation of repetitive action sequences.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04118",
        "title": "Articulated Pose Estimation Using Hierarchical Exemplar-Based Models",
        "authors": [
            "Jiongxin Liu",
            "Yinxiao Li",
            "Peter Allen",
            "Peter Belhumeur"
        ],
        "abstract": "Exemplar-based models have achieved great success on localizing the parts of semi-rigid objects. However, their efficacy on highly articulated objects such as humans is yet to be explored. Inspired by hierarchical object representation and recent application of Deep Convolutional Neural Networks (DCNNs) on human pose estimation, we propose a novel formulation that incorporates both hierarchical exemplar-based models and DCNNs in the spatial terms. Specifically, we obtain more expressive spatial models by assuming independence between exemplars at different levels in the hierarchy; we also obtain stronger spatial constraints by inferring the spatial relations between parts at the same level. As our method strikes a good balance between expressiveness and strength of spatial models, it is both effective and generalizable, achieving state-of-the-art results on different benchmarks: Leeds Sports Dataset and CUB-200-2011.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04133",
        "title": "A Person Re-Identification System For Mobile Devices",
        "authors": [
            "George Cushen"
        ],
        "abstract": "Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04134",
        "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect",
        "authors": [
            "Qifei Wang",
            "Gregorij Kurillo",
            "Ferda Ofli",
            "Ruzena Bajcsy"
        ],
        "abstract": "Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.\n    ",
        "submission_date": "2015-12-13T00:00:00",
        "last_modified_date": "2015-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04143",
        "title": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks",
        "authors": [
            "Sean Bell",
            "C. Lawrence Zitnick",
            "Kavita Bala",
            "Ross Girshick"
        ],
        "abstract": "It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04150",
        "title": "Learning Deep Features for Discriminative Localization",
        "authors": [
            "Bolei Zhou",
            "Aditya Khosla",
            "Agata Lapedriza",
            "Aude Oliva",
            "Antonio Torralba"
        ],
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04205",
        "title": "Compressed Dynamic Mode Decomposition for Background Modeling",
        "authors": [
            "N. Benjamin Erichson",
            "Steven L. Brunton",
            "J. Nathan Kutz"
        ],
        "abstract": "We introduce the method of compressed dynamic mode decomposition (cDMD) for background modeling. The dynamic mode decomposition (DMD) is a regression technique that integrates two of the leading data analysis methods in use today: Fourier transforms and singular value decomposition. Borrowing ideas from compressed sensing and matrix sketching, cDMD eases the computational workload of high resolution video processing. The key principal of cDMD is to obtain the decomposition on a (small) compressed matrix representation of the video feed. Hence, the cDMD algorithm scales with the intrinsic rank of the matrix, rather then the size of the actual video (data) matrix. Selection of the optimal modes characterizing the background is formulated as a sparsity-constrained sparse coding problem. Our results show, that the quality of the resulting background model is competitive, quantified by the F-measure, Recall and Precision. A GPU (graphics processing unit) accelerated implementation is also presented which further boosts the computational efficiency of the algorithm.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04219",
        "title": "On the Relation between two Rotation Metrics",
        "authors": [
            "Thomas Ruland"
        ],
        "abstract": "In their work \"Global Optimization through Rotation Space Search\", Richard Hartley and Fredrik Kahl introduce a global optimization strategy for problems in geometric computer vision, based on rotation space search using a branch-and-bound algorithm. In its core, Lemma 2 of their publication is the important foundation for a class of global optimization algorithms, which is adopted over a wide range of problems in subsequent publications. This lemma relates a metric on rotations represented by rotation matrices with a metric on rotations in axis-angle representation. This work focuses on a proof for this relationship, which is based on Rodrigues' Rotation Theorem for the composition of rotations in axis-angle representation.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04295",
        "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator",
        "authors": [
            "Lukas Cavigelli",
            "Luca Benini"
        ],
        "abstract": "An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04407",
        "title": "We Are Humor Beings: Understanding and Predicting Visual Humor",
        "authors": [
            "Arjun Chandrasekaran",
            "Ashwin K. Vijayakumar",
            "Stanislaw Antol",
            "Mohit Bansal",
            "Dhruv Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2016-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04412",
        "title": "Instance-aware Semantic Segmentation via Multi-task Network Cascades",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems.\n",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04418",
        "title": "Sparse Representation of a Blur Kernel for Blind Image Restoration",
        "authors": [
            "Chia-Chen Lee",
            "Wen-Liang Hwang"
        ],
        "abstract": "Blind image restoration is a non-convex problem which involves restoration of images from an unknown blur kernel. The factors affecting the performance of this restoration are how much prior information about an image and a blur kernel are provided and what algorithm is used to perform the restoration task. Prior information on images is often employed to restore the sharpness of the edges of an image. By contrast, no consensus is still present regarding what prior information to use in restoring from a blur kernel due to complex image blurring processes. In this paper, we propose modelling of a blur kernel as a sparse linear combinations of basic 2-D patterns. Our approach has a competitive edge over the existing blur kernel modelling methods because our method has the flexibility to customize the dictionary design, which makes it well-adaptive to a variety of applications. As a demonstration, we construct a dictionary formed by basic patterns derived from the Kronecker product of Gaussian sequences. We also compare our results with those derived by other state-of-the-art methods, in terms of peak signal to noise ratio (PSNR).\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04509",
        "title": "On non-iterative training of a neural classifier",
        "authors": [
            "K.Eswaran",
            "K.Damodhar Rao"
        ],
        "abstract": "Recently an algorithm, was discovered, which separates points in n-dimension by planes in such a manner that no two points are left un-separated by at least one plane{[}1-3{]}. By using this new algorithm we show that there are two ways of classification by a neural network, for a large dimension feature space, both of which are non-iterative and deterministic. To demonstrate the power of both these methods we apply them exhaustively to the classical pattern recognition problem: The Fisher-Anderson's, IRIS flower data set and present the results.\n",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04582",
        "title": "Interactive Volumetry Of Liver Ablation Zones",
        "authors": [
            "Jan Egger",
            "Harald Busse",
            "Philipp Brandmaier",
            "Daniel Seider",
            "Matthias Gawlitza",
            "Steffen Strocka",
            "Philip Voglreiter",
            "Mark Dokter",
            "Michael Hofmann",
            "Bernhard Kainz",
            "Alexander Hann",
            "Xiaojun Chen",
            "Tuomas Alhonnoro",
            "Mika Pollari",
            "Dieter Schmalstieg",
            "Michael Moche"
        ],
        "abstract": "Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique that destroys cancer cells by heat. The heat results from focusing energy in the radiofrequency spectrum through a needle. Amongst others, this can enable the treatment of patients who are not eligible for an open surgery. However, the possibility of recurrent liver cancer due to incomplete ablation of the tumor makes post-interventional monitoring via regular follow-up scans mandatory. These scans have to be carefully inspected for any conspicuousness. Within this study, the RF ablation zones from twelve post-interventional CT acquisitions have been segmented semi-automatically to support the visual inspection. An interactive, graph-based contouring approach, which prefers spherically shaped regions, has been applied. For the quantitative and qualitative analysis of the algorithm's results, manual slice-by-slice segmentations produced by clinical experts have been used as the gold standard (which have also been compared among each other). As evaluation metric for the statistical validation, the Dice Similarity Coefficient (DSC) has been calculated. The results show that the proposed tool provides lesion segmentation with sufficient accuracy much faster than manual segmentation. The visual feedback and interactivity make the proposed tool well suitable for the clinical workflow.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2015-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04605",
        "title": "Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context",
        "authors": [
            "Marian-Andrei Rizoiu",
            "Julien Velcin",
            "St\u00e9phane Lallich"
        ],
        "abstract": "One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation.\n    ",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04785",
        "title": "On Deep Representation Learning from Noisy Web Images",
        "authors": [
            "Phong D. Vo",
            "Alexandru Ginsca",
            "Herv\u00e9 Le Borgne",
            "Adrian Popescu"
        ],
        "abstract": "The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04958",
        "title": "Context Driven Label Fusion for segmentation of Subcutaneous and Visceral Fat in CT Volumes",
        "authors": [
            "Sarfaraz Hussein",
            "Aileen Green",
            "Arjun Watane",
            "Georgios Papadakis",
            "Medhat Osman",
            "Ulas Bagci"
        ],
        "abstract": "Quantification of adipose tissue (fat) from computed tomography (CT) scans is conducted mostly through manual or semi-automated image segmentation algorithms with limited efficacy. In this work, we propose a completely unsupervised and automatic method to identify adipose tissue, and then separate Subcutaneous Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal region. We offer a three-phase pipeline consisting of (1) Initial boundary estimation using gradient points, (2) boundary refinement using Geometric Median Absolute Deviation and Appearance based Local Outlier Scores (3) Context driven label fusion using Conditional Random Fields (CRF) to obtain the final boundary between SAT and VAT. We evaluate the proposed method on 151 abdominal CT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT and VAT segmentation, as well as significant reduction in fat quantification error measure.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05227",
        "title": "Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop",
        "authors": [
            "Yin Cui",
            "Feng Zhou",
            "Yuanqing Lin",
            "Serge Belongie"
        ],
        "abstract": "Existing fine-grained visual categorization methods often suffer from three challenges: lack of training data, large number of fine-grained categories, and high intraclass vs. low inter-class variance. In this work we propose a generic iterative framework for fine-grained categorization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These anchor points capture intra-class variances and remain discriminative between classes. In each round, images with high confidence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark each candidate image as either a \"true positive\" or a \"false positive\". True positives are added into our current dataset and false positives are regarded as \"hard negatives\" for our metric learning model. Then the model is retrained with an expanded dataset and hard negatives for the next round. To demonstrate the effectiveness of the proposed framework, we bootstrap a fine-grained flower dataset with 620 categories from Instagram images. The proposed deep metric learning scheme is evaluated on both our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state-of-the-art results achieved by the proposed deep metric learning methods.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05246",
        "title": "Blockout: Dynamic Model Selection for Hierarchical Deep Networks",
        "authors": [
            "Calvin Murdock",
            "Zhen Li",
            "Howard Zhou",
            "Tom Duerig"
        ],
        "abstract": "Most deep architectures for image classification--even those that are trained to classify a large number of diverse categories--learn shared image representations with a single model. Intuitively, however, categories that are more similar should share more information than those that are very different. While hierarchical deep networks address this problem by learning separate features for subsets of related categories, current implementations require simplified models using fixed architectures specified via heuristic clustering methods. Instead, we propose Blockout, a method for regularization and model selection that simultaneously learns both the model architecture and parameters. A generalization of Dropout, our approach gives a novel parametrization of hierarchical architectures that allows for structure learning via back-propagation. To demonstrate its utility, we evaluate Blockout on the CIFAR and ImageNet datasets, demonstrating improved classification accuracy, better regularization performance, faster training, and the clear emergence of hierarchical network structures.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05278",
        "title": "Shape and Spatially-Varying Reflectance Estimation From Virtual Exemplars",
        "authors": [
            "Zhuo Hui",
            "Aswin C Sankaranarayanan"
        ],
        "abstract": "This paper addresses the problem of estimating the shape of objects that exhibit spatially-varying reflectance. We assume that multiple images of the object are obtained under a fixed view-point and varying illumination, i.e., the setting of photometric stereo. At the core of our techniques is the assumption that the BRDF at each pixel lies in the non-negative span of a known BRDF ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2016-09-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05300",
        "title": "Multiregion Bilinear Convolutional Neural Networks for Person Re-Identification",
        "authors": [
            "Evgeniya Ustinova",
            "Yaroslav Ganin",
            "Victor Lempitsky"
        ],
        "abstract": "In this work we propose a new architecture for person re-identification. As the task of re-identification is inherently associated with embedding learning and non-rigid appearance description, our architecture is based on the deep bilinear convolutional network (Bilinear-CNN) that has been proposed recently for fine-grained classification of highly non-rigid objects. While the last stages of the original Bilinear-CNN architecture completely removes the geometric information from consideration by performing orderless pooling, we observe that a better embedding can be learned by performing bilinear pooling in a more local way, where each pooling is confined to a predefined region. Our architecture thus represents a compromise between traditional convolutional networks and bilinear CNNs and strikes a balance between rigid matching and completely ignoring spatial information.\n",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2017-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05421",
        "title": "Numerical Demultiplexing of Color Image Sensor Measurements via Non-linear Random Forest Modeling",
        "authors": [
            "Jason Deglint",
            "Farnoud Kazemzadeh",
            "Daniel Cho",
            "David A. Clausi",
            "Alexander Wong"
        ],
        "abstract": "The simultaneous capture of imaging data at multiple wavelengths across the electromagnetic spectrum is highly challenging, requiring complex and costly multispectral image sensors. In this study, we introduce a comprehensive framework for performing simultaneous multispectral imaging using conventional image sensors with color filter arrays via numerical demultiplexing of the color image sensor measurements. A numerical forward model characterizing the formation of sensor measurements from light spectra hitting the sensor is constructed based on a comprehensive spectral characterization of the sensor. A numerical demultiplexer is then learned via non-linear random forest modeling based on the forward model. Given the learned numerical demultiplexer, one can then demultiplex simultaneously-acquired measurements made by the image sensor into reflectance intensities at discrete selectable wavelengths, resulting in a higher resolution reflectance spectrum. Simulation and real-world experimental results demonstrate the efficacy of such a method for simultaneous multispectral imaging.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05430",
        "title": "Large Scale Business Discovery from Street Level Imagery",
        "authors": [
            "Qian Yu",
            "Christian Szegedy",
            "Martin C. Stumpe",
            "Liron Yatziv",
            "Vinay Shet",
            "Julian Ibarz",
            "Sacha Arnoud"
        ],
        "abstract": "Search with local intent is becoming increasingly useful due to the popularity of the mobile device. The creation and maintenance of accurate listings of local businesses worldwide is time consuming and expensive. In this paper, we propose an approach to automatically discover businesses that are visible on street level imagery. Precise business store front detection enables accurate geo-location of businesses, and further provides input for business categorization, listing generation, etc. The large variety of business categories in different countries makes this a very challenging problem. Moreover, manual annotation is prohibitive due to the scale of this problem. We propose the use of a MultiBox based approach that takes input image pixels and directly outputs store front bounding boxes. This end-to-end learning approach instead preempts the need for hand modeling either the proposal generation phase or the post-processing phase, leveraging large labelled training datasets. We demonstrate our approach outperforms the state of the art detection techniques with a large margin in terms of performance and run-time efficiency. In the evaluation, we show this approach achieves human accuracy in the low-recall settings. We also provide an end-to-end evaluation of business discovery in the real world.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05586",
        "title": "Reconstruction of Enhanced Ultrasound Images From Compressed Measurements Using Simultaneous Direction Method of Multipliers",
        "authors": [
            "Zhouye Chen",
            "Adrian Basarab",
            "Denis Kouam\u00e9"
        ],
        "abstract": "High resolution ultrasound image reconstruction from a reduced number of measurements is of great interest in ultrasound imaging, since it could enhance both the frame rate and image resolution. Compressive deconvolution, combining compressed sensing and image deconvolution, represents an interesting possibility to consider this challenging task. The model of compressive deconvolution includes, in addition to the compressive sampling matrix, a 2D convolution operator carrying the information on the system point spread function. Through this model, the resolution of reconstructed ultrasound images from compressed measurements mainly depends on three aspects: the acquisition setup, i.e. the incoherence of the sampling matrix, the image regularization, i.e. the sparsity prior, and the optimization technique. In this paper, we mainly focused on the last two aspects. We proposed a novel simultaneous direction method of multipliers-based optimization scheme to invert the linear model, including two regularization terms expressing the sparsity of the RF images in a given basis and the generalized Gaussian statistical assumption on tissue reflectivity functions. The performance of the method is evaluated on both simulated and in vivo data.\n    ",
        "submission_date": "2015-12-17T00:00:00",
        "last_modified_date": "2015-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05653",
        "title": "Effects of GIMP Retinex Filtering Evaluated by the Image Entropy",
        "authors": [
            "A.C. Sparavigna",
            "R. Marazzato"
        ],
        "abstract": "A GIMP Retinex filtering can be used for enhancing images, with good results on foggy images, as recently discussed. Since this filter has some parameters that can be adjusted to optimize the output image, several approaches can be decided according to desired results. Here, as a criterion for optimizing the filtering parameters, we consider the maximization of the image entropy. We use, besides the Shannon entropy, also a generalized entropy.\n    ",
        "submission_date": "2015-12-16T00:00:00",
        "last_modified_date": "2015-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05830",
        "title": "Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks",
        "authors": [
            "Li Shen",
            "Zhouchen Lin",
            "Qingming Huang"
        ],
        "abstract": "Learning deeper convolutional neural networks becomes a tendency in recent years. However, many empirical evidences suggest that performance improvement cannot be gained by simply stacking more layers. In this paper, we consider the issue from an information theoretical perspective, and propose a novel method Relay Backpropagation, that encourages the propagation of effective information through the network in training stage. By virtue of the method, we achieved the first place in ILSVRC 2015 Scene Classification Challenge. Extensive experiments on two challenging large scale datasets demonstrate the effectiveness of our method is not restricted to a specific dataset or network architecture. Our models will be available to the research community later.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2016-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05844",
        "title": "Domain Adaptation and Transfer Learning in StochasticNets",
        "authors": [
            "Mohammad Javad Shafiee",
            "Parthipan Siva",
            "Paul Fieguth",
            "Alexander Wong"
        ],
        "abstract": "Transfer learning is a recent field of machine learning research that aims to resolve the challenge of dealing with insufficient training data in the domain of interest. This is a particular issue with traditional deep neural networks where a large amount of training data is needed. Recently, StochasticNets was proposed to take advantage of sparse connectivity in order to decrease the number of parameters that needs to be learned, which in turn may relax training data size requirements. In this paper, we study the efficacy of transfer learning on StochasticNet frameworks. Experimental results show ~7% improvement on StochasticNet performance when the transfer learning is applied in training step.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05986",
        "title": "Can Pretrained Neural Networks Detect Anatomy?",
        "authors": [
            "Vlado Menkovski",
            "Zharko Aleksovski",
            "Axel Saalbach",
            "Hannes Nickisch"
        ],
        "abstract": "Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05990",
        "title": "Deformable Distributed Multiple Detector Fusion for Multi-Person Tracking",
        "authors": [
            "Andy J Ma",
            "Pong C Yuen",
            "Suchi Saria"
        ],
        "abstract": "This paper addresses fully automated multi-person tracking in complex environments with challenging occlusion and extensive pose variations. Our solution combines multiple detectors for a set of different regions of interest (e.g., full-body and head) for multi-person tracking. The use of multiple detectors leads to fewer miss detections as it is able to exploit the complementary strengths of the individual detectors. While the number of false positives may increase with the increased number of bounding boxes detected from multiple detectors, we propose to group the detection outputs by bounding box location and depth information. For robustness to significant pose variations, deformable spatial relationship between detectors are learnt in our multi-person tracking system. On RGBD data from a live Intensive Care Unit (ICU), we show that the proposed method significantly improves multi-person tracking performance over state-of-the-art methods.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06009",
        "title": "Face Hallucination using Linear Models of Coupled Sparse Support",
        "authors": [
            "Reuben Farrugia",
            "Christine Guillemot"
        ],
        "abstract": "Most face super-resolution methods assume that low-resolution and high-resolution manifolds have similar local geometrical structure, hence learn local models on the lowresolution manifolds (e.g. sparse or locally linear embedding models), which are then applied on the high-resolution manifold. However, the low-resolution manifold is distorted by the oneto-many relationship between low- and high- resolution patches. This paper presents a method which learns linear models based on the local geometrical structure on the high-resolution manifold rather than on the low-resolution manifold. For this, in a first step, the low-resolution patch is used to derive a globally optimal estimate of the high-resolution patch. The approximated solution is shown to be close in Euclidean space to the ground-truth but is generally smooth and lacks the texture details needed by state-ofthe-art face recognizers. This first estimate allows us to find the support of the high-resolution manifold using sparse coding (SC), which are then used as support for learning a local projection (or upscaling) model between the low-resolution and the highresolution manifolds using Multivariate Ridge Regression (MRR). Experimental results show that the proposed method outperforms six face super-resolution methods in terms of both recognition and quality. These results also reveal that the recognition and quality are significantly affected by the method used for stitching all super-resolved patches together, where quilting was found to better preserve the texture details which helps to achieve higher recognition rates.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06014",
        "title": "Multiclass Classification of Cervical Cancer Tissues by Hidden Markov Model",
        "authors": [
            "Sabyasachi Mukhopadhyay",
            "Sanket Nandan",
            "Indrajit Kurmi"
        ],
        "abstract": "In this paper, we report a hidden Markov model based multiclass classification of cervical cancer tissues. This model has been validated directly over time series generated by the medium refractive index fluctuations extracted from differential interference contrast images of healthy and different stages of cancer tissues. The method shows promising results for multiclass classification with higher accuracy.\n    ",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2016-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06075",
        "title": "Modeling Colors of Single Attribute Variations with Application to Food Appearance",
        "authors": [
            "Yaser Yacoob"
        ],
        "abstract": "This paper considers the intra-image color-space of an object or a scene when these are subject to a dominant single-source of variation. The source of variation can be intrinsic or extrinsic (i.e., imaging conditions) to the object. We observe that the quantized colors for such objects typically lie on a planar subspace of RGB, and in some cases linear or polynomial curves on this plane are effective in capturing these color variations. We also observe that the inter-image color sub-spaces are robust as long as drastic illumination change is not involved.\n",
        "submission_date": "2015-12-18T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06223",
        "title": "Combining patch-based strategies and non-rigid registration-based label fusion methods",
        "authors": [
            "Carlos Platero",
            "M. Carmen Tobar"
        ],
        "abstract": "The objective of this study is to develop a patch-based labeling method that cooperates with a label fusion using non-rigid registrations. We present a novel patch-based label fusion method, whose selected patches and their weights are calculated from a combination of similarity measures between patches using intensity-based distances and labeling-based distances, where a previous labeling of the target image is inferred through a label fusion method using non-rigid registrations. These combined similarity measures result in better selection of the patches, and their weights are more robust, which improves the segmentation results compared to other label fusion methods, including the conventional patch-based labeling method. To evaluate the performance and the robustness of the proposed label fusion method, we employ two available databases of T1-weighted (T1W) magnetic resonance imaging (MRI) of human brains. We compare our approach with other label fusion methods in the automatic hippocampal segmentation from T1W-MRI.\n",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2015-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06235",
        "title": "Multistage SFM: A Coarse-to-Fine Approach for 3D Reconstruction",
        "authors": [
            "Rajvi Shah",
            "Aditya Deshpande",
            "P J Narayanan"
        ],
        "abstract": "Several methods have been proposed for large-scale 3D reconstruction from large, unorganized image collections. A large reconstruction problem is typically divided into multiple components which are reconstructed independently using structure from motion (SFM) and later merged together. Incremental SFM methods are most popular for the basic structure recovery of a single component. They are robust and effective but are strictly sequential in nature. We present a multistage approach for SFM reconstruction of a single component that breaks the sequential nature of the incremental SFM methods. Our approach begins with quickly building a coarse 3D model using only a fraction of features from given images. The coarse model is then enriched by localizing remaining images and matching and triangulating remaining features in subsequent stages. These stages are made efficient and highly parallel by leveraging the geometry of the coarse model. Our method produces similar quality models as compared to incremental SFM methods while being notably fast and parallel.\n    ",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2016-10-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06285",
        "title": "Neutro-Connectedness Cut",
        "authors": [
            "Min Xian",
            "Yingtao Zhang",
            "H. D. Cheng",
            "Fei Xu",
            "Jianrui Ding"
        ],
        "abstract": "Interactive image segmentation is a challenging task and receives increasing attention recently; however, two major drawbacks exist in interactive segmentation approaches. First, the segmentation performance of ROI-based methods is sensitive to the initial ROI: different ROIs may produce results with great difference. Second, most seed-based methods need intense interactions, and are not applicable in many cases. In this work, we generalize the Neutro-Connectedness (NC) to be independent of top-down priors of objects and to model image topology with indeterminacy measurement on image regions, propose a novel method for determining object and background regions, which is applied to exclude isolated background regions and enforce label consistency, and put forward a hybrid interactive segmentation method, Neutro-Connectedness Cut (NC-Cut), which can overcome the above two problems by utilizing both pixel-wise appearance information and region-based NC properties. We evaluate the proposed NC-Cut by employing two image datasets (265 images), and demonstrate that the proposed approach outperforms state-of-the-art interactive image segmentation methods (Grabcut, MILCut, One-Cut, MGC_max^sum and pPBC).\n    ",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2016-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06473",
        "title": "Quantized Convolutional Neural Networks for Mobile Devices",
        "authors": [
            "Jiaxiang Wu",
            "Cong Leng",
            "Yuhang Wang",
            "Qinghao Hu",
            "Jian Cheng"
        ],
        "abstract": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-05-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06492",
        "title": "Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect",
        "authors": [
            "Qifei Wang",
            "Gregorij Kurillo",
            "Ferda Ofli",
            "Ruzena Bajcsy"
        ],
        "abstract": "This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06498",
        "title": "Harnessing the Deep Net Object Models for Enhancing Human Action Recognition",
        "authors": [
            "O. V. Ramana Murthy",
            "Roland Goecke"
        ],
        "abstract": "In this study, the influence of objects is investigated in the scenario of human action recognition with large number of classes. We hypothesize that the objects the humans are interacting will have good say in determining the action being performed. Especially, if the objects are non-moving, such as objects appearing in the background, features such as spatio-temporal interest points, dense trajectories may fail to detect them. Hence we propose to detect objects using pre-trained object detectors in every frame statically. Trained Deep network models are used as object detectors. Information from different layers in conjunction with different encoding techniques is extensively studied to obtain the richest feature vectors. This technique is observed to yield state-of-the-art performance on HMDB51 and UCF101 datasets.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06539",
        "title": "Spatial Phase-Sweep: Increasing temporal resolution of transient imaging using a light source array",
        "authors": [
            "Ryuichi Tadano",
            "Adithya Kumar Pediredla",
            "Kaushik Mitra",
            "Ashok Veeraraghavan"
        ],
        "abstract": "Transient imaging or light-in-flight techniques capture the propagation of an ultra-short pulse of light through a scene, which in effect captures the optical impulse response of the scene. Recently, it has been shown that we can capture transient images using commercially available Time-of-Flight (ToF) systems such as Photonic Mixer Devices (PMD). In this paper, we propose `spatial phase-sweep', a technique that exploits the speed of light to increase the temporal resolution beyond the 100 picosecond limit imposed by current electronics. Spatial phase-sweep uses a linear array of light sources with spatial separation of about 3 mm between them, thereby resulting in a time shift of about 10 picoseconds, which translates into 100 Gfps of transient imaging in theory. We demonstrate a prototype and transient imaging results using spatial phase-sweep.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06559",
        "title": "Analysis of Vessel Connectivities in Retinal Images by Cortically Inspired Spectral Clustering",
        "authors": [
            "Marta Favali",
            "Samaneh Abbasi-Sureshjani",
            "Bart ter Haar Romeny",
            "Alessandro Sarti"
        ],
        "abstract": "Retinal images provide early signs of diabetic retinopathy, glaucoma, and hypertension. These signs can be investigated based on microaneurysms or smaller vessels. The diagnostic biomarkers are the change of vessel widths and angles especially at junctions, which are investigated using the vessel segmentation or tracking. Vessel paths may also be interrupted; crossings and bifurcations may be disconnected. This paper addresses a novel contextual method based on the geometry of the primary visual cortex (V1) to study these difficulties. We have analyzed the specific problems at junctions with a connectivity kernel obtained as the fundamental solution of the Fokker-Planck equation, which is usually used to represent the geometrical structure of multi-orientation cortical connectivity. Using the spectral clustering on a large local affinity matrix constructed by both the connectivity kernel and the feature of intensity, the vessels are identified successfully in a hierarchical topology each representing an individual perceptual unit.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06566",
        "title": "Local and global gestalt laws: A neurally based spectral approach",
        "authors": [
            "Marta Favali",
            "Giovanna Citti",
            "Alessandro Sarti"
        ],
        "abstract": "A mathematical model of figure-ground articulation is presented, taking into account both local and global gestalt laws. The model is compatible with the functional architecture of the primary visual cortex (V1). Particularly the local gestalt law of good continuity is described by means of suitable connectivity kernels, that are derived from Lie group theory and are neurally implemented in long range connectivity in V1. Different kernels are compatible with the geometric structure of cortical connectivity and they are derived as the fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacian and the isotropic Laplacian equations. The kernels are used to construct matrices of connectivity among the features present in a visual stimulus. Global gestalt constraints are then introduced in terms of spectral analysis of the connectivity matrix, showing that this processing can be cortically implemented in V1 by mean field neural equations. This analysis performs grouping of local features and individuates perceptual units with the highest saliency. Numerical simulations are performed and results are obtained applying the technique to a number of stimuli.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06709",
        "title": "Sparse Coding with Fast Image Alignment via Large Displacement Optical Flow",
        "authors": [
            "Xiaoxia Sun",
            "Nasser M. Nasrabadi",
            "Trac D. Tran"
        ],
        "abstract": "Sparse representation-based classifiers have shown outstanding accuracy and robustness in image classification tasks even with the presence of intense noise and occlusion. However, it has been discovered that the performance degrades significantly either when test image is not aligned with the dictionary atoms or the dictionary atoms themselves are not aligned with each other, in which cases the sparse linear representation assumption fails. In this paper, having both training and test images misaligned, we introduce a novel sparse coding framework that is able to efficiently adapt the dictionary atoms to the test image via large displacement optical flow. In the proposed algorithm, every dictionary atom is automatically aligned with the input image and the sparse code is then recovered using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed for the proposed framework. Experimental results on digit datasets recognition verify the efficacy and robustness of the proposed algorithm.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06735",
        "title": "Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs",
        "authors": [
            "Ziyu Zhang",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "Our aim is to provide a pixel-wise instance-level labeling of a monocular image in the context of autonomous driving. We build on recent work [Zhang et al., ICCV15] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image. A simple Markov random field model using several heuristics was then proposed in [Zhang et al., ICCV15] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field and show how to encode various intuitive potentials in a way that is amenable to efficient mean field inference [Kr\u00e4henb\u00fchl et al., NIPS11]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a significant performance boost over the baseline [Zhang et al., ICCV15].\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-04-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06757",
        "title": "GraphConnect: A Regularization Framework for Neural Networks",
        "authors": [
            "Jiaji Huang",
            "Qiang Qiu",
            "Robert Calderbank",
            "Guillermo Sapiro"
        ],
        "abstract": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06790",
        "title": "Car Segmentation and Pose Estimation using 3D Object Models",
        "authors": [
            "Siddharth Mahendran",
            "Ren\u00e9 Vidal"
        ],
        "abstract": "Image segmentation and 3D pose estimation are two key cogs in any algorithm for scene understanding. However, state-of-the-art CRF-based models for image segmentation rely mostly on 2D object models to construct top-down high-order potentials. In this paper, we propose new top-down potentials for image segmentation and pose estimation based on the shape and volume of a 3D object model. We show that these complex top-down potentials can be easily decomposed into standard forms for efficient inference in both the segmentation and pose estimation tasks. Experiments on a car dataset show that knowledge of segmentation helps perform pose estimation better and vice versa.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06925",
        "title": "Transformed Residual Quantization for Approximate Nearest Neighbor Search",
        "authors": [
            "Jiangbo Yuan",
            "Xiuwen Liu"
        ],
        "abstract": "The success of product quantization (PQ) for fast nearest neighbor search depends on the exponentially reduced complexities of both storage and computation with respect to the codebook size. Recent efforts have been focused on employing sophisticated optimization strategies, or seeking more effective models. Residual quantization (RQ) is such an alternative that holds the same property as PQ in terms of the aforementioned complexities. In addition to being a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for approximate nearest neighbor search. This motivated us to propose a novel approach to optimizing RQ and the related hybrid models. With an observation of the general randomness increase in a residual space, we propose a new strategy that jointly learns a local transformation per residual cluster with an ultimate goal to reduce overall quantization errors. We have shown that our approach can achieve significantly better accuracy on nearest neighbor search than both the original and the optimized PQ on several very large scale benchmarks.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06963",
        "title": "Multi-Instance Visual-Semantic Embedding",
        "authors": [
            "Zhou Ren",
            "Hailin Jin",
            "Zhe Lin",
            "Chen Fang",
            "Alan Yuille"
        ],
        "abstract": "Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning, by mapping images into a continuous semantic label space. Although several approaches have been proposed for single-label embedding tasks, handling images with multiple labels (which is a more general setting) still remains an open problem, mainly due to the complex underlying corresponding relationship between image and its labels. In this work, we present Multi-Instance visual-semantic Embedding model (MIE) for embedding images associated with either single or multiple labels. Our model discovers and maps semantically-meaningful image subregions to their corresponding labels. And we demonstrate the superiority of our method over the state-of-the-art on two tasks, including multi-label image annotation and zero-shot learning.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06974",
        "title": "Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels",
        "authors": [
            "Ishan Misra",
            "C. Lawrence Zitnick",
            "Margaret Mitchell",
            "Ross Girshick"
        ],
        "abstract": "When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy \"human-centric\" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting \"what's in the image\" versus \"what's worth saying.\" We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07030",
        "title": "Deep Learning with S-shaped Rectified Linear Activation Units",
        "authors": [
            "Xiaojie Jin",
            "Chunyan Xu",
            "Jiashi Feng",
            "Yunchao Wei",
            "Junjun Xiong",
            "Shuicheng Yan"
        ],
        "abstract": "Rectified linear activation units are important components for state-of-the-art deep convolutional networks. In this paper, we propose a novel S-shaped rectified linear activation unit (SReLU) to learn both convex and non-convex functions, imitating the multiple function forms given by the two fundamental laws, namely the Webner-Fechner law and the Stevens law, in psychophysics and neural sciences. Specifically, SReLU consists of three piecewise linear functions, which are formulated by four learnable parameters. The SReLU is learned jointly with the training of the whole deep network through back propagation. During the training phase, to initialize SReLU in different layers, we propose a \"freezing\" method to degenerate SReLU into a predefined leaky rectified linear unit in the initial several training epochs and then adaptively learn the good initial values. SReLU can be universally used in the existing deep networks with negligible additional parameters and computation cost. Experiments with two popular CNN architectures, Network in Network and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100, MNIST and ImageNet demonstrate that SReLU achieves remarkable improvement compared to other activation functions.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07041",
        "title": "Implementation of deep learning algorithm for automatic detection of brain tumors using intraoperative IR-thermal mapping data",
        "authors": [
            "A.V. Makarenko",
            "M.G. Volovik"
        ],
        "abstract": "The efficiency of deep machine learning for automatic delineation of tumor areas has been demonstrated for intraoperative neuronavigation using active IR-mapping with the use of the cold test. The proposed approach employs a matrix IR-imager to remotely register the space-time distribution of surface temperature pattern, which is determined by the dynamics of local cerebral blood flow. The advantages of this technique are non-invasiveness, zero risks for the health of patients and medical staff, low implementation and operational costs, ease and speed of use. Traditional IR-diagnostic technique has a crucial limitation - it involves a diagnostician who determines the boundaries of tumor areas, which gives rise to considerable uncertainty, which can lead to diagnosis errors that are difficult to control. The current study demonstrates that implementing deep learning algorithms allows to eliminate the explained drawback.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07080",
        "title": "Cost-based Feature Transfer for Vehicle Occupant Classification",
        "authors": [
            "Toby Perrett",
            "Majid Mirmehdi",
            "Eduardo Dias"
        ],
        "abstract": "Knowledge of human presence and interaction in a vehicle is of growing interest to vehicle manufacturers for design and safety purposes. We present a framework to perform the tasks of occupant detection and occupant classification for automatic child locks and airbag suppression. It operates for all passenger seats, using a single overhead camera. A transfer learning technique is introduced to make full use of training data from all seats whilst still maintaining some control over the bias, necessary for a system designed to penalize certain misclassifications more than others. An evaluation is performed on a challenging dataset with both weighted and unweighted classifiers, demonstrating the effectiveness of the transfer process.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07108",
        "title": "Recent Advances in Convolutional Neural Networks",
        "authors": [
            "Jiuxiang Gu",
            "Zhenhua Wang",
            "Jason Kuen",
            "Lianyang Ma",
            "Amir Shahroudy",
            "Bing Shuai",
            "Ting Liu",
            "Xingxing Wang",
            "Li Wang",
            "Gang Wang",
            "Jianfei Cai",
            "Tsuhan Chen"
        ],
        "abstract": "In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2017-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07155",
        "title": "Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web",
        "authors": [
            "Shugao Ma",
            "Sarah Adel Bargal",
            "Jianming Zhang",
            "Leonid Sigal",
            "Stan Sclaroff"
        ],
        "abstract": "Recently, attempts have been made to collect millions of videos to train CNN models for action recognition in videos. However, curating such large-scale video datasets requires immense human labor, and training CNNs on millions of videos demands huge computational resources. In contrast, collecting action images from the Web is much easier and training on images requires much less computation. In addition, labeled web images tend to contain discriminative action poses, which highlight discriminative portions of a video's temporal progression. We explore the question of whether we can utilize web action images to train better CNN models for action recognition in videos. We collect 23.8K manually filtered images from the Web that depict the 101 actions in the UCF101 action video dataset. We show that by utilizing web action images along with videos in training, significant performance boosts of CNN models can be achieved. We then investigate the scalability of the process by leveraging crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M video frames by 393K unfiltered images and get comparable performance.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07314",
        "title": "Mid-level Representation for Visual Recognition",
        "authors": [
            "Moin Nabi"
        ],
        "abstract": "Visual Recognition is one of the fundamental challenges in AI, where the goal is to understand the semantics of visual data. Employing mid-level representation, in particular, shifted the paradigm in visual recognition. The mid-level image/video representation involves discovering and training a set of mid-level visual patterns (e.g., parts and attributes) and represent a given image/video utilizing them. The mid-level patterns can be extracted from images and videos using the motion and appearance information of visual phenomenas. This thesis targets employing mid-level representations for different high-level visual recognition tasks, namely (i)image understanding and (ii)video understanding.\n",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07331",
        "title": "Plug-and-Play Priors for Bright Field Electron Tomography and Sparse Interpolation",
        "authors": [
            "Suhas Sreehari",
            "S. V. Venkatakrishnan",
            "Brendt Wohlberg",
            "Lawrence F. Drummy",
            "Jeffrey P. Simmons",
            "Charles A. Bouman"
        ],
        "abstract": "Many material and biological samples in scientific imaging are characterized by non-local repeating structures. These are studied using scanning electron microscopy and electron tomography. Sparse sampling of individual pixels in a 2D image acquisition geometry, or sparse sampling of projection images with large tilt increments in a tomography experiment, can enable high speed data acquisition and minimize sample damage caused by the electron beam.\n",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07344",
        "title": "A Deep Generative Deconvolutional Image Model",
        "authors": [
            "Yunchen Pu",
            "Xin Yuan",
            "Andrew Stevens",
            "Chunyuan Li",
            "Lawrence Carin"
        ],
        "abstract": "A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07502",
        "title": "Convolutional Architecture Exploration for Action Recognition and Image Classification",
        "authors": [
            "J.T. Turner",
            "David Aha",
            "Leslie Smith",
            "Kalyan Moy Gupta"
        ],
        "abstract": "Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a software package for the training, classifying, and feature extraction of images. The UCF Sports Action dataset is a widely used machine learning dataset that has 200 videos taken in 720x480 resolution of 9 different sporting activities: diving, golf, swinging, kicking, lifting, horseback riding, running, skateboarding, swinging (various gymnastics), and walking. In this report we report on a caffe feature extraction pipeline of images taken from the videos of the UCF Sports Action dataset. A similar test was performed on overfeat, and results were inferior to caffe. This study is intended to explore the architecture and hyper parameters needed for effective static analysis of action in videos and classification over a variety of image datasets.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2015-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07506",
        "title": "Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd",
        "authors": [
            "Andreas Doumanoglou",
            "Rigas Kouskouridas",
            "Sotiris Malassiotis",
            "Tae-Kyun Kim"
        ],
        "abstract": "Object detection and 6D pose estimation in the crowd (scenes with multiple object instances, severe foreground occlusions and background distractors), has become an important problem in many rapidly evolving technological areas such as robotics and augmented reality. Single shot-based 6D pose estimators with manually designed features are still unable to tackle the above challenges, motivating the research towards unsupervised feature learning and next-best-view estimation. In this work, we present a complete framework for both single shot-based 6D object pose estimation and next-best-view prediction based on Hough Forests, the state of the art object pose estimator that performs classification and regression jointly. Rather than using manually designed features we a) propose an unsupervised feature learnt from depth-invariant patches using a Sparse Autoencoder and b) offer an extensive evaluation of various state of the art features. Furthermore, taking advantage of the clustering performed in the leaf nodes of Hough Forests, we learn to estimate the reduction of uncertainty in other views, formulating the problem of selecting the next-best-view. To further improve pose estimation, we propose an improved joint registration and hypotheses verification module as a final refinement step to reject false detections. We provide two additional challenging datasets inspired from realistic scenarios to extensively evaluate the state of the art and our framework. One is related to domestic environments and the other depicts a bin-picking scenario mostly found in industrial settings. We show that our framework significantly outperforms state of the art both on public and on our datasets.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07711",
        "title": "Adaptive Object Detection Using Adjacency and Zoom Prediction",
        "authors": [
            "Yongxi Lu",
            "Tara Javidi",
            "Svetlana Lazebnik"
        ],
        "abstract": "State-of-the-art object detection systems rely on an accurate set of region proposals. Several recent methods use a neural network architecture to hypothesize promising object locations. While these approaches are computationally efficient, they rely on fixed image regions as anchors for predictions. In this paper we propose to use a search strategy that adaptively directs computational resources to sub-regions likely to contain objects. Compared to methods based on fixed anchor locations, our approach naturally adapts to cases where object instances are sparse and small. Our approach is comparable in terms of accuracy to the state-of-the-art Faster R-CNN approach while using two orders of magnitude fewer anchors on average. Code is publicly available.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07712",
        "title": "Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from Non-linear Measurements",
        "authors": [
            "Anupriya Gogna",
            "Angshul Majumdar"
        ],
        "abstract": "This work addresses the problem of estimating proton density and T1 maps from two partially sampled K-space scans such that the total acquisition time remains approximately the same as a single scan. Existing multi parametric non linear curve fitting techniques require a large number (8 or more) of echoes to estimate the maps resulting in prolonged (clinically infeasible) acquisition times. Our simulation results show that our method yields very accurate and robust results from only two partially sampled scans (total scan time being the same as a single echo MRI). We model PD and T1 maps to be sparse in some transform domain. The PD map is recovered via standard Compressed Sensing based recovery technique. Estimating the T1 map requires solving an analysis prior sparse recovery problem from non linear measurements, since the relationship between T1 values and intensity values or K space samples is not linear. For the first time in this work, we propose an algorithm for analysis prior sparse recovery for non linear measurements. We have compared our approach with the only existing technique based on matrix factorization from non linear measurements; our method yields considerably superior results.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07729",
        "title": "G-CNN: an Iterative Grid Based Object Detector",
        "authors": [
            "Mahyar Najibi",
            "Mohammad Rastegari",
            "Larry S. Davis"
        ],
        "abstract": "We introduce G-CNN, an object detection technique based on CNNs which works without proposal algorithms. G-CNN starts with a multi-scale grid of fixed bounding boxes. We train a regressor to move and scale elements of the grid towards objects iteratively. G-CNN models the problem of object detection as finding a path from a fixed grid to boxes tightly surrounding the objects. G-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast R-CNN which uses around 2K bounding boxes generated with a proposal technique. This strategy makes detection faster by removing the object proposal stage as well as reducing the number of boxes to be processed.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2016-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07815",
        "title": "Truncated Max-of-Convex Models",
        "authors": [
            "Pankaj Pansari",
            "M. Pawan Kumar"
        ],
        "abstract": "Truncated convex models (TCM) are a special case of pairwise random fields that have been widely used in computer vision. However, by restricting the order of the potentials to be at most two, they fail to capture useful image statistics. We propose a natural generalization of TCM to high-order random fields, which we call truncated max-of-convex models (TMCM). The energy function of TMCM consistsof two types of potentials: (i) unary potential, which has no restriction on its form; and (ii) clique potential, which is the sum of the m largest truncated convex distances over all label pairs in a clique. The use of a convex distance function encourages smoothness, while truncation allows for discontinuities in the labeling. By using m > 1, TMCM provides robustness towards errors in the definition of the cliques. In order to minimize the energy function of a TMCM over all possible labelings, we design an efficient st-MINCUT based range expansion algorithm. We prove the accuracy of our algorithm by establishing strong multiplicative bounds for several special cases of interest. Using synthetic and standard real data sets, we demonstrate the benefit of our high-order TMCM over pairwise TCM, as well as the benefit of our range expansion algorithm over other st-MINCUT based approaches.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2016-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07928",
        "title": "Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network",
        "authors": [
            "Seunghoon Hong",
            "Junhyuk Oh",
            "Bohyung Han",
            "Honglak Lee"
        ],
        "abstract": "We propose a novel weakly-supervised semantic segmentation algorithm based on Deep Convolutional Neural Network (DCNN). Contrary to existing weakly-supervised approaches, our algorithm exploits auxiliary segmentation annotations available for different categories to guide segmentations on images with only image-level class labels. To make the segmentation knowledge transferrable across categories, we design a decoupled encoder-decoder architecture with attention model. In this architecture, the model generates spatial highlights of each category presented in an image using an attention model, and subsequently generates foreground segmentation for each highlighted region using decoder. Combining attention model, we show that the decoder trained with segmentation annotations in different categories can boost the performance of weakly-supervised semantic segmentation. The proposed algorithm demonstrates substantially improved performance compared to the state-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012 dataset when our model is trained with the annotations in 60 exclusive categories in Microsoft COCO dataset.\n    ",
        "submission_date": "2015-12-24T00:00:00",
        "last_modified_date": "2015-12-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07947",
        "title": "Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain Stochastically Fully Connected Conditional Random Fields",
        "authors": [
            "Edward Li",
            "Farzad Khalvati",
            "Mohammad Javad Shafiee",
            "Masoom A. Haider",
            "Alexander Wong"
        ],
        "abstract": "Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for the screening and diagnosis of frequently occurring cancers. However image quality may suffer by long acquisition times for MRIs due to patient motion, as well as result in great patient discomfort. Reducing MRI acquisition time can reduce patient discomfort and as a result reduces motion artifacts from the acquisition process. Compressive sensing strategies, when applied to MRI, have been demonstrated to be effective at decreasing acquisition times significantly by sparsely sampling the \\emph{k}-space during the acquisition process. However, such a strategy requires advanced reconstruction algorithms to produce high quality and reliable images from compressive sensing MRI. This paper proposes a new reconstruction approach based on cross-domain stochastically fully connected conditional random fields (CD-SFCRF) for compressive sensing MRI. The CD-SFCRF introduces constraints in both \\emph{k}-space and spatial domains within a stochastically fully connected graphical model to produce improved MRI reconstruction. Experimental results using T2-weighted (T2w) imaging and diffusion-weighted imaging (DWI) of the prostate show strong performance in preserving fine details and tissue structures in the reconstructed images when compared to other tested methods even at low sampling rates.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07951",
        "title": "A Combined Deep-Learning and Deformable-Model Approach to Fully Automatic Segmentation of the Left Ventricle in Cardiac MRI",
        "authors": [
            "M. R. Avendi",
            "A. Kheradvar",
            "H. Jafarkhani"
        ],
        "abstract": "Segmentation of the left ventricle (LV) from cardiac magnetic resonance imaging (MRI) datasets is an essential step for calculation of clinical indices such as ventricular volume and ejection fraction. In this work, we employ deep learning algorithms combined with deformable models to develop and evaluate a fully automatic segmentation tool for the LV from short-axis cardiac MRI datasets. The method employs deep learning algorithms to learn the segmentation task from the ground true data. Convolutional networks are employed to automatically detect the LV chamber in MRI dataset. Stacked autoencoders are utilized to infer the shape of the LV. The inferred shape is incorporated into deformable models to improve the accuracy and robustness of the segmentation. We validated our method using 45 cardiac MR datasets taken from the MICCAI 2009 LV segmentation challenge and showed that it outperforms the state-of-the art methods. Excellent agreement with the ground truth was achieved. Validation metrics, percentage of good contours, Dice metric, average perpendicular distance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versus those of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by other methods, respectively.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08047",
        "title": "Assessment of texture measures susceptibility to noise in conventional and contrast enhanced computed tomography lung tumour images",
        "authors": [
            "Omar Sultan Al-Kadi"
        ],
        "abstract": "Noise is one of the major problems that hinder an effective texture analysis of disease in medical images, which may cause variability in the reported diagnosis. In this paper seven texture measurement methods (two wavelet, two model and three statistical based) were applied to investigate their susceptibility to subtle noise caused by acquisition and reconstruction deficiencies in computed tomography (CT) images. Features of lung tumours were extracted from two different conventional and contrast enhanced CT image data-sets under filtered and noisy conditions. When measuring the noise in the background open-air region of the analysed CT images, noise of Gaussian and Rayleigh distributions with varying mean and variance was encountered, and Fisher distance was used to differentiate between an original extracted lung tumour region of interest (ROI) with the filtered and noisy reconstructed versions. It was determined that the wavelet packet (WP) and fractal dimension measures were the least affected, while the Gaussian Markov random field, run-length and co-occurrence matrices were the most affected by noise. Depending on the selected ROI size, it was concluded that texture measures with fewer extracted features can decrease susceptibility to noise, with the WP and the Gabor filter having a stable performance in both filtered and noisy CT versions and for both data-sets. Knowing how robust each texture measure under noise presence is can assist physicians using an automated lung texture classification system in choosing the appropriate feature extraction algorithm for a more accurate diagnosis.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08049",
        "title": "Texture measures combination for improved meningioma classification of histopathological images",
        "authors": [
            "Omar S. Al-Kadi"
        ],
        "abstract": "Providing an improved technique which can assist pathologists in correctly classifying meningioma tumours with a significant accuracy is our main objective. The proposed technique, which is based on optimum texture measure combination, inspects the separability of the RGB colour channels and selects the channel which best segments the cell nuclei of the histopathological images. The morphological gradient was applied to extract the region of interest for each subtype and for elimination of possible noise (e.g. cracks) which might occur during biopsy preparation. Meningioma texture features are extracted by four different texture measures (two model-based and two statistical-based) and then corresponding features are fused together in different combinations after excluding highly correlated features, and a Bayesian classifier was used for meningioma subtype discrimination. The combined Gaussian Markov random field and run-length matrix texture measures outperformed all other combinations in terms of quantitatively characterising the meningioma tissue, achieving an overall classification accuracy of 92.50%, improving from 83.75% which is the best accuracy achieved if the texture measures are used individually.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08051",
        "title": "A Multiresolution Clinical Decision Support System Based on Fractal Model Design for Classification of Histological Brain Tumours",
        "authors": [
            "Omar S. Al-Kadi"
        ],
        "abstract": "Tissue texture is known to exhibit a heterogeneous or non-stationary nature, therefore using a single resolution approach for optimum classification might not suffice. A clinical decision support system that exploits the subband textural fractal characteristics for best bases selection of meningioma brain histopathological image classification is proposed. Each subband is analysed using its fractal dimension instead of energy, which has the advantage of being less sensitive to image intensity and abrupt changes in tissue texture. The most significant subband that best identifies texture discontinuities will be chosen for further decomposition, and its fractal characteristics would represent the optimal feature vector for classification. The performance was tested using the support vector machine (SVM), Bayesian and k-nearest neighbour (kNN) classifiers and a leave-one-patient-out method was employed for validation. Our method outperformed the classical energy based selection approaches, achieving for SVM, Bayesian and kNN classifiers an overall classification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%, 83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69% for the energy texture signatures, respectively. These results indicate the potential usefulness as a decision support system that could complement radiologists diagnostic capability to discriminate higher order statistical textural information, for which it would be otherwise difficult via ordinary human vision.\n    ",
        "submission_date": "2015-12-25T00:00:00",
        "last_modified_date": "2015-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08086",
        "title": "Part-Stacked CNN for Fine-Grained Visual Categorization",
        "authors": [
            "Shaoli Huang",
            "Zhe Xu",
            "Dacheng Tao",
            "Ya Zhang"
        ],
        "abstract": "In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the fine-grained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that en- codes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from both the perspective of classification accuracy and model interpretability.\n    ",
        "submission_date": "2015-12-26T00:00:00",
        "last_modified_date": "2015-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08103",
        "title": "Data Driven Robust Image Guided Depth Map Restoration",
        "authors": [
            "Wei Liu",
            "Yun Gu",
            "Chunhua Shen",
            "Xiaogang Chen",
            "Qiang Wu",
            "Jie Yang"
        ],
        "abstract": "Depth maps captured by modern depth cameras such as Kinect and Time-of-Flight (ToF) are usually contaminated by missing data, noises and suffer from being of low resolution. In this paper, we present a robust method for high-quality restoration of a degraded depth map with the guidance of the corresponding color image. We solve the problem in an energy optimization framework that consists of a novel robust data term and smoothness term. To accommodate not only the noise but also the inconsistency between depth discontinuities and the color edges, we model both the data term and smoothness term with a robust exponential error norm function. We propose to use Iteratively Re-weighted Least Squares (IRLS) methods for efficiently solving the resulting highly non-convex optimization problem. More importantly, we further develop a data-driven adaptive parameter selection scheme to properly determine the parameter in the model. We show that the proposed approach can preserve fine details and sharp depth discontinuities even for a large upsampling factor ($8\\times$ for example). Experimental results on both simulated and real datasets demonstrate that the proposed method outperforms recent state-of-the-art methods in coping with the heavy noise, preserving sharp depth discontinuities and suppressing the texture copy artifacts.\n    ",
        "submission_date": "2015-12-26T00:00:00",
        "last_modified_date": "2015-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08212",
        "title": "Improving Facial Analysis and Performance Driven Animation through Disentangling Identity and Expression",
        "authors": [
            "David Rim",
            "Sina Honari",
            "Md Kamrul Hasan",
            "Chris Pal"
        ],
        "abstract": "We present techniques for improving performance driven facial animation, emotion recognition, and facial key-point or landmark prediction using learned identity invariant representations. Established approaches to these problems can work well if sufficient examples and labels for a particular identity are available and factors of variation are highly controlled. However, labeled examples of facial expressions, emotions and key-points for new individuals are difficult and costly to obtain. In this paper we improve the ability of techniques to generalize to new and unseen individuals by explicitly modeling previously seen variations related to identity and expression. We use a weakly-supervised approach in which identity labels are used to learn the different factors of variation linked to identity separately from factors related to expression. We show how probabilistic modeling of these sources of variation allows one to learn identity-invariant representations for expressions which can then be used to identity-normalize various procedures for facial expression analysis and animation control. We also show how to extend the widely used techniques of active appearance models and constrained local models through replacing the underlying point distribution models which are typically constructed using principal component analysis with identity-expression factorized representations. We present a wide variety of experiments in which we consistently improve performance on emotion recognition, markerless performance-driven facial animation and facial key-point tracking.\n    ",
        "submission_date": "2015-12-27T00:00:00",
        "last_modified_date": "2016-05-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08413",
        "title": "Outlier Detection In Large-scale Traffic Data By Na\u00efve Bayes Method and Gaussian Mixture Model Method",
        "authors": [
            "Philip Lam",
            "Lili Wang",
            "Henry Y.T. Ngan",
            "Nelson H.C. Yung",
            "Anthony G.O. Yeh"
        ],
        "abstract": "It is meaningful to detect outliers in traffic data for traffic management. However, this is a massive task for people from large-scale database to distinguish outliers. In this paper, we present two methods: Kernel Smoothing Na\u00efve Bayes (NB) method and Gaussian Mixture Model (GMM) method to automatically detect any hardware errors as well as abnormal traffic events in traffic data collected at a four-arm junction in Hong Kong. Traffic data was recorded in a video format, and converted to spatial-temporal (ST) traffic signals by statistics. The ST signals are then projected to a two-dimensional (2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimension reduction. We assume that inlier data are normal distributed. As such, the NB and GMM methods are successfully applied in outlier detection (OD) for traffic data. The kernel smooth NB method assumes the existence of kernel distributions in traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMM method believes the traffic data is formed by the mixture of Gaussian distributions and exploits confidence region for OD. This paper would address the modeling of each method and evaluate their respective performances. Experimental results show that the NB algorithm with Triangle kernel and GMM method achieve up to 93.78% and 94.50% accuracies, respectively.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2015-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08424",
        "title": "Graph entropies in texture segmentation of images",
        "authors": [
            "Martin Welk"
        ],
        "abstract": "We study the applicability of a set of texture descriptors introduced in recent work by the author to texture-based segmentation of images. The texture descriptors under investigation result from applying graph indices from quantitative graph theory to graphs encoding the local structure of images. The underlying graphs arise from the computation of morphological amoebas as structuring elements for adaptive morphology, either as weighted or unweighted Dijkstra search trees or as edge-weighted pixel graphs within structuring elements. In the present paper we focus on texture descriptors in which the graph indices are entropy-based, and use them in a geodesic active contour framework for image segmentation. Experiments on several synthetic and one real-world image are shown to demonstrate texture segmentation by this approach. Forthermore, we undertake an attempt to analyse selected entropy-based texture descriptors with regard to what information about texture they actually encode. Whereas this analysis uses some heuristic assumptions, it indicates that the graph-based texture descriptors are related to fractal dimension measures that have been proven useful in texture analysis.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2015-12-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08475",
        "title": "MRF-Based Multispectral Image Fusion Using an Adaptive Approach Based on Edge-Guided Interpolation",
        "authors": [
            "Mohammad Reza Khosravi",
            "Mohammad Sharif-Yazd",
            "Mohammad Kazem Moghimi",
            "Ahmad Keshavarz",
            "Habib Rostami",
            "Suleiman Mansouri"
        ],
        "abstract": "In interpretation of remote sensing images, it is possible that some images which are supplied by different sensors become incomprehensible. For better visual perception of these images, it is essential to operate series of pre-processing and elementary corrections and then operate a series of main processing steps for more precise analysis on the images. There are several approaches for processing which are depended on the type of remote sensing images. The discussed approach in this article, i.e. image fusion, is the use of natural colors of an optical image for adding color to a grayscale satellite image which gives us the ability for better observation of the HR image of OLI sensor of Landsat-8. This process with emphasis on details of fusion technique has previously been performed; however, we are going to apply the concept of the interpolation process. In fact, we see many important software tools such as ENVI and ERDAS as the most famous remote sensing image processing tools have only classical interpolation techniques (such as bi-linear (BL) and bi-cubic/cubic convolution (CC)). Therefore, ENVI- and ERDAS-based researches in image fusion area and even other fusion researches often dont use new and better interpolators and are mainly concentrated on the fusion algorithms details for achieving a better quality, so we only focus on the interpolation impact on fusion quality in Landsat-8 multispectral images. The important feature of this approach is to use a statistical, adaptive, and edge-guided interpolation method for improving the color quality in the images in practice. Numerical simulations show selecting the suitable interpolation techniques in MRF-based images creates better quality than the classical interpolators.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2019-04-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08512",
        "title": "Visually Indicated Sounds",
        "authors": [
            "Andrew Owens",
            "Phillip Isola",
            "Josh McDermott",
            "Antonio Torralba",
            "Edward H. Adelson",
            "William T. Freeman"
        ],
        "abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.\n    ",
        "submission_date": "2015-12-28T00:00:00",
        "last_modified_date": "2016-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08648",
        "title": "A framework for robust object multi-detection with a vote aggregation and a cascade filtering",
        "authors": [
            "Grzegorz Kurzejamski",
            "Jacek Zawistowski",
            "Grzegorz Sarwas"
        ],
        "abstract": "This paper presents a framework designed for the multi-object detection purposes and adjusted for the application of product search on the market shelves. The framework uses a single feedback loop and a pattern resizing mechanism to demonstrate the top effectiveness of the state-of-the-art local features. A high detection rate with a low false detection chance can be achieved with use of only one pattern per object and no manual parameters adjustments. The method incorporates well known local features and a basic matching process to create a reliable voting space. Further steps comprise of metric transformations, graphical vote space representation, two-phase vote aggregation process and a cascade of verifying filters.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08669",
        "title": "Robust Scene Text Recognition Using Sparse Coding based Features",
        "authors": [
            "Da-Han Wang",
            "Hanzi Wang",
            "Dong Zhang",
            "Jonathan Li",
            "David Zhang"
        ],
        "abstract": "In this paper, we propose an effective scene text recognition method using sparse coding based features, called Histograms of Sparse Codes (HSC) features. For character detection, we use the HSC features instead of using the Histograms of Oriented Gradients (HOG) features. The HSC features are extracted by computing sparse codes with dictionaries that are learned from data using K-SVD, and aggregating per-pixel sparse codes to form local histograms. For word recognition, we integrate multiple cues including character detection scores and geometric contexts in an objective function. The final recognition results are obtained by searching for the words which correspond to the maximum value of the objective function. The parameters in the objective function are learned using the Minimum Classification Error (MCE) training method. Experiments on several challenging datasets demonstrate that the proposed HSC-based scene text recognition method outperforms HOG-based methods significantly and outperforms most state-of-the-art methods.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.08814",
        "title": "Combined statistical and model based texture features for improved image classification",
        "authors": [
            "Omar Al-Kadi"
        ],
        "abstract": "This paper aims to improve the accuracy of texture classification based on extracting texture features using five different texture methods and classifying the patterns using a naive Bayesian classifier. Three statistical-based and two model-based methods are used to extract texture features from eight different texture images, then their accuracy is ranked after using each method individually and in pairs. The accuracy improved up to 97.01% when model based -Gaussian Markov random field (GMRF) and fractional Brownian motion (fBm) - were used together for classification as compared to the highest achieved using each of the five different methods alone; and proved to be better in classifying as compared to statistical methods. Also, using GMRF with statistical based methods, such as Gray level co-occurrence (GLCM) and run-length (RLM) matrices, improved the overall accuracy to 96.94% and 96.55%; respectively.\n    ",
        "submission_date": "2015-12-29T00:00:00",
        "last_modified_date": "2015-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09041",
        "title": "Actor-Action Semantic Segmentation with Grouping Process Models",
        "authors": [
            "Chenliang Xu",
            "Jason J. Corso"
        ],
        "abstract": "Actor-action semantic segmentation made an important step toward advanced video understanding problems: what action is happening; who is performing the action; and where is the action in space-time. Current models for this problem are local, based on layered CRFs, and are unable to capture long-ranging interaction of video parts. We propose a new model that combines these local labeling CRFs with a hierarchical supervoxel decomposition. The supervoxels provide cues for possible groupings of nodes, at various scales, in the CRFs to encourage adaptive, high-order groups for more effective labeling. Our model is dynamic and continuously exchanges information during inference: the local CRFs influence what supervoxels in the hierarchy are active, and these active nodes influence the connectivity in the CRF; we hence call it a grouping process model. The experimental results on a recent large-scale video dataset show a large margin of 60% relative improvement over the state of the art, which demonstrates the effectiveness of the dynamic, bidirectional flow between labeling and grouping.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09049",
        "title": "LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing",
        "authors": [
            "Chenliang Xu",
            "Jason J. Corso"
        ],
        "abstract": "Supervoxel segmentation has strong potential to be incorporated into early video analysis as superpixel segmentation has in image analysis. However, there are many plausible supervoxel methods and little understanding as to when and where each is most appropriate. Indeed, we are not aware of a single comparative study on supervoxel segmentation. To that end, we study seven supervoxel algorithms, including both off-line and streaming methods, in the context of what we consider to be a good supervoxel: namely, spatiotemporal uniformity, object/region boundary detection, region compression and parsimony. For the evaluation we propose a comprehensive suite of seven quality metrics to measure these desirable supervoxel characteristics. In addition, we evaluate the methods in a supervoxel classification task as a proxy for subsequent high-level uses of the supervoxels in video analysis. We use six existing benchmark video datasets with a variety of content-types and dense human annotations. Our findings have led us to conclusive evidence that the hierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) and temporal superpixels (TSP) methods are the top-performers among the seven methods. They all perform well in terms of segmentation accuracy, but vary in regard to the other desiderata: GBH captures object boundaries best; SWA has the best potential for region compression; and TSP achieves the best undersegmentation error.\n    ",
        "submission_date": "2015-12-30T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09194",
        "title": "Exploiting Local Structures with the Kronecker Layer in Convolutional Networks",
        "authors": [
            "Shuchang Zhou",
            "Jia-Nan Wu",
            "Yuxin Wu",
            "Xinyu Zhou"
        ],
        "abstract": "In this paper, we propose and study a technique to reduce the number of parameters and computation time in convolutional neural networks. We use Kronecker product to exploit the local structures within convolution and fully-connected layers, by replacing the large weight matrices by combinations of multiple Kronecker products of smaller matrices. Just as the Kronecker product is a generalization of the outer product from vectors to matrices, our method is a generalization of the low rank approximation method for convolution neural networks. We also introduce combinations of different shapes of Kronecker product to increase modeling capacity. Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve $3.3 \\times$ speedup or $3.6 \\times$ parameter reduction with less than 1\\% drop in accuracy, showing the effectiveness and efficiency of our method. Moreover, the computation efficiency of Kronecker layer makes using larger feature map possible, which in turn enables us to outperform the previous state-of-the-art on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2016-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09272",
        "title": "Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions",
        "authors": [
            "Vijay Kumar B G",
            "Gustavo Carneiro",
            "Ian Reid"
        ],
        "abstract": "Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset. Pre-trained models are available online at ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2016-08-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00680",
        "title": "A New Method for Signal and Image Analysis: The Square Wave Method",
        "authors": [
            "Osvaldo Skliar",
            "Ricardo E. Monge",
            "Sherry Gapper"
        ],
        "abstract": "A brief review is provided of the use of the Square Wave Method (SWM) in the field of signal and image analysis and it is specified how results thus obtained are expressed using the Square Wave Transform (SWT), in the frequency domain. To illustrate the new approach introduced in this field, the results of two cases are analyzed: a) a sequence of samples (that is, measured values) of an electromyographic recording; and b) the classic image of Lenna.\n    ",
        "submission_date": "2015-01-04T00:00:00",
        "last_modified_date": "2015-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.00756",
        "title": "Hashing with binary autoencoders",
        "authors": [
            "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n",
            "Ramin Raziperchikolaei"
        ],
        "abstract": "An attractive approach for fast search in image databases is binary hashing, where each high-dimensional, real-valued image is mapped onto a low-dimensional, binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints, and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here, we focus on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps: one that learns the encoder and decoder separately, and one that optimizes the code for each image. Image retrieval experiments, using precision/recall and a measure of code utilization, show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.\n    ",
        "submission_date": "2015-01-05T00:00:00",
        "last_modified_date": "2015-01-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.01266",
        "title": "The Quadrifocal Variety",
        "authors": [
            "Luke Oeding"
        ],
        "abstract": "Multi-view Geometry is reviewed from an Algebraic Geometry perspective and multi-focal tensors are constructed as equivariant projections of the Grassmannian. A connection to the principal minor assignment problem is made by considering several flatlander cameras. The ideal of the quadrifocal variety is computed up to degree 8 (and partially in degree 9) using the representations of $\\operatorname{GL}(3)^{\\times 4}$ in the polynomial ring on the space of $3 \\times 3 \\times 3 \\times 3$ tensors. Further representation-theoretic analysis gives a lower bound for the number of minimal generators.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2016-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02058",
        "title": "HOG based Fast Human Detection",
        "authors": [
            "M. Kachouane",
            "S. Sahki",
            "M. Lakrouf",
            "N. Ouadah"
        ],
        "abstract": "Objects recognition in image is one of the most difficult problems in computer vision. It is also an important step for the implementation of several existing applications that require high-level image interpretation. Therefore, there is a growing interest in this research area during the last years. In this paper, we present an algorithm for human detection and recognition in real-time, from images taken by a CCD camera mounted on a car-like mobile robot. The proposed technique is based on Histograms of Oriented Gradient (HOG) and SVM classifier. The implementation of our detector has provided good results, and can be used in robotics tasks.\n    ",
        "submission_date": "2015-01-09T00:00:00",
        "last_modified_date": "2015-01-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02246",
        "title": "The Effect of Wedge Tip Angles on Stress Intensity Factors in the Contact Problem between Tilted Wedge and a Half Plane with an Edge Crack Using Digital Image Correlation",
        "authors": [
            "Seyedmeysam Khaleghian",
            "Anahita Emami",
            "Mohammad Yadegari",
            "Nasser Soltani"
        ],
        "abstract": "The first and second mode stress intensity factors (SIFs) of a contact problem between a half-plane with an edge crack and an asymmetric tilted wedge were obtained using experimental method of Digital Image Correlation (DIC). In this technique, displacement and strain fields can be measured using two digital images of the same sample at different stages of loading. However, several images were taken consequently in each stage of this experiment to avoid the noise effect. A pair of images of each stage was compared to each other. Then, the correlation coefficients between them were studied using a computer code. The pairs with the correlation coefficient higher than 0.8 were selected as the acceptable match for displacement measurements near the crack tip. Subsequently, the SIFs of specimens were calculated using displacement fields obtained from DIC method. The effect of wedge tips angle on their SIFs was also studied. Moreover, the results of DIC method were compared with the results of photoelasticity method and a close agreement between them was observed.\n    ",
        "submission_date": "2015-01-06T00:00:00",
        "last_modified_date": "2015-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02376",
        "title": "Simplified vision based automatic navigation for wheat harvesting in low income economies",
        "authors": [
            "Muhammad Zubair Ahmad",
            "Ayyaz Akhtar",
            "Abdul Qadeer Khan",
            "Amir A. Khan"
        ],
        "abstract": "Recent developments in the domain of agricultural robotics have resulted in development of complex and efficient systems. Most of the land owners in the South Asian region are low income farmers. The agricultural experience for them is still a completely manual process. However, the extreme weather conditions, heat and flooding, often combine to put a lot of stress on these small land owners and the associated labor. In this paper, we propose a prototype for an automated power reaper for the wheat crop. This automated vehicle is navigated using a simple vision based approach employing the low-cost camera and assisted GPS. The mechanical platform is driven by three motors controlled through an interface between the proposed vision algorithm and the electrical drive. The proposed methodology is applied on some real field scenarios to demonstrate the efficiency of the vision based algorithm.\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02378",
        "title": "Low Cost Semi-Autonomous Agricultural Robots In Pakistan-Vision Based Navigation Scalable methodology for wheat harvesting",
        "authors": [
            "Muhammad Zubair Ahmad",
            "Ayyaz Akhtar",
            "Abdul Qadeer Khan",
            "Amir Ali Khan",
            "Muhammad Murtaza Khan"
        ],
        "abstract": "Robots have revolutionized our way of life in recent ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02379",
        "title": "Autonomous Farm Vehicles: Prototype of Power Reaper",
        "authors": [
            "Abdul Qadeer Khan",
            "Ayyaz Akhtar",
            "Muhammad Zubair Ahmad"
        ],
        "abstract": "Chapter 2 will begin with introduction of Agricultural Robotics. There will be a literature review of the mechanical structure, vision and control algorithms. In chapter 3 we will discuss the methodology in detail using block diagrams and flowcharts. The results of the tested and the proposed algorithms will also be displayed. In chapter 4 we will discuss the results in detail and how they are of significance in our work. In chapter 5 we will conclude our work and discuss some future perspectives. In appendices we will provide some background information necessary regarding this project.\n    ",
        "submission_date": "2015-01-10T00:00:00",
        "last_modified_date": "2015-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02598",
        "title": "Combining Language and Vision with a Multimodal Skip-gram Model",
        "authors": [
            "Angeliki Lazaridou",
            "Nghia The Pham",
            "Marco Baroni"
        ],
        "abstract": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02655",
        "title": "Texture Retrieval via the Scattering Transform",
        "authors": [
            "Alexander Sagel",
            "Dominik Meyer",
            "Hao Shen"
        ],
        "abstract": "This work studies the problem of content-based image retrieval, specifically, texture retrieval. It focuses on feature extraction and similarity measure for texture images. Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval. It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations. Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval.\n",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02714",
        "title": "From Visual Attributes to Adjectives through Decompositional Distributional Semantics",
        "authors": [
            "Angeliki Lazaridou",
            "Georgiana Dinu",
            "Adam Liska",
            "Marco Baroni"
        ],
        "abstract": "As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown...) attracting most attention. By building on the recent \"zero-shot learning\" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as \"visual phrases\", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.\n    ",
        "submission_date": "2015-01-12T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.02995",
        "title": "Improved 8-point Approximate DCT for Image and Video Compression Requiring Only 14 Additions",
        "authors": [
            "U. S. Potluri",
            "A. Madanayake",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Edirisuriya"
        ],
        "abstract": "Video processing systems such as HEVC requiring low energy consumption needed for the multimedia market has lead to extensive development in fast algorithms for the efficient approximation of 2-D DCT transforms. The DCT is employed in a multitude of compression standards due to its remarkable energy compaction properties. Multiplier-free approximate DCT transforms have been proposed that offer superior compression performance at very low circuit complexity. Such approximations can be realized in digital VLSI hardware using additions and subtractions only, leading to significant reductions in chip area and power consumption compared to conventional DCTs and integer transforms. In this paper, we introduce a novel 8-point DCT approximation that requires only 14 addition operations and no multiplications. The proposed transform possesses low computational complexity and is compared to state-of-the-art DCT approximations in terms of both algorithm complexity and peak signal-to-noise ratio. The proposed DCT approximation is a candidate for reconfigurable video standards such as HEVC. The proposed transform and several other DCT approximations are mapped to systolic-array digital architectures and physically realized as digital prototype circuits using FPGA technology and mapped to 45 nm CMOS technology.\n    ",
        "submission_date": "2015-01-13T00:00:00",
        "last_modified_date": "2015-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03302",
        "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Mario Fritz"
        ],
        "abstract": "Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \"question answering about images\" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.\n    ",
        "submission_date": "2015-01-14T00:00:00",
        "last_modified_date": "2015-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03915",
        "title": "Feature Selection based on Machine Learning in MRIs for Hippocampal Segmentation",
        "authors": [
            "Sabina Tangaro",
            "Nicola Amoroso",
            "Massimo Brescia",
            "Stefano Cavuoti",
            "Andrea Chincarini",
            "Rosangela Errico",
            "Paolo Inglese",
            "Giuseppe Longo",
            "Rosalia Maglietta",
            "Andrea Tateo",
            "Giuseppe Riccio",
            "Roberto Bellotti"
        ],
        "abstract": "Neurodegenerative diseases are frequently associated with structural changes in the brain. Magnetic Resonance Imaging (MRI) scans can show these variations and therefore be used as a supportive feature for a number of neurodegenerative diseases. The hippocampus has been known to be a biomarker for Alzheimer disease and other neurological and psychiatric diseases. However, it requires accurate, robust and reproducible delineation of hippocampal structures. Fully automatic methods are usually the voxel based approach, for each voxel a number of local features were calculated. In this paper we compared four different techniques for feature selection from a set of 315 features extracted for each voxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrapper methods, respectively, (ii) Sequential Forward Selection and (iii) Sequential Backward Elimination; and (iv) embedded method based on the Random Forest Classifier on a set of 10 T1-weighted brain MRIs and tested on an independent set of 25 subjects. The resulting segmentations were compared with manual reference labelling. By using only 23 features for each voxel (sequential backward elimination) we obtained comparable state of-the-art performances with respect to the standard tool FreeSurfer.\n    ",
        "submission_date": "2015-01-16T00:00:00",
        "last_modified_date": "2015-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.03997",
        "title": "Holistic random encoding for imaging through multimode fibers",
        "authors": [
            "Hwanchol Jang",
            "Changhyeong Yoon",
            "Euiheon Chung",
            "Wonshik Choi",
            "Heung-No Lee"
        ],
        "abstract": "The input numerical aperture (NA) of multimode fiber (MMF) can be effectively increased by placing turbid media at the input end of the MMF. This provides the potential for high-resolution imaging through the MMF. While the input NA is increased, the number of propagation modes in the MMF and hence the output NA remains the same. This makes the image reconstruction process underdetermined and may limit the quality of the image reconstruction. In this paper, we aim to improve the signal to noise ratio (SNR) of the image reconstruction in imaging through MMF. We notice that turbid media placed in the input of the MMF transforms the incoming waves into a better format for information transmission and information extraction. We call this transformation as holistic random (HR) encoding of turbid media. By exploiting the HR encoding, we make a considerable improvement on the SNR of the image reconstruction. For efficient utilization of the HR encoding, we employ sparse representation (SR), a relatively new signal reconstruction framework when it is provided with a HR encoded signal. This study shows for the first time to our knowledge the benefit of utilizing the HR encoding of turbid media for recovery in the optically underdetermined systems where the output NA of it is smaller than the input NA for imaging through MMF.\n    ",
        "submission_date": "2014-12-30T00:00:00",
        "last_modified_date": "2014-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04158",
        "title": "On the Performance of ConvNet Features for Place Recognition",
        "authors": [
            "Niko S\u00fcnderhauf",
            "Feras Dayoub",
            "Sareh Shirazi",
            "Ben Upcroft",
            "Michael Milford"
        ],
        "abstract": "After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.\n    ",
        "submission_date": "2015-01-17T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04318",
        "title": "Clustering based on the In-tree Graph Structure and Affinity Propagation",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "A recently proposed clustering method, called the Nearest Descent (ND), can organize the whole dataset into a sparsely connected graph, called the In-tree. This ND-based Intree structure proves able to reveal the clustering structure underlying the dataset, except one imperfect place, that is, there are some undesired edges in this In-tree which require to be removed. Here, we propose an effective way to automatically remove the undesired edges in In-tree via an effective combination of the In-tree structure with affinity propagation (AP). The key for the combination is to add edges between the reachable nodes in In-tree before using AP to remove the undesired edges. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed method.\n    ",
        "submission_date": "2015-01-18T00:00:00",
        "last_modified_date": "2018-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04656",
        "title": "Microscopic Advances with Large-Scale Learning: Stochastic Optimization for Cryo-EM",
        "authors": [
            "Ali Punjani",
            "Marcus A. Brubaker"
        ],
        "abstract": "Determining the 3D structures of biological molecules is a key problem for both biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising technique for structure estimation which relies heavily on computational methods to reconstruct 3D structures from 2D images. This paper introduces the challenging Cryo-EM density estimation problem as a novel application for stochastic optimization techniques. Structure discovery is formulated as MAP estimation in a probabilistic latent-variable model, resulting in an optimization problem to which an array of seven stochastic optimization methods are applied. The methods are tested on both real and synthetic data, with some methods recovering reasonable structures in less than one epoch from a random initialization. Complex quasi-Newton methods are found to converge more slowly than simple gradient-based methods, but all stochastic methods are found to converge to similar optima. This method represents a major improvement over existing methods as it is significantly faster and is able to converge from a random initialization.\n    ",
        "submission_date": "2015-01-19T00:00:00",
        "last_modified_date": "2015-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.04870",
        "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to Classifier Trellises",
        "authors": [
            "J. Read",
            "L. Martino",
            "P. Olmos",
            "D. Luengo"
        ],
        "abstract": "Multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years. A popular method for multi-label classification is classifier chains, in which the predictions of individual classifiers are cascaded along a chain, thus taking into account inter-label dependencies and improving the overall performance. Several varieties of classifier chain methods have been introduced, and many of them perform very competitively across a wide range of benchmark datasets. However, scalability limitations become apparent on larger datasets when modeling a fully-cascaded chain. In particular, the methods' strategies for discovering and modeling a good chain structure constitutes a mayor computational bottleneck. In this paper, we present the classifier trellis (CT) method for scalable multi-label classification. We compare CT with several recently proposed classifier chain methods to show that it occupies an important niche: it is highly competitive on standard multi-label problems, yet it can also scale up to thousands or even tens of thousands of labels.\n    ",
        "submission_date": "2015-01-20T00:00:00",
        "last_modified_date": "2015-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05352",
        "title": "Optimizing affinity-based binary hashing using auxiliary coordinates",
        "authors": [
            "Ramin Raziperchikolaei",
            "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"
        ],
        "abstract": "In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2016-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05552",
        "title": "Estimating the Intrinsic Dimension of Hyperspectral Images Using an Eigen-Gap Approach",
        "authors": [
            "A. Halimi",
            "P. Honeine",
            "M. Kharouf",
            "C. Richard",
            "J.-Y. Tourneret"
        ],
        "abstract": "Linear mixture models are commonly used to represent hyperspectral datacube as a linear combinations of endmember spectra. However, determining of the number of endmembers for images embedded in noise is a crucial task. This paper proposes a fully automatic approach for estimating the number of endmembers in hyperspectral images. The estimation is based on recent results of random matrix theory related to the so-called spiked population model. More precisely, we study the gap between successive eigenvalues of the sample covariance matrix constructed from high dimensional noisy samples. The resulting estimation strategy is unsupervised and robust to correlated noise. This strategy is validated on both synthetic and real images. The experimental results are very promising and show the accuracy of this algorithm with respect to state-of-the-art algorithms.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.05684",
        "title": "Bi-Objective Nonnegative Matrix Factorization: Linear Versus Kernel-Based Models",
        "authors": [
            "Paul Honeine",
            "Fei Zhu"
        ],
        "abstract": "Nonnegative matrix factorization (NMF) is a powerful class of feature extraction techniques that has been successfully applied in many fields, namely in signal and image processing. Current NMF techniques have been limited to a single-objective problem in either its linear or nonlinear kernel-based formulation. In this paper, we propose to revisit the NMF as a multi-objective problem, in particular a bi-objective one, where the objective functions defined in both input and feature spaces are taken into account. By taking the advantage of the sum-weighted method from the literature of multi-objective optimization, the proposed bi-objective NMF determines a set of nondominated, Pareto optimal, solutions instead of a single optimal decomposition. Moreover, the corresponding Pareto front is studied and approximated. Experimental results on unmixing real hyperspectral images confirm the efficiency of the proposed bi-objective NMF compared with the state-of-the-art methods.\n    ",
        "submission_date": "2015-01-22T00:00:00",
        "last_modified_date": "2015-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06115",
        "title": "Constrained Extreme Learning Machines: A Study on Classification Cases",
        "authors": [
            "Wentao Zhu",
            "Jun Miao",
            "Laiyun Qing"
        ],
        "abstract": "Extreme learning machine (ELM) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers. However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process. In this paper, we proposed new ways, named \"constrained extreme learning machines\" (CELMs), to randomly select hidden neurons based on sample distribution. Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors. The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods. Additionally, the CELMs have a similar fast learning speed as ELM.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06209",
        "title": "Parallel Magnetic Resonance Imaging",
        "authors": [
            "Martin Uecker"
        ],
        "abstract": "The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scan times and, in consequence, its sensitivity to motion. Exploiting the complementary information from multiple receive coils, parallel imaging is able to recover images from under-sampled k-space data and to accelerate the measurement. Because parallel magnetic resonance imaging can be used to accelerate basically any imaging sequence it has many important applications. Parallel imaging brought a fundamental shift in image reconstruction: Image reconstruction changed from a simple direct Fourier transform to the solution of an ill-conditioned inverse problem. This work gives an overview of image reconstruction from the perspective of inverse problems. After introducing basic concepts such as regularization, discretization, and iterative reconstruction, advanced topics are discussed including algorithms for auto-calibration, the connection to approximation theory, and the combination with compressed sensing.\n    ",
        "submission_date": "2015-01-25T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.06450",
        "title": "IT-map: an Effective Nonlinear Dimensionality Reduction Method for Interactive Clustering",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Scientists in many fields have the common and basic need of dimensionality reduction: visualizing the underlying structure of the massive multivariate data in a low-dimensional space. However, many dimensionality reduction methods confront the so-called \"crowding problem\" that clusters tend to overlap with each other in the embedding. Previously, researchers expect to avoid that problem and seek to make clusters maximally separated in the embedding. However, the proposed in-tree (IT) based method, called IT-map, allows clusters in the embedding to be locally overlapped, while seeking to make them distinguishable by some small yet key parts. IT-map provides a simple, effective and novel solution to cluster-preserving mapping, which makes it possible to cluster the original data points interactively and thus should be of general meaning in science and engineering.\n    ",
        "submission_date": "2015-01-26T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07719",
        "title": "Montblanc: GPU accelerated Radio Interferometer Measurement Equations in support of Bayesian Inference for Radio Observations",
        "authors": [
            "Simon Perkins",
            "Patrick Marais",
            "Jonathan Zwart",
            "Iniyan Natarajan",
            "Cyril Tasse",
            "Oleg Smirnov"
        ],
        "abstract": "We present Montblanc, a GPU implementation of the Radio interferometer measurement equation (RIME) in support of the Bayesian inference for radio observations (BIRO) technique. BIRO uses Bayesian inference to select sky models that best match the visibilities observed by a radio interferometer. To accomplish this, BIRO evaluates the RIME multiple times, varying sky model parameters to produce multiple model visibilities. Chi-squared values computed from the model and observed visibilities are used as likelihood values to drive the Bayesian sampling process and select the best sky model.\n",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1501.07758",
        "title": "Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts",
        "authors": [
            "Elias Kellner",
            "Bibek Dhital",
            "Marco Reisert"
        ],
        "abstract": "Gibbs-ringing is a well known artifact which manifests itself as spurious oscillations in the vicinity of sharp image transients, e.g. at tissue boundaries. The origin can be seen in the truncation of k-space during MRI data-acquisition. Consequently, correction techniques like Gegenbauer reconstruction or extrapolation methods aim at recovering these missing data. Here, we present a simple and robust method which exploits a different view on the Gibbs-phenomena. The truncation in k-space can be interpreted as a convolution with a sinc-function in image space. Hence, the severity of the artifacts depends on how the sinc-function is sampled. We propose to re-interpolate the image based on local, subvoxel shifts to sample the ringing pattern at the zero-crossings of the oscillating sinc-function. With this, the artifact can effectively and robustly be removed with a minimal amount of smoothing.\n    ",
        "submission_date": "2015-01-30T00:00:00",
        "last_modified_date": "2015-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00363",
        "title": "Iterated Support Vector Machines for Distance Metric Learning",
        "authors": [
            "Wangmeng Zuo",
            "Faqiang Wang",
            "David Zhang",
            "Liang Lin",
            "Yuchi Huang",
            "Deyu Meng",
            "Lei Zhang"
        ],
        "abstract": "Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2015-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00555",
        "title": "A Discrete Tchebichef Transform Approximation for Image and Video Coding",
        "authors": [
            "P. A. M. Oliveira",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Madanayake"
        ],
        "abstract": "In this paper, we introduce a low-complexity approximation for the discrete Tchebichef transform (DTT). The proposed forward and inverse transforms are multiplication-free and require a reduced number of additions and bit-shifting operations. Numerical compression simulations demonstrate the efficiency of the proposed transform for image and video coding. Furthermore, Xilinx Virtex-6 FPGA based hardware realization shows 44.9% reduction in dynamic power consumption and 64.7% lower area when compared to the literature.\n    ",
        "submission_date": "2015-01-28T00:00:00",
        "last_modified_date": "2015-01-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00592",
        "title": "A Class of DCT Approximations Based on the Feig-Winograd Algorithm",
        "authors": [
            "C. J. Tablada",
            "F. M. Bayer",
            "R. J. Cintra"
        ],
        "abstract": "A new class of matrices based on a parametrization of the Feig-Winograd factorization of 8-point DCT is proposed. Such parametrization induces a matrix subspace, which unifies a number of existing methods for DCT approximation. By solving a comprehensive multicriteria optimization problem, we identified several new DCT approximations. Obtained solutions were sought to possess the following properties: (i) low multiplierless computational complexity, (ii) orthogonality or near orthogonality, (iii) low complexity invertibility, and (iv) close proximity and performance to the exact DCT. Proposed approximations were submitted to assessment in terms of proximity to the DCT, coding performance, and suitability for image compression. Considering Pareto efficiency, particular new proposed approximations could outperform various existing methods archived in literature.\n    ",
        "submission_date": "2015-02-02T00:00:00",
        "last_modified_date": "2016-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00756",
        "title": "Design of a Mobile Face Recognition System for Visually Impaired Persons",
        "authors": [
            "Shonal Chaudhry",
            "Rohitash Chandra"
        ],
        "abstract": "It is estimated that 285 million people globally are visually impaired. A majority of these people live in developing countries and are among the elderly population. One of the most difficult tasks faced by the visually impaired is identification of people. While naturally, voice recognition is a common method of identification, it is an intuitive and difficult process. The rise of computation capability of mobile devices gives motivation to develop applications that can assist visually impaired persons. With the availability of mobile devices, these people can be assisted by an additional method of identification through intelligent software based on computer vision techniques. In this paper, we present the design and implementation of a face detection and recognition system for the visually impaired through the use of mobile computing. This mobile system is assisted by a server-based support system. The system was tested on a custom video database. Experiment results show high face detection accuracy and promising face recognition accuracy in suitable conditions. The challenges of the system lie in better recognition techniques for difficult situations in terms of lighting and weather.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-06-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.00956",
        "title": "ORB-SLAM: a Versatile and Accurate Monocular SLAM System",
        "authors": [
            "Raul Mur-Artal",
            "J. M. M. Montiel",
            "Juan D. Tardos"
        ],
        "abstract": "This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.\n    ",
        "submission_date": "2015-02-03T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01094",
        "title": "Multimodal Task-Driven Dictionary Learning for Image Classification",
        "authors": [
            "Soheil Bahrampour",
            "Nasser M. Nasrabadi",
            "Asok Ray",
            "W. Kenneth Jenkins"
        ],
        "abstract": "Dictionary learning algorithms have been successfully used for both reconstructive and discriminative tasks, where an input signal is represented with a sparse linear combination of dictionary atoms. While these methods are mostly developed for single-modality scenarios, recent studies have demonstrated the advantages of feature-level fusion based on the joint sparse representation of the multimodal inputs. In this paper, we propose a multimodal task-driven dictionary learning algorithm under the joint sparsity constraint (prior) to enforce collaborations among multiple homogeneous/heterogeneous sources of information. In this task-driven formulation, the multimodal dictionaries are learned simultaneously with their corresponding classifiers. The resulting multimodal dictionaries can generate discriminative latent features (sparse codes) from the data that are optimized for a given task such as binary or multiclass classification. Moreover, we present an extension of the proposed formulation using a mixed joint and independent sparsity prior which facilitates more flexible fusion of the modalities at feature level. The efficacy of the proposed algorithms for multimodal classification is illustrated on four different applications -- multimodal face recognition, multi-view face recognition, multi-view action recognition, and multimodal biometric recognition. It is also shown that, compared to the counterpart reconstructive-based dictionary learning algorithms, the task-driven formulations are more computationally efficient in the sense that they can be equipped with more compact dictionaries and still achieve superior performance.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2015-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01241",
        "title": "A specialized face-processing network consistent with the representational geometry of monkey face patches",
        "authors": [
            "Amirhossein Farzmahdi",
            "Karim Rajaei",
            "Masoud Ghodrati",
            "Reza Ebrahimpour",
            "Seyed-Mahdi Khaligh-Razavi"
        ],
        "abstract": "Ample evidence suggests that face processing in human and non-human primates is performed differently compared with other objects. Converging reports, both physiologically and psychophysically, indicate that faces are processed in specialized neural networks in the brain -i.e. face patches in monkeys and the fusiform face area (FFA) in humans. We are all expert face-processing agents, and able to identify very subtle differences within the category of faces, despite substantial visual and featural similarities. Identification is performed rapidly and accurately after viewing a whole face, while significantly drops if some of the face configurations (e.g. inversion, misalignment) are manipulated or if partial views of faces are shown due to occlusion. This refers to a hotly-debated, yet highly-supported concept, known as holistic face processing. We built a hierarchical computational model of face-processing based on evidence from recent neuronal and behavioural studies on faces processing in primates. Representational geometries of the last three layers of the model have characteristics similar to those observed in monkey face patches (posterior, middle and anterior patches). Furthermore, several face-processing-related phenomena reported in the literature automatically emerge as properties of this model. The representations are evolved through several computational layers, using biologically plausible learning rules. The model satisfies face inversion effect, composite face effect, other race effect, view and identity selectivity, and canonical face views. To our knowledge, no models have so far been proposed with this performance and agreement with biological data.\n    ",
        "submission_date": "2015-02-04T00:00:00",
        "last_modified_date": "2016-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01400",
        "title": "Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation",
        "authors": [
            "Marcelo Pereyra",
            "Steve McLaughlin"
        ],
        "abstract": "This paper presents a new Bayesian estimation technique for hidden Potts-Markov random fields with unknown regularisation parameters, with application to fast unsupervised K-class image segmentation. The technique is derived by first removing the regularisation parameter from the Bayesian model by marginalisation, followed by a small-variance-asymptotic (SVA) analysis in which the spatial regularisation and the integer-constrained terms of the Potts model are decoupled. The evaluation of this SVA Bayesian estimator is then relaxed into a problem that can be computed efficiently by iteratively solving a convex total-variation denoising problem and a least-squares clustering (K-means) problem, both of which can be solved straightforwardly, even in high-dimensions, and with parallel computing techniques. This leads to a fast fully unsupervised Bayesian image segmentation methodology in which the strength of the spatial regularisation is adapted automatically to the observed image during the inference procedure, and that can be easily applied in large 2D and 3D scenarios or in applications requiring low computing times. Experimental results on real images, as well as extensive comparisons with state-of-the-art algorithms, confirm that the proposed methodology offer extremely fast convergence and produces accurate segmentation results, with the important additional advantage of self-adjusting regularisation parameters.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2016-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01480",
        "title": "Ring artifacts correction in compressed sensing tomographic reconstruction",
        "authors": [
            "Pierre Paleo",
            "Alessandro Mirone"
        ],
        "abstract": "We present a novel approach to handle ring artifacts correction in compressed sensing tomographic reconstruction. The correction is part of the reconstruction process, which differs from classical sinogram pre-processing and image post-processing techniques. The principle of compressed sensing tomographic reconstruction is presented. Then, we show that the ring artifacts correction can be integrated in the reconstruction problem formalism. We provide numerical results for both simulated and real data. This technique is included in the PyHST2 code which is used at the European Synchrotron Radiation Facility for tomographic reconstruction.\n    ",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01643",
        "title": "Performance Analysis of Cone Detection Algorithms",
        "authors": [
            "Letizia Mariotti",
            "Nicholas Devaney"
        ],
        "abstract": "Many algorithms have been proposed to help clinicians evaluate cone density and spacing, as these may be related to the onset of retinal diseases. However, there has been no rigorous comparison of the performance of these algorithms. In addition, the performance of such algorithms is typically determined by comparison with human observers. Here we propose a technique to simulate realistic images of the cone mosaic. We use the simulated images to test the performance of two popular cone detection algorithms and we introduce an algorithm which is used by astronomers to detect stars in astronomical images. We use Free Response Operating Characteristic (FROC) curves to evaluate and compare the performance of the three algorithms. This allows us to optimize the performance of each algorithm. We observe that performance is significantly enhanced by up-sampling the images. We investigate the effect of noise and image quality on cone mosaic parameters estimated using the different algorithms, finding that the estimated regularity is the most sensitive parameter.\n",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01659",
        "title": "Learning Articulated Motions From Visual Demonstration",
        "authors": [
            "Sudeep Pillai",
            "Matthew R. Walter",
            "Seth Teller"
        ],
        "abstract": "Many functional elements of human homes and workplaces consist of rigid components which are connected through one or more sliding or rotating linkages. Examples include doors and drawers of cabinets and appliances; laptops; and swivel office chairs. A robotic mobile manipulator would benefit from the ability to acquire kinematic models of such objects from observation. This paper describes a method by which a robot can acquire an object model by capturing depth imagery of the object as a human moves it through its range of motion. We envision that in future, a machine newly introduced to an environment could be shown by its human user the articulated objects particular to that environment, inferring from these \"visual demonstrations\" enough information to actuate each object independently of the user.\n",
        "submission_date": "2015-02-05T00:00:00",
        "last_modified_date": "2015-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01823",
        "title": "Unsupervised Fusion Weight Learning in Multiple Classifier Systems",
        "authors": [
            "Anurag Kumar",
            "Bhiksha Raj"
        ],
        "abstract": "In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision. Unlike most classifier fusion methods where a single weight is learned to weigh all examples our method learns instance-specific weights. The problem is formulated as learning the weight which maximizes a clarity index; subsequently the index itself and the learned weights both are used separately to rank all the test points. Our method gives an unsupervised method of optimizing performance on actual test data, unlike the well known stacking-based methods where optimization is done over a labeled training set. Moreover, we show that our method is tolerant to noisy classifiers and can be used for selecting N-best classifiers.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.01827",
        "title": "Hierarchical Maximum-Margin Clustering",
        "authors": [
            "Guang-Tong Zhou",
            "Sung Ju Hwang",
            "Mark Schmidt",
            "Leonid Sigal",
            "Greg Mori"
        ],
        "abstract": "We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximum-margin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2015-02-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02077",
        "title": "Quantum Energy Regression using Scattering Transforms",
        "authors": [
            "Matthew Hirn",
            "Nicolas Poilvert",
            "St\u00e9phane Mallat"
        ],
        "abstract": "We present a novel approach to the regression of quantum mechanical energies based on a scattering transform of an intermediate electron density representation. A scattering transform is a deep convolution network computed with a cascade of multiscale wavelet transforms. It possesses appropriate invariant and stability properties for quantum energy regression. This new framework removes fundamental limitations of Coulomb matrix based energy regressions, and numerical experiments give state-of-the-art accuracy over planar molecules.\n    ",
        "submission_date": "2015-02-06T00:00:00",
        "last_modified_date": "2016-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02330",
        "title": "Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction",
        "authors": [
            "Yong Luo",
            "Dacheng Tao",
            "Yonggang Wen",
            "Kotagiri Ramamohanarao",
            "Chao Xu"
        ],
        "abstract": "Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features.\n",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02478",
        "title": "Efficient batchwise dropout training using submatrices",
        "authors": [
            "Ben Graham",
            "Jeremy Reizenstein",
            "Leigh Robinson"
        ],
        "abstract": "Dropout is a popular technique for regularizing artificial neural networks. Dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units---a different pattern of dropout is applied to every sample in the minibatch. We explore a very simple alternative to the dropout mask. Instead of masking dropped out units by setting them to zero, we perform matrix multiplication using a submatrix of the weight matrix---unneeded hidden units are never calculated. Performing dropout batchwise, so that one pattern of dropout is used for each sample in a minibatch, we can substantially reduce training times. Batchwise dropout can be used with fully-connected and convolutional neural networks.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2015-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02590",
        "title": "Analysis of classifiers' robustness to adversarial perturbations",
        "authors": [
            "Alhussein Fawzi",
            "Omar Fawzi",
            "Pascal Frossard"
        ],
        "abstract": "The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, namely their instability to adversarial perturbations (Szegedy et. al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on the families of linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured by the distinguishability). Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \\sqrt{d} (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed in the context of neural networks. To the best of our knowledge, our results provide the first theoretical work that addresses the phenomenon of adversarial instability recently observed for deep networks. Our analysis is complemented by experimental results on controlled and real-world data.\n    ",
        "submission_date": "2015-02-09T00:00:00",
        "last_modified_date": "2016-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.02871",
        "title": "Talk to the Hand: Generating a 3D Print from Photographs",
        "authors": [
            "Edward Aboufadel",
            "Sylvanna V. Krawczyk",
            "Melissa Sherman-Bennett"
        ],
        "abstract": "This manuscript presents a linear algebra-based technique that only requires two unique photographs from a digital camera to mathematically construct a 3D surface representation which can then be 3D printed. Basic computer vision theory and manufacturing principles are also briefly discussed.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03044",
        "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
        "authors": [
            "Kelvin Xu",
            "Jimmy Ba",
            "Ryan Kiros",
            "Kyunghyun Cho",
            "Aaron Courville",
            "Ruslan Salakhutdinov",
            "Richard Zemel",
            "Yoshua Bengio"
        ],
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03126",
        "title": "Kernel Task-Driven Dictionary Learning for Hyperspectral Image Classification",
        "authors": [
            "Soheil Bahrampour",
            "Nasser M. Nasrabadi",
            "Asok Ray",
            "Kenneth W. Jenkins"
        ],
        "abstract": "Dictionary learning algorithms have been successfully used in both reconstructive and discriminative tasks, where the input signal is represented by a linear combination of a few dictionary atoms. While these methods are usually developed under $\\ell_1$ sparsity constrain (prior) in the input domain, recent studies have demonstrated the advantages of sparse representation using structured sparsity priors in the kernel domain. In this paper, we propose a supervised dictionary learning algorithm in the kernel domain for hyperspectral image classification. In the proposed formulation, the dictionary and classifier are obtained jointly for optimal classification performance. The supervised formulation is task-driven and provides learned features from the hyperspectral data that are well suited for the classification task. Moreover, the proposed algorithm uses a joint ($\\ell_{12}$) sparsity prior to enforce collaboration among the neighboring pixels. The simulation results illustrate the efficiency of the proposed dictionary learning algorithm.\n    ",
        "submission_date": "2015-02-10T00:00:00",
        "last_modified_date": "2015-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03215",
        "title": "A Hybrid Approach for Improved Content-based Image Retrieval using Segmentation",
        "authors": [
            "Smarajit Bose",
            "Amita Pal",
            "Jhimli Mallick",
            "Sunil Kumar",
            "Pratyaydipta Rudra"
        ],
        "abstract": "The objective of Content-Based Image Retrieval (CBIR) methods is essentially to extract, from large (image) databases, a specified number of images similar in visual and semantic content to a so-called query image. To bridge the semantic gap that exists between the representation of an image by low-level features (namely, colour, shape, texture) and its high-level semantic content as perceived by humans, CBIR systems typically make use of the relevance feedback (RF) mechanism. RF iteratively incorporates user-given inputs regarding the relevance of retrieved images, to improve retrieval efficiency. One approach is to vary the weights of the features dynamically via feature reweighting. In this work, an attempt has been made to improve retrieval accuracy by enhancing a CBIR system based on color features alone, through implicit incorporation of shape information obtained through prior segmentation of the images. Novel schemes for feature reweighting as well as for initialization of the relevant set for improved relevance feedback, have also been proposed for boosting performance of RF- based CBIR. At the same time, new measures for evaluation of retrieval accuracy have been suggested, to overcome the limitations of existing measures in the RF context. Results of extensive experiments have been presented to illustrate the effectiveness of the proposed approaches.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03409",
        "title": "Large-Scale Deep Learning on the YFCC100M Dataset",
        "authors": [
            "Karl Ni",
            "Roger Pearce",
            "Kofi Boakye",
            "Brian Van Essen",
            "Damian Borth",
            "Barry Chen",
            "Eric Wang"
        ],
        "abstract": "We present a work-in-progress snapshot of learning with a 15 billion parameter deep learning network on HPC architectures applied to the largest publicly available natural image and video dataset released to-date. Recent advancements in unsupervised deep neural networks suggest that scaling up such networks in both model and training dataset size can yield significant improvements in the learning of concepts at the highest layers. We train our three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M dataset. The dataset comprises approximately 99.2 million images and 800,000 user-created videos from Yahoo's Flickr image and video sharing platform. Training of our network takes eight days on 98 GPU nodes at the High Performance Computing Center at Lawrence Livermore National Laboratory. Encouraging preliminary results and future research directions are presented and discussed.\n    ",
        "submission_date": "2015-02-11T00:00:00",
        "last_modified_date": "2015-02-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03537",
        "title": "Convergence of gradient based pre-training in Denoising autoencoders",
        "authors": [
            "Vamsi K Ithapu",
            "Sathya Ravi",
            "Vikas Singh"
        ],
        "abstract": "The success of deep architectures is at least in part attributed to the layer-by-layer unsupervised pre-training that initializes the network. Various papers have reported extensive empirical analysis focusing on the design and implementation of good pre-training procedures. However, an understanding pertaining to the consistency of parameter estimates, the convergence of learning procedures and the sample size estimates is still unavailable in the literature. In this work, we study pre-training in classical and distributed denoising autoencoders with these goals in mind. We show that the gradient converges at the rate of $\\frac{1}{\\sqrt{N}}$ and has a sub-linear dependence on the size of the autoencoder network. In a distributed setting where disjoint sections of the whole network are pre-trained synchronously, we show that the convergence improves by at least $\\tau^{3/4}$, where $\\tau$ corresponds to the size of the sections. We provide a broad set of experiments to empirically evaluate the suggested behavior.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03723",
        "title": "Simulation of Color Blindness and a Proposal for Using Google Glass as Color-correcting Tool",
        "authors": [
            "H.M. de Oliveira",
            "J. Ranhel",
            "R.B.A. Alves"
        ],
        "abstract": "The human visual color response is driven by specialized cells called cones, which exist in three types, viz. R, G, and B. Software is developed to simulate how color images are displayed for different types of color blindness. Specified the default color deficiency associated with a user, it generates a preview of the rainbow (in the visible range, from red to violet) and shows up, side by side with a colorful image provided as input, the display correspondent colorblind. The idea is to provide an image processing after image acquisition to enable a better perception ofcolors by the color blind. Examples of pseudo-correction are shown for the case of Protanopia (red blindness). The system is adapted into a screen of an i-pad or a cellphone in which the colorblind observe the camera, the image processed with color detail previously imperceptible by his naked eye. As prospecting, wearable computer glasses could be manufactured to provide a corrected image playback. The approach can also provide augmented reality for human vision by adding the UV or IR responses as a new feature of Google Glass.\n    ",
        "submission_date": "2015-02-12T00:00:00",
        "last_modified_date": "2015-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.03879",
        "title": "Semi-supervised Data Representation via Affinity Graph Learning",
        "authors": [
            "Weiya Ren"
        ],
        "abstract": "We consider the general problem of utilizing both labeled and unlabeled data to improve data representation performance. A new semi-supervised learning framework is proposed by combing manifold regularization and data representation methods such as Non negative matrix factorization and sparse coding. We adopt unsupervised data representation methods as the learning machines because they do not depend on the labeled data, which can improve machine's generation ability as much as possible. The proposed framework forms the Laplacian regularizer through learning the affinity graph. We incorporate the new Laplacian regularizer into the unsupervised data representation to smooth the low dimensional representation of data and make use of label information. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results compared with state-of-art methods.\n    ",
        "submission_date": "2015-02-13T00:00:00",
        "last_modified_date": "2015-02-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04502",
        "title": "Clustering by Descending to the Nearest Neighbor in the Delaunay Graph Space",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "In our previous works, we proposed a physically-inspired rule to organize the data points into an in-tree (IT) structure, in which some undesired edges are allowed to occur. By removing those undesired or redundant edges, this IT structure is divided into several separate parts, each representing one cluster. In this work, we seek to prevent the undesired edges from arising at the source. Before using the physically-inspired rule, data points are at first organized into a proximity graph which restricts each point to select the optimal directed neighbor just among its neighbors. Consequently, separated in-trees or clusters automatically arise, without redundant edges requiring to be removed.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04681",
        "title": "Unsupervised Learning of Video Representations using LSTMs",
        "authors": [
            "Nitish Srivastava",
            "Elman Mansimov",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2016-01-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04726",
        "title": "ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors",
        "authors": [
            "Hojjat S. Mousavi",
            "Vishal Monga",
            "Trac D. Tran"
        ],
        "abstract": "In this letter, we address sparse signal recovery using spike and slab priors. In particular, we focus on a Bayesian framework where sparsity is enforced on reconstruction coefficients via probabilistic priors. The optimization resulting from spike and slab prior maximization is known to be a hard non-convex problem, and existing solutions involve simplifying assumptions and/or relaxations. We propose an approach called Iterative Convex Refinement (ICR) that aims to solve the aforementioned optimization problem directly allowing for greater generality in the sparse structure. Essentially, ICR solves a sequence of convex optimization problems such that sequence of solutions converges to a sub-optimal solution of the original hard optimization problem. We propose two versions of our algorithm: a.) an unconstrained version, and b.) with a non-negativity constraint on sparse coefficients, which may be required in some real-world problems. Experimental validation is performed on both synthetic data and for a real-world image recovery problem, which illustrates merits of ICR over state of the art alternatives.\n    ",
        "submission_date": "2015-02-16T00:00:00",
        "last_modified_date": "2015-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.04837",
        "title": "Nonparametric Nearest Neighbor Descent Clustering based on Delaunay Triangulation",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "In our physically inspired in-tree (IT) based clustering algorithm and the series after it, there is only one free parameter involved in computing the potential value of each point. In this work, based on the Delaunay Triangulation or its dual Voronoi tessellation, we propose a nonparametric process to compute potential values by the local information. This computation, though nonparametric, is relatively very rough, and consequently, many local extreme points will be generated. However, unlike those gradient-based methods, our IT-based methods are generally insensitive to those local extremes. This positively demonstrates the superiority of these parametric (previous) and nonparametric (in this work) IT-based methods.\n    ",
        "submission_date": "2015-02-17T00:00:00",
        "last_modified_date": "2015-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05197",
        "title": "Analysis and approximation of some Shape-from-Shading models for non-Lambertian surfaces",
        "authors": [
            "Silvia Tozza",
            "Maurizio Falcone"
        ],
        "abstract": "The reconstruction of a 3D object or a scene is a classical inverse problem in Computer Vision. In the case of a single image this is called the Shape-from-Shading (SfS) problem and it is known to be ill-posed even in a simplified version like the vertical light source case. A huge number of works deals with the orthographic SfS problem based on the Lambertian reflectance model, the most common and simplest model which leads to an eikonal type equation when the light source is on the vertical axis. In this paper we want to study non-Lambertian models since they are more realistic and suitable whenever one has to deal with different kind of surfaces, rough or specular. We will present a unified mathematical formulation of some popular orthographic non-Lambertian models, considering vertical and oblique light directions as well as different viewer positions. These models lead to more complex stationary nonlinear partial differential equations of Hamilton-Jacobi type which can be regarded as the generalization of the classical eikonal equation corresponding to the Lambertian case. However, all the equations corresponding to the models considered here (Oren-Nayar and Phong) have a similar structure so we can look for weak solutions to this class in the viscosity solution framework. Via this unified approach, we are able to develop a semi-Lagrangian approximation scheme for the Oren-Nayar and the Phong model and to prove a general convergence result. Numerical simulations on synthetic and real images will illustrate the effectiveness of this approach and the main features of the scheme, also comparing the results with previous results in the literature.\n    ",
        "submission_date": "2015-02-18T00:00:00",
        "last_modified_date": "2016-01-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.05957",
        "title": "Web Similarity in Sets of Search Terms using Database Queries",
        "authors": [
            "Andrew R. Cohen",
            "Paul M.B. Vitanyi"
        ],
        "abstract": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or another large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a common similarity (common semantics) on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity of members of a set according to all (upper semi)computable properties. We develop the theory and give applications of classifying using Amazon, Wikipedia, and the NCBI website from the National Institutes of Health. The last gives new correlations between health hazards. A restriction of the NWD to a set of two yields the earlier normalized google distance (NGD) but no combination of the NGD's of pairs in a set can extract the information the NWD extracts from the set. The NWD enables a new contextual (different databases) learning approachbased on Kolmogorov complexity theory that incorporates knowledge from these databases.\n    ",
        "submission_date": "2015-02-20T00:00:00",
        "last_modified_date": "2020-07-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06236",
        "title": "Some enumerations of binary digital images",
        "authors": [
            "P. Christopher Staecker"
        ],
        "abstract": "The topology of digital images has been studied much in recent years, but no attempt has been made to exhaustively catalog the structure of binary images of small numbers of points. We produce enumerations of several classes of digital images up to isomorphism and decide which among them are homotopy equivalent to one another. Noting some patterns in the results, we make some conjectures about digital images which are irreducible but not rigid.\n    ",
        "submission_date": "2015-02-22T00:00:00",
        "last_modified_date": "2015-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.06464",
        "title": "Rectified Factor Networks",
        "authors": [
            "Djork-Arn\u00e9 Clevert",
            "Andreas Mayr",
            "Thomas Unterthiner",
            "Sepp Hochreiter"
        ],
        "abstract": "We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.\n    ",
        "submission_date": "2015-02-23T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07019",
        "title": "Building with Drones: Accurate 3D Facade Reconstruction using MAVs",
        "authors": [
            "Shreyansh Daftry",
            "Christof Hoppe",
            "Horst Bischof"
        ],
        "abstract": "Automatic reconstruction of 3D models from images using multi-view Structure-from-Motion methods has been one of the most fruitful outcomes of computer vision. These advances combined with the growing popularity of Micro Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools ubiquitous for large number of Architecture, Engineering and Construction applications among audiences, mostly unskilled in computer vision. However, to obtain high-resolution and accurate reconstructions from a large-scale object using SfM, there are many critical constraints on the quality of image data, which often become sources of inaccuracy as the current 3D reconstruction pipelines do not facilitate the users to determine the fidelity of input data during the image acquisition. In this paper, we present and advocate a closed-loop interactive approach that performs incremental reconstruction in real-time and gives users an online feedback about the quality parameters like Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We also propose a novel multi-scale camera network design to prevent scene drift caused by incremental map building, and release the first multi-scale image sequence dataset as a benchmark. Further, we evaluate our system on real outdoor scenes, and show that our interactive pipeline combined with a multi-scale camera network approach provides compelling accuracy in multi-view reconstruction tasks when compared against the state-of-the-art methods.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07041",
        "title": "Describing Colors, Textures and Shapes for Content Based Image Retrieval - A Survey",
        "authors": [
            "Jamil Ahmad",
            "Muhammad Sajjad",
            "Irfan Mehmood",
            "Seungmin Rho",
            "Sung Wook Baik"
        ],
        "abstract": "Visual media has always been the most enjoyed way of communication. From the advent of television to the modern day hand held computers, we have witnessed the exponential growth of images around us. Undoubtedly it's a fact that they carry a lot of information in them which needs be utilized in an effective manner. Hence intense need has been felt to efficiently index and store large image collections for effective and on- demand retrieval. For this purpose low-level features extracted from the image contents like color, texture and shape has been used. Content based image retrieval systems employing these features has proven very successful. Image retrieval has promising applications in numerous fields and hence has motivated researchers all over the world. New and improved ways to represent visual content are being developed each day. Tremendous amount of research has been carried out in the last decade. In this paper we will present a detailed overview of some of the powerful color, texture and shape descriptors for content based image retrieval. A comparative analysis will also be carried out for providing an insight into outstanding challenges in this field.\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07241",
        "title": "Proceedings of the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015)",
        "authors": [
            "Frank Hannig",
            "Dietmar Fey",
            "Anton Lokhmotov"
        ],
        "abstract": "This volume contains the papers accepted at the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with the Conference on Design, Automation and Test in Europe (DATE).\n    ",
        "submission_date": "2015-02-25T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07448",
        "title": "Automatic Optimization of Hardware Accelerators for Image Processing",
        "authors": [
            "Oliver Reiche",
            "Konrad H\u00e4ublein",
            "Marc Reichenbach",
            "Frank Hannig",
            "J\u00fcrgen Teich",
            "Dietmar Fey"
        ],
        "abstract": "In the domain of image processing, often real-time constraints are required. In particular, in safety-critical applications, such as X-ray computed tomography in medical imaging or advanced driver assistance systems in the automotive domain, timing is of utmost importance. A common approach to maintain real-time capabilities of compute-intensive applications is to offload those computations to dedicated accelerator hardware, such as Field Programmable Gate Arrays (FPGAs). Programming such architectures is a challenging task, with respect to the typical FPGA-specific design criteria: Achievable overall algorithm latency and resource usage of FPGA primitives (BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies this task by enabling the description of algorithms in well-known higher languages (C/C++) and its automatic synthesis that can be accomplished by HLS tools. However, algorithm developers still need expert knowledge about the target architecture, in order to achieve satisfying results. Therefore, in previous work, we have shown that elevating the description of image algorithms to an even higher abstraction level, by using a Domain-Specific Language (DSL), can significantly cut down the complexity for designing such algorithms for FPGAs. To give the developer even more control over the common trade-off, latency vs. resource usage, we will present an automatic optimization process where these criteria are analyzed and fed back to the DSL compiler, in order to generate code that is closer to the desired design specifications. Finally, we generate code for stereo block matching algorithms and compare it with handwritten implementations to quantify the quality of our results.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07449",
        "title": "Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing",
        "authors": [
            "Lan Shi",
            "Christopher Soell",
            "Andreas Baenisch",
            "Robert Weigel",
            "J\u00fcrgen Seiler",
            "Thomas Ussmueller"
        ],
        "abstract": "A concept for a novel CMOS image sensor suited for analog image pre-processing is presented in this paper. As an example, an image restoration algorithm for reducing image noise is applied as image pre-processing in the analog domain. To supply low-latency data input for analog image preprocessing, the proposed concept for a CMOS image sensor offers a new sensor signal acquisition method in 2D. In comparison to image pre-processing in the digital domain, the proposed analog image pre-processing promises an improved image quality. Furthermore, the image noise at the stage of analog sensor signal acquisition can be used to select the most effective restoration algorithm applied to the analog circuit due to image processing prior to the A/D converter.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07743",
        "title": "Tracking an Object with Unknown Accelerations using a Shadowing Filter",
        "authors": [
            "Kevin Judd"
        ],
        "abstract": "A commonly encountered problem is the tracking of a physical object, like a maneuvering ship, aircraft, land vehicle, spacecraft or animate creature carrying a wireless device. The sensor data is often limited and inaccurate observations of range or bearing. This problem is more difficult than tracking a ballistic trajectory, because an operative affects unknown and arbitrarily changing accelerations. Although stochastic methods of filtering or state estimation (Kalman filters and particle filters) are widely used, out of vogue variational methods are more appropriate in this tracking context, because the objects do not typically display any significant random motions at the length and time scales of interest. This leads us to propose a rather elegant approach based on a \\emph{shadowing filter}. The resulting filter is efficient (reduces to the solution of linear equations) and robust (uneffected by missing data and singular correlations that would cause catastrophic failure of Bayesian filters.) The tracking is so robust, that in some common situations it actually performs better by ignoring error correlations that are so vital to Kalman filters.\n    ",
        "submission_date": "2015-01-21T00:00:00",
        "last_modified_date": "2015-01-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07816",
        "title": "Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms for Localization",
        "authors": [
            "Joshua I. Glaser",
            "Bradley M. Zamft",
            "George M. Church",
            "Konrad P. Kording"
        ],
        "abstract": "Current high-resolution imaging techniques require an intact sample that preserves spatial relationships. We here present a novel approach, \"puzzle imaging,\" that allows imaging a spatially scrambled sample. This technique takes many spatially disordered samples, and then pieces them back together using local properties embedded within the sample. We show that puzzle imaging can efficiently produce high-resolution images using dimensionality reduction algorithms. We demonstrate the theoretical capabilities of puzzle imaging in three biological scenarios, showing that (1) relatively precise 3-dimensional brain imaging is possible; (2) the physical structure of a neural network can often be recovered based only on the neural connectivity matrix; and (3) a chemical map could be reproduced using bacteria with chemosensitive DNA and conjugative transfer. The ability to reconstruct scrambled images promises to enable imaging based on DNA sequencing of homogenized tissue samples.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-06-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07828",
        "title": "Hybrid coding of visual content and local image features",
        "authors": [
            "Luca Baroffio",
            "Matteo Cesana",
            "Alessandro Redondi",
            "Marco Tagliasacchi",
            "Stefano Tubaro"
        ],
        "abstract": "Distributed visual analysis applications, such as mobile visual search or Visual Sensor Networks (VSNs) require the transmission of visual content on a bandwidth-limited network, from a peripheral node to a processing unit. Traditionally, a Compress-Then-Analyze approach has been pursued, in which sensing nodes acquire and encode the pixel-level representation of the visual content, that is subsequently transmitted to a sink node in order to be processed. This approach might not represent the most effective solution, since several analysis applications leverage a compact representation of the content, thus resulting in an inefficient usage of network resources. Furthermore, coding artifacts might significantly impact the accuracy of the visual task at hand. To tackle such limitations, an orthogonal approach named Analyze-Then-Compress has been proposed. According to such a paradigm, sensing nodes are responsible for the extraction of visual features, that are encoded and transmitted to a sink node for further processing. In spite of improved task efficiency, such paradigm implies the central processing node not being able to reconstruct a pixel-level representation of the visual content. In this paper we propose an effective compromise between the two paradigms, namely Hybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual content and local image features. Furthermore, we show how a target tradeoff between image quality and task accuracy might be achieved by accurately allocating the bitrate to either visual content or local features.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.07939",
        "title": "Coding local and global binary visual features extracted from video sequences",
        "authors": [
            "Luca Baroffio",
            "Antonio Canclini",
            "Matteo Cesana",
            "Alessandro Redondi",
            "Marco Tagliasacchi",
            "Stefano Tubaro"
        ],
        "abstract": "Binary local features represent an effective alternative to real-valued descriptors, leading to comparable results for many visual analysis tasks, while being characterized by significantly lower computational complexity and memory requirements. When dealing with large collections, a more compact representation based on global features is often preferred, which can be obtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW) model. Several applications, including for example visual sensor networks and mobile augmented reality, require visual features to be transmitted over a bandwidth-limited network, thus calling for coding techniques that aim at reducing the required bit budget, while attaining a target level of efficiency. In this paper we investigate a coding scheme tailored to both local and global binary features, which aims at exploiting both spatial and temporal redundancy by means of intra- and inter-frame coding. In this respect, the proposed coding scheme can be conveniently adopted to support the Analyze-Then-Compress (ATC) paradigm. That is, visual features are extracted from the acquired content, encoded at remote nodes, and finally transmitted to a central controller that performs visual analysis. This is in contrast with the traditional approach, in which visual content is acquired at a node, compressed and then sent to a central unit for further processing, according to the Compress-Then-Analyze (CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of rate-efficiency curves in the context of two different visual analysis tasks: homography estimation and content-based retrieval. Our results show that the novel ATC paradigm based on the proposed coding primitives can be competitive with CTA, especially in bandwidth limited scenarios.\n    ",
        "submission_date": "2015-02-26T00:00:00",
        "last_modified_date": "2015-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08029",
        "title": "Describing Videos by Exploiting Temporal Structure",
        "authors": [
            "Li Yao",
            "Atousa Torabi",
            "Kyunghyun Cho",
            "Nicolas Ballas",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1502.08039",
        "title": "Probabilistic Zero-shot Classification with Semantic Rankings",
        "authors": [
            "Jihun Hamm",
            "Mikhail Belkin"
        ],
        "abstract": "In this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zero-shot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.\n    ",
        "submission_date": "2015-02-27T00:00:00",
        "last_modified_date": "2015-02-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00087",
        "title": "Macroblock Classification Method for Video Applications Involving Motions",
        "authors": [
            "Weiyao Lin",
            "Ming-Ting Sun",
            "Hongxiang Li",
            "Zhenzhong Chen",
            "Wei Li",
            "Bing Zhou"
        ],
        "abstract": "In this paper, a macroblock classification method is proposed for various video processing applications involving motions. Based on the analysis of the Motion Vector field in the compressed video, we propose to classify Macroblocks of each video frame into different classes and use this class information to describe the frame content. We demonstrate that this low-computation-complexity method can efficiently catch the characteristics of the frame. Based on the proposed macroblock classification, we further propose algorithms for different video processing applications, including shot change detection, motion discontinuity detection, and outlier rejection for global motion estimation. Experimental results demonstrate that the methods based on the proposed approach can work effectively on these applications.\n    ",
        "submission_date": "2015-02-28T00:00:00",
        "last_modified_date": "2015-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00687",
        "title": "A review of mean-shift algorithms for clustering",
        "authors": [
            "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"
        ],
        "abstract": "A natural way to characterize the cluster structure of a dataset is by finding regions containing a high density of data. This can be done in a nonparametric way with a kernel density estimate, whose modes and hence clusters can be found using mean-shift algorithms. We describe the theory and practice behind clustering based on kernel density estimates and mean-shift algorithms. We discuss the blurring and non-blurring versions of mean-shift; theoretical results about mean-shift algorithms and Gaussian mixtures; relations with scale-space theory, spectral clustering and other algorithms; extensions to tracking, to manifold and graph data, and to manifold denoising; K-modes and Laplacian K-modes algorithms; acceleration strategies for large datasets; and applications to image segmentation, manifold denoising and multivalued regression.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2015-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00757",
        "title": "Constrained $H^1$-regularization schemes for diffeomorphic image registration",
        "authors": [
            "Andreas Mang",
            "George Biros"
        ],
        "abstract": "We propose regularization schemes for deformable registration and efficient algorithms for their numerical approximation. We treat image registration as a variational optimal control problem. The deformation map is parametrized by its velocity. Tikhonov regularization ensures well-posedness. Our scheme augments standard smoothness regularization operators based on $H^1$- and $H^2$-seminorms with a constraint on the divergence of the velocity field, which resembles variational formulations for Stokes incompressible flows. In our formulation, we invert for a stationary velocity field and a mass source map. This allows us to explicitly control the compressibility of the deformation map and by that the determinant of the deformation gradient. We also introduce a new regularization scheme that allows us to control shear. We use a globalized, preconditioned, matrix-free, reduced space (Gauss--)Newton--Krylov scheme for numerical optimization. We exploit variable elimination techniques to reduce the number of unknowns of our system; we only iterate on the reduced space of the velocity field. Our current implementation is limited to the two-dimensional case. The numerical experiments demonstrate that we can control the determinant of the deformation gradient without compromising registration quality. This additional control allows us to avoid oversmoothing of the deformation map. We also demonstrate that we can promote or penalize shear while controlling the determinant of the deformation gradient.\n    ",
        "submission_date": "2015-03-02T00:00:00",
        "last_modified_date": "2016-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.00843",
        "title": "A Survey On Video Forgery Detection",
        "authors": [
            "Sowmya K.N.",
            "H.R. Chennamma"
        ],
        "abstract": "The Digital Forgeries though not visibly identifiable to human perception it may alter or meddle with underlying natural statistics of digital content. Tampering involves fiddling with video content in order to cause damage or make unauthorized alteration/modification. Tampering detection in video is cumbersome compared to image when considering the properties of the video. Tampering impacts need to be studied and the applied technique/method is used to establish the factual information for legal course in judiciary. In this paper we give an overview of the prior literature and challenges involved in video forgery detection where passive approach is found.\n    ",
        "submission_date": "2015-03-03T00:00:00",
        "last_modified_date": "2015-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01228",
        "title": "Bethe Learning of Conditional Random Fields via MAP Decoding",
        "authors": [
            "Kui Tang",
            "Nicholas Ruozzi",
            "David Belanger",
            "Tony Jebara"
        ],
        "abstract": "Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable. This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function. The result is a new single loop algorithm MLE-Struct, which is substantially more efficient than previous double-loop methods for approximate maximum likelihood estimation. Our algorithm outperforms existing methods in experiments involving image segmentation, matching problems from vision, and a new dataset of university roommate assignments.\n    ",
        "submission_date": "2015-03-04T00:00:00",
        "last_modified_date": "2015-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01521",
        "title": "Jointly Learning Multiple Measures of Similarities from Triplet Comparisons",
        "authors": [
            "Liwen Zhang",
            "Subhransu Maji",
            "Ryota Tomioka"
        ],
        "abstract": "Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form \"from the t-th view, object A is more similar to B than to C\". Our framework jointly learns view-specific embeddings exploiting correlations between views. Experiments on a number of datasets, including one of multi-view crowdsourced comparison on bird images, show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. Our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi-task metric learning on the ISOLET dataset.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01538",
        "title": "Pyrcca: regularized kernel canonical correlation analysis in Python and its applications to neuroimaging",
        "authors": [
            "Natalia Y. Bilenko",
            "Jack L. Gallant"
        ],
        "abstract": "Canonical correlation analysis (CCA) is a valuable method for interpreting cross-covariance across related datasets of different dimensionality. There are many potential applications of CCA to neuroimaging data analysis. For instance, CCA can be used for finding functional similarities across fMRI datasets collected from multiple subjects without resampling individual datasets to a template anatomy. In this paper, we introduce Pyrcca, an open-source Python module for executing CCA between two or more datasets. Pyrcca can be used to implement CCA with or without regularization, and with or without linear or a Gaussian kernelization of the datasets. We demonstrate an application of CCA implemented with Pyrcca to neuroimaging data analysis. We use CCA to find a data-driven set of functional response patterns that are similar across individual subjects in a natural movie experiment. We then demonstrate how this set of response patterns discovered by CCA can be used to accurately predict subject responses to novel natural movie stimuli.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01558",
        "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision",
        "authors": [
            "Jonathan Malmaud",
            "Jonathan Huang",
            "Vivek Rathod",
            "Nick Johnston",
            "Andrew Rabinovich",
            "Kevin Murphy"
        ],
        "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01626",
        "title": "Inference of hidden structures in complex physical systems by multi-scale clustering",
        "authors": [
            "Z. Nussinov",
            "P. Ronhovde",
            "Dandan Hu",
            "S. Chakrabarty",
            "M. Sahu",
            "Bo Sun",
            "N. A. Mauro",
            "K. K. Sahu"
        ],
        "abstract": "We survey the application of a relatively new branch of statistical physics--\"community detection\"-- to data mining. In particular, we focus on the diagnosis of materials and automated image segmentation. Community detection describes the quest of partitioning a complex system involving many elements into optimally decoupled subsets or communities of such elements. We review a multiresolution variant which is used to ascertain structures at different spatial and temporal scales. Significant patterns are obtained by examining the correlations between different independent solvers. Similar to other combinatorial optimization problems in the NP complexity class, community detection exhibits several phases. Typically, illuminating orders are revealed by choosing parameters that lead to extremal information theory correlations.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2016-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01800",
        "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video",
        "authors": [
            "Samira Ebrahimi Kahou",
            "Xavier Bouthillier",
            "Pascal Lamblin",
            "Caglar Gulcehre",
            "Vincent Michalski",
            "Kishore Konda",
            "S\u00e9bastien Jean",
            "Pierre Froumenty",
            "Yann Dauphin",
            "Nicolas Boulanger-Lewandowski",
            "Raul Chandias Ferrari",
            "Mehdi Mirza",
            "David Warde-Farley",
            "Aaron Courville",
            "Pascal Vincent",
            "Roland Memisevic",
            "Christopher Pal",
            "Yoshua Bengio"
        ],
        "abstract": "The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based \"bag-of-mouths\" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset.\n    ",
        "submission_date": "2015-03-05T00:00:00",
        "last_modified_date": "2015-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.01820",
        "title": "Latent Hierarchical Model for Activity Recognition",
        "authors": [
            "Ninghang Hu",
            "Gwenn Englebienne",
            "Zhongyu Lou",
            "Ben Kr\u00f6se"
        ],
        "abstract": "We present a novel hierarchical model for human activity recognition. In contrast to approaches that successively recognize actions and activities, our approach jointly models actions and activities in a unified framework, and their labels are simultaneously predicted. The model is embedded with a latent layer that is able to capture a richer class of contextual information in both state-state and observation-state pairs. Although loops are present in the model, the model has an overall linear-chain structure, where the exact inference is tractable. Therefore, the model is very efficient in both inference and learning. The parameters of the graphical model are learned with a Structured Support Vector Machine (Structured-SVM). A data-driven approach is used to initialize the latent variables; therefore, no manual labeling for the latent states is required. The experimental results from using two benchmark datasets show that our model outperforms the state-of-the-art approach, and our model is computationally more efficient.\n    ",
        "submission_date": "2015-03-06T00:00:00",
        "last_modified_date": "2015-03-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02302",
        "title": "DESAT: an SSW tool for SDO/AIA image de-saturation",
        "authors": [
            "Richard A Schwartz",
            "Gabriele Torre",
            "Anna Maria Massone",
            "Michele Piana"
        ],
        "abstract": "Saturation affects a significant rate of images recorded by the Atmospheric Imaging Assembly on the Solar Dynamics Observatory. This paper describes a computational method and a technological pipeline for the de-saturation of such images, based on several mathematical ingredients like Expectation Maximization, image correlation and interpolation. An analysis of the computational properties and demands of the pipeline, together with an assessment of its reliability are performed against a set of data recorded from the Feburary 25 2014 flaring event.\n    ",
        "submission_date": "2015-03-08T00:00:00",
        "last_modified_date": "2015-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.02318",
        "title": "Understanding Image Virality",
        "authors": [
            "Arturo Deza",
            "Devi Parikh"
        ],
        "abstract": "Virality of online content on social networking websites is an important but esoteric phenomenon often studied in fields like marketing, psychology and data mining. In this paper we study viral images from a computer vision perspective. We introduce three new image datasets from Reddit, and define a virality score using Reddit metadata. We train classifiers with state-of-the-art image features to predict virality of individual images, relative virality in pairs of images, and the dominant topic of a viral image. We also compare machine performance to human performance on these tasks. We find that computers perform poorly with low level features, and high level information is critical for predicting virality. We encode semantic information through relative attributes. We identify the 5 key visual attributes that correlate with virality. We create an attribute-based characterization of images that can predict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes) -- better than humans at 60.12%. Finally, we study how human prediction of image virality varies with different `contexts' in which the images are viewed, such as the influence of neighbouring images, images recently viewed, as well as the image title or caption. This work is a first step in understanding the complex but important phenomenon of image virality. Our datasets and annotations will be made publicly available.\n    ",
        "submission_date": "2015-03-08T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03016",
        "title": "Remarks on pointed digital homotopy",
        "authors": [
            "Laurence Boxer",
            "P. Christopher Staecker"
        ],
        "abstract": "We present and explore in detail a pair of digital images with $c_u$-adjacencies that are homotopic but not pointed homotopic. For two digital loops $f,g: [0,m]_Z \\rightarrow X$ with the same basepoint, we introduce the notion of {\\em tight at the basepoint (TAB)} pointed homotopy, which is more restrictive than ordinary pointed homotopy and yields some different results.\n",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03231",
        "title": "Adaptive-Rate Sparse Signal Reconstruction With Application in Compressive Background Subtraction",
        "authors": [
            "Joao F. C. Mota",
            "Nikos Deligiannis",
            "Aswin C. Sankaranarayanan",
            "Volkan Cevher",
            "Miguel R. D. Rodrigues"
        ],
        "abstract": "We propose and analyze an online algorithm for reconstructing a sequence of signals from a limited number of linear measurements. The signals are assumed sparse, with unknown support, and evolve over time according to a generic nonlinear dynamical model. Our algorithm, based on recent theoretical results for $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of measurements to be taken at each time on-the-fly. As an example, we apply the algorithm to compressive video background subtraction, a problem that can be stated as follows: given a set of measurements of a sequence of images with a static background, simultaneously reconstruct each image while separating its foreground from the background. The performance of our method is illustrated on sequences of real images: we observe that it allows a dramatic reduction in the number of measurements with respect to state-of-the-art compressive background subtraction schemes.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03492",
        "title": "Parallel Statistical Multi-resolution Estimation",
        "authors": [
            "Jan Lebert",
            "Lutz K\u00fcnneke",
            "Johannes Hagemann",
            "Stephan C. Kramer"
        ],
        "abstract": "We discuss several strategies to implement Dykstra's projection algorithm on NVIDIA's compute unified device architecture (CUDA). Dykstra's algorithm is the central step in and the computationally most expensive part of statistical multi-resolution methods. It projects a given vector onto the intersection of convex sets. Compared with a CPU implementation our CUDA implementation is one order of magnitude faster. For a further speed up and to reduce memory consumption we have developed a new variant, which we call incomplete Dykstra's algorithm. Implemented in CUDA it is one order of magnitude faster than the CUDA implementation of the standard Dykstra algorithm. As sample application we discuss using the incomplete Dykstra's algorithm as preprocessor for the recently developed super-resolution optical fluctuation imaging (SOFI) method (Dertinger et al. 2009). We show that statistical multi-resolution estimation can enhance the resolution improvement of the plain SOFI algorithm just as the Fourier-reweighting of SOFI. The results are compared in terms of their power spectrum and their Fourier ring correlation (Saxton and Baumeister 1982). The Fourier ring correlation indicates that the resolution for typical second order SOFI images can be improved by about 30 per cent. Our results show that a careful parallelization of Dykstra's algorithm enables its use in large-scale statistical multi-resolution analyses.\n    ",
        "submission_date": "2015-03-10T00:00:00",
        "last_modified_date": "2015-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03506",
        "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning",
        "authors": [
            "Christian Wachinger",
            "Polina Golland"
        ],
        "abstract": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\u00f6m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.\n    ",
        "submission_date": "2015-03-11T00:00:00",
        "last_modified_date": "2015-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03562",
        "title": "Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation",
        "authors": [
            "Zhiyong Cheng",
            "Daniel Soudry",
            "Zexi Mao",
            "Zhenzhong Lan"
        ],
        "abstract": "Compared to Multilayer Neural Networks with real weights, Binary Multilayer Neural Networks (BMNNs) can be implemented more efficiently on dedicated hardware. BMNNs have been demonstrated to be effective on binary classification tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text datasets. In this paper, we investigate the capability of BMNNs using the EBP algorithm on multiclass image classification tasks. The performances of binary neural networks with multiple hidden layers and different numbers of hidden units are examined on MNIST. We also explore the effectiveness of image spatial filters and the dropout technique in BMNNs. Experimental results on MNIST dataset show that EBP can obtain 2.12% test error with binary weights and 1.66% test error with real weights, which is comparable to the results of standard BackPropagation algorithm on fully connected MNNs.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.03732",
        "title": "Starting engagement detection towards a companion robot using multimodal features",
        "authors": [
            "Dominique Vaufreydaz",
            "Wafa Johal",
            "Claudine Combe"
        ],
        "abstract": "Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone's intention of starting an interaction. Classically, spatial information like the human position and speed, the human-robot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection.\n    ",
        "submission_date": "2015-03-12T00:00:00",
        "last_modified_date": "2015-03-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04127",
        "title": "Image patch analysis of sunspots and active regions. I. Intrinsic dimension and correlation analysis",
        "authors": [
            "Kevin R. Moon",
            "Jimmy J. Li",
            "Veronique Delouille",
            "Ruben De Visscher",
            "Fraser Watson",
            "Alfred O. Hero III"
        ],
        "abstract": "The flare-productivity of an active region is observed to be related to its spatial complexity. Mount Wilson or McIntosh sunspot classifications measure such complexity but in a categorical way, and may therefore not use all the information present in the observations. Moreover, such categorical schemes hinder a systematic study of an active region's evolution for example. We propose fine-scale quantitative descriptors for an active region's complexity and relate them to the Mount Wilson classification. We analyze the local correlation structure within continuum and magnetogram data, as well as the cross-correlation between continuum and magnetogram data. We compute the intrinsic dimension, partial correlation, and canonical correlation analysis (CCA) of image patches of continuum and magnetogram active region images taken from the SOHO-MDI instrument. We use masks of sunspots derived from continuum as well as larger masks of magnetic active regions derived from the magnetogram to analyze separately the core part of an active region from its surrounding part. We find the relationship between complexity of an active region as measured by Mount Wilson and the intrinsic dimension of its image patches. Partial correlation patterns exhibit approximately a third-order Markov structure. CCA reveals different patterns of correlation between continuum and magnetogram within the sunspots and in the region surrounding the sunspots. These results also pave the way for patch-based dictionary learning with a view towards automatic clustering of active regions.\n    ",
        "submission_date": "2015-03-13T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04253",
        "title": "Novel Super-Resolution Method Based on High Order Nonlocal-Means",
        "authors": [
            "Kang Yong-Rim",
            "Kim Yong-Jin"
        ],
        "abstract": "Super-resolution without explicit sub-pixel motion estimation is a very active subject of image reconstruction containing general motion. The Non-Local Means (NLM) method is a simple image reconstruction method without explicit motion estimation. In this paper we generalize NLM method to higher orders using kernel regression can apply to super-resolution reconstruction. The performance of the generalized method is compared with other methods.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04287",
        "title": "Metric Localization using Google Street View",
        "authors": [
            "Pratik Agarwal",
            "Wolfram Burgard",
            "Luciano Spinello"
        ],
        "abstract": "Accurate metrical localization is one of the central challenges in mobile robotics. Many existing methods aim at localizing after building a map with the robot. In this paper, we present a novel approach that instead uses geotagged panoramas from the Google Street View as a source of global positioning. We model the problem of localization as a non-linear least squares estimation in two phases. The first estimates the 3D position of tracked feature points from short monocular camera sequences. The second computes the rigid body transformation between the Street View panoramas and the estimated points. The only input of this approach is a stream of monocular camera images and odometry estimates. We quantified the accuracy of the method by running the approach on a robotic platform in a parking lot by using visual fiducials as ground truth. Additionally, we applied the approach in the context of personal localization in a real urban scenario by using data from a Google Tango tablet.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-04-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04338",
        "title": "Towards radio astronomical imaging using an arbitrary basis",
        "authors": [
            "Matthias Petschow"
        ],
        "abstract": "The new generation of radio telescopes, such as the Square Kilometer Array (SKA), requires dramatic advances in computer hardware and software, in order to process the large amounts of produced data efficiently. In this document, we explore a new approach to wide-field imaging. By generalizing the image reconstruction, which is performed by an inverse Fourier transform, to arbitrary transformations, we gain enormous new possibilities. In particular, we outline an approach that might allow to obtain a sky image of size P times Q in (optimal) O(PQ) time. This could be a step in the direction of real-time, wide-field sky imaging for future telescopes.\n    ",
        "submission_date": "2015-03-14T00:00:00",
        "last_modified_date": "2015-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04400",
        "title": "Separable and non-separable data representation for pattern discrimination",
        "authors": [
            "Jaros\u0142aw Adam Miszczak"
        ],
        "abstract": "We provide a complete work-flow, based on the language of quantum information theory, suitable for processing data for the purpose of pattern recognition. The main advantage of the introduced scheme is that it can be easily implemented and applied to process real-world data using modest computation resources. At the same time it can be used to investigate the difference in the pattern recognition resulting from the utilization of the tensor product structure of the space of quantum states. We illustrate this difference by providing a simple example based on the classification of 2D data.\n    ",
        "submission_date": "2015-03-15T00:00:00",
        "last_modified_date": "2015-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04585",
        "title": "Statistical Analysis of Loopy Belief Propagation in Random Fields",
        "authors": [
            "Muneki Yasuda",
            "Shun Kataoka",
            "Kazuyuki Tanaka"
        ],
        "abstract": "Loopy belief propagation (LBP), which is equivalent to the Bethe approximation in statistical mechanics, is a message-passing-type inference method that is widely used to analyze systems based on Markov random fields (MRFs). In this paper, we propose a message-passing-type method to analytically evaluate the quenched average of LBP in random fields by using the replica cluster variation method. The proposed analytical method is applicable to general pair-wise MRFs with random fields whose distributions differ from each other and can give the quenched averages of the Bethe free energies over random fields, which are consistent with numerical results. The order of its computational cost is equivalent to that of standard LBP. In the latter part of this paper, we describe the application of the proposed method to Bayesian image restoration, in which we observed that our theoretical results are in good agreement with the numerical results for natural images.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04596",
        "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network",
        "authors": [
            "Mark D. McDonnell",
            "Tony Vladusich"
        ],
        "abstract": "We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.04776",
        "title": "Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic Images",
        "authors": [
            "Mohammad Tofighi",
            "Onur Yorulmaz",
            "A. Enis Cetin"
        ],
        "abstract": "In this article, two closed and convex sets for blind deconvolution problem are proposed. Most blurring functions in microscopy are symmetric with respect to the origin. Therefore, they do not modify the phase of the Fourier transform (FT) of the original image. As a result blurred image and the original image have the same FT phase. Therefore, the set of images with a prescribed FT phase can be used as a constraint set in blind deconvolution problems. Another convex set that can be used during the image reconstruction process is the epigraph set of Total Variation (TV) function. This set does not need a prescribed upper bound on the total variation of the image. The upper bound is automatically adjusted according to the current image of the restoration process. Both of these two closed and convex sets can be used as a part of any blind deconvolution algorithm. Simulation examples are presented.\n    ",
        "submission_date": "2015-03-16T00:00:00",
        "last_modified_date": "2015-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.05947",
        "title": "Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression Algorithm",
        "authors": [
            "Yanlai Chen"
        ],
        "abstract": "Dimension reduction is often needed in the area of data mining. The goal of these methods is to map the given high-dimensional data into a low-dimensional space preserving certain properties of the initial data. There are two kinds of techniques for this purpose. The first, projective methods, builds an explicit linear projection from the high-dimensional space to the low-dimensional one. On the other hand, the nonlinear methods utilizes nonlinear and implicit mapping between the two spaces. In both cases, the methods considered in literature have usually relied on computationally very intensive matrix factorizations, frequently the Singular Value Decomposition (SVD). The computational burden of SVD quickly renders these dimension reduction methods infeasible thanks to the ever-increasing sizes of the practical datasets.\n",
        "submission_date": "2015-03-19T00:00:00",
        "last_modified_date": "2015-03-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06561",
        "title": "A Comparative Analysis of Tensor Decomposition Models Using Hyper Spectral Image",
        "authors": [
            "Ankit Gupta",
            "Ashish Oberoi"
        ],
        "abstract": "Hyper spectral imaging is a remote sensing technology, providing variety of applications such as material identification, space object identification, planetary exploitation etc. It deals with capturing continuum of images of the earth surface from different angles. Due to the multidimensional nature of the image, multi-way arrays are one of the possible solutions for analyzing hyper spectral data. This multi-way array is called tensor. Our approach deals with implementing three decomposition models LMLRA, BTD and CPD to the sample data for choosing the best decomposition of the data set. The results have proved that Block Term Decomposition (BTD) is the best tensor model for decomposing the hyper spectral image in to resultant factor matrices.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.06775",
        "title": "Non-contact transmittance photoplethysmographic imaging (PPGI) for long-distance cardiovascular monitoring",
        "authors": [
            "Robert Amelard",
            "Christian Scharfenberger",
            "Farnoud Kazemzadeh",
            "Kaylen J. Pfisterer",
            "Bill S. Lin",
            "Alexander Wong",
            "David A. Clausi"
        ],
        "abstract": "Photoplethysmography (PPG) devices are widely used for monitoring cardiovascular function. However, these devices require skin contact, which restrict their use to at-rest short-term monitoring using single-point measurements. Photoplethysmographic imaging (PPGI) has been recently proposed as a non-contact monitoring alternative by measuring blood pulse signals across a spatial region of interest. Existing systems operate in reflectance mode, of which many are limited to short-distance monitoring and are prone to temporal changes in ambient illumination. This paper is the first study to investigate the feasibility of long-distance non-contact cardiovascular monitoring at the supermeter level using transmittance PPGI. For this purpose, a novel PPGI system was designed at the hardware and software level using ambient correction via temporally coded illumination (TCI) and signal processing for PPGI signal extraction. Experimental results show that the processing steps yield a substantially more pulsatile PPGI signal than the raw acquired signal, resulting in statistically significant increases in correlation to ground-truth PPG in both short- ($p \\in [<0.0001, 0.040]$) and long-distance ($p \\in [<0.0001, 0.056]$) monitoring. The results support the hypothesis that long-distance heart rate monitoring is feasible using transmittance PPGI, allowing for new possibilities of monitoring cardiovascular function in a non-contact manner.\n    ",
        "submission_date": "2015-03-23T00:00:00",
        "last_modified_date": "2015-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07077",
        "title": "Rotation-invariant convolutional neural networks for galaxy morphology prediction",
        "authors": [
            "Sander Dieleman",
            "Kyle W. Willett",
            "Joni Dambre"
        ],
        "abstract": "Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey (SDSS) have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is time-consuming and does not scale to large ($\\gtrsim10^4$) numbers of images.\n",
        "submission_date": "2015-03-24T00:00:00",
        "last_modified_date": "2015-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07790",
        "title": "Transductive Multi-label Zero-shot Learning",
        "authors": [
            "Yanwei Fu",
            "Yongxin Yang",
            "Tim Hospedales",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the first time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique compositionality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07884",
        "title": "Transductive Multi-class and Multi-label Zero-shot Learning",
        "authors": [
            "Yanwei Fu",
            "Yongxin Yang",
            "Timothy M. Hospedales",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "Recently, zero-shot learning (ZSL) has received increasing interest. The key idea underpinning existing ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation which is assumed to be shared between the auxiliary and target datasets, and is used to bridge between these domains for knowledge transfer. The semantic representation used in existing approaches varies from visual attributes to semantic word vectors and semantic relatedness. However, the overall pipeline is similar: a projection mapping low-level features to the semantic representation is learned from the auxiliary dataset by either classification or regression models and applied directly to map each instance into the same semantic representation space where a zero-shot classifier is used to recognise the unseen target class instances with a single known 'prototype' of each target class. In this paper we discuss two related lines of work improving the conventional approach: exploiting transductive learning ZSL, and generalising ZSL to the multi-label case.\n    ",
        "submission_date": "2015-03-26T00:00:00",
        "last_modified_date": "2015-03-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.07998",
        "title": "Real-time multi-view deconvolution",
        "authors": [
            "Benjamin Schmid",
            "Jan Huisken"
        ],
        "abstract": "In light-sheet microscopy, overall image content and resolution are improved by acquiring and fusing multiple views of the sample from different directions. State-of-the-art multi-view (MV) deconvolution employs the point spread functions (PSF) of the different views to simultaneously fuse and deconvolve the images in 3D, but processing takes a multiple of the acquisition time and constitutes the bottleneck in the imaging pipeline. Here we show that MV deconvolution in 3D can finally be achieved in real-time by reslicing the acquired data and processing cross-sectional planes individually on the massively parallel architecture of a graphics processing unit (GPU).\n    ",
        "submission_date": "2015-03-27T00:00:00",
        "last_modified_date": "2015-03-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1503.08248",
        "title": "Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval",
        "authors": [
            "Xirong Li",
            "Tiberio Uricchio",
            "Lamberto Ballan",
            "Marco Bertini",
            "Cees G. M. Snoek",
            "Alberto Del Bimbo"
        ],
        "abstract": "Where previous reviews on content-based image retrieval emphasize on what can be seen in an image to bridge the semantic gap, this survey considers what people tag about an image. A comprehensive treatise of three closely linked problems, i.e., image tag assignment, refinement, and tag-based image retrieval is presented. While existing works vary in terms of their targeted tasks and methodology, they rely on the key functionality of tag relevance, i.e. estimating the relevance of a specific tag with respect to the visual content of a given image and its social context. By analyzing what information a specific method exploits to construct its tag relevance function and how such information is exploited, this paper introduces a taxonomy to structure the growing literature, understand the ingredients of the main works, clarify their connections and difference, and recognize their merits and limitations. For a head-to-head comparison between the state-of-the-art, a new experimental protocol is presented, with training sets containing 10k, 100k and 1m images and an evaluation on three test sets, contributed by various research groups. Eleven representative works are implemented and evaluated. Putting all this together, the survey aims to provide an overview of the past and foster progress for the near future.\n    ",
        "submission_date": "2015-03-28T00:00:00",
        "last_modified_date": "2016-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00430",
        "title": "Direct l_(2,p)-Norm Learning for Feature Selection",
        "authors": [
            "Hanyang Peng",
            "Yong Fan"
        ],
        "abstract": "In this paper, we propose a novel sparse learning based feature selection method that directly optimizes a large margin linear classification model sparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints, rather than using the sparsity as a regularization term. To solve the direct sparsity optimization problem that is non-smooth and non-convex when 0<p<1, we provide an efficient iterative algorithm with proved convergence by converting it to a convex and smooth optimization problem at every iteration step. The proposed algorithm has been evaluated based on publicly available datasets, and extensive comparison experiments have demonstrated that our algorithm could achieve feature selection performance competitive to state-of-the-art algorithms.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00580",
        "title": "Quantum image classification using principal component analysis",
        "authors": [
            "Mateusz Ostaszewski",
            "Przemys\u0142aw Sadowski",
            "Piotr Gawron"
        ],
        "abstract": "We present a novel quantum algorithm for classification of images. The algorithm is constructed using principal component analysis and von Neuman quantum measurements. In order to apply the algorithm we present a new quantum representation of grayscale images.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00593",
        "title": "The Approximation of the Dissimilarity Projection",
        "authors": [
            "Emanuele Olivetti",
            "Thien Bao Nguyen",
            "Paolo Avesani"
        ],
        "abstract": "Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3D pathways of axons within the white matter of the brain as a tractography. The analysis of tractographies has drawn attention from the machine learning and pattern recognition communities providing novel challenges such as finding an appropriate representation space for the data. Many of the current learning algorithms require the input to be from a vectorial space. This requirement contrasts with the intrinsic nature of the tractography because its basic elements, called streamlines or tracks, have different lengths and different number of points and for this reason they cannot be directly represented in a common vectorial space. In this work we propose the adoption of the dissimilarity representation which is an Euclidean embedding technique defined by selecting a set of streamlines called prototypes and then mapping any new streamline to the vector of distances from prototypes. We investigate the degree of approximation of this projection under different prototype selection policies and prototype set sizes in order to characterise its use on tractography data. Additionally we propose the use of a scalable approximation of the most effective prototype selection policy that provides fast and accurate dissimilarity approximations of complete tractographies.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00641",
        "title": "A Probabilistic Theory of Deep Learning",
        "authors": [
            "Ankit B. Patel",
            "Tan Nguyen",
            "Richard G. Baraniuk"
        ],
        "abstract": "A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2015-04-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00702",
        "title": "End-to-End Training of Deep Visuomotor Policies",
        "authors": [
            "Sergey Levine",
            "Chelsea Finn",
            "Trevor Darrell",
            "Pieter Abbeel"
        ],
        "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.\n    ",
        "submission_date": "2015-04-02T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00905",
        "title": "Robust Anomaly Detection Using Semidefinite Programming",
        "authors": [
            "Jose A. Lopez",
            "Octavia Camps",
            "Mario Sznaier"
        ],
        "abstract": "This paper presents a new approach, based on polynomial optimization and the method of moments, to the problem of anomaly detection. The proposed technique only requires information about the statistical moments of the normal-state distribution of the features of interest and compares favorably with existing approaches (such as Parzen windows and 1-class SVM). In addition, it provides a succinct description of the normal state. Thus, it leads to a substantial simplification of the the anomaly detection problem when working with higher dimensional datasets.\n    ",
        "submission_date": "2015-04-03T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.00923",
        "title": "A Unified Deep Neural Network for Speaker and Language Recognition",
        "authors": [
            "Fred Richardson",
            "Douglas Reynolds",
            "Najim Dehak"
        ],
        "abstract": "Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge speaker recognition task (55% reduction in EER for the out-of-domain condition) and on the NIST 2011 Language Recognition Evaluation (48% reduction in EER for the 30s test condition).\n    ",
        "submission_date": "2015-04-03T00:00:00",
        "last_modified_date": "2015-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01025",
        "title": "Preprint Extending Touch-less Interaction on Vision Based Wearable Device",
        "authors": [
            "Zhihan Lv",
            "Liangbing Feng",
            "Shengzhong Feng",
            "Haibo Li"
        ],
        "abstract": "This is the preprint version of our paper on IEEE Virtual Reality Conference 2015. A touch-less interaction technology on vision based wearable device is designed and evaluated. Users interact with the application with dynamic hands/feet gestures in front of the camera. Several proof-of-concept prototypes with eleven dynamic gestures are developed based on the touch-less interaction. At last, a comparing user study evaluation is proposed to demonstrate the usability of the touch-less approach, as well as the impact on user's emotion, running on a wearable framework or Google Glass.\n    ",
        "submission_date": "2015-04-04T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.01716",
        "title": "An Empirical Evaluation of Deep Learning on Highway Driving",
        "authors": [
            "Brody Huval",
            "Tao Wang",
            "Sameep Tandon",
            "Jeff Kiske",
            "Will Song",
            "Joel Pazhayampallil",
            "Mykhaylo Andriluka",
            "Pranav Rajpurkar",
            "Toki Migimatsu",
            "Royce Cheng-Yue",
            "Fernando Mujica",
            "Adam Coates",
            "Andrew Y. Ng"
        ],
        "abstract": "Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.\n    ",
        "submission_date": "2015-04-07T00:00:00",
        "last_modified_date": "2015-04-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02164",
        "title": "Linearly Supporting Feature Extraction For Automated Estimation Of Stellar Atmospheric Parameters",
        "authors": [
            "Xiangru Li",
            "Yu Lu",
            "Georges Comte",
            "Ali Luo",
            "Yongheng Zhao",
            "Yongjun Wang"
        ],
        "abstract": "We describe a scheme to extract linearly supporting (LSU) features from stellar spectra to automatically estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H]. \"Linearly supporting\" means that the atmospheric parameters can be accurately estimated from the extracted features through a linear model. The successive steps of the process are as follow: first, decompose the spectrum using a wavelet packet (WP) and represent it by the derived decomposition coefficients; second, detect representative spectral features from the decomposition coefficients using the proposed method Least Absolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detected features using a linear regression method. One prominent characteristic of this scheme is its ability to evaluate quantitatively the contribution of each detected feature to the atmospheric parameter estimate and also to trace back the physical significance of that feature. This work also shows that the usefulness of a component depends on both wavelength and frequency. The proposed scheme has been evaluated on both real spectra from the Sloan Digital Sky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODF models. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62 features for log$~g$, and 68 features for [Fe/H]. Test consistencies between our estimates and those provided by the Spectroscopic Sarameter Pipeline of SDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$ (83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. For the synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$ (32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02206",
        "title": "A Multiphase Image Segmentation Based on Fuzzy Membership Functions and L1-norm Fidelity",
        "authors": [
            "Fang Li",
            "Stanley Osher",
            "Jing Qin",
            "Ming Yan"
        ],
        "abstract": "In this paper, we propose a variational multiphase image segmentation model based on fuzzy membership functions and L1-norm fidelity. Then we apply the alternating direction method of multipliers to solve an equivalent problem. All the subproblems can be solved efficiently. Specifically, we propose a fast method to calculate the fuzzy median. Experimental results and comparisons show that the L1-norm based method is more robust to outliers such as impulse noise and keeps better contrast than its L2-norm counterpart. Theoretically, we prove the existence of the minimizer and analyze the convergence of the algorithm.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2016-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02356",
        "title": "Exploring EEG for Object Detection and Retrieval",
        "authors": [
            "Eva Mohedano",
            "Amaia Salvador",
            "Sergi Porta",
            "Xavier Gir\u00f3-i-Nieto",
            "Graham Healy",
            "Kevin McGuinness",
            "Noel O'Connor",
            "Alan F. Smeaton"
        ],
        "abstract": "This paper explores the potential for using Brain Computer Interfaces (BCI) as a relevance feedback mechanism in content-based image retrieval. We investigate if it is possible to capture useful EEG signals to detect if relevant objects are present in a dataset of realistic and complex images. We perform several experiments using a rapid serial visual presentation (RSVP) of images at different rates (5Hz and 10Hz) on 8 users with different degrees of familiarization with BCI and the dataset. We then use the feedback from the BCI and mouse-based interfaces to retrieve localized objects in a subset of TRECVid images. We show that it is indeed possible to detect such objects in complex images and, also, that users with previous knowledge on the dataset or experience with the RSVP outperform others. When the users have limited time to annotate the images (100 seconds in our experiments) both interfaces are comparable in performance. Comparing our best users in a retrieval task, we found that EEG-based relevance feedback outperforms mouse-based feedback. The realistic and complex image dataset differentiates our work from previous studies on EEG for image retrieval.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02398",
        "title": "Real-time Monocular Object SLAM",
        "authors": [
            "Dorian G\u00e1lvez-L\u00f3pez",
            "Marta Salas",
            "Juan D. Tard\u00f3s",
            "J. M. M. Montiel"
        ],
        "abstract": "We present a real-time object-based SLAM system that leverages the largest object database to date. Our approach comprises two main components: 1) a monocular SLAM algorithm that exploits object rigidity constraints to improve the map and find its real scale, and 2) a novel object recognition algorithm based on bags of binary words, which provides live detections with a database of 500 3D objects. The two components work together and benefit each other: the SLAM algorithm accumulates information from the observations of the objects, anchors object features to especial map landmarks and sets constrains on the optimization. At the same time, objects partially or fully located within the map are used as a prior to guide the recognition algorithm, achieving higher recall. We evaluate our proposal on five real environments showing improvements on the accuracy of the map and efficiency with respect to other state-of-the-art techniques.\n    ",
        "submission_date": "2015-04-09T00:00:00",
        "last_modified_date": "2015-04-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02756",
        "title": "Discrimination and characterization of Parkinsonian rest tremors by analyzing long-term correlations and multifractal signatures",
        "authors": [
            "Lorenzo Livi",
            "Alireza Sadeghian",
            "Hamid Sadeghian"
        ],
        "abstract": "In this paper, we analyze 48 signals of rest tremor velocity related to 12 distinct subjects affected by Parkinson's disease. The subjects belong to two different groups, formed by four and eight subjects with, respectively, high- and low-amplitude rest tremors. Each subject is tested in four settings, given by combining the use of deep brain stimulation and L-DOPA medication. We develop two main feature-based representations of such signals, which are obtained by considering (i) the long-term correlations and multifractal properties, and (ii) the power spectra. The feature-based representations are initially utilized for the purpose of characterizing the subjects under different settings. In agreement with previous studies, we show that deep brain stimulation does not significantly characterize neither of the two groups, regardless of the adopted representation. On the other hand, the medication effect yields statistically significant differences in both high- and low-amplitude tremor groups. We successively test several different instances of the two feature-based representations of the signals in the setting of supervised classification and (nonlinear) feature transformation. We consider three different classification problems, involving the recognition of (i) the presence of medication, (ii) the use of deep brain stimulation, and (iii) the membership to the high- and low-amplitude tremor groups. Classification results show that the use of medication can be discriminated with higher accuracy, considering many of the feature-based representations. Notably, we show that the best results are obtained with a parsimonious, two-dimensional representation encoding the long-term correlations and multifractal character of the signals.\n    ",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.02762",
        "title": "Image patch analysis of sunspots and active regions. II. Clustering via matrix factorization",
        "authors": [
            "Kevin R. Moon",
            "Veronique Delouille",
            "Jimmy J. Li",
            "Ruben De Visscher",
            "Fraser Watson",
            "Alfred O. Hero III"
        ],
        "abstract": "Separating active regions that are quiet from potentially eruptive ones is a key issue in Space Weather applications. Traditional classification schemes such as Mount Wilson and McIntosh have been effective in relating an active region large scale magnetic configuration to its ability to produce eruptive events. However, their qualitative nature prevents systematic studies of an active region's evolution for example. We introduce a new clustering of active regions that is based on the local geometry observed in Line of Sight magnetogram and continuum images. We use a reduced-dimension representation of an active region that is obtained by factoring the corresponding data matrix comprised of local image patches. Two factorizations can be compared via the definition of appropriate metrics on the resulting factors. The distances obtained from these metrics are then used to cluster the active regions. We find that these metrics result in natural clusterings of active regions. The clusterings are related to large scale descriptors of an active region such as its size, its local magnetic field distribution, and its complexity as measured by the Mount Wilson classification scheme. We also find that including data focused on the neutral line of an active region can result in an increased correspondence between our clustering results and other active region descriptors such as the Mount Wilson classifications and the $R$ value. We provide some recommendations for which metrics, matrix factorization techniques, and regions of interest to use to study active regions.\n    ",
        "submission_date": "2015-04-10T00:00:00",
        "last_modified_date": "2015-12-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03106",
        "title": "Learning Multiple Visual Tasks while Discovering their Structure",
        "authors": [
            "Carlo Ciliberto",
            "Lorenzo Rosasco",
            "Silvia Villa"
        ],
        "abstract": "Multi-task learning is a natural approach for computer vision applications that require the simultaneous solution of several distinct but related problems, e.g. object detection, classification, tracking of multiple agents, or denoising, to name a few. The key idea is that exploring task relatedness (structure) can lead to improved performances.\n",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.03154",
        "title": "Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How Many Objects can iCub Learn?",
        "authors": [
            "Giulia Pasquale",
            "Carlo Ciliberto",
            "Francesca Odone",
            "Lorenzo Rosasco",
            "Lorenzo Natale"
        ],
        "abstract": "The ability to visually recognize objects is a fundamental skill for robotics systems. Indeed, a large variety of tasks involving manipulation, navigation or interaction with other agents, deeply depends on the accurate understanding of the visual scene. Yet, at the time being, robots are lacking good visual perceptual systems, which often become the main bottleneck preventing the use of autonomous agents for real-world applications.\n",
        "submission_date": "2015-04-13T00:00:00",
        "last_modified_date": "2015-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.04343",
        "title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning",
        "authors": [
            "Stefan Hadjis",
            "Firas Abuzaid",
            "Ce Zhang",
            "Christopher R\u00e9"
        ],
        "abstract": "We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals. We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures. We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet. Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs.\n    ",
        "submission_date": "2015-04-16T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05035",
        "title": "F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation",
        "authors": [
            "Xiaohe Wu",
            "Wangmeng Zuo",
            "Yuanyuan Zhu",
            "Liang Lin"
        ],
        "abstract": "The generalization error bound of support vector machine (SVM) depends on the ratio of radius and margin, while standard SVM only considers the maximization of the margin but ignores the minimization of the radius. Several approaches have been proposed to integrate radius and margin for joint learning of feature transformation and SVM classifier. However, most of them either require the form of the transformation matrix to be diagonal, or are non-convex and computationally expensive. In this paper, we suggest a novel approximation for the radius of minimum enclosing ball (MEB) in feature space, and then propose a convex radius-margin based SVM model for joint learning of feature transformation and SVM classifier, i.e., F-SVM. An alternating minimization method is adopted to solve the F-SVM model, where the feature transformation is updatedvia gradient descent and the classifier is updated by employing the existing SVM solver. By incorporating with kernel principal component analysis, F-SVM is further extended for joint learning of nonlinear transformation and classifier. Experimental results on the UCI machine learning datasets and the LFW face datasets show that F-SVM outperforms the standard SVM and the existing radius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05241",
        "title": "Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection",
        "authors": [
            "Yi Hou",
            "Hong Zhang",
            "Shilin Zhou"
        ],
        "abstract": "Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU.\n    ",
        "submission_date": "2015-04-20T00:00:00",
        "last_modified_date": "2015-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.05632",
        "title": "Self-Tuned Deep Super Resolution",
        "authors": [
            "Zhangyang Wang",
            "Yingzhen Yang",
            "Zhaowen Wang",
            "Shiyu Chang",
            "Wei Han",
            "Jianchao Yang",
            "Thomas S. Huang"
        ],
        "abstract": "Deep learning has been successfully applied to image super resolution (SR). In this paper, we propose a deep joint super resolution (DJSR) model to exploit both external and self similarities for SR. A Stacked Denoising Convolutional Auto Encoder (SDCAE) is first pre-trained on external examples with proper data augmentations. It is then fine-tuned with multi-scale self examples from each input, where the reliability of self examples is explicitly taken into account. We also enhance the model performance by sub-model training and selection. The DJSR model is extensively evaluated and compared with state-of-the-arts, and show noticeable performance improvements both quantitatively and perceptually on a wide range of images.\n    ",
        "submission_date": "2015-04-22T00:00:00",
        "last_modified_date": "2015-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06785",
        "title": "Complete Dictionary Recovery over the Sphere",
        "authors": [
            "Ju Sun",
            "Qing Qu",
            "John Wright"
        ],
        "abstract": "We consider the problem of recovering a complete (i.e., square and invertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$ with $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is sufficiently sparse. This recovery problem is central to the theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals, and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\\mathbf X_0$. In contrast, prior results based on efficient algorithms provide recovery guarantees when $\\mathbf X_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta \\in (0, 1)$.\n",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06786",
        "title": "Deviation Based Pooling Strategies For Full Reference Image Quality Assessment",
        "authors": [
            "Hossein Ziaei Nafchi",
            "Rachid Hedjam",
            "Atena Shahkolaei",
            "Mohamed Cheriet"
        ],
        "abstract": "The state-of-the-art pooling strategies for perceptual image quality assessment (IQA) are based on the mean and the weighted mean. They are robust pooling strategies which usually provide a moderate to high performance for different IQAs. Recently, standard deviation (SD) pooling was also proposed. Although, this deviation pooling provides a very high performance for a few IQAs, its performance is lower than mean poolings for many other IQAs. In this paper, we propose to use the mean absolute deviation (MAD) and show that it is a more robust and accurate pooling strategy for a wider range of IQAs. In fact, MAD pooling has the advantages of both mean pooling and SD pooling. The joint computation and use of the MAD and SD pooling strategies is also considered in this paper. Experimental results provide useful information on the choice of the proper deviation pooling strategy for different IQA models.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.06787",
        "title": "Max-margin Deep Generative Models",
        "authors": [
            "Chongxuan Li",
            "Jun Zhu",
            "Tianlin Shi",
            "Bo Zhang"
        ],
        "abstract": "Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.\n    ",
        "submission_date": "2015-04-26T00:00:00",
        "last_modified_date": "2015-12-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07116",
        "title": "Meta learning of bounds on the Bayes classifier error",
        "authors": [
            "Kevin R. Moon",
            "Veronique Delouille",
            "Alfred O. Hero III"
        ],
        "abstract": "Meta learning uses information from base learners (e.g. classifiers or estimators) as well as information about the learning problem to improve upon the performance of a single base learner. For example, the Bayes error rate of a given feature space, if known, can be used to aid in choosing a classifier, as well as in feature selection and model selection for the base classifiers and the meta classifier. Recent work in the field of f-divergence functional estimation has led to the development of simple and rapidly converging estimators that can be used to estimate various bounds on the Bayes error. We estimate multiple bounds on the Bayes error using an estimator that applies meta learning to slowly converging plug-in estimators to obtain the parametric convergence rate. We compare the estimated bounds empirically on simulated data and then estimate the tighter bounds on features extracted from an image patch analysis of sunspot continuum and magnetogram images.\n    ",
        "submission_date": "2015-04-27T00:00:00",
        "last_modified_date": "2015-07-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07643",
        "title": "A novel variational model for image registration using Gaussian curvature",
        "authors": [
            "Mazlinda Ibrahim",
            "Ke Chen",
            "Carlos Brito-Loeza"
        ],
        "abstract": "Image registration is one important task in many image processing applications. It aims to align two or more images so that useful information can be extracted through comparison, combination or superposition. This is achieved by constructing an optimal trans- formation which ensures that the template image becomes similar to a given reference image. Although many models exist, designing a model capable of modelling large and smooth deformation field continues to pose a challenge. This paper proposes a novel variational model for image registration using the Gaussian curvature as a regulariser. The model is motivated by the surface restoration work in geometric processing [Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. An effective numerical solver is provided for the model using an augmented Lagrangian method. Numerical experiments can show that the new model outperforms three competing models based on, respectively, a linear curvature [Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the mean curvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp. 89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage, (2009), pp. 61-72] in terms of robustness and accuracy.\n    ",
        "submission_date": "2015-04-28T00:00:00",
        "last_modified_date": "2015-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07786",
        "title": "Projected Iterative Soft-thresholding Algorithm for Tight Frames in Compressed Sensing Magnetic Resonance Imaging",
        "authors": [
            "Yunsong Liu",
            "Zhifang Zhan",
            "Jian-Feng Cai",
            "Di Guo",
            "Zhong Chen",
            "Xiaobo Qu"
        ],
        "abstract": "Compressed sensing has shown great potentials in accelerating magnetic resonance imaging. Fast image reconstruction and high image quality are two main issues faced by this new technology. It has been shown that, redundant image representations, e.g. tight frames, can significantly improve the image quality. But how to efficiently solve the reconstruction problem with these redundant representation systems is still challenging. This paper attempts to address the problem of applying iterative soft-thresholding algorithm (ISTA) to tight frames based magnetic resonance image reconstruction. By introducing the canonical dual frame to construct the orthogonal projection operator on the range of the analysis sparsity operator, we propose a projected iterative soft-thresholding algorithm (pISTA) and further accelerate it by incorporating the strategy proposed by Beck and Teboulle in 2009. We theoretically prove that pISTA converges to the minimum of a function with a balanced tight frame sparsity. Experimental results demonstrate that the proposed algorithm achieves better reconstruction than the widely used synthesis sparse model and the accelerated pISTA converges faster or comparable to the state-of-art smoothing FISTA. One major advantage of pISTA is that only one extra parameter, the step size, is introduced and the numerical solution is stable to it in terms of image reconstruction errors, thus allowing easily setting in many fast magnetic resonance imaging applications.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07857",
        "title": "Probabilistic Depth Image Registration incorporating Nonvisual Information",
        "authors": [
            "Manuel W\u00fcthrich",
            "Peter Pastor",
            "Ludovic Righetti",
            "Aude Billard",
            "Stefan Schaal"
        ],
        "abstract": "In this paper, we derive a probabilistic registration algorithm for object modeling and tracking. In many robotics applications, such as manipulation tasks, nonvisual information about the movement of the object is available, which we will combine with the visual information. Furthermore we do not only consider observations of the object, but we also take space into account which has been observed to not be part of the object. Furthermore we are computing a posterior distribution over the relative alignment and not a point estimate as typically done in for example Iterative Closest Point (ICP). To our knowledge no existing algorithm meets these three conditions and we thus derive a novel registration algorithm in a Bayesian framework. Experimental results suggest that the proposed methods perform favorably in comparison to PCL implementations of feature mapping and ICP, especially if nonvisual information is available.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.07874",
        "title": "Visual Information Retrieval in Endoscopic Video Archives",
        "authors": [
            "Jennifer Roldan-Carlos",
            "Mathias Lux",
            "Xavier Gir\u00f3-i-Nieto",
            "Pia Mu\u00f1oz",
            "Nektarios Anagnostopoulos"
        ],
        "abstract": "In endoscopic procedures, surgeons work with live video streams from the inside of their subjects. A main source for documentation of procedures are still frames from the video, identified and taken during the surgery. However, with growing demands and technical means, the streams are saved to storage servers and the surgeons need to retrieve parts of the videos on demand. In this submission we present a demo application allowing for video retrieval based on visual features and late fusion, which allows surgeons to re-find shots taken during the procedure.\n    ",
        "submission_date": "2015-04-29T00:00:00",
        "last_modified_date": "2015-04-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1504.08142",
        "title": "Semi-Orthogonal Multilinear PCA with Relaxed Start",
        "authors": [
            "Qiquan Shi",
            "Haiping Lu"
        ],
        "abstract": "Principal component analysis (PCA) is an unsupervised method for learning low-dimensional features with orthogonal projections. Multilinear PCA methods extend PCA to deal with multidimensional data (tensors) directly via tensor-to-tensor projection or tensor-to-vector projection (TVP). However, under the TVP setting, it is difficult to develop an effective multilinear PCA method with the orthogonality constraint. This paper tackles this problem by proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA learns low-dimensional features directly from tensors via TVP by imposing the orthogonality constraint in only one mode. This formulation results in more captured variance and more learned features than full orthogonality. For better generalization, we further introduce a relaxed start (RS) strategy to get SO-MPCA-RS by fixing the starting projection vectors, which increases the bias and reduces the variance of the learning model. Experiments on both face (2D) and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing algorithms on the whole, and the relaxed start strategy is also effective for other TVP-based PCA methods.\n    ",
        "submission_date": "2015-04-30T00:00:00",
        "last_modified_date": "2015-05-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00468",
        "title": "VQA: Visual Question Answering",
        "authors": [
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Stanislaw Antol",
            "Margaret Mitchell",
            "C. Lawrence Zitnick",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (",
        "submission_date": "2015-05-03T00:00:00",
        "last_modified_date": "2016-10-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00824",
        "title": "Self-Expressive Decompositions for Matrix Approximation and Clustering",
        "authors": [
            "Eva L. Dyer",
            "Tom A. Goldstein",
            "Raajen Patel",
            "Konrad P. Kording",
            "Richard G. Baraniuk"
        ],
        "abstract": "Data-aware methods for dimensionality reduction and matrix decomposition aim to find low-dimensional structure in a collection of data. Classical approaches discover such structure by learning a basis that can efficiently express the collection. Recently, \"self expression\", the idea of using a small subset of data vectors to represent the full collection, has been developed as an alternative to learning. Here, we introduce a scalable method for computing sparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that constructs a basis by sequentially selecting incoherent vectors from the dataset. After forming a basis from a subset of vectors in the dataset, SEED then computes a sparse representation of the dataset with respect to this basis. We develop sufficient conditions under which SEED exactly represents low rank matrices and vectors sampled from a unions of independent subspaces. We show how SEED can be used in applications ranging from matrix approximation and denoising to clustering, and apply it to numerous real-world datasets. Our results demonstrate that SEED is an attractive low-complexity alternative to other sparse matrix factorization approaches such as sparse PCA and self-expressive methods for clustering.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.00853",
        "title": "Empirical Evaluation of Rectified Activations in Convolutional Network",
        "authors": [
            "Bing Xu",
            "Naiyan Wang",
            "Tianqi Chen",
            "Mu Li"
        ],
        "abstract": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01214",
        "title": "Learning Style Similarity for Searching Infographics",
        "authors": [
            "Babak Saleh",
            "Mira Dontcheva",
            "Aaron Hertzmann",
            "Zhicheng Liu"
        ],
        "abstract": "Infographics are complex graphic designs integrating text, images, charts and sketches. Despite the increasing popularity of infographics and the rapid growth of online design portfolios, little research investigates how we can take advantage of these design resources. In this paper we present a method for measuring the style similarity between infographics. Based on human perception data collected from crowdsourced experiments, we use computer vision and machine learning algorithms to learn a style similarity metric for infographic designs. We evaluate different visual features and learning algorithms and find that a combination of color histograms and Histograms-of-Gradients (HoG) features is most effective in characterizing the style of infographics. We demonstrate our similarity metric on a preliminary image retrieval test.\n    ",
        "submission_date": "2015-05-05T00:00:00",
        "last_modified_date": "2015-05-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01335",
        "title": "Comparing persistence diagrams through complex vectors",
        "authors": [
            "Barbara Di Fabio",
            "Massimo Ferri"
        ],
        "abstract": "The natural pseudo-distance of spaces endowed with filtering functions is precious for shape classification and retrieval; its optimal estimate coming from persistence diagrams is the bottleneck distance, which unfortunately suffers from combinatorial explosion. A possible algebraic representation of persistence diagrams is offered by complex polynomials; since far polynomials represent far persistence diagrams, a fast comparison of the coefficient vectors can reduce the size of the database to be classified by the bottleneck distance. This article explores experimentally three transformations from diagrams to polynomials and three distances between the complex vectors of coefficients.\n    ",
        "submission_date": "2015-05-06T00:00:00",
        "last_modified_date": "2015-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01809",
        "title": "Language Models for Image Captioning: The Quirks and What Works",
        "authors": [
            "Jacob Devlin",
            "Hao Cheng",
            "Hao Fang",
            "Saurabh Gupta",
            "Li Deng",
            "Xiaodong He",
            "Geoffrey Zweig",
            "Margaret Mitchell"
        ],
        "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.\n    ",
        "submission_date": "2015-05-07T00:00:00",
        "last_modified_date": "2015-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.01953",
        "title": "The structure of optimal parameters for image restoration problems",
        "authors": [
            "Juan Carlos De Los Reyes",
            "Carola-Bibiane Sch\u00f6nlieb",
            "Tuomo Valkonen"
        ],
        "abstract": "We study the qualitative properties of optimal regularisation parameters in variational models for image restoration. The parameters are solutions of bilevel optimisation problems with the image restoration problem as constraint. A general type of regulariser is considered, which encompasses total variation (TV), total generalized variation (TGV) and infimal-convolution total variation (ICTV). We prove that under certain conditions on the given data optimal parameters derived by bilevel optimisation problems exist. A crucial point in the existence proof turns out to be the boundedness of the optimal parameters away from $0$ which we prove in this paper. The analysis is done on the original -- in image restoration typically non-smooth variational problem -- as well as on a smoothed approximation set in Hilbert space which is the one considered in numerical computations. For the smoothed bilevel problem we also prove that it $\\Gamma$ converges to the original problem as the smoothing vanishes. All analysis is done in function spaces rather than on the discretised learning problem.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02000",
        "title": "Deep Learning for Medical Image Segmentation",
        "authors": [
            "Matthew Lai"
        ],
        "abstract": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \"tri-planar\" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02074",
        "title": "Exploring Models and Data for Image Question Answering",
        "authors": [
            "Mengye Ren",
            "Ryan Kiros",
            "Richard Zemel"
        ],
        "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.02120",
        "title": "Bilevel approaches for learning of variational imaging models",
        "authors": [
            "Luca Calatroni",
            "Cao Chung",
            "Juan Carlos De Los Reyes",
            "Carola-Bibiane Sch\u00f6nlieb",
            "Tuomo Valkonen"
        ],
        "abstract": "We review some recent learning approaches in variational imaging, based on bilevel optimisation, and emphasize the importance of their treatment in function space. The paper covers both analytical and numerical techniques. Analytically, we include results on the existence and structure of minimisers, as well as optimality conditions for their characterisation. Based on this information, Newton type methods are studied for the solution of the problems at hand, combining them with sampling techniques in case of large databases. The computational verification of the developed techniques is extensively documented, covering instances with different type of regularisers, several noise models, spatially dependent weights and large image databases.\n    ",
        "submission_date": "2015-05-08T00:00:00",
        "last_modified_date": "2015-05-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03093",
        "title": "A new Level-set based Protocol for Accurate Bone Segmentation from CT Imaging",
        "authors": [
            "Manuel Pinheiro",
            "J.L. Alves"
        ],
        "abstract": "In this work it is proposed a medical image segmentation pipeline for accurate bone segmentation from CT imaging. It is a two-step methodology, with a pre-segmentation step and a segmentation refinement step. First, the user performs a rough segmenting of the desired region of interest. Next, a fully automatic refinement step is applied to the pre-segmented data. The automatic segmentation refinement is composed by several sub-stpng, namely image deconvolution, image cropping and interpolation. The user-defined pre-segmentation is then refined over the deconvolved, cropped, and up-sampled version of the image. The algorithm is applied in the segmentation of CT images of a composite femur bone, reconstructed with different reconstruction protocols. Segmentation outcomes are validated against a gold standard model obtained with coordinate measuring machine Nikon Metris LK V20 with a digital line scanner LC60-D that guarantees an accuracy of 28 $\\mu m$. High sub-pixel accuracy models were obtained for all tested Datasets. The algorithm is able to produce high quality segmentation of the composite femur regardless of the surface meshing strategy used.\n    ",
        "submission_date": "2015-05-12T00:00:00",
        "last_modified_date": "2015-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03358",
        "title": "An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures",
        "authors": [
            "Rossano Schifanella",
            "Miriam Redi",
            "Luca Aiello"
        ],
        "abstract": "The dynamics of attention in social media tend to obey power laws. Attention concentrates on a relatively small number of popular items and neglecting the vast majority of content produced by the crowd. Although popularity can be an indication of the perceived value of an item within its community, previous research has hinted to the fact that popularity is distinct from intrinsic quality. As a result, content with low visibility but high quality lurks in the tail of the popularity distribution. This phenomenon can be particularly evident in the case of photo-sharing communities, where valuable photographers who are not highly engaged in online social interactions contribute with high-quality pictures that remain unseen. We propose to use a computer vision method to surface beautiful pictures from the immense pool of near-zero-popularity items, and we test it on a large dataset of creative-commons photos on Flickr. By gathering a large crowdsourced ground truth of aesthetics scores for Flickr images, we show that our method retrieves photos whose median perceived beauty score is equal to the most popular ones, and whose average is lower by only 1.5%.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2015-05-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03504",
        "title": "Loop-corrected belief propagation for lattice spin models",
        "authors": [
            "Hai-Jun Zhou",
            "Wei-Mou Zheng"
        ],
        "abstract": "Belief propagation (BP) is a message-passing method for solving probabilistic graphical models. It is very successful in treating disordered models (such as spin glasses) on random graphs. On the other hand, finite-dimensional lattice models have an abundant number of short loops, and the BP method is still far from being satisfactory in treating the complicated loop-induced correlations in these systems. Here we propose a loop-corrected BP method to take into account the effect of short loops in lattice spin models. We demonstrate, through an application to the square-lattice Ising model, that loop-corrected BP improves over the naive BP method significantly. We also implement loop-corrected BP at the coarse-grained region graph level to further boost its performance.\n    ",
        "submission_date": "2015-05-13T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03703",
        "title": "A PCA-Based Convolutional Network",
        "authors": [
            "Yanhai Gan",
            "Jun Liu",
            "Junyu Dong",
            "Guoqiang Zhong"
        ],
        "abstract": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-the-art deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.03891",
        "title": "Task-Based Optimization of Computed Tomography Imaging Systems",
        "authors": [
            "Adrian A. Sanchez"
        ],
        "abstract": "The goal of this thesis is to provide a framework for the use of task-based metrics of image quality to aid in the design, implementation, and evaluation of CT image reconstruction algorithms and CT systems in general. We support the view that task-based metrics of image quality can be useful in guiding the algorithm design and implementation process in order to yield images of objectively superior quality and higher utility for a given task. Further, we believe that metrics such as the Hotelling observer (HO) SNR can be used as summary scalar metrics of image quality for the evaluation of images produced by novel reconstruction algorithms. In this work, we aim to construct a concise and versatile formalism for image reconstruction algorithm design, implementation, and assessment. The bulk of the work focuses on linear analytical algorithms, specifically the ubiquitous filtered back-projection (FBP) algorithm. However, due to the demonstrated importance of optimization-based algorithms in a wide variety of CT applications, we devote one chapter to the characterization of noise properties in TV-based iterative reconstruction, as the understanding of image statistics in optimization-based reconstruction is the limiting factor in applying HO metrics.\n    ",
        "submission_date": "2015-05-14T00:00:00",
        "last_modified_date": "2015-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04548",
        "title": "Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM",
        "authors": [
            "Michael Milford",
            "Hanme Kim",
            "Michael Mangan",
            "Stefan Leutenegger",
            "Tom Stone",
            "Barbara Webb",
            "Andrew Davison"
        ],
        "abstract": "Event-based cameras offer much potential to the fields of robotics and computer vision, in part due to their large dynamic range and extremely high \"frame rates\". These attributes make them, at least in theory, particularly suitable for enabling tasks like navigation and mapping on high speed robotic platforms under challenging lighting conditions, a task which has been particularly challenging for traditional algorithms and camera sensors. Before these tasks become feasible however, progress must be made towards adapting and innovating current RGB-camera-based algorithms to work with event-based cameras. In this paper we present ongoing research investigating two distinct approaches to incorporating event-based cameras for robotic navigation: the investigation of suitable place recognition / loop closure techniques, and the development of efficient neural implementations of place recognition techniques that enable the possibility of place recognition using event-based cameras at very high frame rates using neuromorphic computing hardware.\n    ",
        "submission_date": "2015-05-18T00:00:00",
        "last_modified_date": "2015-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.04938",
        "title": "Convective regularization for optical flow",
        "authors": [
            "Jos\u00e9 A. Iglesias",
            "Clemens Kirisits"
        ],
        "abstract": "We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \\nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\\|v_t + \\nabla v v\\|^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models.\n    ",
        "submission_date": "2015-05-19T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05489",
        "title": "A Sparse Gaussian Process Framework for Photometric Redshift Estimation",
        "authors": [
            "Ibrahim A. Almosallam",
            "Sam N. Lindsay",
            "Matt J. Jarvis",
            "Stephen J. Roberts"
        ],
        "abstract": "Accurate photometric redshifts are a lynchpin for many future experiments to pin down the cosmological model and for studies of galaxy evolution. In this study, a novel sparse regression framework for photometric redshift estimation is presented. Simulated and real data from SDSS DR12 were used to train and test the proposed models. We show that approaches which include careful data preparation and model design offer a significant improvement in comparison with several competing machine learning algorithms. Standard implementations of most regression algorithms have as the objective the minimization of the sum of squared errors. For redshift inference, however, this induces a bias in the posterior mean of the output distribution, which can be problematic. In this paper we directly target minimizing $\\Delta z = (z_\\textrm{s} - z_\\textrm{p})/(1+z_\\textrm{s})$ and address the bias problem via a distribution-based weighting scheme, incorporated as part of the optimization objective. The results are compared with other machine learning algorithms in the field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs) and sparse GPs. The proposed framework reaches a mean absolute $\\Delta z = 0.0026(1+z_\\textrm{s})$, over the redshift range of $0 \\le z_\\textrm{s} \\le 2$ on the simulated data, and $\\Delta z = 0.0178(1+z_\\textrm{s})$ over the entire redshift range on the SDSS DR12 survey, outperforming the standard ANNz used in the literature. We also investigate how the relative size of the training set affects the photometric redshift accuracy. We find that a training set of \\textgreater 30 per cent of total sample size, provides little additional constraint on the photometric redshifts, and note that our GP formalism strongly outperforms ANNz in the sparse data regime for the simulated data set.\n    ",
        "submission_date": "2015-05-20T00:00:00",
        "last_modified_date": "2015-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05561",
        "title": "Why Regularized Auto-Encoders learn Sparse Representation?",
        "authors": [
            "Devansh Arpit",
            "Yingbo Zhou",
            "Hung Ngo",
            "Venu Govindaraju"
        ],
        "abstract": "While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \\textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \\textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2016-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05740",
        "title": "Graph edit distance : a new binary linear programming formulation",
        "authors": [
            "Julien Lerouge",
            "Zeina Abu-Aisheh",
            "Romain Raveaux",
            "Pierre H\u00e9roux",
            "S\u00e9bastien Adam"
        ],
        "abstract": "Graph edit distance (GED) is a powerful and flexible graph matching paradigm that can be used to address different tasks in structural pattern recognition, machine learning, and data mining. In this paper, some new binary linear programming formulations for computing the exact GED between two graphs are proposed. A major strength of the formulations lies in their genericity since the GED can be computed between directed or undirected fully attributed graphs (i.e. with attributes on both vertices and edges). Moreover, a relaxation of the domain constraints in the formulations provides efficient lower bound approximations of the GED. A complete experimental study comparing the proposed formulations with 4 state-of-the-art algorithms for exact and approximate graph edit distances is provided. By considering both the quality of the proposed solution and the efficiency of the algorithms as performance criteria, the results show that none of the compared methods dominates the others in the Pareto sense. As a consequence, faced to a given real-world problem, a trade-off between quality and efficiency has to be chosen w.r.t. the application constraints. In this context, this paper provides a guide that can be used to choose the appropriate method.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2015-05-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.05901",
        "title": "Randomized Robust Subspace Recovery for High Dimensional Data Matrices",
        "authors": [
            "Mostafa Rahmani",
            "George Atia"
        ],
        "abstract": "This paper explores and analyzes two randomized designs for robust Principal Component Analysis (PCA) employing low-dimensional data sketching. In one design, a data sketch is constructed using random column sampling followed by low dimensional embedding, while in the other, sketching is based on random column and row sampling. Both designs are shown to bring about substantial savings in complexity and memory requirements for robust subspace learning over conventional approaches that use the full scale data. A characterization of the sample and computational complexity of both designs is derived in the context of two distinct outlier models, namely, sparse and independent outlier models. The proposed randomized approach can provably recover the correct subspace with computational and sample complexity that are almost independent of the size of the data. The results of the mathematical analysis are confirmed through numerical simulations using both synthetic and real data.\n    ",
        "submission_date": "2015-05-21T00:00:00",
        "last_modified_date": "2016-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06072",
        "title": "Diffusion Methods for Classification with Pairwise Relationships",
        "authors": [
            "Pedro F. Felzenszwalb",
            "Benar F. Svaiter"
        ],
        "abstract": "We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid.\n    ",
        "submission_date": "2015-05-22T00:00:00",
        "last_modified_date": "2019-05-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06621",
        "title": "Machine learning based data mining for Milky Way filamentary structures reconstruction",
        "authors": [
            "Giuseppe Riccio",
            "Stefano Cavuoti",
            "Eugenio Schisano",
            "Massimo Brescia",
            "Amata Mercurio",
            "Davide Elia",
            "Milena Benedettini",
            "Stefano Pezzuto",
            "Sergio Molinari",
            "Anna Maria Di Giorgio"
        ],
        "abstract": "We present an innovative method called FilExSeC (Filaments Extraction, Selection and Classification), a data mining tool developed to investigate the possibility to refine and optimize the shape reconstruction of filamentary structures detected with a consolidated method based on the flux derivative analysis, through the column-density maps computed from Herschel infrared Galactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present methodology is based on a feature extraction module followed by a machine learning model (Random Forest) dedicated to select features and to classify the pixels of the input images. From tests on both simulations and real observations the method appears reliable and robust with respect to the variability of shape and distribution of filaments. In the cases of highly defined filament structures, the presented method is able to bridge the gaps among the detected fragments, thus improving their shape reconstruction. From a preliminary \"a posteriori\" analysis of derived filament physical parameters, the method appears potentially able to add a sufficient contribution to complete and refine the filament reconstruction.\n    ",
        "submission_date": "2015-05-25T00:00:00",
        "last_modified_date": "2016-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.06907",
        "title": "Using Dimension Reduction to Improve the Classification of High-dimensional Data",
        "authors": [
            "Andreas Gr\u00fcnauer",
            "Markus Vincze"
        ],
        "abstract": "In this work we show that the classification performance of high-dimensional structural MRI data with only a small set of training examples is improved by the usage of dimension reduction methods. We assessed two different dimension reduction variants: feature selection by ANOVA F-test and feature transformation by PCA. On the reduced datasets, we applied common learning algorithms using 5-fold cross-validation. Training, tuning of the hyperparameters, as well as the performance evaluation of the classifiers was conducted using two different performance measures: Accuracy, and Receiver Operating Characteristic curve (AUC). Our hypothesis is supported by experimental results.\n    ",
        "submission_date": "2015-05-26T00:00:00",
        "last_modified_date": "2015-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07522",
        "title": "Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures",
        "authors": [
            "Miriam Redi",
            "Daniele Quercia",
            "Lindsay T. Graham",
            "Samuel D. Gosling"
        ],
        "abstract": "To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.07690",
        "title": "Invertible Orientation Scores of 3D Images",
        "authors": [
            "Michiel Janssen",
            "Remco Duits",
            "Marcel Breeuwer"
        ],
        "abstract": "The enhancement and detection of elongated structures in noisy image data is relevant for many biomedical applications. To handle complex crossing structures in 2D images, 2D orientation scores were introduced, which already showed their use in a variety of applications. Here we extend this work to 3D orientation scores. First, we construct the orientation score from a given dataset, which is achieved by an invertible coherent state type of transform. For this transformation we introduce 3D versions of the 2D cake-wavelets, which are complex wavelets that can simultaneously detect oriented structures and oriented edges. For efficient implementation of the different steps in the wavelet creation we use a spherical harmonic transform. Finally, we show some first results of practical applications of 3D orientation scores.\n    ",
        "submission_date": "2015-05-28T00:00:00",
        "last_modified_date": "2015-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08019",
        "title": "Research on the fast Fourier transform of image based on GPU",
        "authors": [
            "Feifei Shen",
            "Zhenjian Song",
            "Congrui Wu",
            "Jiaqi Geng",
            "Qingyun Wang"
        ],
        "abstract": "Study of general purpose computation by GPU (Graphics Processing Unit) can improve the image processing capability of micro-computer system. This paper studies the parallelism of the different stages of decimation in time radix 2 FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT on GPU. The experiment result demonstrates the validity and advantage over general CPU, especially in the condition of large input size. The approach can also be generalized to other transforms alike.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-05-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1505.08098",
        "title": "CURL: Co-trained Unsupervised Representation Learning for Image Classification",
        "authors": [
            "Simone Bianco",
            "Gianluigi Ciocca",
            "Claudio Cusano"
        ],
        "abstract": "In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from Co-trained Unsupervised Representation Learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features. To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art.\n    ",
        "submission_date": "2015-05-29T00:00:00",
        "last_modified_date": "2015-09-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00176",
        "title": "An Open Source Testing Tool for Evaluating Handwriting Input Methods",
        "authors": [
            "Liquan Qiu",
            "Lianwen Jin",
            "Ruifen Dai",
            "Yuxiang Zhang",
            "Lei Li"
        ],
        "abstract": "This paper presents an open source tool for testing the recognition accuracy of Chinese handwriting input methods. The tool consists of two modules, namely the PC and Android mobile client. The PC client reads handwritten samples in the computer, and transfers them individually to the Android client in accordance with the socket communication protocol. After the Android client receives the data, it simulates the handwriting on screen of client device, and triggers the corresponding handwriting recognition method. The recognition accuracy is recorded by the Android client. We present the design principles and describe the implementation of the test platform. We construct several test datasets for evaluating different handwriting recognition systems, and conduct an objective and comprehensive test using six Chinese handwriting input methods with five datasets. The test results for the recognition accuracy are then compared and analyzed.\n    ",
        "submission_date": "2015-05-30T00:00:00",
        "last_modified_date": "2015-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00333",
        "title": "Learning to Answer Questions From Image Using Convolutional Neural Network",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "abstract": "In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00511",
        "title": "Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions",
        "authors": [
            "Jimmy Ba",
            "Kevin Swersky",
            "Sanja Fidler",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00527",
        "title": "User Preferences Modeling and Learning for Pleasing Photo Collage Generation",
        "authors": [
            "Simone Bianco",
            "Gianluigi Ciocca"
        ],
        "abstract": "In this paper we consider how to automatically create pleasing photo collages created by placing a set of images on a limited canvas area. The task is formulated as an optimization problem. Differently from existing state-of-the-art approaches, we here exploit subjective experiments to model and learn pleasantness from user preferences. To this end, we design an experimental framework for the identification of the criteria that need to be taken into account to generate a pleasing photo collage. Five different thematic photo datasets are used to create collages using state-of-the-art criteria. A first subjective experiment where several subjects evaluated the collages, emphasizes that different criteria are involved in the subjective definition of pleasantness. We then identify new global and local criteria and design algorithms to quantify them. The relative importance of these criteria are automatically learned by exploiting the user preferences, and new collages are generated. To validate our framework, we performed several psycho-visual experiments involving different users. The results shows that the proposed framework allows to learn a novel computational model which effectively encodes an inter-user definition of pleasantness. The learned definition of pleasantness generalizes well to new photo datasets of different themes and sizes not used in the learning. Moreover, compared with two state of the art approaches, the collages created using our framework are preferred by the majority of the users.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2015-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00575",
        "title": "A Riemannian low-rank method for optimization over semidefinite matrices with block-diagonal constraints",
        "authors": [
            "Nicolas Boumal"
        ],
        "abstract": "We propose a new algorithm to solve optimization problems of the form $\\min f(X)$ for a smooth function $f$ under the constraints that $X$ is positive semidefinite and the diagonal blocks of $X$ are small identity matrices. Such problems often arise as the result of relaxing a rank constraint (lifting). In particular, many estimation tasks involving phases, rotations, orthonormal bases or permutations fit in this framework, and so do certain relaxations of combinatorial problems such as Max-Cut. The proposed algorithm exploits the facts that (1) such formulations admit low-rank solutions, and (2) their rank-restricted versions are smooth optimization problems on a Riemannian manifold. Combining insights from both the Riemannian and the convex geometries of the problem, we characterize when second-order critical points of the smooth problem reveal KKT points of the semidefinite problem. We compare against state of the art, mature software and find that, on certain interesting problem instances, what we call the staircase method is orders of magnitude faster, is more accurate and scales better. Code is available.\n    ",
        "submission_date": "2015-06-01T00:00:00",
        "last_modified_date": "2016-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00711",
        "title": "Quantifying Creativity in Art Networks",
        "authors": [
            "Ahmed Elgammal",
            "Babak Saleh"
        ],
        "abstract": "Can we develop a computer algorithm that assesses the creativity of a painting given its context within art history? This paper proposes a novel computational framework for assessing the creativity of creative products, such as paintings, sculptures, poetry, etc. We use the most common definition of creativity, which emphasizes the originality of the product and its influential value. The proposed computational framework is based on constructing a network between creative products and using this network to infer about the originality and influence of its nodes. Through a series of transformations, we construct a Creativity Implication Network. We show that inference about creativity in this network reduces to a variant of network centrality problems which can be solved efficiently. We apply the proposed framework to the task of quantifying creativity of paintings (and sculptures). We experimented on two datasets with over 62K paintings to illustrate the behavior of the proposed framework. We also propose a methodology for quantitatively validating the results of the proposed algorithm, which we call the \"time machine experiment\".\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.00768",
        "title": "Soft Computing Techniques for Change Detection in remotely sensed images : A Review",
        "authors": [
            "Madhu Khurana",
            "Vikas Saxena"
        ],
        "abstract": "With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2018-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01072",
        "title": "Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition",
        "authors": [
            "Xinyu Wu",
            "Vishal Saxena",
            "Kehan Zhu"
        ],
        "abstract": "A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing.\n    ",
        "submission_date": "2015-06-02T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.01732",
        "title": "Monocular SLAM Supported Object Recognition",
        "authors": [
            "Sudeep Pillai",
            "John Leonard"
        ],
        "abstract": "In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.\n    ",
        "submission_date": "2015-06-04T00:00:00",
        "last_modified_date": "2015-06-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02083",
        "title": "Automatic tracking of protein vesicles",
        "authors": [
            "Min Xu"
        ],
        "abstract": "With the advance of fluorescence imaging technologies, recently cell biologists are able to record the movement of protein vesicles within a living cell. Automatic tracking of the movements of these vesicles become key for qualitative analysis of dynamics of theses vesicles. In this thesis, we formulate such tracking problem as video object tracking problem, and design a dynamic programming method for tracking single object. Our experiments on simulation data show that the method can identify a track with high accuracy which is robust to the choose of tracking parameters and presence of high level noise. We then extend this method to the tracking multiple objects using the track elimination strategy. In multiple object tracking, the above approach often fails to correctly identify a track when two tracks cross. We solve this problem by incorporating the Kalman filter into the dynamic programming framework. Our experiments on simulated data show that the tracking accuracy is significantly improved.\n    ",
        "submission_date": "2015-06-05T00:00:00",
        "last_modified_date": "2015-06-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02264",
        "title": "Visual Learning of Arithmetic Operations",
        "authors": [
            "Yedid Hoshen",
            "Shmuel Peleg"
        ],
        "abstract": "A simple Neural Network model is presented for end-to-end visual learning of arithmetic operations from pictures of numbers. The input consists of two pictures, each showing a 7-digit number. The output, also a picture, displays the number showing the result of an arithmetic operation (e.g., addition or subtraction) on the two input numbers. The concepts of a number, or of an operator, are not explicitly introduced. This indicates that addition is a simple cognitive task, which can be learned visually using a very small number of neurons.\n",
        "submission_date": "2015-06-07T00:00:00",
        "last_modified_date": "2015-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02509",
        "title": "SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet",
        "authors": [
            "Lei Zhang",
            "David Zhang"
        ],
        "abstract": "Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4% of the average accuracy. The features and codes are available in ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02544",
        "title": "Learning with Group Invariant Features: A Kernel Perspective",
        "authors": [
            "Youssef Mroueh",
            "Stephen Voinea",
            "Tomaso Poggio"
        ],
        "abstract": "We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02617",
        "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks",
        "authors": [
            "Behnam Neyshabur",
            "Ruslan Salakhutdinov",
            "Nathan Srebro"
        ],
        "abstract": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02626",
        "title": "Learning both Weights and Connections for Efficient Neural Networks",
        "authors": [
            "Song Han",
            "Jeff Pool",
            "John Tran",
            "William J. Dally"
        ],
        "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.\n    ",
        "submission_date": "2015-06-08T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.02753",
        "title": "Inverting Visual Representations with Convolutional Networks",
        "authors": [
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "abstract": "Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03099",
        "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
        "authors": [
            "Samy Bengio",
            "Oriol Vinyals",
            "Navdeep Jaitly",
            "Noam Shazeer"
        ],
        "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03124",
        "title": "Multiscale edge detection and parametric shape modeling for boundary delineation in optoacoustic images",
        "authors": [
            "Subhamoy Mandal",
            "Viswanath Pamulakanty Sudarshan",
            "Yeshaswini Nagaraj",
            "Xose Luis Dean Ben",
            "Daniel Razansky"
        ],
        "abstract": "In this article, we present a novel scheme for segmenting the image boundary (with the background) in optoacoustic small animal in vivo imaging systems. The method utilizes a multiscale edge detection algorithm to generate a binary edge map. A scale dependent morphological operation is employed to clean spurious edges. Thereafter, an ellipse is fitted to the edge map through constrained parametric transformations and iterative goodness of fit calculations. The method delimits the tissue edges through the curve fitting model, which has shown high levels of accuracy. Thus, this method enables segmentation of optoacoutic images with minimal human intervention, by eliminating need of scale selection for multiscale processing and seed point determination for contour mapping.\n    ",
        "submission_date": "2015-06-09T00:00:00",
        "last_modified_date": "2015-06-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03358",
        "title": "Optical Flow on Evolving Sphere-Like Surfaces",
        "authors": [
            "Lukas F. Lang",
            "Otmar Scherzer"
        ],
        "abstract": "In this work we consider optical flow on evolving Riemannian 2-manifolds which can be parametrised from the 2-sphere. Our main motivation is to estimate cell motion in time-lapse volumetric microscopy images depicting fluorescently labelled cells of a live zebrafish embryo. We exploit the fact that the recorded cells float on the surface of the embryo and allow for the extraction of an image sequence together with a sphere-like surface. We solve the resulting variational problem by means of a Galerkin method based on vector spherical harmonics and present numerical results computed from the aforementioned microscopy data.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03412",
        "title": "Convergence rates for pretraining and dropout: Guiding learning parameters using network structure",
        "authors": [
            "Vamsi K. Ithapu",
            "Sathya Ravi",
            "Vikas Singh"
        ],
        "abstract": "Unsupervised pretraining and dropout have been well studied, especially with respect to regularization and output consistency. However, our understanding about the explicit convergence rates of the parameter estimates, and their dependence on the learning (like denoising and dropout rate) and structural (like depth and layer lengths) aspects of the network is less mature. An interesting question in this context is to ask if the network structure could \"guide\" the choices of such learning parameters. In this work, we explore these gaps between network structure, the learning mechanisms and their interaction with parameter convergence rates. We present a way to address these issues based on the backpropagation convergence rates for general nonconvex objectives using first-order information. We then incorporate two learning mechanisms into this general framework -- denoising autoencoder and dropout, and subsequently derive the convergence rates of deep networks. Building upon these bounds, we provide insights into the choices of learning parameters and network sizes that achieve certain levels of convergence accuracy. The results derived here support existing empirical observations, and we also conduct a set of experiments to evaluate them.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2017-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03478",
        "title": "Generative Image Modeling Using Spatial LSTMs",
        "authors": [
            "Lucas Theis",
            "Matthias Bethge"
        ],
        "abstract": "Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.\n    ",
        "submission_date": "2015-06-10T00:00:00",
        "last_modified_date": "2015-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03852",
        "title": "Tree-Cut for Probabilistic Image Segmentation",
        "authors": [
            "Shell X. Hu",
            "Christopher K. I. Williams",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper presents a new probabilistic generative model for image segmentation, i.e. the task of partitioning an image into homogeneous regions. Our model is grounded on a mid-level image representation, called a region tree, in which regions are recursively split into subregions until superpixels are reached. Given the region tree, image segmentation is formalized as sampling cuts in the tree from the model. Inference for the cuts is exact, and formulated using dynamic programming. Our tree-cut model can be tuned to sample segmentations at a particular scale of interest out of many possible multiscale image segmentations. This generalizes the common notion that there should be only one correct segmentation per image. Also, it allows moving beyond the standard single-scale evaluation, where the segmentation result for an image is averaged against the corresponding set of coarse and fine human annotations, to conduct a scale-specific evaluation. Our quantitative results are comparable to those of the leading gPb-owt-ucm method, with the notable advantage that we additionally produce a distribution over all possible tree-consistent segmentations of the image.\n    ",
        "submission_date": "2015-06-11T00:00:00",
        "last_modified_date": "2015-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.03899",
        "title": "Place classification with a graph regularized deep neural network model",
        "authors": [
            "Yiyi Liao",
            "Sarath Kodagoda",
            "Yue Wang",
            "Lei Shi",
            "Yong Liu"
        ],
        "abstract": "Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. It is a nontrivial classification problem which has attracted many research. In recent years, there is a high exploitation of Artificial Intelligent algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With the deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. Firstly, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Secondly, each layer of data is fed into a deep neural network model for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all the layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effective- ness of our end-to-end place classification framework in which both the multi-layer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.\n    ",
        "submission_date": "2015-06-12T00:00:00",
        "last_modified_date": "2015-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.04449",
        "title": "Compressing Convolutional Neural Networks",
        "authors": [
            "Wenlin Chen",
            "James T. Wilson",
            "Stephen Tyree",
            "Kilian Q. Weinberger",
            "Yixin Chen"
        ],
        "abstract": "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.\n    ",
        "submission_date": "2015-06-14T00:00:00",
        "last_modified_date": "2015-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05011",
        "title": "Bayesian representation learning with oracle constraints",
        "authors": [
            "Theofanis Karaletsos",
            "Serge Belongie",
            "Gunnar R\u00e4tsch"
        ],
        "abstract": "Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \\emph{generative unsupervised feature learning} with a \\emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05085",
        "title": "Time Series Classification using the Hidden-Unit Logistic Model",
        "authors": [
            "Wenjie Pei",
            "Hamdi Dibeklio\u011flu",
            "David M.J. Tax",
            "Laurens van der Maaten"
        ],
        "abstract": "We present a new model for time series classification, called the hidden-unit logistic model, that uses binary stochastic hidden units to model latent structure in the data. The hidden units are connected in a chain structure that models temporal dependencies in the data. Compared to the prior models for time series classification such as the hidden conditional random field, our model can model very complex decision boundaries because the number of latent states grows exponentially with the number of hidden units. We demonstrate the strong performance of our model in experiments on a variety of (computer vision) tasks, including handwritten character recognition, speech recognition, facial expression, and action recognition. We also present a state-of-the-art system for facial action unit detection based on the hidden-unit logistic model.\n    ",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05163",
        "title": "Deep Convolutional Networks on Graph-Structured Data",
        "authors": [
            "Mikael Henaff",
            "Joan Bruna",
            "Yann LeCun"
        ],
        "abstract": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.\n",
        "submission_date": "2015-06-16T00:00:00",
        "last_modified_date": "2015-06-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05393",
        "title": "MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance Fingerprinting",
        "authors": [
            "Ze Wang"
        ],
        "abstract": "Magnetic resonance fingerprinting (MRF) is a new technique for simultaneously quantifying multiple MR parameters using one temporally resolved MR scan. But its brute-force dictionary generating and searching (DGS) process causes a huge disk space demand and computational burden, prohibiting it from a practical multiple slice high-definition imaging. The purpose of this paper was to provide a fast and space efficient DGS algorithm for MRF. Based on an empirical analysis of properties of the distance function of the acquired MRF signal and the pre-defined MRF dictionary entries, we proposed a parameter separable MRF DGS method, which breaks the multiplicative computation complexity into an additive one and enabling a resolution scalable multi-resolution DGS process, which was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM was hundreds or thousands of times faster than the original brute-force DGS method. The acceleration was even higher when considering the time difference for generating the dictionary. Using a high precision quantification, MRF can find the right parameter values for a 64x64 imaging slice in 117 secs. Our data also showed that spatial constraints can be used to further speed up MRF ZOOM.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.05439",
        "title": "Learning with a Wasserstein Loss",
        "authors": [
            "Charlie Frogner",
            "Chiyuan Zhang",
            "Hossein Mobahi",
            "Mauricio Araya-Polo",
            "Tomaso Poggio"
        ],
        "abstract": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.\n    ",
        "submission_date": "2015-06-17T00:00:00",
        "last_modified_date": "2015-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06155",
        "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits",
        "authors": [
            "Mohammad Norouzi",
            "Maxwell D. Collins",
            "David J. Fleet",
            "Pushmeet Kohli"
        ],
        "abstract": "We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.\n    ",
        "submission_date": "2015-06-19T00:00:00",
        "last_modified_date": "2015-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.06833",
        "title": "A Survey of Current Datasets for Vision and Language Research",
        "authors": [
            "Francis Ferraro",
            "Nasrin Mostafazadeh",
            "Ting-Hao",
            "Huang",
            "Lucy Vanderwende",
            "Jacob Devlin",
            "Michel Galley",
            "Margaret Mitchell"
        ],
        "abstract": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.\n    ",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07194",
        "title": "Advanced statistical methods for eye movement analysis and modeling: a gentle introduction",
        "authors": [
            "Giuseppe Boccignone"
        ],
        "abstract": "In this Chapter we show that by considering eye movements, and in particular, the resulting sequence of gaze shifts, a stochastic process, a wide variety of tools become available for analyses and modelling beyond conventional statistical methods. Such tools encompass random walk analyses and more complex techniques borrowed from the pattern recognition and machine learning fields.\n",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2017-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07236",
        "title": "Incremental RANSAC for Online Relocation in Large Dynamic Environments",
        "authors": [
            "Kanji Tanaka",
            "Eiji Kondo"
        ],
        "abstract": "Vehicle relocation is the problem in which a mobile robot has to estimate the self-position with respect to an a priori map of landmarks using the perception and the motion measurements without using any knowledge of the initial self-position. Recently, RANdom SAmple Consensus (RANSAC), a robust multi-hypothesis estimator, has been successfully applied to offline relocation in static environments. On the other hand, online relocation in dynamic environments is still a difficult problem, for available computation time is always limited, and for measurement include many outliers. To realize real time algorithm for such an online process, we have developed an incremental version of RANSAC algorithm by extending an efficient preemption RANSAC scheme. This novel scheme named incremental RANSAC is able to find inlier hypotheses of self-positions out of large number of outlier hypotheses contaminated by outlier measurements.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-06-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07365",
        "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
        "authors": [
            "Manuel Watter",
            "Jost Tobias Springenberg",
            "Joschka Boedecker",
            "Martin Riedmiller"
        ],
        "abstract": "We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.\n    ",
        "submission_date": "2015-06-24T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.07950",
        "title": "Bag-of-Features Image Indexing and Classification in Microsoft SQL Server Relational Database",
        "authors": [
            "Marcin Korytkowski",
            "Rafal Scherer",
            "Pawel Staszewski",
            "Piotr Woldan"
        ],
        "abstract": "This paper presents a novel relational database architecture aimed to visual objects classification and retrieval. The framework is based on the bag-of-features image representation model combined with the Support Vector Machine classification and is integrated in a Microsoft SQL Server database.\n    ",
        "submission_date": "2015-06-26T00:00:00",
        "last_modified_date": "2015-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08316",
        "title": "Keypoint Encoding for Improved Feature Extraction from Compressed Video at Low Bitrates",
        "authors": [
            "Jianshu Chao",
            "Eckehard Steinbach"
        ],
        "abstract": "In many mobile visual analysis applications, compressed video is transmitted over a communication network and analyzed by a server. Typical processing steps performed at the server include keypoint detection, descriptor calculation, and feature matching. Video compression has been shown to have an adverse effect on feature-matching performance. The negative impact of compression can be reduced by using the keypoints extracted from the uncompressed video to calculate descriptors from the compressed video. Based on this observation, we propose to provide these keypoints to the server as side information and to extract only the descriptors from the compressed video. First, we introduce four different frame types for keypoint encoding to address different types of changes in video content. These frame types represent a new scene, the same scene, a slowly changing scene, or a rapidly moving scene and are determined by comparing features between successive video frames. Then, we propose Intra, Skip and Inter modes of encoding the keypoints for different frame types. For example, keypoints for new scenes are encoded using the Intra mode, and keypoints for unchanged scenes are skipped. As a result, the bitrate of the side information related to keypoint encoding is significantly reduced. Finally, we present pairwise matching and image retrieval experiments conducted to evaluate the performance of the proposed approach using the Stanford mobile augmented reality dataset and 720p format videos. The results show that the proposed approach offers significantly improved feature matching and image retrieval performance at a given bitrate.\n    ",
        "submission_date": "2015-06-27T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08615",
        "title": "Coercive functions from a topological viewpoint and properties of minimizing sets of convex functions appearing in image restoration",
        "authors": [
            "Ren\u00e9 Ciak"
        ],
        "abstract": "Many tasks in image processing can be tackled by modeling an appropriate data fidelity term $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ and then solve one of the regularized minimization problems \\begin{align*}\n",
        "submission_date": "2015-06-23T00:00:00",
        "last_modified_date": "2015-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08928",
        "title": "Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty",
        "authors": [
            "Changkyu Song",
            "Sejong Yoon",
            "Vladimir Pavlovic"
        ],
        "abstract": "We propose new methods to speed up convergence of the Alternating Direction Method of Multipliers (ADMM), a common optimization tool in the context of large scale and distributed learning. The proposed method accelerates the speed of convergence by automatically deciding the constraint penalty needed for parameter consensus in each iteration. In addition, we also propose an extension of the method that adaptively determines the maximum number of iterations to update the penalty. We show that this approach effectively leads to an adaptive, dynamic network topology underlying the distributed optimization. The utility of the new penalty update schemes is demonstrated on both synthetic and real data, including a computer vision application of distributed structure from motion.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.08956",
        "title": "Lens Factory: Automatic Lens Generation Using Off-the-shelf Components",
        "authors": [
            "Libin Sun",
            "Brian Guenter",
            "Neel Joshi",
            "Patrick Therien",
            "James Hays"
        ],
        "abstract": "Custom optics is a necessity for many imaging applications. Unfortunately, custom lens design is costly (thousands to tens of thousands of dollars), time consuming (10-12 weeks typical lead time), and requires specialized optics design expertise. By using only inexpensive, off-the-shelf lens components the Lens Factory automatic design system greatly reduces cost and time. Design, ordering of parts, delivery, and assembly can be completed in a few days, at a cost in the low hundreds of dollars. Lens design constraints, such as focal length and field of view, are specified in terms familiar to the graphics community so no optics expertise is necessary. Unlike conventional lens design systems, which only use continuous optimization methods, Lens Factory adds a discrete optimization stage. This stage searches the combinatorial space of possible combinations of lens elements to find novel designs, evolving simple canonical lens designs into more complex, better designs. Intelligent pruning rules make the combinatorial search feasible. We have designed and built several high performance optical systems which demonstrate the practicality of the system.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2015-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1506.09016",
        "title": "Online Learning to Sample",
        "authors": [
            "Guillaume Bouchard",
            "Th\u00e9o Trouillon",
            "Julien Perez",
            "Adrien Gaidon"
        ],
        "abstract": "Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm.\n    ",
        "submission_date": "2015-06-30T00:00:00",
        "last_modified_date": "2016-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.00501",
        "title": "Distributed image reconstruction for very large arrays in radio astronomy",
        "authors": [
            "Andr\u00e9 Ferrari",
            "David Mary",
            "R\u00e9mi Flamary",
            "C\u00e9dric Richard"
        ],
        "abstract": "Current and future radio interferometric arrays such as LOFAR and SKA are characterized by a paradox. Their large number of receptors (up to millions) allow theoretically unprecedented high imaging resolution. In the same time, the ultra massive amounts of samples makes the data transfer and computational loads (correlation and calibration) order of magnitudes too high to allow any currently existing image reconstruction algorithm to achieve, or even approach, the theoretical resolution. We investigate here decentralized and distributed image reconstruction strategies which select, transfer and process only a fraction of the total data. The loss in MSE incurred by the proposed approach is evaluated theoretically and numerically on simple test cases.\n    ",
        "submission_date": "2015-07-02T00:00:00",
        "last_modified_date": "2015-07-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.01053",
        "title": "Describing Multimedia Content using Attention-based Encoder--Decoder Networks",
        "authors": [
            "Kyunghyun Cho",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.\n    ",
        "submission_date": "2015-07-04T00:00:00",
        "last_modified_date": "2015-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02084",
        "title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost",
        "authors": [
            "Iago Landesa-V\u00e1zquez",
            "Jos\u00e9 Luis Alba-Castro"
        ],
        "abstract": "In this paper, we propose a different insight to analyze AdaBoost. This analysis reveals that, beyond some preconceptions, AdaBoost can be directly used as an asymmetric learning algorithm, preserving all its theoretical properties. A novel class-conditional description of AdaBoost, which models the actual asymmetric behavior of the algorithm, is presented.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02150",
        "title": "SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional Autofocus",
        "authors": [
            "Xinhua Mao"
        ],
        "abstract": "Due to uncertainty on target's motion, the range cell migration (RCM) and azimuth phase error (APE) of moving targets can't be completely compensated in synthetic aperture radar (SAR) processing. Therefore, moving targets often appear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-D autofocus method for refocusing defocused moving targets in SAR images is presented. The new method only requires a direct estimate of APE, while the residual 2-D phase error ( or RCM) is computed from the estimated APE by exploiting the analytical relationship between the 2-D phase error ( or RCM) and APE. Because the parameter estimation is performed in the reduced-dimension space by exploiting prior knowledge on phase error structure, the proposed approach offers clear advantages in both computational efficiency and estimation accuracy.\n    ",
        "submission_date": "2015-07-08T00:00:00",
        "last_modified_date": "2015-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02355",
        "title": "The Shadows of a Cycle Cannot All Be Paths",
        "authors": [
            "Prosenjit Bose",
            "Jean-Lou De Carufel",
            "Michael G. Dobbins",
            "Heuna Kim",
            "Giovanni Viglietta"
        ],
        "abstract": "A \"shadow\" of a subset $S$ of Euclidean space is an orthogonal projection of $S$ into one of the coordinate hyperplanes. In this paper we show that it is not possible for all three shadows of a cycle (i.e., a simple closed curve) in $\\mathbb R^3$ to be paths (i.e., simple open curves).\n",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.02407",
        "title": "Planar Ultrametric Rounding for Image Segmentation",
        "authors": [
            "Julian Yarkony",
            "Charless C. Fowlkes"
        ],
        "abstract": "We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of an LP relaxation of ultrametric rounding. To solve this LP efficiently we introduce a dual cutting plane scheme that uses minimum cost perfect matching as a subroutine in order to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.\n    ",
        "submission_date": "2015-07-09T00:00:00",
        "last_modified_date": "2015-09-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.04913",
        "title": "Tree-based Visualization and Optimization for Image Collection",
        "authors": [
            "Xintong Han",
            "Chongyang Zhang",
            "Weiyao Lin",
            "Mingliang Xu",
            "Bin Sheng",
            "Tao Mei"
        ],
        "abstract": "The visualization of an image collection is the process of displaying a collection of images on a screen under some specific layout requirements. This paper focuses on an important problem that is not well addressed by the previous methods: visualizing image collections into arbitrary layout shapes while arranging images according to user-defined semantic or visual correlations (e.g., color or object category). To this end, we first propose a property-based tree construction scheme to organize images of a collection into a tree structure according to user-defined properties. In this way, images can be adaptively placed with the desired semantic or visual correlations in the final visualization layout. Then, we design a two-step visualization optimization scheme to further optimize image layouts. As a result, multiple layout effects including layout shape and image overlap ratio can be effectively controlled to guarantee a satisfactory visualization. Finally, we also propose a tree-transfer scheme such that visualization layouts can be adaptively changed when users select different \"images of interest\". We demonstrate the effectiveness of our proposed approach through the comparisons with state-of-the-art visualization techniques.\n    ",
        "submission_date": "2015-07-17T00:00:00",
        "last_modified_date": "2015-07-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.05775",
        "title": "Compression of Fully-Connected Layer in Neural Network by Kronecker Product",
        "authors": [
            "Shuchang Zhou",
            "Jia-Nan Wu"
        ],
        "abstract": "In this paper we propose and study a technique to reduce the number of parameters and computation time in fully-connected layers of neural networks using Kronecker product, at a mild cost of the prediction quality. The technique proceeds by replacing Fully-Connected layers with so-called Kronecker Fully-Connected layers, where the weight matrices of the FC layers are approximated by linear combinations of multiple Kronecker products of smaller matrices. In particular, given a model trained on SVHN dataset, we are able to construct a new KFC model with 73\\% reduction in total number of parameters, while the error only rises mildly. In contrast, using low-rank method can only achieve 35\\% reduction in total number of parameters given similar quality degradation allowance. If we only compare the KFC layer with its counterpart fully-connected layer, the reduction in the number of parameters exceeds 99\\%. The amount of computation is also reduced as we replace matrix product of the large matrices in FC layers with matrix products of a few smaller matrices in KFC layers. Further experiments on MNIST, SVHN and some Chinese Character recognition models also demonstrate effectiveness of our technique.\n    ",
        "submission_date": "2015-07-21T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.06105",
        "title": "Banzhaf Random Forests",
        "authors": [
            "Jianyuan Sun",
            "Guoqiang Zhong",
            "Junyu Dong",
            "Yajuan Cai"
        ],
        "abstract": "Random forests are a type of ensemble method which makes predictions by combining the results of several independent trees. However, the theory of random forests has long been outpaced by their application. In this paper, we propose a novel random forests algorithm based on cooperative game theory. Banzhaf power index is employed to evaluate the power of each feature by traversing possible feature coalitions. Unlike the previously used information gain rate of information theory, which simply chooses the most informative feature, the Banzhaf power index can be considered as a metric of the importance of each feature on the dependency among a group of features. More importantly, we have proved the consistency of the proposed algorithm, named Banzhaf random forests (BRF). This theoretical analysis takes a step towards narrowing the gap between the theory and practice of random forests for classification problems. Experiments on several UCI benchmark data sets show that BRF is competitive with state-of-the-art classifiers and dramatically outperforms previous consistent random forests. Particularly, it is much more efficient than previous consistent random forests.\n    ",
        "submission_date": "2015-07-22T00:00:00",
        "last_modified_date": "2015-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07077",
        "title": "Making sense of randomness: an approach for fast recovery of compressively sensed signals",
        "authors": [
            "V. Abrol",
            "P. Sharma",
            "A. K Sao"
        ],
        "abstract": "In compressed sensing (CS) framework, a signal is sampled below Nyquist rate, and the acquired compressed samples are generally random in nature. However, for efficient estimation of the actual signal, the sensing matrix must preserve the relative distances among the acquired compressed samples. Provided this condition is fulfilled, we show that CS samples will preserve the envelope of the actual signal even at different compression ratios. Exploiting this envelope preserving property of CS samples, we propose a new fast dictionary learning (DL) algorithm which is able to extract prototype signals from compressive samples for efficient sparse representation and recovery of signals. These prototype signals are orthogonal intrinsic mode functions (IMFs) extracted using empirical mode decomposition (EMD), which is one of the popular methods to capture the envelope of a signal. The extracted IMFs are used to build the dictionary without even comprehending the original signal or the sensing matrix. Moreover, one can build the dictionary on-line as new CS samples are available. In particularly, to recover first $L$ signals ($\\in\\mathbb{R}^n$) at the decoder, one can build the dictionary in just $\\mathcal{O}(nL\\log n)$ operations, that is far less as compared to existing approaches. The efficiency of the proposed approach is demonstrated experimentally for recovery of speech signals.\n    ",
        "submission_date": "2015-07-25T00:00:00",
        "last_modified_date": "2015-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07760",
        "title": "A Hyperelastic Two-Scale Optimization Model for Shape Matching",
        "authors": [
            "Konrad Simon",
            "Sameer Sheorey",
            "David Jacobs",
            "Ronen Basri"
        ],
        "abstract": "We suggest a novel shape matching algorithm for three-dimensional surface meshes of disk or sphere topology. The method is based on the physical theory of nonlinear elasticity and can hence handle large rotations and deformations. Deformation boundary conditions that supplement the underlying equations are usually unknown. Given an initial guess, these are optimized such that the mechanical boundary forces that are responsible for the deformation are of a simple nature. We show a heuristic way to approximate the nonlinear optimization problem by a sequence of convex problems using finite elements. The deformation cost, i.e, the forces, is measured on a coarse scale while ICP-like matching is done on the fine scale. We demonstrate the plausibility of our algorithm on examples taken from different datasets.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.07830",
        "title": "Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian",
        "authors": [
            "Yongxin Yang",
            "Timothy Hospedales"
        ],
        "abstract": "Most visual recognition methods implicitly assume the data distribution remains unchanged from training to testing. However, in practice domain shift often exists, where real-world factors such as lighting and sensor type change between train and test, and classifiers do not generalise from source to target domains. It is impractical to train separate models for all possible situations because collecting and labelling the data is expensive. Domain adaptation algorithms aim to ameliorate domain shift, allowing a model trained on a source to perform well on a different target domain. However, even for the setting of unsupervised domain adaptation, where the target domain is unlabelled, collecting data for every possible target domain is still costly. In this paper, we propose a new domain adaptation method that has no need to access either data or labels of the target domain when it can be described by a parametrised vector and there exits several related source domains within the same parametric space. It greatly reduces the burden of data collection and annotation, and our experiments show some promising results.\n    ",
        "submission_date": "2015-07-28T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08155",
        "title": "IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Previously, we proposed a physically-inspired method to construct data points into an effective in-tree (IT) structure, in which the underlying cluster structure in the dataset is well revealed. Although there are some edges in the IT structure requiring to be removed, such undesired edges are generally distinguishable from other edges and thus are easy to be determined. For instance, when the IT structures for the 2-dimensional (2D) datasets are graphically presented, those undesired edges can be easily spotted and interactively determined. However, in practice, there are many datasets that do not lie in the 2D Euclidean space, thus their IT structures cannot be graphically presented. But if we can effectively map those IT structures into a visualized space in which the salient features of those undesired edges are preserved, then the undesired edges in the IT structures can still be visually determined in a visualization environment. Previously, this purpose was reached by our method called IT-map. The outstanding advantage of IT-map is that clusters can still be found even with the so-called crowding problem in the embedding.\n",
        "submission_date": "2015-07-29T00:00:00",
        "last_modified_date": "2015-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08736",
        "title": "A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion Cortex",
        "authors": [
            "Stephen G. Odaibo"
        ],
        "abstract": "Visual perception results from a systematic transformation of the information flowing through the visual system. In the neuronal hierarchy, the response properties of single neurons are determined by neurons located one level below, and in turn, determine the responses of neurons located one level above. Therefore in modeling receptive fields, it is essential to ensure that the response properties of neurons in a given level can be generated by combining the response models of neurons in its input levels. However, existing response models of neurons in the motion cortex do not inherently yield the temporal frequency filtering gradient (TFFG) property that is known to emerge along the primary visual cortex (V1) to middle temporal (MT) motion processing stream. TFFG is the change from predominantly lowpass to predominantly bandpass temporal frequency filtering character along the V1 to MT pathway (Foster et al 1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, the sinc wavelet model (Odaibo, 2014), which logically and efficiently generates the TFFG. The model replaces the Gabor function's sine wave carrier with a sinc (sin(x)/x) function, and has the same or fewer number of parameters as existing models. Because of its logical consistency with the emergent network property of TFFG, we conclude that the sinc wavelet is a better model for the receptive fields of motion cortex neurons. This model will provide new physiological insights into how the brain represents visual information.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08750",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
        "authors": [
            "Junhyuk Oh",
            "Xiaoxiao Guo",
            "Honglak Lee",
            "Richard Lewis",
            "Satinder Singh"
        ],
        "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08847",
        "title": "A novel multivariate performance optimization method based on sparse coding and hyper-predictor learning",
        "authors": [
            "Jiachen Yanga",
            "Zhiyong Dinga",
            "Fei Guoa",
            "Huogen Wanga",
            "Nick Hughesb"
        ],
        "abstract": "In this paper, we investigate the problem of optimization multivariate performance measures, and propose a novel algorithm for it. Different from traditional machine learning methods which optimize simple loss functions to learn prediction function, the problem studied in this paper is how to learn effective hyper-predictor for a tuple of data points, so that a complex loss function corresponding to a multivariate performance measure can be minimized. We propose to present the tuple of data points to a tuple of sparse codes via a dictionary, and then apply a linear function to compare a sparse code against a give candidate class label. To learn the dictionary, sparse codes, and parameter of the linear function, we propose a joint optimization problem. In this problem, the both the reconstruction error and sparsity of sparse code, and the upper bound of the complex loss function are minimized. Moreover, the upper bound of the loss function is approximated by the sparse codes and the linear function parameter. To optimize this problem, we develop an iterative algorithm based on descent gradient methods to learn the sparse codes and hyper-predictor parameter alternately. Experiment results on some benchmark data sets show the advantage of the proposed methods over other state-of-the-art algorithms.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2015-07-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1507.08861",
        "title": "Mobile Multi-View Object Image Search",
        "authors": [
            "Fatih Calisir",
            "Muhammet Bastan",
            "Ozgur Ulusoy",
            "Ugur Gudukbay"
        ],
        "abstract": "High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search.\n    ",
        "submission_date": "2015-07-31T00:00:00",
        "last_modified_date": "2018-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00451",
        "title": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines",
        "authors": [
            "Rein Houthooft",
            "Filip De Turck"
        ],
        "abstract": "Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors.\n    ",
        "submission_date": "2015-08-03T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.00761",
        "title": "Recognition of Emotions using Kinects",
        "authors": [
            "Shun Li",
            "Changye Zhu",
            "Liqing Cui",
            "Nan Zhao",
            "Baobin Li",
            "Tingshao Zhu"
        ],
        "abstract": "Psychological studies indicate that emotional states are expressed in the way people walk and the human gait is investigated in terms of its ability to reveal a person's emotional state. And Microsoft Kinect is a rapidly developing, inexpensive, portable and no-marker motion capture system. This paper gives a new referable method to do emotion recognition, by using Microsoft Kinect to do gait pattern analysis, which has not been reported. $59$ subjects are recruited in this study and their gait patterns are record by two Kinect cameras. Significant joints selecting, Coordinate system transforming, Slider window gauss filter, Differential operation, and Data segmentation are used in data preprocessing. Feature extracting is based on Fourier transformation. By using the NaiveBayes, RandomForests, libSVM and SMO classification, the recognition rate of natural and unnatural emotions can reach above 70%.It is concluded that using the Kinect system can be a new method in recognition of emotions.\n    ",
        "submission_date": "2015-08-04T00:00:00",
        "last_modified_date": "2015-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01055",
        "title": "Estimating snow cover from publicly available images",
        "authors": [
            "Roman Fedorov",
            "Alessandro Camerada",
            "Piero Fraternali",
            "Marco Tagliasacchi"
        ],
        "abstract": "In this paper we study the problem of estimating snow cover in mountainous regions, that is, the spatial extent of the earth surface covered by snow. We argue that publicly available visual content, in the form of user generated photographs and image feeds from outdoor webcams, can both be leveraged as additional measurement sources, complementing existing ground, satellite and airborne sensor data. To this end, we describe two content acquisition and processing pipelines that are tailored to such sources, addressing the specific challenges posed by each of them, e.g., identifying the mountain peaks, filtering out images taken in bad weather conditions, handling varying illumination conditions. The final outcome is summarized in a snow cover index, which indicates for a specific mountain and day of the year, the fraction of visible area covered by snow, possibly at different elevations. We created a manually labelled dataset to assess the accuracy of the image snow covered area estimation, achieving 90.0% precision at 91.1% recall. In addition, we show that seasonal trends related to air temperature are captured by the snow cover index.\n    ",
        "submission_date": "2015-08-05T00:00:00",
        "last_modified_date": "2015-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01534",
        "title": "Nonlinear Metric Learning for kNN and SVMs through Geometric Transformations",
        "authors": [
            "Bibo Shi",
            "Jundong Liu"
        ],
        "abstract": "In recent years, research efforts to extend linear metric learning models to handle nonlinear structures have attracted great interests. In this paper, we propose a novel nonlinear solution through the utilization of deformable geometric models to learn spatially varying metrics, and apply the strategy to boost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS) are chosen as the geometric model due to their remarkable versatility and representation power in accounting for high-order deformations. By transforming the input space through TPS, we can pull same-class neighbors closer while pushing different-class points farther away in kNN, as well as make the input data points more linearly separable in SVMs. Improvements in the performance of kNN classification are demonstrated through experiments on synthetic and real world datasets, with comparisons made with several state-of-the-art metric learning solutions. Our SVM-based models also achieve significant improvements over traditional linear and kernel SVMs with the same datasets.\n    ",
        "submission_date": "2015-08-06T00:00:00",
        "last_modified_date": "2015-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.01720",
        "title": "Mismatch in the Classification of Linear Subspaces: Sufficient Conditions for Reliable Classification",
        "authors": [
            "Jure Sokolic",
            "Francesco Renna",
            "Robert Calderbank",
            "Miguel R. D. Rodrigues"
        ],
        "abstract": "This paper considers the classification of linear subspaces with mismatched classifiers. In particular, we assume a model where one observes signals in the presence of isotropic Gaussian noise and the distribution of the signals conditioned on a given class is Gaussian with a zero mean and a low-rank covariance matrix. We also assume that the classifier knows only a mismatched version of the parameters of input distribution in lieu of the true parameters. By constructing an asymptotic low-noise expansion of an upper bound to the error probability of such a mismatched classifier, we provide sufficient conditions for reliable classification in the low-noise regime that are able to sharply predict the absence of a classification error floor. Such conditions are a function of the geometry of the true signal distribution, the geometry of the mismatched signal distributions as well as the interplay between such geometries, namely, the principal angles and the overlap between the true and the mismatched signal subspaces. Numerical results demonstrate that our conditions for reliable classification can sharply predict the behavior of a mismatched classifier both with synthetic data and in a motion segmentation and a hand-written digit classification applications.\n    ",
        "submission_date": "2015-08-07T00:00:00",
        "last_modified_date": "2016-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02091",
        "title": "Image Representations and New Domains in Neural Image Captioning",
        "authors": [
            "Jack Hessel",
            "Nicolas Savva",
            "Michael J. Wilber"
        ],
        "abstract": "We examine the possibility that recent promising results in automatic caption generation are due primarily to language models. By varying image representation quality produced by a convolutional neural network, we find that a state-of-the-art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations. We replicate this result in a new, fine-grained, transfer learned captioning domain, consisting of 66K recipe image/title pairs. We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and find that having multiple captions per image is beneficial, but not an absolute requirement.\n    ",
        "submission_date": "2015-08-09T00:00:00",
        "last_modified_date": "2015-08-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.02849",
        "title": "Manifold regularization in structured output space for semi-supervised structured output prediction",
        "authors": [
            "Fei Jiang",
            "Lili Jia",
            "Xiaobao Sheng",
            "Riley LeMieux"
        ],
        "abstract": "Structured output prediction aims to learn a predictor to predict a structured output from a input data vector. The structured outputs include vector, tree, sequence, etc. We usually assume that we have a training set of input-output pairs to train the predictor. However, in many real-world appli- cations, it is difficult to obtain the output for a input, thus for many training input data points, the structured outputs are missing. In this paper, we dis- cuss how to learn from a training set composed of some input-output pairs, and some input data points without outputs. This problem is called semi- supervised structured output prediction. We propose a novel method for this problem by constructing a nearest neighbor graph from the input space to present the manifold structure, and using it to regularize the structured out- put space directly. We define a slack structured output for each training data point, and proposed to predict it by learning a structured output predictor. The learning of both slack structured outputs and the predictor are unified within one single minimization problem. In this problem, we propose to mini- mize the structured loss between the slack structured outputs of neighboring data points, and the prediction error measured by the structured loss. The problem is optimized by an iterative algorithm. Experiment results over three benchmark data sets show its advantage.\n    ",
        "submission_date": "2015-08-12T00:00:00",
        "last_modified_date": "2015-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03276",
        "title": "Talking about the Moving Image: A Declarative Model for Image Schema Based Embodied Perception Grounding and Language Generation",
        "authors": [
            "Jakob Suchan",
            "Mehul Bhatt",
            "Harshita Jhavar"
        ],
        "abstract": "We present a general theory and corresponding declarative model for the embodied grounding and natural language based analytical summarisation of dynamic visuo-spatial imagery. The declarative model ---ecompassing spatio-linguistic abstractions, image schemas, and a spatio-temporal feature based language generator--- is modularly implemented within Constraint Logic Programming (CLP). The implemented model is such that primitives of the theory, e.g., pertaining to space and motion, image schemata, are available as first-class objects with `deep semantics' suited for inference and query. We demonstrate the model with select examples broadly motivated by areas such as film, design, geography, smart environments where analytical natural language based externalisations of the moving image are central from the viewpoint of human interaction, evidence-based qualitative analysis, and sensemaking.\n",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03590",
        "title": "Light-field Microscopy with a Consumer Light-field Camera",
        "authors": [
            "Lois Mignard-Debise",
            "Ivo Ihrke"
        ],
        "abstract": "We explore the use of inexpensive consumer light- field camera technology for the purpose of light-field mi- croscopy. Our experiments are based on the Lytro (first gen- eration) camera. Unfortunately, the optical systems of the Lytro and those of microscopes are not compatible, lead- ing to a loss of light-field information due to angular and spatial vignetting when directly recording microscopic pic- tures. We therefore consider an adaptation of the Lytro op- tical system. We demonstrate that using the Lytro directly as an oc- ular replacement, leads to unacceptable spatial vignetting. However, we also found a setting that allows the use of the Lytro camera in a virtual imaging mode which prevents the information loss to a large extent. We analyze the new vir- tual imaging mode and use it in two different setups for im- plementing light-field microscopy using a Lytro camera. As a practical result, we show that the camera can be used for low magnification work, as e.g. common in quality control, surface characterization, etc. We achieve a maximum spa- tial resolution of about 6.25{\\mu}m, albeit at a limited SNR for the side views.\n    ",
        "submission_date": "2015-05-04T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03649",
        "title": "Borobudur was Built Algorithmically",
        "authors": [
            "Hokky Situngkir"
        ],
        "abstract": "The self-similarity of Indonesian Borobudur Temple is observed through the dimensionality of stupa that is hypothetically closely related to whole architectural body. Fractal dimension is calculated by using the cube counting method and found that the dimension is 2.325, which is laid between the two-dimensional plane and three dimensional space. The applied fractal geometry and self-similarity of the building is emerged as the building process implement the metric rules, since there is no universal metric standard known in ancient traditional Javanese culture thus the architecture is not based on final master plan. The paper also proposes how the hypothetical algorithmic architecture might be applied computationally in order to see some experimental generations of similar building. The paper ends with some conjectures for further challenge and insights related to fractal geometry in Javanese traditional cultural heritages.\n    ",
        "submission_date": "2015-08-13T00:00:00",
        "last_modified_date": "2015-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.03868",
        "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology",
        "authors": [
            "Brendan Jou",
            "Tao Chen",
            "Nikolaos Pappas",
            "Miriam Redi",
            "Mercan Topkara",
            "Shih-Fu Chang"
        ],
        "abstract": "Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of >15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, >7.36M images and their metadata is also released.\n    ",
        "submission_date": "2015-08-16T00:00:00",
        "last_modified_date": "2015-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04221",
        "title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification",
        "authors": [
            "Xuejie Liu",
            "Jingbin Wang",
            "Ming Yin",
            "Benjamin Edwards",
            "Peijuan Xu"
        ],
        "abstract": "Context of data points, which is usually defined as the other data points in a data set, has been found to play important roles in data representation and classification. In this paper, we study the problem of using context of a data point for its classification problem. Our work is inspired by the observation that actually only very few data points are critical in the context of a data point for its representation and classification. We propose to represent a data point as the sparse linear combination of its context, and learn the sparse context in a supervised way to increase its discriminative ability. To this end, we proposed a novel formulation for context learning, by modeling the learning of context parameter and classifier in a unified objective, and optimizing it with an alternative strategy in an iterative algorithm. Experiments on three benchmark data set show its advantage over state-of-the-art context-based data representation and classification methods.\n    ",
        "submission_date": "2015-08-18T00:00:00",
        "last_modified_date": "2015-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04554",
        "title": "Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification",
        "authors": [
            "Bokai Cao",
            "Xiangnan Kong",
            "Jingyuan Zhang",
            "Philip S. Yu",
            "Ann B. Ragin"
        ],
        "abstract": "Mining discriminative subgraph patterns from graph data has attracted great interest in recent years. It has a wide variety of applications in disease diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the graph representation alone. However, in many real-world applications, the side information is available along with the graph data. For example, for neurological disorder identification, in addition to the brain networks derived from neuroimaging data, hundreds of clinical, immunologic, serologic and cognitive measures may also be documented for each subject. These measures compose multiple side views encoding a tremendous amount of supplemental information for diagnostic purposes, yet are often ignored. In this paper, we study the problem of discriminative subgraph selection using multiple side views and propose a novel solution to find an optimal set of subgraph features for graph classification by exploring a plurality of side views. We derive a feature evaluation criterion, named gSide, to estimate the usefulness of subgraph patterns based upon side views. Then we develop a branch-and-bound algorithm, called gMSV, to efficiently search for optimal subgraph features by integrating the subgraph mining process and the procedure of discriminative feature selection. Empirical studies on graph classification tasks for neurological disorders using brain networks demonstrate that subgraph patterns selected by the multi-side-view guided subgraph selection approach can effectively boost graph classification performances and are relevant to disease diagnosis.\n    ",
        "submission_date": "2015-08-19T00:00:00",
        "last_modified_date": "2015-08-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.04924",
        "title": "Distributed Compressive Sensing: A Deep Learning Approach",
        "authors": [
            "Hamid Palangi",
            "Rabab Ward",
            "Li Deng"
        ],
        "abstract": "Various studies that address the compressed sensing problem with Multiple Measurement Vectors (MMVs) have been recently carried. These studies assume the vectors of the different channels to be jointly sparse. In this paper, we relax this condition. Instead we assume that these sparse vectors depend on each other but that this dependency is unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the \"residuals\" of all previous vectors. To estimate these probabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], a data driven model for sequence modelling that is deep in time. To calculate the model parameters, we minimize a cross entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used just at the decoder. As the proposed method is a data driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g. in recorded images and videos.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2016-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05056",
        "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual Sentiment Prediction",
        "authors": [
            "Victor Campos",
            "Amaia Salvador",
            "Brendan Jou",
            "Xavier Gir\u00f3-i-Nieto"
        ],
        "abstract": "Visual media are powerful means of expressing emotions and sentiments. The constant generation of new content in social networks highlights the need of automated visual sentiment analysis tools. While Convolutional Neural Networks (CNNs) have established a new state-of-the-art in several vision problems, their application to the task of sentiment analysis is mostly unexplored and there are few studies regarding how to design CNNs for this purpose. In this work, we study the suitability of fine-tuning a CNN for visual sentiment prediction as well as explore performance boosting techniques within this deep learning setting. Finally, we provide a deep-dive analysis into a benchmark, state-of-the-art network architecture to gain insight about how to design patterns for CNNs on the task of visual sentiment prediction.\n    ",
        "submission_date": "2015-08-20T00:00:00",
        "last_modified_date": "2015-08-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.05514",
        "title": "Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence",
        "authors": [
            "Tohid Ardeshiri",
            "Umut Orguner",
            "Emre \u00d6zkan"
        ],
        "abstract": "We propose a greedy mixture reduction algorithm which is capable of pruning mixture components as well as merging them based on the Kullback-Leibler divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD based method since it is not restricted to merging operations. The capability of pruning (in addition to merging) gives the algorithm the ability of preserving the peaks of the original mixture during the reduction. Analytical approximations are derived to circumvent the computational intractability of the KLD which results in a computationally efficient method. The proposed algorithm is compared with Runnalls' and Williams' methods in two numerical examples, using both simulated and real world data. The results indicate that the performance and computational complexity of the proposed approach make it an efficient alternative to existing mixture reduction methods.\n    ",
        "submission_date": "2015-08-22T00:00:00",
        "last_modified_date": "2015-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06264",
        "title": "Multiple kernel multivariate performance learning using cutting plane algorithm",
        "authors": [
            "Jingbin Wang",
            "Haoxiang Wang",
            "Yihua Zhou",
            "Nancy McDonald"
        ],
        "abstract": "In this paper, we propose a multi-kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure. Moreover, to solve the problem of kernel function selection and kernel parameter tuning, we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods.\n    ",
        "submission_date": "2015-08-25T00:00:00",
        "last_modified_date": "2015-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06904",
        "title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks",
        "authors": [
            "Markus Thom",
            "Franz Gritschneder"
        ],
        "abstract": "A rigorous formulation of the dynamics of a signal processing scheme aimed at dense signal scanning without any loss in accuracy is introduced and analyzed. Related methods proposed in the recent past lack a satisfactory analysis of whether they actually fulfill any exactness constraints. This is improved through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The proposed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors. This is demonstrated both theoretically in a computational complexity analysis and empirically on modern parallel processors.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2017-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.06936",
        "title": "Validation of neural spike sorting algorithms without ground-truth information",
        "authors": [
            "Alex H. Barnett",
            "Jeremy F. Magland",
            "Leslie F. Greengard"
        ],
        "abstract": "We describe a suite of validation metrics that assess the credibility of a given automatic spike sorting algorithm applied to a given electrophysiological recording, when ground-truth is unavailable. By rerunning the spike sorter two or more times, the metrics measure stability under various perturbations consistent with variations in the data itself, making no assumptions about the noise model, nor about the internal workings of the sorting algorithm. Such stability is a prerequisite for reproducibility of results. We illustrate the metrics on standard sorting algorithms for both in vivo and ex vivo recordings. We believe that such metrics could reduce the significant human labor currently spent on validation, and should form an essential part of large-scale automated spike sorting and systematic benchmarking of algorithms.\n    ",
        "submission_date": "2015-08-27T00:00:00",
        "last_modified_date": "2015-08-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07243",
        "title": "Bilevel parameter learning for higher-order total variation regularisation models",
        "authors": [
            "J.C. De los Reyes",
            "C.-B. Sch\u00f6nlieb",
            "T. Valkonen"
        ],
        "abstract": "We consider a bilevel optimisation approach for parameter learning in higher-order total variation image reconstruction models. Apart from the least squares cost functional, naturally used in bilevel learning, we propose and analyse an alternative cost, based on a Huber regularised TV-seminorm. Differentiability properties of the solution operator are verified and a first-order optimality system is derived. Based on the adjoint information, a quasi-Newton algorithm is proposed for the numerical solution of the bilevel problems. Numerical experiments are carried out to show the suitability of our approach and the improved performance of the new cost functional. Thanks to the bilevel optimisation framework, also a detailed comparison between TGV$^2$ and ICTV is carried out, showing the advantages and shortcomings of both regularisers, depending on the structure of the processed images and their noise level.\n    ",
        "submission_date": "2015-08-28T00:00:00",
        "last_modified_date": "2015-08-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1508.07569",
        "title": "Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing",
        "authors": [
            "Gary Pui-Tung Choi",
            "Kin Tat Ho",
            "Lok Ming Lui"
        ],
        "abstract": "Point cloud is the most fundamental representation of 3D geometric objects. Analyzing and processing point cloud surfaces is important in computer graphics and computer vision. However, most of the existing algorithms for surface analysis require connectivity information. Therefore, it is desirable to develop a mesh structure on point clouds. This task can be simplified with the aid of a parameterization. In particular, conformal parameterizations are advantageous in preserving the geometric information of the point cloud data. In this paper, we extend a state-of-the-art spherical conformal parameterization algorithm for genus-0 closed meshes to the case of point clouds, using an improved approximation of the Laplace-Beltrami operator on data points. Then, we propose an iterative scheme called the North-South reiteration for achieving a spherical conformal parameterization. A balancing scheme is introduced to enhance the distribution of the spherical parameterization. High quality triangulations and quadrangulations can then be built on the point clouds with the aid of the parameterizations. Also, the meshes generated are guaranteed to be genus-0 closed meshes. Moreover, using our proposed spherical conformal parameterization, multilevel representations of point clouds can be easily constructed. Experimental results demonstrate the effectiveness of our proposed framework.\n    ",
        "submission_date": "2015-08-30T00:00:00",
        "last_modified_date": "2016-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00083",
        "title": "Metastatic liver tumour segmentation from discriminant Grassmannian manifolds",
        "authors": [
            "Samuel Kadoury",
            "Eugene Vorontsov",
            "An Tang"
        ],
        "abstract": "The early detection, diagnosis and monitoring of liver cancer progression can be achieved with the precise delineation of metastatic tumours. However, accurate automated segmentation remains challenging due to the presence of noise, inhomogeneity and the high appearance variability of malignant tissue. In this paper, we propose an unsupervised metastatic liver tumour segmentation framework using a machine learning approach based on discriminant Grassmannian manifolds which learns the appearance of tumours with respect to normal tissue. First, the framework learns within-class and between-class similarity distributions from a training set of images to discover the optimal manifold discrimination between normal and pathological tissue in the liver. Second, a conditional optimisation scheme computes nonlocal pairwise as well as pattern-based clique potentials from the manifold subspace to recognise regions with similar labelings and to incorporate global consistency in the segmentation process. The proposed framework was validated on a clinical database of 43 CT images from patients with metastatic liver cancer. Compared to state-of-the-art methods, our method achieves a better performance on two separate datasets of metastatic liver tumours from different clinical sites, yielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50 tumours with an average volume of 27.3 mm3.\n    ",
        "submission_date": "2015-08-31T00:00:00",
        "last_modified_date": "2015-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00151",
        "title": "Learning A Task-Specific Deep Architecture For Clustering",
        "authors": [
            "Zhangyang Wang",
            "Shiyu Chang",
            "Jiayu Zhou",
            "Meng Wang",
            "Thomas S. Huang"
        ],
        "abstract": "While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods.\n    ",
        "submission_date": "2015-09-01T00:00:00",
        "last_modified_date": "2015-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.00728",
        "title": "On Transitive Consistency for Linear Invertible Transformations between Euclidean Coordinate Systems",
        "authors": [
            "Johan Thunberg",
            "Florian Bernard",
            "Jorge Goncalves"
        ],
        "abstract": "Transitive consistency is an intrinsic property for collections of linear invertible transformations between Euclidean coordinate frames. In practice, when the transformations are estimated from data, this property is lacking. This work addresses the problem of synchronizing transformations that are not transitively consistent. Once the transformations have been synchronized, they satisfy the transitive consistency condition - a transformation from frame $A$ to frame $C$ is equal to the composite transformation of first transforming A to B and then transforming B to C. The coordinate frames correspond to nodes in a graph and the transformations correspond to edges in the same graph. Two direct or centralized synchronization methods are presented for different graph topologies; the first one for quasi-strongly connected graphs, and the second one for connected graphs. As an extension of the second method, an iterative Gauss-Newton method is presented, which is later adapted to the case of affine and Euclidean transformations. Two distributed synchronization methods are also presented for orthogonal matrices, which can be seen as distributed versions of the two direct or centralized methods; they are similar in nature to standard consensus protocols used for distributed averaging. When the transformations are orthogonal matrices, a bound on the optimality gap can be computed. Simulations show that the gap is almost right, even for noise large in magnitude. This work also contributes on a theoretical level by providing linear algebraic relationships for transitively consistent transformations. One of the benefits of the proposed methods is their simplicity - basic linear algebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a wide range of parameter settings, the methods are numerically validated.\n    ",
        "submission_date": "2015-09-02T00:00:00",
        "last_modified_date": "2015-09-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01220",
        "title": "Light Efficient Flutter Shutter",
        "authors": [
            "Moshe Ben-Ezra"
        ],
        "abstract": "Flutter shutter is a technique in which the exposure is chopped into segments and light is only integrated part of the time. By carefully selecting the chopping sequence it is possible to better condition the data for reconstruction problems such as motion deblurring, focal sweeping, and compressed sensing. The partial exposure trades better conditioning for less energy. In problems such as motion deblurring the available energy is what caused the problem in the first place (as strong illumination allows short exposure thus eliminates motion blur). It is still beneficial because the benefit from the better conditioning outweighs the cost in energy.\n",
        "submission_date": "2015-08-23T00:00:00",
        "last_modified_date": "2015-08-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.01404",
        "title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization",
        "authors": [
            "Arnaud Vandaele",
            "Nicolas Gillis",
            "Qi Lei",
            "Kai Zhong",
            "Inderjit Dhillon"
        ],
        "abstract": "Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods.\n    ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2016-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.02805",
        "title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Previously in 2014, we proposed the Nearest Descent (ND) method, capable of generating an efficient Graph, called the in-tree (IT). Due to some beautiful and effective features, this IT structure proves well suited for data clustering. Although there exist some redundant edges in IT, they usually have salient features and thus it is not hard to remove them.\n",
        "submission_date": "2015-09-09T00:00:00",
        "last_modified_date": "2016-03-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03257",
        "title": "Rigid Multiview Varieties",
        "authors": [
            "Michael Joswig",
            "Joe Kileel",
            "Bernd Sturmfels",
            "Andr\u00e9 Wagner"
        ],
        "abstract": "The multiview variety from computer vision is generalized to images by $n$ cameras of points linked by a distance constraint. The resulting five-dimensional variety lives in a product of $2n$ projective planes. We determine defining polynomial equations, and we explore generalizations of this variety to scenarios of interest in applications.\n    ",
        "submission_date": "2015-09-10T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03503",
        "title": "NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining",
        "authors": [
            "Marco Winkler"
        ],
        "abstract": "The detection of triadic subgraph motifs is a common methodology in complex-networks research. The procedure usually applied in order to detect motifs evaluates whether a certain subgraph pattern is overrepresented in a network as a whole. However, motifs do not necessarily appear frequently in every region of a graph. For this reason, we recently introduced the framework of Node-Specific Pattern Mining (NoSPaM). This work is a manual for an implementation of NoSPaM which can be downloaded from ",
        "submission_date": "2015-09-04T00:00:00",
        "last_modified_date": "2015-09-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03789",
        "title": "Bio-Inspired Human Action Recognition using Hybrid Max-Product Neuro-Fuzzy Classifier and Quantum-Behaved PSO",
        "authors": [
            "Bardia Yousefi",
            "Chu Kiong Loo"
        ],
        "abstract": "Studies on computational neuroscience through functional magnetic resonance imaging (fMRI) and following biological inspired system stated that human action recognition in the brain of mammalian leads two distinct pathways in the model, which are specialized for analysis of motion (optic flow) and form information. Principally, we have defined a novel and robust form features applying active basis model as form extractor in form pathway in the biological inspired model. An unbalanced synergetic neural net-work classifies shapes and structures of human objects along with tuning its attention parameter by quantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi Tessellations. These tools utilized and justified as strong tools for following biological system model in form pathway. But the final decision has done by combination of ultimate outcomes of both pathways via fuzzy inference which increases novality of proposed model. Combination of these two brain pathways is done by considering each feature sets in Gaussian membership functions with fuzzy product inference method. Two configurations have been proposed for form pathway: applying multi-prototype human action templates using two time synergetic neural network for obtaining uniform template regarding each actions, and second scenario that it uses abstracting human action in four key-frames. Experimental results showed promising accuracy performance on different datasets (KTH and Weizmann).\n    ",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2016-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03844",
        "title": "Vectors of Locally Aggregated Centers for Compact Video Representation",
        "authors": [
            "Alhabib Abbas",
            "Nikos Deligiannis",
            "Yiannis Andreopoulos"
        ],
        "abstract": "We propose a novel vector aggregation technique for compact video representation, with application in accurate similarity detection within large video datasets. The current state-of-the-art in visual search is formed by the vector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates compact video representations based on scale-invariant feature transform (SIFT) vectors (extracted per frame) and local feature centers computed over a training set. With the aim to increase robustness to visual distortions, we propose a new approach that operates at a coarser level in the feature representation. We create vectors of locally aggregated centers (VLAC) by first clustering SIFT features to obtain local feature centers (LFCs) and then encoding the latter with respect to given centers of local feature centers (CLFCs), extracted from a training set. The sum-of-differences between the LFCs and the CLFCs are aggregated to generate an extremely-compact video description used for accurate video segment similarity detection. Experimentation using a video dataset, comprising more than 1000 minutes of content from the Open Video Project, shows that VLAC obtains substantial gains in terms of mean Average Precision (mAP) against VLAD and the hyper-pooling method of Douze et. al., under the same compaction factor and the same set of distortions.\n    ",
        "submission_date": "2015-09-13T00:00:00",
        "last_modified_date": "2015-09-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03942",
        "title": "Geometry and dimensionality reduction of feature spaces in primary visual cortex",
        "authors": [
            "Davide Barbieri"
        ],
        "abstract": "Some geometric properties of the wavelet analysis performed by visual neurons are discussed and compared with experimental data. In particular, several relationships between the cortical morphologies and the parametric dependencies of extracted features are formalized and considered from a harmonic analysis point of view.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.03970",
        "title": "Natural scene statistics mediate the perception of image complexity",
        "authors": [
            "Nicolas Gauvrit",
            "Fernando Soler-Toscano",
            "Hector Zenil"
        ],
        "abstract": "Humans are sensitive to complexity and regularity in patterns. The subjective perception of pattern complexity is correlated to algorithmic (Kolmogorov-Chaitin) complexity as defined in computer science, but also to the frequency of naturally occurring patterns. However, the possible mediational role of natural frequencies in the perception of algorithmic complexity remains unclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis, and complement their results in a new experiment. We conclude that human perception of complexity seems partly shaped by natural scenes statistics, thereby establishing a link between the perception of complexity and the effect of natural scene statistics.\n    ",
        "submission_date": "2015-09-14T00:00:00",
        "last_modified_date": "2015-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04612",
        "title": "Adapting Resilient Propagation for Deep Learning",
        "authors": [
            "Alan Mosca",
            "George D. Magoulas"
        ],
        "abstract": "The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04648",
        "title": "Comparative Design Space Exploration of Dense and Semi-Dense SLAM",
        "authors": [
            "M. Zeeshan Zia",
            "Luigi Nardi",
            "Andrew Jack",
            "Emanuele Vespa",
            "Bruno Bodin",
            "Paul H.J. Kelly",
            "Andrew J. Davison"
        ],
        "abstract": "SLAM has matured significantly over the past few years, and is beginning to appear in serious commercial products. While new SLAM systems are being proposed at every conference, evaluation is often restricted to qualitative visualizations or accuracy estimation against a ground truth. This is due to the lack of benchmarking methodologies which can holistically and quantitatively evaluate these systems. Further investigation at the level of individual kernels and parameter spaces of SLAM pipelines is non-existent, which is absolutely essential for systems research and integration. We extend the recently introduced SLAMBench framework to allow comparing two state-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the metrics of accuracy, energy consumption, and processing frame rate on two different hardware platforms, namely a desktop and an embedded device. We also analyze the pipelines at the level of individual kernels and explore their algorithmic and hardware design spaces for the first time, yielding valuable insights.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2016-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04751",
        "title": "Free-body Gesture Tracking and Augmented Reality Improvisation for Floor and Aerial Dance",
        "authors": [
            "Tammuz Dubnov",
            "Cheng-i Wang"
        ],
        "abstract": "This paper describes an updated interactive performance system for floor and Aerial Dance that controls visual and sonic aspects of the presentation via a depth sensing camera (MS Kinect). In order to detect, measure and track free movement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground and in the air) is performed using IR markers with a method for multi target tracking capabilities added and described in detail. An improved gesture tracking and recognition system, called Action Graph (AG), is described in the paper. Action Graph uses an efficient incremental construction from a single long sequence of movement features and automatically captures repeated sub-segments in the movement from start to finish with no manual interaction needed with other advanced capabilities discussed as well. By using the new model for the gesture we can unify an entire choreography piece by dynamically tracking and recognizing gestures and sub-portions of the piece. This gives the performer the freedom to improvise based on a set of recorded gestures/portions of the choreography and have the system dynamically respond in relation to the performer within a set of related rehearsed actions, an ability that has not been seen in any other system to date.\n    ",
        "submission_date": "2015-09-15T00:00:00",
        "last_modified_date": "2015-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.04863",
        "title": "Fast Template Matching by Subsampled Circulant Matrix",
        "authors": [
            "Sung-Hsien Hsieh",
            "Chun-Shien Lu",
            "and Soo-Chang Pei"
        ],
        "abstract": "Template matching is widely used for many applications in image and signal processing and usually is time-critical. Traditional methods usually focus on how to reduce the search locations by coarse-to-fine strategy or full search combined with pruning strategy. However, the computation cost of those methods is easily dominated by the size of signal N instead of that of template K. This paper proposes a probabilistic and fast matching scheme, which computation costs requires O(N) additions and O(K \\log K) multiplications, based on cross-correlation. The nuclear idea is to first downsample signal, which size becomes O(K), and then subsequent operations only involves downsampled signals. The probability of successful match depends on cross-correlation between signal and the template. We show the sufficient condition for successful match and prove that the probability is high for binary signals with K^2/log K >= O(N). The experiments shows this proposed scheme is fast and efficient and supports the theoretical results.\n    ",
        "submission_date": "2015-09-16T00:00:00",
        "last_modified_date": "2015-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05301",
        "title": "Humans Are Easily Fooled by Digital Images",
        "authors": [
            "Victor Schetinger",
            "Manuel M. Oliveira",
            "Roberto da Silva",
            "Tiago J. Carvalho"
        ],
        "abstract": "Digital images are ubiquitous in our modern lives, with uses ranging from social media to news, and even scientific papers. For this reason, it is crucial evaluate how accurate people are when performing the task of identify doctored images. In this paper, we performed an extensive user study evaluating subjects capacity to detect fake images. After observing an image, users have been asked if it had been altered or not. If the user answered the image has been altered, he had to provide evidence in the form of a click on the image. We collected 17,208 individual answers from 383 users, using 177 images selected from public forensic databases. Different from other previously studies, our method propose different ways to avoid lucky guess when evaluating users answers. Our results indicate that people show inaccurate skills at differentiating between altered and non-altered images, with an accuracy of 58%, and only identifying the modified images 46.5% of the time. We also track user features such as age, answering time, confidence, providing deep analysis of how such variables influence on the users' performance.\n    ",
        "submission_date": "2015-09-17T00:00:00",
        "last_modified_date": "2015-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05715",
        "title": "MAGMA: Multi-level accelerated gradient mirror descent algorithm for large-scale convex composite minimization",
        "authors": [
            "Vahan Hovhannisyan",
            "Panos Parpas",
            "Stefanos Zafeiriou"
        ],
        "abstract": "Composite convex optimization models arise in several applications, and are especially prevalent in inverse problems with a sparsity inducing norm and in general convex optimization with simple constraints. The most widely used algorithms for convex composite models are accelerated first order methods, however they can take a large number of iterations to compute an acceptable solution for large-scale problems. In this paper we propose to speed up first order methods by taking advantage of the structure present in many applications and in image processing in particular. Our method is based on multi-level optimization methods and exploits the fact that many applications that give rise to large scale models can be modelled using varying degrees of fidelity. We use Nesterov's acceleration techniques together with the multi-level approach to achieve $\\mathcal{O}(1/\\sqrt{\\epsilon})$ convergence rate, where $\\epsilon$ denotes the desired accuracy. The proposed method has a better convergence rate than any other existing multi-level method for convex problems, and in addition has the same rate as accelerated methods, which is known to be optimal for first-order methods. Moreover, as our numerical experiments show, on large-scale face recognition problems our algorithm is several times faster than the state of the art.\n    ",
        "submission_date": "2015-09-18T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.05962",
        "title": "Telugu OCR Framework using Deep Learning",
        "authors": [
            "Rakesh Achanta",
            "Trevor Hastie"
        ],
        "abstract": "In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex alphasyllabary and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve state-of-the-art error rates. We also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make Deep Learning work.\n    ",
        "submission_date": "2015-09-20T00:00:00",
        "last_modified_date": "2017-02-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06113",
        "title": "Deep Spatial Autoencoders for Visuomotor Learning",
        "authors": [
            "Chelsea Finn",
            "Xin Yu Tan",
            "Yan Duan",
            "Trevor Darrell",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "abstract": "Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.\n    ",
        "submission_date": "2015-09-21T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06576",
        "title": "Homotopy relations for digital images",
        "authors": [
            "Laurence Boxer",
            "P. Christopher Staecker"
        ],
        "abstract": "We introduce three generalizations of homotopy equivalence in digital images, to allow us to express whether a finite and an infinite digital image are similar with respect to homotopy.\n",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06690",
        "title": "Invariants of objects and their images under surjective maps",
        "authors": [
            "Irina A. Kogan",
            "Peter J. Olver"
        ],
        "abstract": "We examine the relationships between the differential invariants of objects and of their images under a surjective map. We analyze both the case when the underlying transformation group is projectable and hence induces an action on the image, and the case when only a proper subgroup of the entire group acts projectably. In the former case, we establish a constructible isomorphism between the algebra of differential invariants of the images and the algebra of fiber-wise constant (gauge) differential invariants of the objects. In the latter case, we describe residual effects of the full transformation group on the image invariants. Our motivation comes from the problem of reconstruction of an object from multiple-view images, with central and parallel projections of curves from three-dimensional space to the two-dimensional plane serving as our main examples.\n    ",
        "submission_date": "2015-09-22T00:00:00",
        "last_modified_date": "2015-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06825",
        "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours",
        "authors": [
            "Lerrel Pinto",
            "Abhinav Gupta"
        ],
        "abstract": "Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.06939",
        "title": "Enabling Depth-driven Visual Attention on the iCub Humanoid Robot: Instructions for Use and New Perspectives",
        "authors": [
            "Giulia Pasquale",
            "Tanis Mar",
            "Carlo Ciliberto",
            "Lorenzo Rosasco",
            "Lorenzo Natale"
        ],
        "abstract": "The importance of depth perception in the interactions that humans have within their nearby space is a well established fact. Consequently, it is also well known that the possibility of exploiting good stereo information would ease and, in many cases, enable, a large variety of attentional and interactive behaviors on humanoid robotic platforms. However, the difficulty of computing real-time and robust binocular disparity maps from moving stereo cameras often prevents from relying on this kind of cue to visually guide robots' attention and actions in real-world scenarios. The contribution of this paper is two-fold: first, we show that the Efficient Large-scale Stereo Matching algorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map is well suited to be used on a humanoid robotic platform as the iCub robot; second, we show how, provided with a fast and reliable stereo system, implementing relatively challenging visual behaviors in natural settings can require much less effort. As a case of study we consider the common situation where the robot is asked to focus the attention on one object close in the scene, showing how a simple but effective disparity-based segmentation solves the problem in this case. Indeed this example paves the way to a variety of other similar applications.\n    ",
        "submission_date": "2015-09-23T00:00:00",
        "last_modified_date": "2015-09-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07543",
        "title": "On Optimizing Human-Machine Task Assignments",
        "authors": [
            "Andreas Veit",
            "Michael Wilber",
            "Rajan Vaish",
            "Serge Belongie",
            "James Davis",
            "Vishal Anand",
            "Anshu Aviral",
            "Prithvijit Chakrabarty",
            "Yash Chandak",
            "Sidharth Chaturvedi",
            "Chinmaya Devaraj",
            "Ankit Dhall",
            "Utkarsh Dwivedi",
            "Sanket Gupte",
            "Sharath N. Sridhar",
            "Karthik Paga",
            "Anuj Pahuja",
            "Aditya Raisinghani",
            "Ayush Sharma",
            "Shweta Sharma",
            "Darpana Sinha",
            "Nisarg Thakkar",
            "K. Bala Vignesh",
            "Utkarsh Verma",
            "Kanniganti Abhishek",
            "Amod Agrawal",
            "Arya Aishwarya",
            "Aurgho Bhattacharjee",
            "Sarveshwaran Dhanasekar",
            "Venkata Karthik Gullapalli",
            "Shuchita Gupta",
            "Chandana G",
            "Kinjal Jain",
            "Simran Kapur",
            "Meghana Kasula",
            "Shashi Kumar",
            "Parth Kundaliya",
            "Utkarsh Mathur",
            "Alankrit Mishra",
            "Aayush Mudgal",
            "Aditya Nadimpalli",
            "Munakala Sree Nihit",
            "Akanksha Periwal",
            "Ayush Sagar",
            "Ayush Shah",
            "Vikas Sharma",
            "Yashovardhan Sharma",
            "Faizal Siddiqui",
            "Virender Singh",
            "Abhinav S.",
            "Anurag. D. Yadav"
        ],
        "abstract": "When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with \"off-the-shelf\" machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance.\n    ",
        "submission_date": "2015-09-24T00:00:00",
        "last_modified_date": "2015-09-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07831",
        "title": "Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories",
        "authors": [
            "Jaeyong Sung",
            "Ian Lenz",
            "Ashutosh Saxena"
        ],
        "abstract": "A robot operating in a real-world environment needs to perform reasoning over a variety of sensor modalities such as vision, language and motion trajectories. However, it is extremely challenging to manually design features relating such disparate modalities. In this work, we introduce an algorithm that learns to embed point-cloud, natural language, and manipulation trajectory data into a shared embedding space with a deep neural network. To learn semantically meaningful spaces throughout our network, we use a loss-based margin to bring embeddings of relevant pairs closer together while driving less-relevant cases from different modalities further apart. We use this both to pre-train its lower layers and fine-tune our final embedding space, leading to a more robust representation. We test our algorithm on the task of manipulating novel objects and appliances based on prior experience with other objects. On a large dataset, we achieve significant improvements in both accuracy and inference time over the previous state of the art. We also perform end-to-end experiments on a PR2 robot utilizing our learned embedding space.\n    ",
        "submission_date": "2015-09-25T00:00:00",
        "last_modified_date": "2017-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.07975",
        "title": "Modeling Curiosity in a Mobile Robot for Long-Term Autonomous Exploration and Monitoring",
        "authors": [
            "Yogesh Girdhar",
            "Gregory Dudek"
        ],
        "abstract": "This paper presents a novel approach to modeling curiosity in a mobile robot, which is useful for monitoring and adaptive data collection tasks, especially in the context of long term autonomous missions where pre-programmed missions are likely to have limited utility. We use a realtime topic modeling technique to build a semantic perception model of the environment, using which, we plan a path through the locations in the world with high semantic information content. The life-long learning behavior of the proposed perception model makes it suitable for long-term exploration missions. We validate the approach using simulated exploration experiments using aerial and underwater data, and demonstrate an implementation on the Aqua underwater robot in a variety of scenarios. We find that the proposed exploration paths that are biased towards locations with high topic perplexity, produce better terrain models with high discriminative power. Moreover, we show that the proposed algorithm implemented on Aqua robot is able to do tasks such as coral reef inspection, diver following, and sea floor exploration, without any prior training or preparation.\n    ",
        "submission_date": "2015-09-26T00:00:00",
        "last_modified_date": "2015-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08038",
        "title": "Deep Trans-layer Unsupervised Networks for Representation Learning",
        "authors": [
            "Wentao Zhu",
            "Jun Miao",
            "Laiyun Qing",
            "Xilin Chen"
        ],
        "abstract": "Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset.\n    ",
        "submission_date": "2015-09-27T00:00:00",
        "last_modified_date": "2015-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08329",
        "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs",
        "authors": [
            "Alberto N. Escalante-B.",
            "Laurenz Wiskott"
        ],
        "abstract": "Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a time series. Graph-based SFA (GSFA) is a supervised extension that can solve regression problems if followed by a post-processing regression algorithm. A training graph specifies arbitrary connections between the training samples. The connections in current graphs, however, only depend on the rank of the involved labels. Exploiting the exact label values makes further improvements in estimation accuracy possible.\n",
        "submission_date": "2015-09-28T00:00:00",
        "last_modified_date": "2015-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08745",
        "title": "Compression of Deep Neural Networks on the Fly",
        "authors": [
            "Guillaume Souli\u00e9",
            "Vincent Gripon",
            "Ma\u00eblys Robert"
        ],
        "abstract": "Thanks to their state-of-the-art performance, deep neural networks are increasingly used for object recognition. To achieve these results, they use millions of parameters to be trained. However, when targeting embedded applications the size of these models becomes problematic. As a consequence, their usage on smartphones or other resource limited devices is prohibited. In this paper we introduce a novel compression method for deep neural networks that is performed during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve significantly larger compression rates than state-of-the-art methods.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2016-03-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1509.08973",
        "title": "Symbol Emergence in Robotics: A Survey",
        "authors": [
            "Tadahiro Taniguchi",
            "Takayuki Nagai",
            "Tomoaki Nakamura",
            "Naoto Iwahashi",
            "Tetsuya Ogata",
            "Hideki Asoh"
        ],
        "abstract": "Humans can learn the use of language through physical interaction with their environment and semiotic communication with other people. It is very important to obtain a computational understanding of how humans can form a symbol system and obtain semiotic skills through their autonomous mental development. Recently, many studies have been conducted on the construction of robotic systems and machine-learning methods that can learn the use of language through embodied multimodal interaction with their environment and other systems. Understanding human social interactions and developing a robot that can smoothly communicate with human users in the long term, requires an understanding of the dynamics of symbol systems and is crucially important. The embodied cognition and social interaction of participants gradually change a symbol system in a constructive manner. In this paper, we introduce a field of research called symbol emergence in robotics (SER). SER is a constructive approach towards an emergent symbol system. The emergent symbol system is socially self-organized through both semiotic communications and physical interactions with autonomous cognitive developmental agents, i.e., humans and developmental robots. Specifically, we describe some state-of-art research topics concerning SER, e.g., multimodal categorization, word discovery, and a double articulation analysis, that enable a robot to obtain words and their embodied meanings from raw sensory--motor information, including visual information, haptic information, auditory information, and acoustic speech signals, in a totally unsupervised manner. Finally, we suggest future directions of research in SER.\n    ",
        "submission_date": "2015-09-29T00:00:00",
        "last_modified_date": "2015-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01077",
        "title": "Nonlinear Spectral Analysis via One-homogeneous Functionals - Overview and Future Prospects",
        "authors": [
            "Guy Gilboa",
            "Michael Moeller",
            "Martin Burger"
        ],
        "abstract": "We present in this paper the motivation and theory of nonlinear spectral representations, based on convex regularizing functionals. Some comparisons and analogies are drawn to the fields of signal processing, harmonic analysis and sparse representations. The basic approach, main results and initial applications are shown. A discussion of open problems and future directions concludes this work.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01113",
        "title": "RAID: A Relation-Augmented Image Descriptor",
        "authors": [
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Peter Wonka"
        ],
        "abstract": "As humans, we regularly interpret images based on the relations between image regions. For example, a person riding object X, or a plank bridging two objects. Current methods provide limited support to search for images based on such relations. We present RAID, a relation-augmented image descriptor that supports queries based on inter-region relations. The key idea of our descriptor is to capture the spatial distribution of simple point-to-region relationships to describe more complex relationships between two image regions. We evaluate the proposed descriptor by querying into a large subset of the Microsoft COCO database and successfully extract nontrivial images demonstrating complex inter-region relations, which are easily missed or erroneously classified by existing methods.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01130",
        "title": "Bregman Iteration for Correspondence Problems: A Study of Optical Flow",
        "authors": [
            "Laurent Hoeltgen",
            "Michael Breu\u00df"
        ],
        "abstract": "Bregman iterations are known to yield excellent results for denoising, deblurring and compressed sensing tasks, but so far this technique has rarely been used for other image processing problems. In this paper we give a thorough description of the Bregman iteration, unifying thereby results of different authors within a common framework. Then we show how to adapt the split Bregman iteration, originally developed by Goldstein and Osher for image restoration purposes, to optical flow which is a fundamental correspondence problem in computer vision. We consider some classic and modern optical flow models and present detailed algorithms that exhibit the benefits of the Bregman iteration. By making use of the results of the Bregman framework, we address the issues of convergence and error estimation for the algorithms. Numerical examples complement the theoretical part.\n    ",
        "submission_date": "2015-10-05T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01628",
        "title": "Large-scale subspace clustering using sketching and validation",
        "authors": [
            "Panagiotis A. Traganitis",
            "Konstantinos Slavakis",
            "Georgios B. Giannakis"
        ],
        "abstract": "The nowadays massive amounts of generated and communicated data present major challenges in their processing. While capable of successfully classifying nonlinearly separable objects in various settings, subspace clustering (SC) methods incur prohibitively high computational complexity when processing large-scale data. Inspired by the random sampling and consensus (RANSAC) approach to robust regression, the present paper introduces a randomized scheme for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale data. At the heart of SkeVa-SC lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments. Sparsity in data representations is also exploited to reduce the computational burden of SC, while achieving high clustering accuracy. Performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of SkeVa-SC and its competitive performance relative to state-of-the-art scalable SC approaches. Keywords: Subspace clustering, big data, kernel smoothing, randomization, sketching, validation, sparsity.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.01722",
        "title": "Structured Transforms for Small-Footprint Deep Learning",
        "authors": [
            "Vikas Sindhwani",
            "Tara N. Sainath",
            "Sanjiv Kumar"
        ],
        "abstract": "We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.\n    ",
        "submission_date": "2015-10-06T00:00:00",
        "last_modified_date": "2015-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02173",
        "title": "Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models",
        "authors": [
            "John-Alexander M. Assael",
            "Niklas Wahlstr\u00f6m",
            "Thomas B. Sch\u00f6n",
            "Marc Peter Deisenroth"
        ],
        "abstract": "Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (\"torques\") from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.\n    ",
        "submission_date": "2015-10-08T00:00:00",
        "last_modified_date": "2015-10-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02923",
        "title": "On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image Rician Denoising",
        "authors": [
            "Adrian Martin",
            "Emanuele Schiavi",
            "Sergio Segura de Leon"
        ],
        "abstract": "Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2016-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02934",
        "title": "Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A framework for single-subject analysis in diffusion tensor imaging",
        "authors": [
            "Cheng Guan Koay",
            "Ping-Hong Yeh",
            "John M. Ollinger",
            "M. Okan \u0130rfano\u011flu",
            "Carlo Pierpaoli",
            "Peter J. Basser",
            "Terrence R. Oakes",
            "Gerard Riedy"
        ],
        "abstract": "The purpose of this work is to develop a framework for single-subject analysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI) is capable of testing whether an individual tract as represented by the major eigenvector of the diffusion tensor and its corresponding angular dispersion are significantly different from a group of tracts on a voxel-by-voxel basis. This work develops two complementary statistical tests based on the elliptical cone of uncertainty (COU), which is a model of uncertainty or dispersion of the major eigenvector of the diffusion tensor. The orientation deviation test examines whether the major eigenvector from a single subject is within the average elliptical COU formed by a collection of elliptical COUs. The shape deviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test between the normalized shape measures (area and circumference) of the elliptical cones of uncertainty of the single subject against a group of controls. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR) were incorporated in the orientation deviation test. The shape deviation test uses FDR only. TOADDI was found to be numerically accurate and statistically effective. Clinical data from two Traumatic Brain Injury (TBI) patients and one non-TBI subject were tested against the data obtained from a group of 45 non-TBI controls to illustrate the application of the proposed framework in single-subject analysis. The frontal portion of the superior longitudinal fasciculus seemed to be implicated in both tests as significantly different from that of the control group. The TBI patients and the single non-TBI subject were well separated under the shape deviation test at the chosen FDR level of 0.0005. TOADDI is a simple but novel geometrically based statistical framework for analyzing DTI data.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.02975",
        "title": "Optimal Piecewise Linear Function Approximation for GPU-based Applications",
        "authors": [
            "Daniel Berj\u00f3n",
            "Guillermo Gallego",
            "Carlos Cuevas",
            "Francisco Mor\u00e1n",
            "Narciso Garc\u00eda"
        ],
        "abstract": "Many computer vision and human-computer interaction applications developed in recent years need evaluating complex and continuous mathematical functions as an essential step toward proper operation. However, rigorous evaluation of this kind of functions often implies a very high computational cost, unacceptable in real-time applications. To alleviate this problem, functions are commonly approximated by simpler piecewise-polynomial representations. Following this idea, we propose a novel, efficient, and practical technique to evaluate complex and continuous functions using a nearly optimal design of two types of piecewise linear approximations in the case of a large budget of evaluation subintervals. To this end, we develop a thorough error analysis that yields asymptotically tight bounds to accurately quantify the approximation performance of both representations. It provides an improvement upon previous error estimates and allows the user to control the trade-off between the approximation error and the number of evaluation subintervals. To guarantee real-time operation, the method is suitable for, but not limited to, an efficient implementation in modern Graphics Processing Units (GPUs), where it outperforms previous alternative approaches by exploiting the fixed-function interpolation routines present in their texture units. The proposed technique is a perfect match for any application requiring the evaluation of continuous functions, we have measured in detail its quality and efficiency on several functions, and, in particular, the Gaussian function because it is extensively used in many areas of computer vision and cybernetics, and it is expensive to evaluate.\n    ",
        "submission_date": "2015-10-10T00:00:00",
        "last_modified_date": "2015-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.03730",
        "title": "Fast sequential forensic camera identification",
        "authors": [
            "Fernando P\u00e9rez-Gonz\u00e1lez",
            "Iria Gonz\u00e1lez-Iglesias",
            "Miguel Masciopinto",
            "Pedro Comesa\u00f1a"
        ],
        "abstract": "Two sequential camera source identification methods are proposed. Sequential tests implement a log-likelihood ratio test in an incremental way, thus enabling a reliable decision with a minimal number of observations. One of our methods adapts Goljan et al.'s to sequential operation. The second, which offers better performance in terms of error probabilities and average number of test observations, is based on treating the alternative hypothesis as a doubly stochastic model. We also discuss how the standard sequential test can be corrected to account for the event of weak fingerprints. Finally, we validate the goodness of our methods with experiments.\n    ",
        "submission_date": "2015-10-13T00:00:00",
        "last_modified_date": "2015-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.04709",
        "title": "Multilingual Image Description with Neural Sequence Models",
        "authors": [
            "Desmond Elliott",
            "Stella Frank",
            "Eva Hasler"
        ],
        "abstract": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.\n    ",
        "submission_date": "2015-10-15T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05879",
        "title": "What's the point? Frame-wise Pointing Gesture Recognition with Latent-Dynamic Conditional Random Fields",
        "authors": [
            "Christian Wittner",
            "Boris Schauerte",
            "Rainer Stiefelhagen"
        ],
        "abstract": "We use Latent-Dynamic Conditional Random Fields to perform skeleton-based pointing gesture classification at each time instance of a video sequence, where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently, we determine continuous time sequences of arbitrary length that form individual pointing gestures and this way reliably detect pointing gestures at a false positive detection rate of 0.63%.\n    ",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2015-10-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.05893",
        "title": "Online Unmixing of Multitemporal Hyperspectral Images accounting for Spectral Variability",
        "authors": [
            "Pierre-Antoine Thouvenin",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "abstract": "Hyperspectral unmixing is aimed at identifying the reference spectral signatures composing an hyperspectral image and their relative abundance fractions in each pixel. In practice, the identified signatures may vary spectrally from an image to another due to varying acquisition conditions, thus inducing possibly significant estimation errors. Against this background, hyperspectral unmixing of several images acquired over the same area is of considerable interest. Indeed, such an analysis enables the endmembers of the scene to be tracked and the corresponding endmember variability to be characterized. Sequential endmember estimation from a set of hyperspectral images is expected to provide improved performance when compared to methods analyzing the images independently. However, the significant size of hyperspectral data precludes the use of batch procedures to jointly estimate the mixture parameters of a sequence of hyperspectral images. Provided that each elementary component is present in at least one image of the sequence, we propose to perform an online hyperspectral unmixing accounting for temporal endmember variability. The online hyperspectral unmixing is formulated as a two-stage stochastic program, which can be solved using a stochastic approximation. The performance of the proposed method is evaluated on synthetic and real data. A comparison with independent unmixing algorithms finally illustrates the interest of the proposed strategy.\n    ",
        "submission_date": "2015-10-20T00:00:00",
        "last_modified_date": "2016-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06223",
        "title": "Predicting popularity of online videos using Support Vector Regression",
        "authors": [
            "Tomasz Trzcinski",
            "Przemyslaw Rokita"
        ],
        "abstract": "In this work, we propose a regression method to predict the popularity of an online video based on temporal and visual cues. Our method uses Support Vector Regression with Gaussian Radial Basis Functions. We show that modelling popularity patterns with this approach provides higher and more stable prediction results, mainly thanks to the non-linearity character of the proposed method as well as its resistance against overfitting. We compare our method with the state of the art on datasets containing over 14,000 videos from YouTube and Facebook. Furthermore, we show that results obtained relying only on the early distribution patterns, can be improved by adding social and visual metadata.\n    ",
        "submission_date": "2015-10-21T00:00:00",
        "last_modified_date": "2017-05-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06479",
        "title": "Generic decoding of seen and imagined objects using hierarchical visual features",
        "authors": [
            "Tomoyasu Horikawa",
            "Yukiyasu Kamitani"
        ],
        "abstract": "Object recognition is a key function in both human and machine vision. While recent studies have achieved fMRI decoding of seen and imagined contents, the prediction is limited to training examples. We present a decoding approach for arbitrary objects, using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features including those from a convolutional neural network can be predicted from fMRI patterns and that greater accuracy is achieved for low/high-level features with lower/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, the decoding of imagined objects reveals progressive recruitment of higher to lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2016-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06706",
        "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines",
        "authors": [
            "Aleksandar Zlateski",
            "Kisuk Lee",
            "H. Sebastian Seung"
        ],
        "abstract": "Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent's theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming.\n    ",
        "submission_date": "2015-10-22T00:00:00",
        "last_modified_date": "2015-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.06895",
        "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm",
        "authors": [
            "Canyi Lu",
            "Jinhui Tang",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "The nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of $L_0$-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.\n    ",
        "submission_date": "2015-10-23T00:00:00",
        "last_modified_date": "2015-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07390",
        "title": "Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for Mobile Security Robots",
        "authors": [
            "YongChol Sin",
            "MyongSong Choe",
            "GyongIl Ryang"
        ],
        "abstract": "One of fundamental issues for security robots is to detect and track people in the surroundings. The main problems of this task are real-time constraints, a changing background, varying illumination conditions and a non-rigid shape of the person to be tracked. In this paper, we propose a solution for tracking with a pan-tilt camera and a passive infrared range (PIR) sensor to detect the moving object based on consecutive frame difference. The proposed method is excellent in real-time performance because it requires only a little memory and computation. Experiment results show that this method can detect the moving object such as human efficiently and accurately in non-stationary and complex indoor environment.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07573",
        "title": "Generalized Regressive Motion: a Visual Cue to Collision",
        "authors": [
            "Krzysztof Chalupka",
            "Michael Dickinson",
            "Pietro Perona"
        ],
        "abstract": "Brains and sensory systems evolved to guide motion. Central to this task is controlling the approach to stationary obstacles and detecting moving organisms. Looming has been proposed as the main monocular visual cue for detecting the approach of other animals and avoiding collisions with stationary obstacles. Elegant neural mechanisms for looming detection have been found in the brain of insects and vertebrates. However, looming has not been analyzed in the context of collisions between two moving animals. We propose an alternative strategy, Generalized Regressive Motion (GRM), which is consistent with recently observed behavior in fruit flies. Geometric analysis proves that GRM is a reliable cue to collision among conspecifics, whereas agent-based modeling suggests that GRM is a better cue than looming as a means to detect approach, prevent collisions and maintain mobility.\n    ",
        "submission_date": "2015-10-26T00:00:00",
        "last_modified_date": "2015-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.07740",
        "title": "The Wilson Machine for Image Modeling",
        "authors": [
            "Saeed Saremi",
            "Terrence J. Sejnowski"
        ],
        "abstract": "Learning the distribution of natural images is one of the hardest and most important problems in machine learning. The problem remains open, because the enormous complexity of the structures in natural images spans all length scales. We break down the complexity of the problem and show that the hierarchy of structures in natural images fuels a new class of learning algorithms based on the theory of critical phenomena and stochastic processes. We approach this problem from the perspective of the theory of critical phenomena, which was developed in condensed matter physics to address problems with infinite length-scale fluctuations, and build a framework to integrate the criticality of natural images into a learning algorithm. The problem is broken down by mapping images into a hierarchy of binary images, called bitplanes. In this representation, the top bitplane is critical, having fluctuations in structures over a vast range of scales. The bitplanes below go through a gradual stochastic heating process to disorder. We turn this representation into a directed probabilistic graphical model, transforming the learning problem into the unsupervised learning of the distribution of the critical bitplane and the supervised learning of the conditional distributions for the remaining bitplanes. We learnt the conditional distributions by logistic regression in a convolutional architecture. Conditioned on the critical binary image, this simple architecture can generate large, natural-looking images, with many shades of gray, without the use of hidden units, unprecedented in the studies of natural images. The framework presented here is a major step in bringing criticality and stochastic processes to machine learning and in studying natural image statistics.\n    ",
        "submission_date": "2015-10-27T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08174",
        "title": "Visual Quality Enhancement in Optoacoustic Tomography using Active Contour Segmentation Priors",
        "authors": [
            "Subhamoy Mandal",
            "Xos\u00e9 Lu\u00eds De\u00e1n-Ben",
            "Daniel Razansky"
        ],
        "abstract": "Segmentation of biomedical images is essential for studying and characterizing anatomical structures, detection and evaluation of pathological tissues. Segmentation has been further shown to enhance the reconstruction performance in many tomographic imaging modalities by accounting for heterogeneities of the excitation field and tissue properties in the imaged region. This is particularly relevant in optoacoustic tomography, where discontinuities in the optical and acoustic tissue properties, if not properly accounted for, may result in deterioration of the imaging performance. Efficient segmentation of optoacoustic images is often hampered by the relatively low intrinsic contrast of large anatomical structures, which is further impaired by the limited angular coverage of some commonly employed tomographic imaging configurations. Herein, we analyze the performance of active contour models for boundary segmentation in cross-sectional optoacoustic tomography. The segmented mask is employed to construct a two compartment model for the acoustic and optical parameters of the imaged tissues, which is subsequently used to improve accuracy of the image reconstruction routines. The performance of the suggested segmentation and modeling approach are showcased in tissue-mimicking phantoms and small animal imaging experiments.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2016-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08233",
        "title": "A Fast Randomized Method to Find Homotopy Classes for Socially-Aware Navigation",
        "authors": [
            "Luigi Palmieri",
            "Andrey Rudenko",
            "Kai O. Arras"
        ],
        "abstract": "We introduce and show preliminary results of a fast randomized method that finds a set of K paths lying in distinct homotopy classes. We frame the path planning task as a graph search problem, where the navigation graph is based on a Voronoi diagram. The search is biased by a cost function derived from the social force model that is used to generate and select the paths. We compare our method to Yen's algorithm, and empirically show that our approach is faster to find a subset of homotopy classes. Furthermore our approach computes a set of more diverse paths with respect to the baseline while obtaining a negligible loss in path quality.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.08520",
        "title": "Learning with $\\ell^{0}$-Graph: $\\ell^{0}$-Induced Sparse Subspace Clustering",
        "authors": [
            "Yingzhen Yang",
            "Jiashi Feng",
            "Jianchao Yang",
            "Thomas S. Huang"
        ],
        "abstract": "Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC) \\cite{ElhamifarV13} and $\\ell^{1}$-graph \\cite{YanW09,ChengYYFH10}, are effective in partitioning the data that lie in a union of subspaces. Most of those methods use $\\ell^{1}$-norm or $\\ell^{2}$-norm with thresholding to impose the sparsity of the constructed sparse similarity graph, and certain assumptions, e.g. independence or disjointness, on the subspaces are required to obtain the subspace-sparse representation, which is the key to their success. Such assumptions are not guaranteed to hold in practice and they limit the application of sparse subspace clustering on subspaces with general location. In this paper, we propose a new sparse subspace clustering method named $\\ell^{0}$-graph. In contrast to the required assumptions on subspaces for most existing sparse subspace clustering methods, it is proved that subspace-sparse representation can be obtained by $\\ell^{0}$-graph for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We develop a proximal method to obtain the sub-optimal solution to the optimization problem of $\\ell^{0}$-graph with proved guarantee of convergence. Moreover, we propose a regularized $\\ell^{0}$-graph that encourages nearby data to have similar neighbors so that the similarity graph is more aligned within each cluster and the graph connectivity issue is alleviated. Extensive experimental results on various data sets demonstrate the superiority of $\\ell^{0}$-graph compared to other competing clustering methods, as well as the effectiveness of regularized $\\ell^{0}$-graph.\n    ",
        "submission_date": "2015-10-28T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1510.09171",
        "title": "Accurate Vision-based Vehicle Localization using Satellite Imagery",
        "authors": [
            "Hang Chu",
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "We propose a method for accurately localizing ground vehicles with the aid of satellite imagery. Our approach takes a ground image as input, and outputs the location from which it was taken on a georeferenced satellite image. We perform visual localization by estimating the co-occurrence probabilities between the ground and satellite images based on a ground-satellite feature dictionary. The method is able to estimate likelihoods over arbitrary locations without the need for a dense ground image database. We present a ranking-loss based algorithm that learns location-discriminative feature projection matrices that result in further improvements in accuracy. We evaluate our method on the Malaga and KITTI public datasets and demonstrate significant improvements over a baseline that performs exhaustive search.\n    ",
        "submission_date": "2015-10-30T00:00:00",
        "last_modified_date": "2015-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00363",
        "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
        "authors": [
            "Matthieu Courbariaux",
            "Yoshua Bengio",
            "Jean-Pierre David"
        ],
        "abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\n    ",
        "submission_date": "2015-11-02T00:00:00",
        "last_modified_date": "2016-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.00758",
        "title": "High-Performance and Tunable Stereo Reconstruction",
        "authors": [
            "Sudeep Pillai",
            "Srikumar Ramalingam",
            "John J. Leonard"
        ],
        "abstract": "Traditional stereo algorithms have focused their efforts on reconstruction quality and have largely avoided prioritizing for run time performance. Robots, on the other hand, require quick maneuverability and effective computation to observe its immediate environment and perform tasks within it. In this work, we propose a high-performance and tunable stereo disparity estimation method, with a peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can potentially enable robots to quickly reconstruct their immediate surroundings and maneuver at high-speeds. Our key contribution is a disparity estimation algorithm that iteratively approximates the scene depth via a piece-wise planar mesh from stereo imagery, with a fast depth validation step for semi-dense reconstruction. The mesh is initially seeded with sparsely matched keypoints, and is recursively tessellated and refined as needed (via a resampling stage), to provide the desired stereo disparity accuracy. The inherent simplicity and speed of our approach, with the ability to tune it to a desired reconstruction quality and runtime performance makes it a compelling solution for applications in high-speed vehicles.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2016-02-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01029",
        "title": "Understanding symmetries in deep networks",
        "authors": [
            "Vijay Badrinarayanan",
            "Bamdev Mishra",
            "Roberto Cipolla"
        ],
        "abstract": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.\n    ",
        "submission_date": "2015-11-03T00:00:00",
        "last_modified_date": "2015-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01754",
        "title": "Symmetry-invariant optimization in deep networks",
        "authors": [
            "Vijay Badrinarayanan",
            "Bamdev Mishra",
            "Roberto Cipolla"
        ],
        "abstract": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2015-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.01865",
        "title": "Convolutional Neural Network for Stereotypical Motor Movement Detection in Autism",
        "authors": [
            "Nastaran Mohammadian Rad",
            "Andrea Bizzego",
            "Seyed Mostafa Kia",
            "Giuseppe Jurman",
            "Paola Venuti",
            "Cesare Furlanello"
        ],
        "abstract": "Autism Spectrum Disorders (ASDs) are often associated with specific atypical postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have a specific visibility. While the identification and the quantification of SMM patterns remain complex, its automation would provide support to accurate tuning of the intervention in the therapy of autism. Therefore, it is essential to develop automatic SMM detection systems in a real world setting, taking care of strong inter-subject and intra-subject variability. Wireless accelerometer sensing technology can provide a valid infrastructure for real-time SMM detection, however such variability remains a problem also for machine learning methods, in particular whenever handcrafted features extracted from accelerometer signal are considered. Here, we propose to employ the deep learning paradigm in order to learn discriminating features from multi-sensor accelerometer signals. Our results provide preliminary evidence that feature learning and transfer learning embedded in the deep architecture achieve higher accurate SMM detectors in longitudinal scenarios.\n    ",
        "submission_date": "2015-11-05T00:00:00",
        "last_modified_date": "2016-06-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02274",
        "title": "Stacked Attention Networks for Image Question Answering",
        "authors": [
            "Zichao Yang",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng",
            "Alex Smola"
        ],
        "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.\n    ",
        "submission_date": "2015-11-07T00:00:00",
        "last_modified_date": "2016-01-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02589",
        "title": "Parkinson's disease patient rehabilitation using gaming platforms: lessons learnt",
        "authors": [
            "Ioannis Pachoulakis",
            "Nikolaos Papadopoulos",
            "Cleanthe Spanaki"
        ],
        "abstract": "Parkinson's disease (PD) is a progressive neurodegenerative movement disorder where motor dysfunction gradually increases as the disease progress. In addition to administering dopaminergic PD-specific drugs, attending neurologists strongly recommend regular exercise combined with physiotherapy. However, because of the long-term nature of the disease, patients following traditional rehabilitation programs may get bored, lose interest and eventually drop out as a direct result of the repeatability and predictability of the prescribed exercises. Technology supported opportunities to liven up a daily exercise schedule have appeared in the form of character-based, virtual reality games which promote physical training in a non-linear and looser fashion and provide an experience that varies from one game loop the next. Such \"exergames\", a word that results from the amalgamation of the words \"exercise\" and \"game\" challenge patients into performing movements of varying complexity in a playful and immersive virtual environment. Today's game consoles such as Nintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new opportunities to infuse motivation and variety to an otherwise mundane physiotherapy routine. In this paper we present some of these approaches, discuss their suitability for these PD patients, mainly on the basis of demands made on balance, agility and gesture precision, and present design principles that exergame platforms must comply with in order to be suitable for PD patients.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2015-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02793",
        "title": "Generating Images from Captions with Attention",
        "authors": [
            "Elman Mansimov",
            "Emilio Parisotto",
            "Jimmy Lei Ba",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02821",
        "title": "Partial Membership Latent Dirichlet Allocation",
        "authors": [
            "Chao Chen",
            "Alina Zare",
            "J. Tory Cobb"
        ],
        "abstract": "Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting imagery. These models are confined to crisp segmentation. Yet, there are many images in which some regions cannot be assigned a crisp label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and associated parameter estimation algorithms. Experimental results on two natural image datasets and one SONAR image dataset show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability existing methods do not have.\n    ",
        "submission_date": "2015-11-09T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.02986",
        "title": "Experimental robustness of Fourier Ptychography phase retrieval algorithms",
        "authors": [
            "Li-Hao Yeh",
            "Jonathan Dong",
            "Jingshan Zhong",
            "Lei Tian",
            "Michael Chen",
            "Gongguo Tang",
            "Mahdi Soltanolkotabi",
            "Laura Waller"
        ],
        "abstract": "Fourier ptychography is a new computational microscopy technique that provides gigapixel-scale intensity and phase images with both wide field-of-view and high resolution. By capturing a stack of low-resolution images under different illumination angles, a nonlinear inverse algorithm can be used to computationally reconstruct the high-resolution complex field. Here, we compare and classify multiple proposed inverse algorithms in terms of experimental robustness. We find that the main sources of error are noise, aberrations and mis-calibration (i.e. model mis-match). Using simulations and experiments, we demonstrate that the choice of cost function plays a critical role, with amplitude-based cost functions performing better than intensity-based ones. The reason for this is that Fourier ptychography datasets consist of images from both brightfield and darkfield illumination, representing a large range of measured intensities. Both noise (e.g. Poisson noise) and model mis-match errors are shown to scale with intensity. Hence, algorithms that use an appropriate cost function will be more tolerant to both noise and model mis-match. Given these insights, we propose a global Newton's method algorithm which is robust and computationally efficient. Finally, we discuss the impact of procedures for algorithmic correction of aberrations and mis-calibration.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03055",
        "title": "Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing",
        "authors": [
            "Jie Lin",
            "Olivier Mor\u00e8re",
            "Julie Petta",
            "Vijay Chandrasekhar",
            "Antoine Veillard"
        ],
        "abstract": "A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements: providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification, descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless, the dimensionality of a typical DCNN descriptor --extracted either from the visual feature pyramid or the fully-connected layers-- remains quite high at several thousands of scalar values. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully unsupervised method to compute extremely compact binary hashes --in the 32-256 bits range-- from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First, Stacked Restricted Boltzmann Machines (SRBM), a type of unsupervised deep neural nets, are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then, triplet networks, a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range.\n    ",
        "submission_date": "2015-11-10T00:00:00",
        "last_modified_date": "2015-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03355",
        "title": "Principal Autoparallel Analysis: Data Analysis in Weitzenb\u00f6ck Space",
        "authors": [
            "Stephen Marsland",
            "Carole J Twining"
        ],
        "abstract": "The statistical analysis of data lying on a differentiable, locally Euclidean, manifold introduces a variety of challenges because the analogous measures to standard Euclidean statistics are local, that is only defined within a neighbourhood of each datapoint. This is because the curvature of the space means that the connection of Riemannian geometry is path dependent. In this paper we transfer the problem to Weitzenb\u00f6ck space, which has torsion, but not curvature, meaning that parallel transport is path independent, and rather than considering geodesics, it is natural to consider autoparallels, which are `straight' in the sense that they follow the local basis vectors. We demonstrate how to generate these autoparallels in a data-driven fashion, and show that the resulting representation of the data is a useful space in which to perform further analysis.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03398",
        "title": "A GMM-Based Stair Quality Model for Human Perceived JPEG Images",
        "authors": [
            "Sudeng Hu",
            "Haiqiang Wang",
            "C.-C. Jay Kuo"
        ],
        "abstract": "Based on the notion of just noticeable differences (JND), a stair quality function (SQF) was recently proposed to model human perception on JPEG images. Furthermore, a k-means clustering algorithm was adopted to aggregate JND data collected from multiple subjects to generate a single SQF. In this work, we propose a new method to derive the SQF using the Gaussian Mixture Model (GMM). The newly derived SQF can be interpreted as a way to characterize the mean viewer experience. Furthermore, it has a lower information criterion (BIC) value than the previous one, indicating that it offers a better model. A specific example is given to demonstrate the advantages of the new approach.\n    ",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2015-11-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03607",
        "title": "Complete Dictionary Recovery over the Sphere I: Overview and the Geometric Picture",
        "authors": [
            "Ju Sun",
            "Qing Qu",
            "John Wright"
        ],
        "abstract": "We consider the problem of recovering a complete (i.e., square and invertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$ with $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is sufficiently sparse. This recovery problem is central to theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\\mathbf X_0$. In contrast, prior results based on efficient algorithms either only guarantee recovery when $\\mathbf X_0$ has $O(\\sqrt{n})$ zeros per column, or require multiple rounds of SDP relaxation to work when $\\mathbf X_0$ has $O(n^{1-\\delta})$ nonzeros per column (for any constant $\\delta \\in (0, 1)$). }\n",
        "submission_date": "2015-11-11T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03791",
        "title": "Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control",
        "authors": [
            "Fangyi Zhang",
            "J\u00fcrgen Leitner",
            "Michael Milford",
            "Ben Upcroft",
            "Peter Corke"
        ],
        "abstract": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03855",
        "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels",
        "authors": [
            "Wu-Jun Li",
            "Sheng Wang",
            "Wang-Cheng Kang"
        ],
        "abstract": "Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03908",
        "title": "Learning Human Identity from Motion Patterns",
        "authors": [
            "Natalia Neverova",
            "Christian Wolf",
            "Griffin Lacey",
            "Lex Fridman",
            "Deepak Chandra",
            "Brandon Barbello",
            "Graham Taylor"
        ],
        "abstract": "We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.03979",
        "title": "Representational Distance Learning for Deep Neural Networks",
        "authors": [
            "Patrick McClure",
            "Nikolaus Kriegeskorte"
        ],
        "abstract": "Deep neural networks (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (student) to learn from the internal representational spaces of a reference model (teacher), which could be another DNN or, in the future, a biological brain. Representational spaces of the student and the teacher are characterized by representational distance matrices (RDMs). We propose representational distance learning (RDL), a stochastic gradient descent method that drives the RDMs of the student to approximate the RDMs of the teacher. We demonstrate that RDL is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets (MNIST and CIFAR-100), while allowing for architectural differences between student and teacher. By pulling the student's RDMs towards those of the teacher, RDL significantly improved visual classification performance when compared to baseline networks that did not use transfer learning. In the future, RDL may enable combined supervised training of deep neural networks using task constraints (e.g. images and category labels) and constraints from brain-activity measurements, so as to build models that replicate the internal representational spaces of biological brains.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04024",
        "title": "Multimodal Skip-gram Using Convolutional Pseudowords",
        "authors": [
            "Zachary Seymour",
            "Yingming Li",
            "Zhongfei Zhang"
        ],
        "abstract": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional \"pseudowords:\" embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04056",
        "title": "Efficient non-greedy optimization of decision trees",
        "authors": [
            "Mohammad Norouzi",
            "Maxwell D. Collins",
            "Matthew Johnson",
            "David J. Fleet",
            "Pushmeet Kohli"
        ],
        "abstract": "Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04110",
        "title": "Going Deeper in Facial Expression Recognition using Deep Neural Networks",
        "authors": [
            "Ali Mollahosseini",
            "David Chan",
            "Mohammad H. Mahoor"
        ],
        "abstract": "Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2015-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04119",
        "title": "Action Recognition using Visual Attention",
        "authors": [
            "Shikhar Sharma",
            "Ryan Kiros",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.\n    ",
        "submission_date": "2015-11-12T00:00:00",
        "last_modified_date": "2016-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04150",
        "title": "Deep Mean Maps",
        "authors": [
            "Junier B. Oliva",
            "Danica J. Sutherland",
            "Barnab\u00e1s P\u00f3czos",
            "Jeff Schneider"
        ],
        "abstract": "The use of distributions and high-level features from deep architecture has become commonplace in modern computer vision. Both of these methodologies have separately achieved a great deal of success in many computer vision tasks. However, there has been little work attempting to leverage the power of these to methodologies jointly. To this end, this paper presents the Deep Mean Maps (DMMs) framework, a novel family of methods to non-parametrically represent distributions of features in convolutional neural network models.\n",
        "submission_date": "2015-11-13T00:00:00",
        "last_modified_date": "2021-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04599",
        "title": "DeepFool: a simple and accurate method to fool deep neural networks",
        "authors": [
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Alhussein Fawzi",
            "Pascal Frossard"
        ],
        "abstract": "State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.\n    ",
        "submission_date": "2015-11-14T00:00:00",
        "last_modified_date": "2016-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04685",
        "title": "Semi-Inner-Products for Convex Functionals and Their Use in Image Decomposition",
        "authors": [
            "Guy Gilboa"
        ],
        "abstract": "Semi-inner-products in the sense of Lumer are extended to convex functionals. This yields a Hilbert-space like structure to convex functionals in Banach spaces. In particular, a general expression for semi-inner-products with respect to one homogeneous functionals is given. Thus one can use the new operator for the analysis of total variation and higher order functionals like total-generalized-variation (TGV). Having a semi-inner-product, an angle between functions can be defined in a straightforward manner. It is shown that in the one homogeneous case the Bregman distance can be expressed in terms of this newly defined angle. In addition, properties of the semi-inner-product of nonlinear eigenfunctions induced by the functional are derived. We use this construction to state a sufficient condition for a perfect decomposition of two signals and suggest numerical measures which indicate when those conditions are approximately met.\n    ",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2015-11-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04777",
        "title": "Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian Trust-region Method",
        "authors": [
            "Ju Sun",
            "Qing Qu",
            "John Wright"
        ],
        "abstract": "We consider the problem of recovering a complete (i.e., square and invertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$ with $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is sufficiently sparse. This recovery problem is central to theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\\mathbf X_0$.\n",
        "submission_date": "2015-11-15T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04855",
        "title": "Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch",
        "authors": [
            "Lionel Pibre",
            "Pasquet J\u00e9r\u00f4me",
            "Dino Ienco",
            "Marc Chaumont"
        ],
        "abstract": "Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best \" shape \" of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2018-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04898",
        "title": "Fast clustering for scalable statistical analysis on structured images",
        "authors": [
            "Bertrand Thirion",
            "Andr\u00e9s Hoyos-Idrobo",
            "Jonas Kahn",
            "Gael Varoquaux"
        ],
        "abstract": "The use of brain images as markers for diseases or behavioral differences is challenged by the small effects size and the ensuing lack of power, an issue that has incited researchers to rely more systematically on large cohorts. Coupled with resolution increases, this leads to very large datasets. A striking example in the case of brain imaging is that of the Human Connectome Project: 20 Terabytes of data and growing. The resulting data deluge poses severe challenges regarding the tractability of some processing steps (discriminant analysis, multivariate models) due to the memory demands posed by these data. In this work, we revisit dimension reduction approaches, such as random projections, with the aim of replacing costly function evaluations by cheaper ones while decreasing the memory requirements. Specifically, we investigate the use of alternate schemes, based on fast clustering, that are well suited for signals exhibiting a strong spatial structure, such as anatomical and functional brain images. Our contribution is twofold: i) we propose a linear-time clustering scheme that bypasses the percolation issues inherent in these algorithms and thus provides compressions nearly as good as traditional quadratic-complexity variance-minimizing clustering schemes, ii) we show that cluster-based compression can have the virtuous effect of removing high-frequency noise, actually improving subsequent estimations steps. As a consequence, the proposed approach yields very accurate models on several large-scale problems yet with impressive gains in computational efficiency, making it possible to analyze large datasets.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2015-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.04906",
        "title": "Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges",
        "authors": [
            "Jaime Zaratiegui",
            "Ana Montoro",
            "Federico Castanedo"
        ],
        "abstract": "We investigated how the application of deep learning, specifically the use of convolutional networks trained with GPUs, can help to build better predictive models in telecommunication business environments, and fill this gap. In particular, we focus on the non-trivial problem of predicting customer churn in telecommunication operators. Our model, called WiseNet, consists of a convolutional network and a novel encoding method that transforms customer activity data and Call Detail Records (CDRs) into images. Experimental evaluation with several machine learning classifiers supports the ability of WiseNet for learning features when using structured input data. For this type of telecommunication business problems, we found that WiseNet outperforms machine learning models with hand-crafted features, and does not require the labor-intensive step of feature engineering. Furthermore, the same model has been applied without retraining to a different market, achieving consistent results. This confirms the generalization property of WiseNet and the ability to extract useful representations.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05099",
        "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
        "authors": [
            "Peng Zhang",
            "Yash Goyal",
            "Douglas Summers-Stay",
            "Dhruv Batra",
            "Devi Parikh"
        ],
        "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.\n    ",
        "submission_date": "2015-11-16T00:00:00",
        "last_modified_date": "2016-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05440",
        "title": "Deep multi-scale video prediction beyond mean square error",
        "authors": [
            "Michael Mathieu",
            "Camille Couprie",
            "Yann LeCun"
        ],
        "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-02-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05497",
        "title": "Learning Neural Network Architectures using Backpropagation",
        "authors": [
            "Suraj Srinivas",
            "R. Venkatesh Babu"
        ],
        "abstract": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05526",
        "title": "Learning Articulated Motion Models from Visual and Lingual Signals",
        "authors": [
            "Zhengyang Wu",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "abstract": "In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common within environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiple-part objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05622",
        "title": "Predicting distributions with Linearizing Belief Networks",
        "authors": [
            "Yann N. Dauphin",
            "David Grangier"
        ],
        "abstract": "Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output $Y$ given the input $X$. It can predict a distribution of outputs $Y$ which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset.\n    ",
        "submission_date": "2015-11-17T00:00:00",
        "last_modified_date": "2016-05-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.05846",
        "title": "Eigenspectra optoacoustic tomography achieves quantitative blood oxygenation imaging deep in tissues",
        "authors": [
            "Stratis Tzoumas",
            "Antonio Nunes",
            "Ivan Olefir",
            "Stefan Stangl",
            "Panagiotis Symvoulidis",
            "Sarah Glasl",
            "Christine Bayer",
            "Gabriele Multhoff",
            "Vasilis Ntziachristos"
        ],
        "abstract": "Light propagating in tissue attains a spectrum that varies with location due to wavelength-dependent fluence attenuation by tissue optical properties, an effect that causes spectral corruption. Predictions of the spectral variations of light fluence in tissue are challenging since the spatial distribution of optical properties in tissue cannot be resolved in high resolution or with high accuracy by current methods. Spectral corruption has fundamentally limited the quantification accuracy of optical and optoacoustic methods and impeded the long sought-after goal of imaging blood oxygen saturation (sO2) deep in tissues; a critical but still unattainable target for the assessment of oxygenation in physiological processes and disease. We discover a new principle underlying light fluence in tissues, which describes the wavelength dependence of light fluence as an affine function of a few reference base spectra, independently of the specific distribution of tissue optical properties. This finding enables the introduction of a previously undocumented concept termed eigenspectra Multispectral Optoacoustic Tomography (eMSOT) that can effectively account for wavelength dependent light attenuation without explicit knowledge of the tissue optical properties. We validate eMSOT in more than 2000 simulations and with phantom and animal measurements. We find that eMSOT can quantitatively image tissue sO2 reaching in many occasions a better than 10-fold improved accuracy over conventional spectral optoacoustic methods. Then, we show that eMSOT can spatially resolve sO2 in muscle and tumor; revealing so far unattainable tissue physiology patterns. Last, we related eMSOT readings to cancer hypoxia and found congruence between eMSOT tumor sO2 images and tissue perfusion and hypoxia maps obtained by correlative histological analysis.\n    ",
        "submission_date": "2015-11-18T00:00:00",
        "last_modified_date": "2015-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06036",
        "title": "Stochastic gradient method with accelerated stochastic dynamics",
        "authors": [
            "Masayuki Ohzeki"
        ],
        "abstract": "In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06049",
        "title": "What Objective Does Self-paced Learning Indeed Optimize?",
        "authors": [
            "Deyu Meng",
            "Qian Zhao",
            "Lu Jiang"
        ],
        "abstract": "Self-paced learning (SPL) is a recently raised methodology designed through simulating the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically substantiated to be effective in these applications. However, the investigation on its theoretical insight is still a blank. To this issue, this study attempts to provide some new theoretical understanding under the SPL scheme. Specifically, we prove that the solving strategy on SPL accords with a majorization minimization algorithm implemented on a latent objective function. Furthermore, we find that the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty (NSPR) known in statistics and machine learning. Such connection inspires us discovering more intrinsic relationship between SPL regimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under SPL can then be finely explained. We also analyze the capability of SPL on its easy loss prior embedding property, and provide an insightful interpretation to the effectiveness mechanism under previous SPL variations. Besides, we design a group-partial-order loss prior, which is especially useful to weakly labeled large-scale data processing tasks. Through applying SPL with this loss prior to the FCVID dataset, which is currently one of the biggest manually annotated video dataset, our method achieves state-of-the-art performance beyond previous methods, which further helps supports the proposed theoretical arguments.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-11-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06065",
        "title": "Deep Learning for Tactile Understanding From Visual and Haptic Data",
        "authors": [
            "Yang Gao",
            "Lisa Anne Hendricks",
            "Katherine J. Kuchenbecker",
            "Trevor Darrell"
        ],
        "abstract": "Robots which interact with the physical world will benefit from a fine-grained tactile understanding of objects and surfaces. Additionally, for certain tasks, robots may need to know the haptic properties of an object before touching it. To enable better tactile understanding for robots, we propose a method of classifying surfaces with haptic adjectives (e.g., compressible or smooth) from both visual and physical interaction data. Humans typically combine visual predictions and feedback from physical interactions to accurately predict haptic properties and interact with the world. Inspired by this cognitive pattern, we propose and explore a purely visual haptic prediction model. Purely visual models enable a robot to \"feel\" without physical interaction. Furthermore, we demonstrate that using both visual and physical interaction signals together yields more accurate haptic classification. Our models take advantage of recent advances in deep neural networks by employing a unified approach to learning features for physical interaction and visual observations. Even though we employ little domain specific knowledge, our model still achieves better results than methods based on hand-designed features.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06067",
        "title": "Convolutional neural networks with low-rank regularization",
        "authors": [
            "Cheng Tai",
            "Tong Xiao",
            "Yi Zhang",
            "Xiaogang Wang",
            "Weinan E"
        ],
        "abstract": "Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06104",
        "title": "Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction",
        "authors": [
            "Sheng-Yi Bai",
            "Sebastian Agethen",
            "Ting-Hsuan Chao",
            "Winston Hsu"
        ],
        "abstract": "The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06238",
        "title": "Multimodal sparse representation learning and applications",
        "authors": [
            "Miriam Cha",
            "Youngjune Gwon",
            "H.T. Kung"
        ],
        "abstract": "Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06241",
        "title": "Convolutional Clustering for Unsupervised Learning",
        "authors": [
            "Aysegul Dundar",
            "Jonghoon Jin",
            "Eugenio Culurciello"
        ],
        "abstract": "The task of labeling data for training deep neural networks is daunting and tedious, requiring millions of labels to achieve the current state-of-the-art results. Such reliance on large amounts of labeled data can be relaxed by exploiting hierarchical features via unsupervised learning techniques. In this work, we propose to train a deep convolutional network based on an enhanced version of the k-means clustering algorithm, which reduces the number of correlated parameters in the form of similar filters, and thus increases test categorization accuracy. We call our algorithm convolutional k-means clustering. We further show that learning the connection between the layers of a deep convolutional neural network improves its ability to be trained on a smaller amount of labeled data. Our experiments show that the proposed algorithm outperforms other techniques that learn filters unsupervised. Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test error of 0.5% on MNIST.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06281",
        "title": "Density Modeling of Images using a Generalized Normalization Transformation",
        "authors": [
            "Johannes Ball\u00e9",
            "Valero Laparra",
            "Eero P. Simoncelli"
        ],
        "abstract": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06292",
        "title": "Foveation-based Mechanisms Alleviate Adversarial Examples",
        "authors": [
            "Yan Luo",
            "Xavier Boix",
            "Gemma Roig",
            "Tomaso Poggio",
            "Qi Zhao"
        ],
        "abstract": "We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNNs) fail, can be alleviated with a mechanism based on foveations---applying the CNN in different image regions. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting as a linear classifier: CNNs act locally linearly to changes in the image regions with objects recognized by the CNN, and in other regions the CNN may act non-linearly. Then, we corroborate that when the neural responses are linear, applying the foveation mechanism to the adversarial example tends to significantly reduce the effect of the perturbation. This is because, hypothetically, the CNNs for ImageNet are robust to changes of scale and translation of the object produced by the foveation, but this property does not generalize to transformations of the perturbation. As a result, the accuracy after a foveation is almost the same as the accuracy of the CNN without the adversarial perturbation, even if the adversarial perturbation is calculated taking into account a foveation.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06306",
        "title": "Robust Convolutional Neural Networks under Adversarial Noise",
        "authors": [
            "Jonghoon Jin",
            "Aysegul Dundar",
            "Eugenio Culurciello"
        ],
        "abstract": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called \"adversarial examples\". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06309",
        "title": "Spatio-temporal video autoencoder with differentiable memory",
        "authors": [
            "Viorica Patraucean",
            "Ankur Handa",
            "Roberto Cipolla"
        ],
        "abstract": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We present one direct application of the proposed framework in weakly-supervised semantic segmentation of videos through label propagation using optical flow.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06335",
        "title": "Unsupervised Deep Embedding for Clustering Analysis",
        "authors": [
            "Junyuan Xie",
            "Ross Girshick",
            "Ali Farhadi"
        ],
        "abstract": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06340",
        "title": "Robust Classification by Pre-conditioned LASSO and Transductive Diffusion Component Analysis",
        "authors": [
            "Yanwei Fu",
            "De-An Huang",
            "Leonid Sigal"
        ],
        "abstract": "Modern machine learning-based recognition approaches require large-scale datasets with large number of labelled training images. However, such datasets are inherently difficult and costly to collect and annotate. Hence there is a great and growing interest in automatic dataset collection methods that can leverage the web. % which are collected % in a cheap, efficient and yet unreliable way. Collecting datasets in this way, however, requires robust and efficient ways for detecting and excluding outliers that are common and prevalent. % Outliers are thus a % prominent treat of using these dataset. So far, there have been a limited effort in machine learning community to directly detect outliers for robust classification. Inspired by the recent work on Pre-conditioned LASSO, this paper formulates the outlier detection task using Pre-conditioned LASSO and employs \\red{unsupervised} transductive diffusion component analysis to both integrate the topological structure of the data manifold, from labeled and unlabeled instances, and reduce the feature dimensionality. Synthetic experiments as well as results on two real-world classification tasks show that our framework can robustly detect the outliers and improve classification.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2019-12-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06348",
        "title": "How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?",
        "authors": [
            "Junghwan Cho",
            "Kyewook Lee",
            "Ellie Shin",
            "Garry Choy",
            "Synho Do"
        ],
        "abstract": "The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06359",
        "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications",
        "authors": [
            "Bihan Wen",
            "Saiprasad Ravishankar",
            "Yoram Bresler"
        ],
        "abstract": "Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2017-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06361",
        "title": "Order-Embeddings of Images and Language",
        "authors": [
            "Ivan Vendrov",
            "Ryan Kiros",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06362",
        "title": "Efficient inference in occlusion-aware generative models of images",
        "authors": [
            "Jonathan Huang",
            "Kevin Murphy"
        ],
        "abstract": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image --- and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06380",
        "title": "Unsupervised Learning of Visual Structure using Predictive Generative Networks",
        "authors": [
            "William Lotter",
            "Gabriel Kreiman",
            "David Cox"
        ],
        "abstract": "The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06381",
        "title": "Manifold Regularized Deep Neural Networks using Adversarial Examples",
        "authors": [
            "Taehoon Lee",
            "Minsuk Choi",
            "Sungroh Yoon"
        ],
        "abstract": "Learning meaningful representations using deep neural networks involves designing efficient training schemes and well-structured networks. Currently, the method of stochastic gradient descent that has a momentum with dropout is one of the most popular training protocols. Based on that, more advanced methods (i.e., Maxout and Batch Normalization) have been proposed in recent years, but most still suffer from performance degradation caused by small perturbations, also known as adversarial examples. To address this issue, we propose manifold regularized networks (MRnet) that utilize a novel training objective function that minimizes the difference between multi-layer embedding results of samples and those adversarial. Our experimental results demonstrated that MRnet is more resilient to adversarial examples and helps us to generalize representations on manifolds. Furthermore, combining MRnet and dropout allowed us to achieve competitive classification performances for three well-known benchmarks: MNIST, CIFAR-10, and SVHN.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06409",
        "title": "Learning to Generate Images with Perceptual Similarity Metrics",
        "authors": [
            "Jake Snell",
            "Karl Ridgeway",
            "Renjie Liao",
            "Brett D. Roads",
            "Michael C. Mozer",
            "Richard S. Zemel"
        ],
        "abstract": "Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures ($\\ell_1$ and $\\ell_2$ distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Finally, we demonstrate the superiority of perceptually-optimized networks for super-resolution imaging. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2017-01-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06412",
        "title": "QBDC: Query by dropout committee for training deep supervised architecture",
        "authors": [
            "Melanie Ducoffe",
            "Frederic Precioso"
        ],
        "abstract": "  While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 % from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2015-11-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06421",
        "title": "Deep Manifold Traversal: Changing Labels with Convolutional Features",
        "authors": [
            "Jacob R. Gardner",
            "Paul Upchurch",
            "Matt J. Kusner",
            "Yixuan Li",
            "Kilian Q. Weinberger",
            "Kavita Bala",
            "John E. Hopcroft"
        ],
        "abstract": "Many tasks in computer vision can be cast as a \"label changing\" problem, where the goal is to make a semantic change to the appearance of an image or some subject in an image in order to alter the class membership. Although successful task-specific methods have been developed for some label changing applications, to date no general purpose method exists. Motivated by this we propose deep manifold traversal, a method that addresses the problem in its most general form: it first approximates the manifold of natural images then morphs a test image along a traversal path away from a source class and towards a target class while staying near the manifold throughout. The resulting algorithm is surprisingly effective and versatile. It is completely data driven, requiring only an example set of images from the desired source and target domains. We demonstrate deep manifold traversal on highly diverse label changing tasks: changing an individual's appearance (age and hair color), changing the season of an outdoor image, and transforming a city skyline towards nighttime.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06428",
        "title": "A Controller-Recognizer Framework: How necessary is recognition for control?",
        "authors": [
            "Marcin Moczulski",
            "Kelvin Xu",
            "Aaron Courville",
            "Kyunghyun Cho"
        ],
        "abstract": "Recently there has been growing interest in building active visual object recognizers, as opposed to the usual passive recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize these recently proposed end-to-end active visual recognizers into a controller-recognizer framework. A model in the controller-recognizer framework consists of a controller, which interfaces with an external manipulator, and a recognizer which classifies the visual input adjusted by the manipulator. We describe two most recently proposed controller-recognizer models: recurrent attention model and spatial transformer network as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly, or completely, couple a controller and recognizer. We ask a question whether this tight coupling is necessary, and try to answer this empirically by building a controller-recognizer model with a decoupled controller and recognizer. Our experiments revealed that it is not always necessary to tightly couple them and that by decoupling a controller and recognizer, there is a possibility of building a generic controller that is pretrained and works together with any subsequent recognizer.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06434",
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "authors": [
            "Alec Radford",
            "Luke Metz",
            "Soumith Chintala"
        ],
        "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06442",
        "title": "Fast Metric Learning For Deep Neural Networks",
        "authors": [
            "Henry Gouk",
            "Bernhard Pfahringer",
            "Michael Cree"
        ],
        "abstract": "Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06448",
        "title": "Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks",
        "authors": [
            "Pouya Bashivan",
            "Irina Rish",
            "Mohammed Yeasin",
            "Noel Codella"
        ],
        "abstract": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.\n    ",
        "submission_date": "2015-11-19T00:00:00",
        "last_modified_date": "2016-02-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06566",
        "title": "Acceleration of the PDHGM on strongly convex subspaces",
        "authors": [
            "Tuomo Valkonen",
            "Thomas Pock"
        ],
        "abstract": "We propose several variants of the primal-dual method due to Chambolle and Pock. Without requiring full strong convexity of the objective functions, our methods are accelerated on subspaces with strong convexity. This yields mixed rates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to the dual sequence, and the residual part of the primal sequence. We demonstrate the efficacy of the proposed methods on image processing problems lacking strong convexity, such as total generalised variation denoising and total variation deblurring.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06624",
        "title": "TEMPO: Feature-Endowed Teichm\u00fcller Extremal Mappings of Point Clouds",
        "authors": [
            "Ting Wei Meng",
            "Gary Pui-Tung Choi",
            "Lok Ming Lui"
        ],
        "abstract": "In recent decades, the use of 3D point clouds has been widespread in computer industry. The development of techniques in analyzing point clouds is increasingly important. In particular, mapping of point clouds has been a challenging problem. In this paper, we develop a discrete analogue of the Teichm\u00fcller extremal mappings, which guarantee uniform conformality distortions, on point cloud surfaces. Based on the discrete analogue, we propose a novel method called TEMPO for computing Teichm\u00fcller extremal mappings between feature-endowed point clouds. Using our proposed method, the Teichm\u00fcller metric is introduced for evaluating the dissimilarity of point clouds. Consequently, our algorithm enables accurate recognition and classification of point clouds. Experimental results demonstrate the effectiveness of our proposed method.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2016-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06631",
        "title": "Multi-Contrast MRI Reconstruction with Structure-Guided Total Variation",
        "authors": [
            "Matthias J. Ehrhardt",
            "Marta M. Betcke"
        ],
        "abstract": "Magnetic resonance imaging (MRI) is a versatile imaging technique that allows different contrasts depending on the acquisition parameters. Many clinical imaging studies acquire MRI data for more than one of these contrasts---such as for instance T1 and T2 weighted images---which makes the overall scanning procedure very time consuming. As all of these images show the same underlying anatomy one can try to omit unnecessary measurements by taking the similarity into account during reconstruction. We will discuss two modifications of total variation---based on i) location and ii) direction---that take structural a priori knowledge into account and reduce to total variation in the degenerate case when no structural knowledge is available. We solve the resulting convex minimization problem with the alternating direction method of multipliers that separates the forward operator from the prior. For both priors the corresponding proximal operator can be implemented as an extension of the fast gradient projection method on the dual problem for total variation. We tested the priors on six data sets that are based on phantoms and real MRI images. In all test cases exploiting the structural information from the other contrast yields better results than separate reconstruction with total variation in terms of standard metrics like peak signal-to-noise ratio and structural similarity index. Furthermore, we found that exploiting the two dimensional directional information results in images with well defined edges, superior to those reconstructed solely using a priori information about the edge location.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06683",
        "title": "Top-k Multiclass SVM",
        "authors": [
            "Maksim Lapin",
            "Matthias Hein",
            "Bernt Schiele"
        ],
        "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.\n    ",
        "submission_date": "2015-11-20T00:00:00",
        "last_modified_date": "2015-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06811",
        "title": "Learning visual groups from co-occurrences in space and time",
        "authors": [
            "Phillip Isola",
            "Daniel Zoran",
            "Dilip Krishnan",
            "Edward H. Adelson"
        ],
        "abstract": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2015-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06855",
        "title": "Unsupervised learning of object semantic parts from internal states of CNNs by population encoding",
        "authors": [
            "Jianyu Wang",
            "Zhishuai Zhang",
            "Cihang Xie",
            "Vittal Premachandran",
            "Alan Yuille"
        ],
        "abstract": "We address the key question of how object part representations can be found from the internal states of CNNs that are trained for high-level tasks, such as object classification. This work provides a new unsupervised method to learn semantic parts and gives new understanding of the internal representations of CNNs. Our technique is based on the hypothesis that semantic parts are represented by populations of neurons rather than by single filters. We propose a clustering technique to extract part representations, which we call Visual Concepts. We show that visual concepts are semantically coherent in that they represent semantic parts, and visually coherent in that corresponding image patches appear very similar. Also, visual concepts provide full spatial coverage of the parts of an object, rather than a few sparse parts as is typically found in keypoint annotations. Furthermore, We treat single visual concept as part detector and evaluate it for keypoint detection using the PASCAL3D+ dataset and for part detection using our newly annotated ImageNetPart dataset. The experiments demonstrate that visual concepts can be used to detect parts. We also show that some visual concepts respond to several semantic parts, provided these parts are visually similar. Thus visual concepts have the essential properties: semantic meaning and detection capability. Note that our ImageNetPart dataset gives rich part annotations which cover the whole object, making it useful for other part-related applications.\n    ",
        "submission_date": "2015-11-21T00:00:00",
        "last_modified_date": "2016-11-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.06951",
        "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks",
        "authors": [
            "Leslie N. Smith",
            "Emily M. Hand",
            "Timothy Doster"
        ],
        "abstract": "We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.\n    ",
        "submission_date": "2015-11-22T00:00:00",
        "last_modified_date": "2015-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07106",
        "title": "Multi-Volume High Resolution RGB-D Mapping with Dynamic Volume Placement",
        "authors": [
            "Michael Salvato",
            "Ross Finman",
            "John Leonard"
        ],
        "abstract": "We present a novel RGB-D mapping system for generating 3D maps over spatially extended regions with higher resolution than current methods using multiple, dynamically placed mapping volumes. Our method takes in RGB-D frames and dynamically assigns multiple mapping volumes to the environment, exchanging mapping volumes between the CPU and GPU. Mapping volumes are added or removed as needed to allow for spatially extended, high resolution mapping. Our system is designed to maximize the resolution possible for such volumetric methods, while working on an unbounded space.\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07125",
        "title": "What Happened to My Dog in That Network: Unraveling Top-down Generators in Convolutional Neural Networks",
        "authors": [
            "Patrick W. Gallagher",
            "Shuai Tang",
            "Zhuowen Tu"
        ],
        "abstract": "Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using \"generators\" corresponding to the network internal effects of three types of transformation (each a restriction of a general affine transformation): rotation, scaling, and translation. We demonstrate how these learned generators can be used to transfer top-down information to novel settings, as mediated by the \"feature flows\" that the transformations (and the associated generators) correspond to inside the network. Specifically, we explore three aspects: 1) using generators as part of a method for synthesizing transformed images --- given a previously unseen image, produce versions of that image corresponding to one or more specified transformations, 2) \"zero-shot learning\" --- when provided with a feature flow corresponding to the effect of a transformation of unknown amount, leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation, even for amounts never observed during training, and 3) (inside-CNN) \"data augmentation\" --- improve the classification performance of an existing network by using the learned generators to directly provide additional training \"inside the CNN\".\n    ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2015-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07376",
        "title": "CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android",
        "authors": [
            "Seyyed Salar Latifi Oskouei",
            "Hossein Golestani",
            "Matin Hashemi",
            "Soheil Ghiasi"
        ],
        "abstract": "Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid, for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at ",
        "submission_date": "2015-11-23T00:00:00",
        "last_modified_date": "2016-10-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07607",
        "title": "Fine-Grain Annotation of Cricket Videos",
        "authors": [
            "Rahul Anand Sharma",
            "Pramod Sankar K",
            "CV Jawahar"
        ],
        "abstract": "The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of ctions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into \"scenes\", by utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labeled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2017-09-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.07608",
        "title": "Picking a Conveyor Clean by an Autonomously Learning Robot",
        "authors": [
            "Janne V. Kujala",
            "Tuomas J. Lukka",
            "Harri Holopainen"
        ],
        "abstract": "We present a research picking prototype related to our company's industrial waste sorting application. The goal of the prototype is to be as autonomous as possible and it both calibrates itself and improves its picking with minimal human intervention. The system learns to pick objects better based on a feedback sensor in its gripper and uses machine learning to choosing the best proposal from a random sample produced by simple hard-coded geometric models. We show experimentally the system improving its picking autonomously by measuring the pick success rate as function of time. We also show how this system can pick a conveyor belt clean, depositing 70 out of 80 objects in a difficult to manipulate pile of novel objects into the correct chute. We discuss potential improvements and next steps in this direction.\n    ",
        "submission_date": "2015-11-24T00:00:00",
        "last_modified_date": "2015-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08032",
        "title": "Learning to detect video events from zero or very few video examples",
        "authors": [
            "Christos Tzelepis",
            "Damianos Galanopoulos",
            "Vasileios Mezaris",
            "Ioannis Patras"
        ],
        "abstract": "In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of ``related'' videos. For learning only from an event's textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit ``related'' event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.08458",
        "title": "An Introduction to Convolutional Neural Networks",
        "authors": [
            "Keiron O'Shea",
            "Ryan Nash"
        ],
        "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.\n",
        "submission_date": "2015-11-26T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09120",
        "title": "Coresets for Kinematic Data: From Theorems to Real-Time Systems",
        "authors": [
            "Soliman Nasser",
            "Ibrahim Jubran",
            "Dan Feldman"
        ],
        "abstract": "A coreset (or core-set) of a dataset is its semantic compression with respect to a set of queries, such that querying the (small) coreset provably yields an approximate answer to querying the original (full) dataset. In the last decade, coresets provided breakthroughs in theoretical computer science for approximation algorithms, and more recently, in the machine learning community for learning \"Big data\". However, we are not aware of real-time systems that compute coresets in a rate of dozens of frames per second. In this paper we suggest a framework to turn theorems to such systems using coresets. We begin with a proof of independent interest, that any set of $n$ matrices in $\\mathbb{R}^{d\\times d}$ whose sum is $S$, has a positively weighted subset whose sum has the same center of mass (mean) and orientation (left+right singular vectors) as $S$, and consists of $O(dr)$ matrices (independent of $n$), where $r\\leq d$ is the rank of $S$. We provide an algorithm that computes this (core) set in one pass over possibly infinite stream of matrices in $d^{O(1)}$ time per matrix insertion. By maintaining such a coreset for kinematic (moving) set of $n$ points, we can run pose-estimation algorithms, such as Kabsch or PnP, on the small coresets, instead of the $n$ points, in real-time using weak devices, while obtaining the same results. This enabled us to implement a low-cost ($<\\$100$) IoT wireless system that tracks a toy (and harmless) quadcopter which guides guests to a desired room (in a hospital, mall, hotel, museum, etc.) with no help of additional human or remote controller. We hope that our framework will encourage researchers outside the theoretical community to design and use coresets in future systems and papers. To this end, we provide extensive experimental results on both synthetic and real data, as well as a link to the open code of our system and algorithms.\n    ",
        "submission_date": "2015-11-30T00:00:00",
        "last_modified_date": "2017-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1511.09123",
        "title": "A Short Survey on Data Clustering Algorithms",
        "authors": [
            "Ka-Chun Wong"
        ],
        "abstract": "With rapidly increasing data, clustering algorithms are important tools for data analytics in modern research. They have been successfully applied to a wide range of domains; for instance, bioinformatics, speech recognition, and financial analysis. Formally speaking, given a set of data instances, a clustering algorithm is expected to divide the set of data instances into the subsets which maximize the intra-subset similarity and inter-subset dissimilarity, where a similarity measure is defined beforehand. In this work, the state-of-the-arts clustering algorithms are reviewed from design concept to methodology; Different clustering paradigms are discussed. Advanced clustering algorithms are also discussed. After that, the existing clustering evaluation metrics are reviewed. A summary with future insights is provided at the end.\n    ",
        "submission_date": "2015-11-25T00:00:00",
        "last_modified_date": "2015-11-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00101",
        "title": "Dynamic Parallel and Distributed Graph Cuts",
        "authors": [
            "Miao Yu",
            "Shuhan Shen",
            "Zhanyi Hu"
        ],
        "abstract": "Graph-cuts are widely used in computer vision. In order to speed up the optimization process and improve the scalability for large graphs, Strandmark and Kahl introduced a splitting method to split a graph into multiple subgraphs for parallel computation in both shared and distributed memory models. However, this parallel algorithm (parallel BK-algorithm) does not have a polynomial bound on the number of iterations and is found non-convergent in some cases due to the possible multiple optimal solutions of its sub-problems.\n",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00242",
        "title": "Towards Dropout Training for Convolutional Neural Networks",
        "authors": [
            "Haibing Wu",
            "Xiaodong Gu"
        ],
        "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00298",
        "title": "On Optical Flow Models for Variational Motion Estimation",
        "authors": [
            "Martin Burger",
            "Hendrik Dirks",
            "Lena Frerking"
        ],
        "abstract": "The aim of this paper is to discuss and evaluate total variation based regularization methods for motion estimation, with particular focus on optical flow models. In addition to standard $L^2$ and $L^1$ data fidelities we give an overview of different variants of total variation regularization obtained from combination with higher order models and a unified computational optimization approach based on primal-dual methods. Moreover, we extend the models by Bregman iterations and provide an inverse problems perspective to the analysis of variational optical flow models. A particular focus of the paper is the quantitative evaluation of motion estimation, which is a difficult and often underestimated task. We discuss several approaches for quality measures of motion estimation and apply them to compare the previously discussed regularization approaches.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00486",
        "title": "Loss Functions for Top-k Error: Analysis and Insights",
        "authors": [
            "Maksim Lapin",
            "Matthias Hein",
            "Bernt Schiele"
        ],
        "abstract": "In order to push the performance on realistic computer vision tasks, the number of classes in modern benchmark datasets has significantly increased in recent years. This increase in the number of classes comes along with increased ambiguity between the class labels, raising the question if top-1 error is the right performance measure. In this paper, we provide an extensive comparison and evaluation of established multiclass methods comparing their top-k performance both from a practical as well as from a theoretical perspective. Moreover, we introduce novel top-k loss functions as modifications of the softmax and the multiclass SVM losses and provide efficient optimization schemes for them. In the experiments, we compare on various datasets all of the proposed and established methods for top-k error optimization. An interesting insight of this paper is that the softmax loss yields competitive top-k performance for all k simultaneously. For a specific top-k error, our new top-k losses lead typically to further improvements while being faster to train than the softmax.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2016-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00504",
        "title": "Efficient Edge Detection on Low-Cost FPGAs",
        "authors": [
            "Jamie Schiel",
            "Andrew Bainbridge-Smith"
        ],
        "abstract": "Improving the efficiency of edge detection in embedded applications, such as UAV control, is critical for reducing system cost and power dissipation. Field programmable gate arrays (FPGA) are a good platform for making improvements because of their specialised internal structure. However, current FPGA edge detectors do not exploit this structure well. A new edge detection architecture is proposed that is better optimised for FPGAs. The basis of the architecture is the Sobel edge kernels that are shown to be the most suitable because of their separability and absence of multiplications. Edge intensities are calculated with a new 4:2 compressor that consists of two custom-designed 3:2 compressors. Addition speed is increased by breaking carry propagation chains with look-ahead logic. Testing of the design showed it gives a 28% increase in speed and 4.4% reduction in area over previous equivalent designs, which demonstrated that it will lower the cost of edge detection systems, dissipate less power and still maintain high-speed control.\n    ",
        "submission_date": "2015-12-01T00:00:00",
        "last_modified_date": "2015-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00570",
        "title": "Attribute2Image: Conditional Image Generation from Visual Attributes",
        "authors": [
            "Xinchen Yan",
            "Jimei Yang",
            "Kihyuk Sohn",
            "Honglak Lee"
        ],
        "abstract": "This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-10-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00622",
        "title": "Continuous and Simultaneous Gesture and Posture Recognition for Commanding a Robotic Wheelchair; Towards Spotting the Signal Patterns",
        "authors": [
            "Ali Boyali",
            "Naohisa Hashimoto",
            "Manolya Kavakli"
        ],
        "abstract": "Spotting signal patterns with varying lengths has been still an open problem in the literature. In this study, we describe a signal pattern recognition approach for continuous and simultaneous classification of a tracked hand's posture and gestures and map them to steering commands for control of a robotic wheelchair. The developed methodology not only affords 100\\% recognition accuracy on a streaming signal for continuous recognition, but also brings about a new perspective for building a training dictionary which eliminates human intervention to spot the gesture or postures on a training signal. In the training phase we employ a state of art subspace clustering method to find the most representative state samples. The recognition and training framework reveal boundaries of the patterns on the streaming signal with a successive decision tree structure intrinsically. We make use of the Collaborative ans Block Sparse Representation based classification methods for continuous gesture and posture recognition.\n    ",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2015-12-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.00743",
        "title": "Recognizing Semantic Features in Faces using Deep Learning",
        "authors": [
            "Amogh Gudi"
        ],
        "abstract": "The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images.\n",
        "submission_date": "2015-12-02T00:00:00",
        "last_modified_date": "2016-10-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01400",
        "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks",
        "authors": [
            "Haibing Wu",
            "Xiaodong Gu"
        ],
        "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01413",
        "title": "Computational Imaging for VLBI Image Reconstruction",
        "authors": [
            "Katherine L. Bouman",
            "Michael D. Johnson",
            "Daniel Zoran",
            "Vincent L. Fish",
            "Sheperd S. Doeleman",
            "William T. Freeman"
        ],
        "abstract": "Very long baseline interferometry (VLBI) is a technique for imaging celestial radio emissions by simultaneously observing a source from telescopes distributed across Earth. The challenges in reconstructing images from fine angular resolution VLBI data are immense. The data is extremely sparse and noisy, thus requiring statistical image models such as those designed in the computer vision community. In this paper we present a novel Bayesian approach for VLBI image reconstruction. While other methods often require careful tuning and parameter selection for different types of data, our method (CHIRP) produces good results under different settings such as low SNR or extended emission. The success of our method is demonstrated on realistic synthetic experiments as well as publicly available real data. We present this problem in a way that is accessible to members of the community, and provide a dataset website (",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01525",
        "title": "Learning the Semantics of Manipulation Action",
        "authors": [
            "Yezhou Yang",
            "Yiannis Aloimonos",
            "Cornelia Fermuller",
            "Eren Erdal Aksoy"
        ],
        "abstract": "In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs $\\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn the $\\lambda$-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2015-12-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01596",
        "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe",
        "authors": [
            "Volodymyr Turchenko",
            "Artur Luczak"
        ],
        "abstract": "The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe. The proposed model of convolutional auto-encoder does not have pooling/unpooling layers yet. The results of our experimental research show comparable accuracy of dimensionality reduction in comparison with a classic auto-encoder on the example of MNIST dataset.\n    ",
        "submission_date": "2015-12-04T00:00:00",
        "last_modified_date": "2016-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01927",
        "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation",
        "authors": [
            "Haoran Chen",
            "Yanfeng Sun",
            "Junbin Gao",
            "Yongli Hu"
        ],
        "abstract": "The paper addresses the problem of optimizing a class of composite functions on Riemannian manifolds and a new first order optimization algorithm (FOA) with a fast convergence rate is proposed. Through the theoretical analysis for FOA, it has been proved that the algorithm has quadratic convergence. The experiments in the matrix completion task show that FOA has better performance than other first order optimization methods on Riemannian manifolds. A fast subspace pursuit method based on FOA is proposed to solve the low-rank representation model based on augmented Lagrange method on the low rank matrix variety. Experimental results on synthetic and real data sets are presented to demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in terms of faster convergence and higher accuracy.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.01979",
        "title": "Hyperspectral Chemical Plume Detection Algorithms Based On Multidimensional Iterative Filtering Decomposition",
        "authors": [
            "Antonio Cicone",
            "Jingfang Liu",
            "Haomin Zhou"
        ],
        "abstract": "Chemicals released in the air can be extremely dangerous for human beings and the environment. Hyperspectral images can be used to identify chemical plumes, however the task can be extremely challenging. Assuming we know a priori that some chemical plume, with a known frequency spectrum, has been photographed using a hyperspectral sensor, we can use standard techniques like the so called matched filter or adaptive cosine estimator, plus a properly chosen threshold value, to identify the position of the chemical plume. However, due to noise and sensors fault, the accurate identification of chemical pixels is not easy even in this apparently simple situation. In this paper we present a post-processing tool that, in a completely adaptive and data driven fashion, allows to improve the performance of any classification methods in identifying the boundaries of a plume. This is done using the Multidimensional Iterative Filtering (MIF) algorithm (",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02097",
        "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based Parameter-Insensitive Clustering Method",
        "authors": [
            "Teng Qiu",
            "Yongjie Li"
        ],
        "abstract": "Most density-based clustering methods largely rely on how well the underlying density is estimated. However, density estimation itself is also a challenging problem, especially the determination of the kernel bandwidth. A large bandwidth could lead to the over-smoothed density estimation in which the number of density peaks could be less than the true clusters, while a small bandwidth could lead to the under-smoothed density estimation in which spurious density peaks, or called the \"ripple noise\", would be generated in the estimated density. In this paper, we propose a density-based hierarchical clustering method, called the Deep Nearest Neighbor Descent (D-NND), which could learn the underlying density structure layer by layer and capture the cluster structure at the same time. The over-smoothed density estimation could be largely avoided and the negative effect of the under-estimated cases could be also largely reduced. Overall, D-NND presents not only the strong capability of discovering the underlying cluster structure but also the remarkable reliability due to its insensitivity to parameters.\n    ",
        "submission_date": "2015-12-07T00:00:00",
        "last_modified_date": "2015-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02766",
        "title": "Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple Motion Models",
        "authors": [
            "Erkan Bostanci",
            "Betul Bostanci",
            "Nadia Kanwal",
            "Adrian F. Clark"
        ],
        "abstract": "A tracking system that will be used for Augmented Reality (AR) applications has two main requirements: accuracy and frame rate. The first requirement is related to the performance of the pose estimation algorithm and how accurately the tracking system can find the position and orientation of the user in the environment. Accuracy problems of current tracking devices, considering that they are low-cost devices, cause static errors during this motion estimation process. The second requirement is related to dynamic errors (the end-to-end system delay; occurring because of the delay in estimating the motion of the user and displaying images based on this estimate. This paper investigates combining the vision-based estimates with measurements from other sensors, GPS and IMU, in order to improve the tracking accuracy in outdoor environments. The idea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a novel fuzzy rule-based approach to decide on the model that results in improved accuracy and faster convergence for the fusion filter. Results show that the developed tracking system is more accurate than a conventional GPS-IMU fusion approach due to additional estimates from a camera and fuzzy motion models. The paper also presents an application in cultural heritage context.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.02914",
        "title": "Yet Another Statistical Analysis of Bob Ross Paintings",
        "authors": [
            "Christopher Steven Marcum"
        ],
        "abstract": "In this paper, we analyze a sample of clippings from paintings by the late artist Bob Ross. Previous work focused on the qualitative themes of his paintings (Hickey, 2014); here, we expand on that line of research by considering the colorspace and luminosity values as our data. Our results demonstrate the subtle aesthetics of the average Ross painting, the common variation shared by his paintings, and the structure of the relationships between each painting in our sample. We reveal, for the first time, renderings of the average paintings and introduce \"eigenross\" components to identify and evaluate shared variance. Additionally, all data and code are embedded in this document to encourage future research, and, in the spirit of Bob Ross, to teach others how to do so.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.03012",
        "title": "ShapeNet: An Information-Rich 3D Model Repository",
        "authors": [
            "Angel X. Chang",
            "Thomas Funkhouser",
            "Leonidas Guibas",
            "Pat Hanrahan",
            "Qixing Huang",
            "Zimo Li",
            "Silvio Savarese",
            "Manolis Savva",
            "Shuran Song",
            "Hao Su",
            "Jianxiong Xiao",
            "Li Yi",
            "Fisher Yu"
        ],
        "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.\n    ",
        "submission_date": "2015-12-09T00:00:00",
        "last_modified_date": "2015-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04208",
        "title": "Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten Actions",
        "authors": [
            "Chenxia Wu",
            "Jiemi Zhang",
            "Bart Selman",
            "Silvio Savarese",
            "Ashutosh Saxena"
        ],
        "abstract": "We present a robotic system that watches a human using a Kinect v2 RGB-D sensor, detects what he forgot to do while performing an activity, and if necessary reminds the person using a laser pointer to point out the related object. Our simple setup can be easily deployed on any assistive robot.\n",
        "submission_date": "2015-12-14T00:00:00",
        "last_modified_date": "2015-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04354",
        "title": "A proposal project for a blind image quality assessment by learning distortions from the full reference image quality assessments",
        "authors": [
            "St\u00e9fane Paris"
        ],
        "abstract": "This short paper presents a perspective plan to build a null reference image quality assessment. Its main goal is to deliver both the objective score and the distortion map for a given distorted image without the knowledge of its reference image.\n    ",
        "submission_date": "2015-11-04T00:00:00",
        "last_modified_date": "2015-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.04636",
        "title": "Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic Resonance Imaging via a Stochastically Fully-Connected Joint Conditional Random Field Model",
        "authors": [
            "Ameneh Boroomand",
            "Mohammad Javad Shafiee",
            "Farzad Khalvati",
            "Masoom A. Haider",
            "Alexander Wong"
        ],
        "abstract": "Diffusion weighted magnetic resonance imaging (DW-MR) is a powerful tool in imaging-based prostate cancer screening and detection. Endorectal coils are commonly used in DW-MR imaging to improve the signal-to-noise ratio (SNR) of the acquisition, at the expense of significant intensity inhomogeneities (bias field) that worsens as we move away from the endorectal coil. The presence of bias field can have a significant negative impact on the accuracy of different image analysis tasks, as well as prostate tumor localization, thus leading to increased inter- and intra-observer variability. Retrospective bias correction approaches are introduced as a more efficient way of bias correction compared to the prospective methods such that they correct for both of the scanner and anatomy-related bias fields in MR imaging. Previously proposed retrospective bias field correction methods suffer from undesired noise amplification that can reduce the quality of bias-corrected DW-MR image. Here, we propose a unified data reconstruction approach that enables joint compensation of bias field as well as data noise in DW-MR imaging. The proposed noise-compensated, bias-corrected (NCBC) data reconstruction method takes advantage of a novel stochastically fully connected joint conditional random field (SFC-JCRF) model to mitigate the effects of data noise and bias field in the reconstructed MR data. The proposed NCBC reconstruction method was tested on synthetic DW-MR data, physical DW-phantom as well as real DW-MR data all acquired using endorectal MR coil. Both qualitative and quantitative analysis illustrated that the proposed NCBC method can achieve improved image quality when compared to other tested bias correction methods. As such, the proposed NCBC method may have potential as a useful retrospective approach for improving the consistency of image interpretations.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2016-07-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.05010",
        "title": "Multiple penalized principal curves: analysis and computation",
        "authors": [
            "Slav Kirov",
            "Dejan Slep\u010dev"
        ],
        "abstract": "We study the problem of finding the one-dimensional structure in a given data set. In other words we consider ways to approximate a given measure (data) by curves. We consider an objective functional whose minimizers are a regularization of principal curves and introduce a new functional which allows for multiple curves. We prove the existence of minimizers and establish their basic properties. We develop an efficient algorithm for obtaining (near) minimizers of the functional. While both of the functionals used are nonconvex, we argue that enlarging the configuration space to allow for multiple curves leads to a simpler energy landscape with fewer undesirable (high-energy) local minima. Furthermore we note that the approach proposed is able to find the one-dimensional structure even for data with considerable amount of noise.\n    ",
        "submission_date": "2015-12-15T00:00:00",
        "last_modified_date": "2016-08-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06216",
        "title": "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines",
        "authors": [
            "Hao Zhang",
            "Zhiting Hu",
            "Jinliang Wei",
            "Pengtao Xie",
            "Gunhee Kim",
            "Qirong Ho",
            "Eric Xing"
        ],
        "abstract": "Deep learning (DL) has achieved notable successes in many machine learning tasks. A number of frameworks have been developed to expedite the process of designing and training deep neural networks (DNNs), such as Caffe, Torch and Theano. Currently they can harness multiple GPUs on a single machine, but are unable to use GPUs that are distributed across multiple machines; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up DL. However, the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization.\n",
        "submission_date": "2015-12-19T00:00:00",
        "last_modified_date": "2015-12-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06337",
        "title": "Kernel principal component analysis network for image classification",
        "authors": [
            "Dan Wu",
            "Jiasong Wu",
            "Rui Zeng",
            "Longyu Jiang",
            "Lotfi Senhadji",
            "Huazhong Shu"
        ],
        "abstract": "In order to classify the nonlinear feature with linear classifier and improve the classification accuracy, a deep learning network named kernel principal component analysis network (KPCANet) is proposed. First, mapping the data into higher space with kernel principal component analysis to make the data linearly separable. Then building a two-layer KPCANet to obtain the principal components of image. Finally, classifying the principal components with linearly classifier. Experimental results show that the proposed KPCANet is effective in face recognition, object recognition and hand-writing digits recognition, it also outperforms principal component analysis network (PCANet) generally as well. Besides, KPCANet is invariant to illumination and stable to occlusion and slight deformation.\n    ",
        "submission_date": "2015-12-20T00:00:00",
        "last_modified_date": "2015-12-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06658",
        "title": "Deep Learning for Surface Material Classification Using Haptic And Visual Information",
        "authors": [
            "Haitian Zheng",
            "Lu Fang",
            "Mengqi Ji",
            "Matti Strese",
            "Yigitcan Ozer",
            "Eckehard Steinbach"
        ],
        "abstract": "When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2016-05-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06730",
        "title": "Multilinear Subspace Clustering",
        "authors": [
            "Eric Kernfeld",
            "Nathan Majumder",
            "Shuchin Aeron",
            "Misha Kilmer"
        ],
        "abstract": "In this paper we present a new model and an algorithm for unsupervised clustering of 2-D data such as images. We assume that the data comes from a union of multilinear subspaces (UOMS) model, which is a specific structured case of the much studied union of subspaces (UOS) model. For segmentation under this model, we develop Multilinear Subspace Clustering (MSC) algorithm and evaluate its performance on the YaleB and Olivietti image data sets. We show that MSC is highly competitive with existing algorithms employing the UOS model in terms of clustering performance while enjoying improvement in computational complexity.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.06785",
        "title": "Beyond Classification: Latent User Interests Profiling from Visual Contents Analysis",
        "authors": [
            "Longqi Yang",
            "Cheng-Kang Hsieh",
            "Deborah Estrin"
        ],
        "abstract": "User preference profiling is an important task in modern online social networks (OSN). With the proliferation of image-centric social platforms, such as Pinterest, visual contents have become one of the most informative data streams for understanding user preferences. Traditional approaches usually treat visual content analysis as a general classification problem where one or more labels are assigned to each image. Although such an approach simplifies the process of image analysis, it misses the rich context and visual cues that play an important role in people's perception of images. In this paper, we explore the possibilities of learning a user's latent visual preferences directly from image contents. We propose a distance metric learning method based on Deep Convolutional Neural Networks (CNN) to directly extract similarity information from visual contents and use the derived distance metric to mine individual users' fine-grained visual preferences. Through our preliminary experiments using data from 5,790 Pinterest users, we show that even for the images within the same category, each user possesses distinct and individually-identifiable visual preferences that are consistent over their lifetime. Our results underscore the untapped potential of finer-grained visual preference profiling in understanding users' preferences.\n    ",
        "submission_date": "2015-12-21T00:00:00",
        "last_modified_date": "2015-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07143",
        "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation",
        "authors": [
            "Mariella Dimiccoli",
            "Marc Bola\u00f1os",
            "Estefania Talavera",
            "Maedeh Aghaei",
            "Stavri G. Nikolov",
            "Petia Radeva"
        ],
        "abstract": "While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments. First, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, by integrating language processing, a vocabulary of concepts is defined in a semantic space. Finally, by exploiting the temporal coherence in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from activity and event recognition to semantic indexing and summarization. Experiments over egocentric sets of nearly 17,000 images, show that the proposed approach outperforms state-of-the-art methods.\n    ",
        "submission_date": "2015-12-22T00:00:00",
        "last_modified_date": "2016-10-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.07587",
        "title": "A Latent-Variable Lattice Model",
        "authors": [
            "Rajasekaran Masatran"
        ],
        "abstract": "Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF.\n    ",
        "submission_date": "2015-12-23T00:00:00",
        "last_modified_date": "2016-06-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09227",
        "title": "Denoising and Completion of 3D Data via Multidimensional Dictionary Learning",
        "authors": [
            "Zemin Zhang",
            "Shuchin Aeron"
        ],
        "abstract": "In this paper a new dictionary learning algorithm for multidimensional data is proposed. Unlike most conventional dictionary learning methods which are derived for dealing with vectors or matrices, our algorithm, named KTSVD, learns a multidimensional dictionary directly via a novel algebraic approach for tensor factorization as proposed in [3, 12, 13]. Using this approach one can define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D data to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based on the idea of sparse coding (using group-sparsity over multidimensional coefficient vectors), alternates between estimating a compact representation and dictionary learning. We analyze our KTSVD algorithm and demonstrate its result on video completion and multispectral image denoising.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2015-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1512.09300",
        "title": "Autoencoding beyond pixels using a learned similarity metric",
        "authors": [
            "Anders Boesen Lindbo Larsen",
            "S\u00f8ren Kaae S\u00f8nderby",
            "Hugo Larochelle",
            "Ole Winther"
        ],
        "abstract": "We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.\n    ",
        "submission_date": "2015-12-31T00:00:00",
        "last_modified_date": "2016-02-10T00:00:00"
    }
]