[
    {
        "url": "https://arxiv.org/abs/1101.0237",
        "title": "A Framework for Real-Time Face and Facial Feature Tracking using Optical Flow Pre-estimation and Template Tracking",
        "authors": [
            "E.R. Gast",
            "Michael S. Lew"
        ],
        "abstract": "This work presents a framework for tracking head movements and capturing the movements of the mouth and both the eyebrows in real-time. We present a head tracker which is a combination of a optical flow and a template based tracker. The estimation of the optical flow head tracker is used as starting point for the template tracker which fine-tunes the head estimation. This approach together with re-updating the optical flow points prevents the head tracker from drifting. This combination together with our switching scheme, makes our tracker very robust against fast movement and motion-blur. We also propose a way to reduce the influence of partial occlusion of the head. In both the optical flow and the template based tracker we identify and exclude occluded points.\n    ",
        "submission_date": "2010-12-31T00:00:00",
        "last_modified_date": "2010-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0242",
        "title": "Binary and nonbinary description of hypointensity in human brain MR images",
        "authors": [
            "Xiaojing Chen",
            "Michael S. Lew"
        ],
        "abstract": "Accumulating evidence has shown that iron is involved in the mechanism underlying many neurodegenerative diseases, such as Alzheimer's disease, Parkinson's disease and Huntington's disease. Abnormal (higher) iron accumulation has been detected in the brains of most neurodegenerative patients, especially in the basal ganglia region. Presence of iron leads to changes in MR signal in both magnitude and phase. Accordingly, tissues with high iron concentration appear hypo-intense (darker than usual) in MR contrasts. In this report, we proposed an improved binary hypointensity description and a novel nonbinary hypointensity description based on principle components analysis. Moreover, Kendall's rank correlation coefficient was used to compare the complementary and redundant information provided by the two methods in order to better understand the individual descriptions of iron accumulation in the brain.\n    ",
        "submission_date": "2010-12-31T00:00:00",
        "last_modified_date": "2010-12-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0384",
        "title": "Combining Neural Networks for Skin Detection",
        "authors": [
            "Chelsia Amy Doukim",
            "Jamal Ahmad Dargham",
            "Ali Chekima",
            "Sigeru Omatu"
        ],
        "abstract": "Two types of combining strategies were evaluated namely combining skin features and combining skin classifiers. Several combining rules were applied where the outputs of the skin classifiers are combined using binary operators such as the AND and the OR operators, \"Voting\", \"Sum of Weights\" and a new neural network. Three chrominance components from the YCbCr colour space that gave the highest correct detection on their single feature MLP were selected as the combining parameters. A major issue in designing a MLP neural network is to determine the optimal number of hidden units given a set of training patterns. Therefore, a \"coarse to fine search\" method to find the number of neurons in the hidden layer is proposed. The strategy of combining Cb/Cr and Cr features improved the correct detection by 3.01% compared to the best single feature MLP given by Cb-Cr. The strategy of combining the outputs of three skin classifiers using the \"Sum of Weights\" rule further improved the correct detection by 4.38% compared to the best single feature MLP.\n    ",
        "submission_date": "2011-01-02T00:00:00",
        "last_modified_date": "2011-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0457",
        "title": "Segmentation of Camera Captured Business Card Images for Mobile Devices",
        "authors": [
            "Ayatullah Faruk Mollah",
            "Subhadip Basu",
            "Mita Nasipuri"
        ],
        "abstract": "Due to huge deformation in the camera captured images, variety in nature of the business cards and the computational constraints of the mobile devices, design of an efficient Business Card Reader (BCR) is challenging to the researchers. Extraction of text regions and segmenting them into characters is one of such challenges. In this paper, we have presented an efficient character segmentation technique for business card images captured by a cell-phone camera, designed in our present work towards developing an efficient BCR. At first, text regions are extracted from the card images and then the skewed ones are corrected using a computationally efficient skew correction technique. At last, these skew corrected text regions are segmented into lines and characters based on horizontal and vertical histogram. Experiments show that the present technique is efficient and applicable for mobile devices, and the mean segmentation accuracy of 97.48% is achieved with 3 mega-pixel (500-600 dpi) images. It takes only 1.1 seconds for segmentation including all the preprocessing steps on a moderately powerful notebook (DualCore T2370, 1.73 GHz, 1GB RAM, 1MB L2 Cache).\n    ",
        "submission_date": "2011-01-03T00:00:00",
        "last_modified_date": "2011-01-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.1602",
        "title": "Application of Freeman Chain Codes: An Alternative Recognition Technique for Malaysian Car Plates",
        "authors": [
            "Nor Amizam Jusoh",
            "Jasni Mohamad Zain"
        ],
        "abstract": "Various applications of car plate recognition systems have been developed using various kinds of methods and techniques by researchers all over the world. The applications developed were only suitable for specific country due to its standard specification endorsed by the transport department of particular countries. The Road Transport Department of Malaysia also has endorsed a specification for car plates that includes the font and size of characters that must be followed by car owners. However, there are cases where this specification is not followed. Several applications have been developed in Malaysia to overcome this problem. However, there is still problem in achieving 100% recognition accuracy. This paper is mainly focused on conducting an experiment using chain codes technique to perform recognition for different types of fonts used in Malaysian car plates.\n    ",
        "submission_date": "2011-01-08T00:00:00",
        "last_modified_date": "2011-01-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2243",
        "title": "Illustrating Color Evolution and Color Blindness by the Decoding Model of Color Vision",
        "authors": [
            "Chenguang Lu"
        ],
        "abstract": "A symmetrical model of color vision, the decoding model as a new version of zone model, was introduced. The model adopts new continuous-valued logic and works in a way very similar to the way a 3-8 decoder in a numerical circuit works. By the decoding model, Young and Helmholtz's tri-pigment theory and Hering's opponent theory are unified more naturally; opponent process, color evolution, and color blindness are illustrated more concisely. According to the decoding model, we can obtain a transform from RGB system to HSV system, which is formally identical to the popular transform for computer graphics provided by Smith (1978). Advantages, problems, and physiological tests of the decoding model are also discussed.\n    ",
        "submission_date": "2010-12-12T00:00:00",
        "last_modified_date": "2010-12-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2312",
        "title": "Automatic segmentation of HeLa cell images",
        "authors": [
            "Jan Urban"
        ],
        "abstract": "In this work, the possibilities for segmentation of cells from their background and each other in digital image were tested, combined and improoved. Lot of images with young, adult and mixture cells were able to prove the quality of described algorithms. Proper segmentation is one of the main task of image analysis and steps order differ from work to work, depending on input images. Reply for biologicaly given question was looking for in this work, including filtration, details emphasizing, segmentation and sphericity computing. Order of algorithms and way to searching for them was also described. Some questions and ideas for further work were mentioned in the conclusion part.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2427",
        "title": "Content-Based Filtering for Video Sharing Social Networks",
        "authors": [
            "Eduardo Valle",
            "Sandra de Avila",
            "Antonio da Luz Jr.",
            "Fillipe de Souza",
            "Marcelo Coelho",
            "Arnaldo Ara\u00fajo"
        ],
        "abstract": "In this paper we compare the use of several features in the task of content filtering for video social networks, a very challenging task, not only because the unwanted content is related to very high-level semantic concepts (e.g., pornography, violence, etc.) but also because videos from social networks are extremely assorted, preventing the use of constrained a priori information. We propose a simple method, able to combine diverse evidence, coming from different features and various video elements (entire video, shots, frames, keyframes, etc.). We evaluate our method in three social network applications, related to the detection of unwanted content - pornographic videos, violent videos, and videos posted to artificially manipulate popularity scores. Using challenging test databases, we show that this simple scheme is able to obtain good results, provided that adequate features are chosen. Moreover, we establish a representation using codebooks of spatiotemporal local descriptors as critical to the success of the method in all three contexts. This is consequential, since the state-of-the-art still relies heavily on static features for the tasks addressed.\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2011-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2491",
        "title": "A Review of Research on Devnagari Character Recognition",
        "authors": [
            "V J Dongre",
            "V H Mankar"
        ],
        "abstract": "English Character Recognition (CR) has been extensively studied in the last half century and progressed to a level, sufficient to produce technology driven applications. But same is not the case for Indian languages which are complicated in terms of structure and computations. Rapidly growing computational power may enable the implementation of Indic CR methodologies. Digital document processing is gaining popularity for application to office and library automation, bank and postal services, publishing houses and communication technology. Devnagari being the national language of India, spoken by more than 500 million people, should be given special attention so that document retrieval and analysis of rich ancient and modern Indian literature can be effectively done. This article is intended to serve as a guide and update for the readers, working in the Devnagari Optical Character Recognition (DOCR) area. An overview of DOCR systems is presented and the available DOCR techniques are reviewed. The current status of DOCR is discussed and directions for future research are suggested.\n    ",
        "submission_date": "2011-01-13T00:00:00",
        "last_modified_date": "2011-01-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2987",
        "title": "Support vector machines/relevance vector machine for remote sensing classification: A review",
        "authors": [
            "Mahesh Pal"
        ],
        "abstract": "Kernel-based machine learning algorithms are based on mapping data from the original input feature space to a kernel feature space of higher dimensionality to solve a linear problem in that space. Over the last decade, kernel based classification and regression approaches such as support vector machines have widely been used in remote sensing as well as in various civil engineering applications. In spite of their better performance with different datasets, support vector machines still suffer from shortcomings such as visualization/interpretation of model, choice of kernel and kernel specific parameter as well as the regularization parameter. Relevance vector machines are another kernel based approach being explored for classification and regression with in last few years. The advantages of the relevance vector machines over the support vector machines is the availability of probabilistic predictions, using arbitrary kernel functions and not requiring setting of the regularization parameter. This paper presents a state-of-the-art review of SVM and RVM in remote sensing and provides some details of their use in other civil engineering application also.\n    ",
        "submission_date": "2011-01-15T00:00:00",
        "last_modified_date": "2011-01-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3354",
        "title": "Introduction to the Bag of Features Paradigm for Image Classification and Retrieval",
        "authors": [
            "Stephen O'Hara",
            "Bruce A. Draper"
        ],
        "abstract": "The past decade has seen the growing popularity of Bag of Features (BoF) approaches to many computer vision tasks, including image classification, video search, robot localization, and texture recognition. Part of the appeal is simplicity. BoF methods are based on orderless collections of quantized local image descriptors; they discard spatial information and are therefore conceptually and computationally simpler than many alternative methods. Despite this, or perhaps because of this, BoF-based systems have set new performance standards on popular image classification benchmarks and have achieved scalability breakthroughs in image retrieval. This paper presents an introduction to BoF image representations, describes critical design choices, and surveys the BoF literature. Emphasis is placed on recent techniques that mitigate quantization errors, improve feature detection, and speed up image retrieval. At the same time, unresolved issues and fundamental challenges are raised. Among the unresolved issues are determining the best techniques for sampling images, describing local image features, and evaluating system performance. Among the more fundamental challenges are how and whether BoF methods can contribute to localizing objects in complex images, or to associating high-level semantics with natural images. This survey should be useful both for introducing new investigators to the field and for providing existing researchers with a consolidated reference to related work.\n    ",
        "submission_date": "2011-01-17T00:00:00",
        "last_modified_date": "2011-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3391",
        "title": "Automated Image Processing for the Analysis of DNA Repair Dynamics",
        "authors": [
            "Thorsten Riess",
            "Christian Dietz",
            "Martin Tomas",
            "Elisa Ferrando-May",
            "Dorit Merhof"
        ],
        "abstract": "The efficient repair of cellular DNA is essential for the maintenance and inheritance of genomic information. In order to cope with the high frequency of spontaneous and induced DNA damage, a multitude of repair mechanisms have evolved. These are enabled by a wide range of protein factors specifically recognizing different types of lesions and finally restoring the normal DNA sequence. This work focuses on the repair factor XPC (xeroderma pigmentosum complementation group C), which identifies bulky DNA lesions and initiates their removal via the nucleotide excision repair pathway. The binding of XPC to damaged DNA can be visualized in living cells by following the accumulation of a fluorescent XPC fusion at lesions induced by laser microirradiation in a fluorescence microscope. In this work, an automated image processing pipeline is presented which allows to identify and quantify the accumulation reaction without any user interaction. The image processing pipeline comprises a preprocessing stage where the image stack data is filtered and the nucleus of interest is segmented. Afterwards, the images are registered to each other in order to account for movements of the cell, and then a bounding box enclosing the XPC-specific signal is automatically determined. Finally, the time-dependent relocation of XPC is evaluated by analyzing the intensity change within this box. Comparison of the automated processing results with the manual evaluation yields qualitatively similar results. However, the automated analysis provides more accurate, reproducible data with smaller standard errors. The image processing pipeline presented in this work allows for an efficient analysis of large amounts of experimental data with no user interaction required.\n    ",
        "submission_date": "2011-01-18T00:00:00",
        "last_modified_date": "2011-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3755",
        "title": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev Inequality",
        "authors": [
            "Shriprakash Sinha"
        ],
        "abstract": "Approximating adequate number of clusters in multidimensional data is an open area of research, given a level of compromise made on the quality of acceptable results. The manuscript addresses the issue by formulating a transductive inductive learning algorithm which uses multivariate Chebyshev inequality. Considering clustering problem in imaging, theoretical proofs for a particular level of compromise are derived to show the convergence of the reconstruction error to a finite value with increasing (a) number of unseen examples and (b) the number of clusters, respectively. Upper bounds for these error rates are also proved. Non-parametric estimates of these error from a random sample of sequences empirically point to a stable number of clusters. Lastly, the generalization of algorithm can be applied to multidimensional data sets from different fields.\n    ",
        "submission_date": "2011-01-19T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4301",
        "title": "Diffusion framework for geometric and photometric data fusion in non-rigid shape analysis",
        "authors": [
            "Artiom Kovnatsky",
            "Michael M. Bronstein",
            "Alexander M. Bronstein",
            "Ron Kimmel"
        ],
        "abstract": "In this paper, we explore the use of the diffusion geometry framework for the fusion of geometric and photometric information in local and global shape descriptors. Our construction is based on the definition of a diffusion process on the shape manifold embedded into a high-dimensional space where the embedding coordinates represent the photometric information. Experimental results show that such data fusion is useful in coping with different challenges of shape analysis where pure geometric and pure photometric methods fail.\n    ",
        "submission_date": "2011-01-22T00:00:00",
        "last_modified_date": "2011-01-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4749",
        "title": "Online Adaptive Decision Fusion Framework Based on Entropic Projections onto Convex Sets with Application to Wildfire Detection in Video",
        "authors": [
            "Osman Gunay",
            "Behcet Ugur Toreyin",
            "Kivanc Kose",
            "A. Enis Cetin"
        ],
        "abstract": "In this paper, an Entropy functional based online Adaptive Decision Fusion (EADF) framework is developed for image analysis and computer vision applications. In this framework, it is assumed that the compound algorithm consists of several sub-algorithms each of which yielding its own decision as a real number centered around zero, representing the confidence level of that particular sub-algorithm. Decision values are linearly combined with weights which are updated online according to an active fusion method based on performing entropic projections onto convex sets describing sub-algorithms. It is assumed that there is an oracle, who is usually a human operator, providing feedback to the decision fusion method. A video based wildfire detection system is developed to evaluate the performance of the algorithm in handling the problems where data arrives sequentially. In this case, the oracle is the security guard of the forest lookout tower verifying the decision of the combined algorithm. Simulation results are presented. The EADF framework is also tested with a standard dataset.\n    ",
        "submission_date": "2011-01-25T00:00:00",
        "last_modified_date": "2011-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5320",
        "title": "A Panorama on Multiscale Geometric Representations, Intertwining Spatial, Directional and Frequency Selectivity",
        "authors": [
            "Laurent Jacques",
            "Laurent Duval",
            "Caroline Chaux",
            "Gabriel Peyr\u00e9"
        ],
        "abstract": "The richness of natural images makes the quest for optimal representations in image processing and computer vision challenging. The latter observation has not prevented the design of image representations, which trade off between efficiency and complexity, while achieving accurate rendering of smooth regions as well as reproducing faithful contours and textures. The most recent ones, proposed in the past decade, share an hybrid heritage highlighting the multiscale and oriented nature of edges and patterns in images. This paper presents a panorama of the aforementioned literature on decompositions in multiscale, multi-orientation bases or dictionaries. They typically exhibit redundancy to improve sparsity in the transformed domain and sometimes its invariance with respect to simple geometric deformations (translation, rotation). Oriented multiscale dictionaries extend traditional wavelet processing and may offer rotation invariance. Highly redundant dictionaries require specific algorithms to simplify the search for an efficient (sparse) representation. We also discuss the extension of multiscale geometric decompositions to non-Euclidean domains such as the sphere or arbitrary meshed surfaces. The etymology of panorama suggests an overview, based on a choice of partially overlapping \"pictures\". We hope that this paper will contribute to the appreciation and apprehension of a stream of current research directions in image understanding.\n    ",
        "submission_date": "2011-01-27T00:00:00",
        "last_modified_date": "2011-04-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5687",
        "title": "A correspondence-less approach to matching of deformable shapes",
        "authors": [
            "Jonathan Pokrass",
            "Alexander M. Bronstein",
            "Michael M. Bronstein"
        ],
        "abstract": "Finding a match between partially available deformable shapes is a challenging problem with numerous applications. The problem is usually approached by computing local descriptors on a pair of shapes and then establishing a point-wise correspondence between the two. In this paper, we introduce an alternative correspondence-less approach to matching fragments to an entire shape undergoing a non-rigid deformation. We use diffusion geometric descriptors and optimize over the integration domains on which the integral descriptors of the two parts match. The problem is regularized using the Mumford-Shah functional. We show an efficient discretization based on the Ambrosio-Tortorelli approximation generalized to triangular meshes. Experiments demonstrating the success of the proposed method are presented.\n    ",
        "submission_date": "2011-01-29T00:00:00",
        "last_modified_date": "2011-01-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5766",
        "title": "Geometric Models with Co-occurrence Groups",
        "authors": [
            "Joan Bruna",
            "St\u00e9phane Mallat"
        ],
        "abstract": "A geometric model of sparse signal representations is introduced for classes of signals. It is computed by optimizing co-occurrence groups with a maximum likelihood estimate calculated with a Bernoulli mixture model. Applications to face image compression and MNIST digit classification illustrate the applicability of this model.\n    ",
        "submission_date": "2011-01-30T00:00:00",
        "last_modified_date": "2011-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.5785",
        "title": "Statistical Compressed Sensing of Gaussian Mixture Models",
        "authors": [
            "Guoshen Yu",
            "Guillermo Sapiro"
        ],
        "abstract": "A novel framework of compressed sensing, namely statistical compressed sensing (SCS), that aims at efficiently sampling a collection of signals that follow a statistical distribution, and achieving accurate reconstruction on average, is introduced. SCS based on Gaussian models is investigated in depth. For signals that follow a single Gaussian model, with Gaussian or Bernoulli sensing matrices of O(k) measurements, considerably smaller than the O(k log(N/k)) required by conventional CS based on sparse models, where N is the signal dimension, and with an optimal decoder implemented via linear filtering, significantly faster than the pursuit decoders applied in conventional CS, the error of SCS is shown tightly upper bounded by a constant times the best k-term approximation error, with overwhelming probability. The failure probability is also significantly smaller than that of conventional sparsity-oriented CS. Stronger yet simpler results further show that for any sensing matrix, the error of Gaussian SCS is upper bounded by a constant times the best k-term approximation with probability one, and the bound constant can be efficiently calculated. For Gaussian mixture models (GMMs), that assume multiple Gaussian distributions and that each signal follows one of them with an unknown index, a piecewise linear estimator is introduced to decode SCS. The accuracy of model selection, at the heart of the piecewise linear decoder, is analyzed in terms of the properties of the Gaussian distributions and the number of sensing measurements. A maximum a posteriori expectation-maximization algorithm that iteratively estimates the Gaussian models parameters, the signals model selection, and decodes the signals, is presented for GMM-based SCS. In real image sensing applications, GMM-based SCS is shown to lead to improved results compared to conventional CS, at a considerably lower computational cost.\n    ",
        "submission_date": "2011-01-30T00:00:00",
        "last_modified_date": "2011-01-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1101",
        "title": "Total variation regularization for fMRI-based prediction of behaviour",
        "authors": [
            "Vincent Michel",
            "Alexandre Gramfort",
            "Ga\u00ebl Varoquaux",
            "Evelyn Eger",
            "Bertrand Thirion"
        ],
        "abstract": "While medical imaging typically provides massive amounts of data, the extraction of relevant information for predictive diagnosis remains a difficult challenge. Functional MRI (fMRI) data, that provide an indirect measure of task-related or spontaneous neuronal activity, are classically analyzed in a mass-univariate procedure yielding statistical parametric maps. This analysis framework disregards some important principles of brain organization: population coding, distributed and overlapping representations. Multivariate pattern analysis, i.e., the prediction of behavioural variables from brain activation patterns better captures this structure. To cope with the high dimensionality of the data, the learning method has to be regularized. However, the spatial structure of the image is not taken into account in standard regularization methods, so that the extracted features are often hard to interpret. More informative and interpretable results can be obtained with the l_1 norm of the image gradient, a.k.a. its Total Variation (TV), as regularization. We apply for the first time this method to fMRI data, and show that TV regularization is well suited to the purpose of brain mapping while being a powerful tool for brain decoding. Moreover, this article presents the first use of TV regularization for classification.\n    ",
        "submission_date": "2011-02-05T00:00:00",
        "last_modified_date": "2011-02-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.1292",
        "title": "Modeling Dynamic Swarms",
        "authors": [
            "Bernard Ghanem",
            "Narendra Ahuja"
        ],
        "abstract": "This paper proposes the problem of modeling video sequences of dynamic swarms (DS). We define DS as a large layout of stochastically repetitive spatial configurations of dynamic objects (swarm elements) whose motions exhibit local spatiotemporal interdependency and stationarity, i.e., the motions are similar in any small spatiotemporal neighborhood. Examples of DS abound in nature, e.g., herds of animals and flocks of birds. To capture the local spatiotemporal properties of the DS, we present a probabilistic model that learns both the spatial layout of swarm elements and their joint dynamics that are modeled as linear transformations. To this end, a spatiotemporal neighborhood is associated with each swarm element, in which local stationarity is enforced both spatially and temporally. We assume that the prior on the swarm dynamics is distributed according to an MRF in both space and time. Embedding this model in a MAP framework, we iterate between learning the spatial layout of the swarm and its dynamics. We learn the swarm transformations using ICM, which iterates between estimating these transformations and updating their distribution in the spatiotemporal neighborhoods. We demonstrate the validity of our method by conducting experiments on real video sequences. Real sequences of birds, geese, robot swarms, and pedestrians evaluate the applicability of our model to real world data.\n    ",
        "submission_date": "2011-02-07T00:00:00",
        "last_modified_date": "2011-02-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2382",
        "title": "A Comparison of Two Human Brain Tumor Segmentation Methods for MRI Data",
        "authors": [
            "Jan Egger",
            "D\u017eenan Zuki\u0107",
            "Miriam H. A. Bauer",
            "Daniela Kuhnt",
            "Barbara Carl",
            "Bernd Freisleben",
            "Andreas Kolb",
            "Christopher Nimsky"
        ],
        "abstract": "The most common primary brain tumors are gliomas, evolving from the cerebral supportive cells. For clinical follow-up, the evaluation of the preoperative tumor volume is essential. Volumetric assessment of tumor volume with manual segmentation of its outlines is a time-consuming process that can be overcome with the help of computerized segmentation methods. In this contribution, two methods for World Health Organization (WHO) grade IV glioma segmentation in the human brain are compared using magnetic resonance imaging (MRI) patient data from the clinical routine. One method uses balloon inflation forces, and relies on detection of high intensity tumor boundaries that are coupled with the use of contrast agent gadolinium. The other method sets up a directed and weighted graph and performs a min-cut for optimal segmentation results. The ground truth of the tumor boundaries - for evaluating the methods on 27 cases - is manually extracted by neurosurgeons with several years of experience in the resection of gliomas. A comparison is performed using the Dice Similarity Coefficient (DSC), a measure for the spatial overlap of different segmentation results.\n    ",
        "submission_date": "2011-02-09T00:00:00",
        "last_modified_date": "2011-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2739",
        "title": "A General Framework for Development of the Cortex-like Visual Object Recognition System: Waves of Spikes, Predictive Coding and Universal Dictionary of Features",
        "authors": [
            "Sergey S. Tarasenko"
        ],
        "abstract": "This study is focused on the development of the cortex-like visual object recognition system. We propose a general framework, which consists of three hierarchical levels (modules). These modules functionally correspond to the V1, V4 and IT areas. Both bottom-up and top-down connections between the hierarchical levels V4 and IT are employed. The higher the degree of matching between the input and the preferred stimulus, the shorter the response time of the neuron. Therefore information about a single stimulus is distributed in time and is transmitted by the waves of spikes. The reciprocal connections and waves of spikes implement predictive coding: an initial hypothesis is generated on the basis of information delivered by the first wave of spikes and is tested with the information carried by the consecutive waves. The development is considered as extraction and accumulation of features in V4 and objects in IT. Once stored a feature can be disposed, if rarely activated. This cause update of feature repository. Consequently, objects in IT are also updated. This illustrates the growing process and dynamical change of topological structures of V4, IT and connections between these areas.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2743",
        "title": "Feature selection via simultaneous sparse approximation for person specific face verification",
        "authors": [
            "Yixiong Liang",
            "Lei Wang",
            "Shenghui Liao",
            "Beiji Zou"
        ],
        "abstract": "There is an increasing use of some imperceivable and redundant local features for face recognition. While only a relatively small fraction of them is relevant to the final recognition task, the feature selection is a crucial and necessary step to select the most discriminant ones to obtain a compact face representation. In this paper, we investigate the sparsity-enforced regularization-based feature selection methods and propose a multi-task feature selection method for building person specific models for face verification. We assume that the person specific models share a common subset of features and novelly reformulated the common subset selection problem as a simultaneous sparse approximation problem. To the best of our knowledge, it is the first time to apply the sparsity-enforced regularization methods for person specific face verification. The effectiveness of the proposed methods is verified with the challenging LFW face databases.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2748",
        "title": "Feature Selection via Sparse Approximation for Face Recognition",
        "authors": [
            "Yixiong Liang",
            "Lei Wang",
            "Yao Xiang",
            "Beiji Zou"
        ],
        "abstract": "Inspired by biological vision systems, the over-complete local features with huge cardinality are increasingly used for face recognition during the last decades. Accordingly, feature selection has become more and more important and plays a critical role for face data description and recognition. In this paper, we propose a trainable feature selection algorithm based on the regularized frame for face recognition. By enforcing a sparsity penalty term on the minimum squared error (MSE) criterion, we cast the feature selection problem into a combinatorial sparse approximation problem, which can be solved by greedy methods or convex relaxation methods. Moreover, based on the same frame, we propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed methods are used for selecting the most informative Gabor features of face images for recognition and the experimental results on benchmark face databases demonstrate the effectiveness of the proposed methods.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2749",
        "title": "Multi-task GLOH feature selection for human age estimation",
        "authors": [
            "Yixiong Liang",
            "Lingbo Liu",
            "Ying Xu",
            "Yao Xiang",
            "Beiji Zou"
        ],
        "abstract": "In this paper, we propose a novel age estimation method based on GLOH feature descriptor and multi-task learning (MTL). The GLOH feature descriptor, one of the state-of-the-art feature descriptor, is used to capture the age-related local and spatial information of face image. As the exacted GLOH features are often redundant, MTL is designed to select the most informative feature bins for age estimation problem, while the corresponding weights are determined by ridge regression. This approach largely reduces the dimensions of feature, which can not only improve performance but also decrease the computational burden. Experiments on the public available FG-NET database show that the proposed method can achieve comparable performance over previous approaches while using much fewer features.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3830",
        "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
        "authors": [
            "Thomas Schoenemann",
            "Fredrik Kahl",
            "Simon Masnou",
            "Daniel Cremers"
        ],
        "abstract": "We present the first method to handle curvature regularity in region-based image segmentation and inpainting that is independent of initialization.\n",
        "submission_date": "2011-02-18T00:00:00",
        "last_modified_date": "2011-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4258",
        "title": "SHREC 2011: robust feature detection and description benchmark",
        "authors": [
            "E. Boyer",
            "A. M. Bronstein",
            "M. M. Bronstein",
            "B. Bustos",
            "T. Darom",
            "R. Horaud",
            "I. Hotz",
            "Y. Keller",
            "J. Keustermans",
            "A. Kovnatsky",
            "R. Litman",
            "J. Reininghaus",
            "I. Sipiran",
            "D. Smeets",
            "P. Suetens",
            "D. Vandermeulen",
            "A. Zaharescu",
            "V. Zobel"
        ],
        "abstract": "Feature-based approaches have recently become very popular in computer vision and image analysis applications, and are becoming a promising direction in shape retrieval. SHREC'11 robust feature detection and description benchmark simulates the feature detection and description stages of feature-based shape retrieval algorithms. The benchmark tests the performance of shape feature detectors and descriptors under a wide variety of transformations. The benchmark allows evaluating how algorithms cope with certain classes of transformations and strength of the transformations that can be dealt with. The present paper is a report of the SHREC'11 robust feature detection and description benchmark results.\n    ",
        "submission_date": "2011-02-21T00:00:00",
        "last_modified_date": "2011-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5448",
        "title": "Continuous Multiclass Labeling Approaches and Algorithms",
        "authors": [
            "Jan Lellmann",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "We study convex relaxations of the image labeling problem on a continuous domain with regularizers based on metric interaction potentials. The generic framework ensures existence of minimizers and covers a wide range of relaxations of the originally combinatorial problem. We focus on two specific relaxations that differ in flexibility and simplicity -- one can be used to tightly relax any metric interaction potential, while the other one only covers Euclidean metrics but requires less computational effort. For solving the nonsmooth discretized problem, we propose a globally convergent Douglas-Rachford scheme, and show that a sequence of dual iterates can be recovered in order to provide a posteriori optimality bounds. In a quantitative comparison to two other first-order methods, the approach shows competitive performance on synthetical and real-world images. By combining the method with an improved binarization technique for nonstandard potentials, we were able to routinely recover discrete solutions within 1%--5% of the global optimum for the combinatorial image labeling problem.\n    ",
        "submission_date": "2011-02-26T00:00:00",
        "last_modified_date": "2011-03-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.5688",
        "title": "A novel super resolution reconstruction of low reoslution images progressively using dct and zonal filter based denoising",
        "authors": [
            "Liyakathunisa",
            "C.N .Ravi Kumar"
        ],
        "abstract": "Due to the factors like processing power limitations and channel capabilities images are often down sampled and transmitted at low bit rates resulting in a low resolution compressed image. High resolution images can be reconstructed from several blurred, noisy and down sampled low resolution images using a computational process know as super resolution reconstruction. Super-resolution is the process of combining multiple aliased low-quality images to produce a high resolution, high-quality image. The problem of recovering a high resolution image progressively from a sequence of low resolution compressed images is considered. In this paper we propose a novel DCT based progressive image display algorithm by stressing on the encoding and decoding process. At the encoder we consider a set of low resolution images which are corrupted by additive white Gaussian noise and motion blur. The low resolution images are compressed using 8 by 8 blocks DCT and noise is filtered using our proposed novel zonal filter. Multiframe fusion is performed in order to obtain a single noise free image. At the decoder the image is reconstructed progressively by transmitting the coarser image first followed by the detail image. And finally a super resolution image is reconstructed by applying our proposed novel adaptive interpolation technique. We have performed both objective and subjective analysis of the reconstructed image, and the resultant image has better super resolution factor, and a higher ISNR and PSNR. A comparative study done with Iterative Back Projection (IBP) and Projection on to Convex Sets (POCS),Papoulis Grechberg, FFT based Super resolution Reconstruction shows that our method has out performed the previous contributions.\n    ",
        "submission_date": "2011-02-28T00:00:00",
        "last_modified_date": "2011-02-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0120",
        "title": "Automatic Detection of Ringworm using Local Binary Pattern (LBP)",
        "authors": [
            "Srimanta Kundu",
            "Nibaran Das",
            "Mita Nasipuri"
        ],
        "abstract": "In this paper we present a novel approach for automatic recognition of ring worm skin disease based on LBP (Local Binary Pattern) feature extracted from the affected skin images. The proposed method is evaluated by extensive experiments on the skin images collected from internet. The dataset is tested using three different classifiers i.e. Bayesian, MLP and SVM. Experimental results show that the proposed methodology efficiently discriminates between a ring worm skin and a normal skin. It is a low cost technique and does not require any special imaging devices.\n    ",
        "submission_date": "2011-03-01T00:00:00",
        "last_modified_date": "2011-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0540",
        "title": "An Algorithm for Repairing Low-Quality Video Enhancement Techniques Based on Trained Filter",
        "authors": [
            "Lijun Wang",
            "Ling Shao"
        ],
        "abstract": "Multifarious image enhancement algorithms have been used in different applications. Still, some algorithms or modules are imperfect for practical use. When the image enhancement modules have been fixed or combined by a series of algorithms, we need to repair them as a whole part without changing the inside. This report aims to find an algorithm based on trained filters to repair low-quality image enhancement modules. A brief review on basic image enhancement techniques and pixel classification methods will be presented, and the procedure of trained filters will be described step by step. The experiments and result comparisons for this algorithm will be described in detail.\n    ",
        "submission_date": "2011-03-02T00:00:00",
        "last_modified_date": "2011-03-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.0738",
        "title": "A Medial Axis Based Thinning Strategy for Character Images",
        "authors": [
            "Soumen Bag",
            "Gaurav Harit"
        ],
        "abstract": "Thinning of character images is a big challenge. Removal of strokes or deformities in thinning is a difficult problem. In this paper, we have proposed a medial axis based thinning strategy used for performing skeletonization of printed and handwritten character images. In this method, we have used shape characteristics of text to get skeleton of nearly same as the true character shape. This approach helps to preserve the local features and true shape of the character images. The proposed algorithm produces one pixel width thin skeleton. As a by-product of our thinning approach, the skeleton also gets segmented into strokes in vector form. Hence further stroke segmentation is not required. Experiment is done on printed English and Bengali characters and we obtain less spurious branches comparing with other thinning methods without any post processing.\n    ",
        "submission_date": "2011-03-03T00:00:00",
        "last_modified_date": "2011-03-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1077",
        "title": "Submodular Decomposition Framework for Inference in Associative Markov Networks with Global Constraints",
        "authors": [
            "Anton Osokin",
            "Dmitry Vetrov",
            "Vladimir Kolmogorov"
        ],
        "abstract": "In the paper we address the problem of finding the most probable state of discrete Markov random field (MRF) with associative pairwise terms. Although of practical importance, this problem is known to be NP-hard in general. We propose a new type of MRF decomposition, submodular decomposition (SMD). Unlike existing decomposition approaches SMD decomposes the initial problem into subproblems corresponding to a specific class label while preserving the graph structure of each subproblem. Such decomposition enables us to take into account several types of global constraints in an efficient manner. We study theoretical properties of the proposed approach and demonstrate its applicability on a number of problems.\n    ",
        "submission_date": "2011-03-05T00:00:00",
        "last_modified_date": "2011-03-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1474",
        "title": "Evaluation of a Novel Approach for Automatic Volume Determination of Glioblastomas Based on Several Manual Expert Segmentations",
        "authors": [
            "Jan Egger",
            "Miriam H. A. Bauer",
            "Daniela Kuhnt",
            "Barbara Carl",
            "Christoph Kappus",
            "Bernd Freisleben",
            "Christopher Nimsky"
        ],
        "abstract": "The glioblastoma multiforme is the most common malignant primary brain tumor and is one of the highest malignant human neoplasms. During the course of disease, the evaluation of tumor volume is an essential part of the clinical follow-up. However, manual segmentation for acquisition of tumor volume is a time-consuming process. In this paper, a new approach for the automatic segmentation and volume determination of glioblastomas (glioblastoma multiforme) is presented and evaluated. The approach uses a user-defined seed point inside the glioma to set up a directed 3D graph. The nodes of the graph are obtained by sampling along rays that are sent through the surface points of a polyhedron. After the graph has been constructed, the minimal s-t cut is calculated to separate the glioblastoma from the background. For evaluation, 12 Magnetic Resonance Imaging (MRI) data sets were manually segmented slice by slice, by neurosurgeons with several years of experience in the resection of gliomas. Afterwards, the manual segmentations were compared with the results of the presented approach via the Dice Similarity Coefficient (DSC). For a better assessment of the DSC results, the manual segmentations of the experts were also compared with each other and evaluated via the DSC. In addition, the 12 data sets were segmented once again by one of the neurosurgeons after a period of two weeks, to also measure the intra-physician deviation of the DSC.\n    ",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2011-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1475",
        "title": "A Semi-Automatic Graph-Based Approach for Determining the Boundary of Eloquent Fiber Bundles in the Human Brain",
        "authors": [
            "Miriam H. A. Bauer",
            "Jan Egger",
            "Daniela Kuhnt",
            "Sebastiano Barbieri",
            "Jan Klein",
            "Horst K. Hahn",
            "Bernd Freisleben",
            "Christopher Nimsky"
        ],
        "abstract": "Diffusion Tensor Imaging (DTI) allows estimating the position, orientation and dimension of bundles of nerve pathways. This non-invasive imaging technique takes advantage of the diffusion of water molecules and determines the diffusion coefficients for every voxel of the data set. The identification of the diffusion coefficients and the derivation of information about fiber bundles is of major interest for planning and performing neurosurgical interventions. To minimize the risk of neural deficits during brain surgery as tumor resection (e.g. glioma), the segmentation and integration of the results in the operating room is of prime importance. In this contribution, a robust and efficient graph-based approach for segmentating tubular fiber bundles in the human brain is presented. To define a cost function, the fractional anisotropy (FA) is used, derived from the DTI data, but this value may differ from patient to patient. Besides manually definining seed regions describing the structure of interest, additionally a manual definition of the cost function by the user is necessary. To improve the approach the contribution introduces a solution for automatically determining the cost function by using different 3D masks for each individual data set.\n    ",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2011-03-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1587",
        "title": "All Roads Lead To Rome",
        "authors": [
            "Xin Li"
        ],
        "abstract": "This short article presents a class of projection-based solution algorithms to the problem considered in the pioneering work on compressed sensing - perfect reconstruction of a phantom image from 22 radial lines in the frequency domain. Under the framework of projection-based image reconstruction, we will show experimentally that several old and new tools of nonlinear filtering (including Perona-Malik diffusion, nonlinear diffusion, Translation-Invariant thresholding and SA-DCT thresholding) all lead to perfect reconstruction of the phantom image.\n    ",
        "submission_date": "2011-03-08T00:00:00",
        "last_modified_date": "2011-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1773",
        "title": "Aorta Segmentation for Stent Simulation",
        "authors": [
            "Jan Egger",
            "Bernd Freisleben",
            "Randolph Setser",
            "Rahul Renapuraar",
            "Christina Biermann",
            "Thomas O'Donnell"
        ],
        "abstract": "Simulation of arterial stenting procedures prior to intervention allows for appropriate device selection as well as highlights potential complications. To this end, we present a framework for facilitating virtual aortic stenting from a contrast computer tomography (CT) scan. More specifically, we present a method for both lumen and outer wall segmentation that may be employed in determining both the appropriateness of intervention as well as the selection and localization of the device. The more challenging recovery of the outer wall is based on a novel minimal closure tracking algorithm. Our aortic segmentation method has been validated on over 3000 multiplanar reformatting (MPR) planes from 50 CT angiography data sets yielding a Dice Similarity Coefficient (DSC) of 90.67%.\n    ",
        "submission_date": "2011-03-09T00:00:00",
        "last_modified_date": "2011-03-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.1952",
        "title": "Ray-Based and Graph-Based Methods for Fiber Bundle Boundary Estimation",
        "authors": [
            "Miriam H. A. Bauer",
            "Jan Egger",
            "Daniela Kuhnt",
            "Sebastiano Barbieri",
            "Jan Klein",
            "Horst K. Hahn",
            "Bernd Freisleben",
            "Christopher Nimsky"
        ],
        "abstract": "Diffusion Tensor Imaging (DTI) provides the possibility of estimating the location and course of eloquent structures in the human brain. Knowledge about this is of high importance for preoperative planning of neurosurgical interventions and for intraoperative guidance by neuronavigation in order to minimize postoperative neurological deficits. Therefore, the segmentation of these structures as closed, three-dimensional object is necessary. In this contribution, two methods for fiber bundle segmentation between two defined regions are compared using software phantoms (abstract model and anatomical phantom modeling the right corticospinal tract). One method uses evaluation points from sampled rays as candidates for boundary points, the other method sets up a directed and weighted (depending on a scalar measure) graph and performs a min-cut for optimal segmentation results. Comparison is done by using the Dice Similarity Coefficient (DSC), a measure for spatial overlap of different segmentation results.\n    ",
        "submission_date": "2011-03-10T00:00:00",
        "last_modified_date": "2011-03-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3228",
        "title": "Multi-parameter acoustic imaging of uniform objects in inhomogeneous media",
        "authors": [
            "H. Emre Guven",
            "Eric L. Miller",
            "Robin O. Cleveland"
        ],
        "abstract": "The problem studied in this paper is ultrasound image reconstruction from frequency-domain measurements of the scattered field from an object with contrast in attenuation and sound speed. The case where the object has uniform but unknown contrast in these properties relative to the background is considered. Background clutter is taken into account in a physically realistic manner by considering an exact scattering model for randomly located small scatterers that vary in sound speed. The resulting statistical characteristics of the interference is incorporated into the imaging solution, which includes applying a total-variation minimization based approach where the relative effect of perturbation in sound speed to attenuation is included as a parameter. Convex optimization methods provide the basis for the reconstruction algorithm. Numerical data for inversion examples are generated by solving the discretized Lippman-Schwinger equation for the object and speckle-forming scatterers in the background. A statistical model based on the Born approximation is used for reconstruction of the object profile. Results are presented for a two dimensional problem in terms of classification performance and compared to minimum-l2-norm reconstruction. Classification using the proposed method is shown to be robust down to a signal-to-clutter ratio of less than 1 dB.\n    ",
        "submission_date": "2011-03-16T00:00:00",
        "last_modified_date": "2011-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3440",
        "title": "Off-Line Handwritten Signature Identification Using Rotated Complex Wavelet Filters",
        "authors": [
            "M.S. Shirdhonkar",
            "Manesh Kokare"
        ],
        "abstract": "In this paper, a new method for handwritten signature identification based on rotated complex wavelet filters is proposed. We have proposed to use the rotated complex wavelet filters (RCWF) and dual tree complex wavelet transform(DTCWT) together to derive signature feature extraction, which captures information in twelve different directions. In identification phase, Canberra distance measure is used. The proposed method is compared with discrete wavelet transform (DWT). From experimental results it is found that signature identification rate of proposed method is superior over DWT\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4723",
        "title": "Automatic Extraction of Open Space Area from High Resolution Urban Satellite Imagery",
        "authors": [
            "B. G. Kodge",
            "P. S. Hiremath"
        ],
        "abstract": "In the 21st century, Aerial and satellite images are information rich. They are also complex to analyze. For GIS systems, many features require fast and reliable extraction of open space area from high resolution satellite imagery. In this paper we will study efficient and reliable automatic extraction algorithm to find out the open space area from the high resolution urban satellite imagery. This automatic extraction algorithm uses some filters and segmentations and grouping is applying on satellite images. And the result images may use to calculate the total available open space area and the built up area. It may also use to compare the difference between present and past open space area using historical urban satellite images of that same projection\n    ",
        "submission_date": "2011-03-24T00:00:00",
        "last_modified_date": "2022-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4913",
        "title": "Automatic Open Space Area Extraction and Change Detection from High Resolution Urban Satellite Images",
        "authors": [
            "B.G. Kodge",
            "P.S. Hiremath"
        ],
        "abstract": "In this paper, we study efficient and reliable automatic extraction algorithm to find out the open space area from the high resolution urban satellite imagery, and to detect changes from the extracted open space area during the period 2003, 2006 and 2008. This automatic extraction and change detection algorithm uses some filters, segmentation and grouping that are applied on satellite images. The resultant images may be used to calculate the total available open space area and the built up area. It may also be used to compare the difference between present and past open space area using historical urban satellite images of that same projection, which is an important geo spatial data management application.\n    ",
        "submission_date": "2011-03-25T00:00:00",
        "last_modified_date": "2011-03-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5621",
        "title": "Application of Threshold Techniques for Readability Improvement of Jawi Historical Manuscript Images",
        "authors": [
            "Hafizan Mat Som",
            "Jasni Mohamad Zain",
            "Amzari Jihadi Ghazali"
        ],
        "abstract": "Historical documents such as old books and manuscripts have a high aesthetic value and highly appreciated. Unfortunately, there are some documents cannot be read due to quality problems like faded paper, ink expand, uneven colour tone, torn paper and other elements disruption such as the existence of small spots. The study aims to produce a copy of manuscript that shows clear wordings so they can easily be read and the copy can also be displayed for visitors. 16 samples of Jawi historical manuscript with different quality problems were obtained from The Royal Museum of Pahang, Malaysia. We applied three binarization techniques; Otsu's method represents global threshold technique; Sauvola and Niblack method which are categorized as local threshold techniques. We compared the binarized images with the original manuscript to be visually inspected by the museum's curator. The unclear features were marked and analyzed. Most of the examined images show that with optimal parameters and effective pre processing technique, local thresholding methods are work well compare with the other one. Niblack's and Sauvola's techniques seem to be the suitable approaches for these types of images. Most of binarized images with these two methods show improvement for readability and character recognition. For this research, even the differences of image result were hard to be distinguished by human capabilities, after comparing the time cost and overall achievement rate of recognized symbols, Niblack's method is performing better than Sauvola's. We could improve the post processing step by adding edge detection techniques and further enhanced by an innovative image refinement technique and a formulation of a class proper method.\n    ",
        "submission_date": "2011-03-29T00:00:00",
        "last_modified_date": "2011-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5776",
        "title": "A Parametric Level Set Approach to Simultaneous Object Identification and Background Reconstruction for Dual Energy Computed Tomography",
        "authors": [
            "Oguz Semerci",
            "Eric L. Miller"
        ],
        "abstract": "Dual energy computerized tomography has gained great interest because of its ability to characterize the chemical composition of a material rather than simply providing relative attenuation images as in conventional tomography. The purpose of this paper is to introduce a novel polychromatic dual energy processing algorithm with an emphasis on detection and characterization of piecewise constant objects embedded in an unknown, cluttered background. Physical properties of the objects, specifically the Compton scattering and photoelectric absorption coefficients, are assumed to be known with some level of uncertainty. Our approach is based on a level-set representation of the characteristic function of the object and encompasses a number of regularization techniques for addressing both the prior information we have concerning the physical properties of the object as well as fundamental, physics-based limitations associated with our ability to jointly recover the Compton scattering and photoelectric absorption properties of the scene. In the absence of an object with appropriate physical properties, our approach returns a null characteristic function and thus can be viewed as simultaneously solving the detection and characterization problems. Unlike the vast majority of methods which define the level set function non-parametrically, i.e., as a dense set of pixel values), we define our level set parametrically via radial basis functions (RBF's) and employ a Gauss-Newton type algorithm for cost minimization. Numerical results show that the algorithm successfully detects objects of interest, finds their shape and location, and gives a adequate reconstruction of the background.\n    ",
        "submission_date": "2011-03-29T00:00:00",
        "last_modified_date": "2011-03-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.5808",
        "title": "Improved Edge Awareness in Discontinuity Preserving Smoothing",
        "authors": [
            "Stuart B. Heinrich",
            "Wesley E. Snyder"
        ],
        "abstract": "Discontinuity preserving smoothing is a fundamentally important procedure that is useful in a wide variety of image processing contexts. It is directly useful for noise reduction, and frequently used as an intermediate step in higher level algorithms. For example, it can be particularly useful in edge detection and segmentation. Three well known algorithms for discontinuity preserving smoothing are nonlinear anisotropic diffusion, bilateral filtering, and mean shift filtering. Although slight differences make them each better suited to different tasks, all are designed to preserve discontinuities while smoothing. However, none of them satisfy this goal perfectly: they each have exception cases in which smoothing may occur across hard edges. The principal contribution of this paper is the identification of a property we call edge awareness that should be satisfied by any discontinuity preserving smoothing algorithm. This constraint can be incorporated into existing algorithms to improve quality, and usually has negligible changes in runtime performance and/or complexity. We present modifications necessary to augment diffusion and mean shift, as well as a new formulation of the bilateral filter that unifies the spatial and range spaces to achieve edge awareness.\n    ",
        "submission_date": "2011-03-30T00:00:00",
        "last_modified_date": "2011-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.6052",
        "title": "Internal Constraints of the Trifocal Tensor",
        "authors": [
            "Stuart B. Heinrich",
            "Wesley E. Snyder"
        ],
        "abstract": "The fundamental matrix and trifocal tensor are convenient algebraic representations of the epipolar geometry of two and three view configurations, respectively. The estimation of these entities is central to most reconstruction algorithms, and a solid understanding of their properties and constraints is therefore very important. The fundamental matrix has 1 internal constraint which is well understood, whereas the trifocal tensor has 8 independent algebraic constraints. The internal tensor constraints can be represented in many ways, although there is only one minimal and sufficient set of 8 constraints known. In this paper, we derive a second set of minimal and sufficient constraints that is simpler. We also show how this can be used in a new parameterization of the trifocal tensor. We hope that this increased understanding of the internal constraints may lead to improved algorithms for estimating the trifocal tensor, although the primary contribution is an improved theoretical understanding.\n    ",
        "submission_date": "2011-03-30T00:00:00",
        "last_modified_date": "2011-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0579",
        "title": "Image Retrieval Method Using Top-surf Descriptor",
        "authors": [
            "Ye Ji"
        ],
        "abstract": "This report presents the results and details of a content-based image retrieval project using the Top-surf descriptor. The experimental results are preliminary, however, it shows the capability of deducing objects from parts of the objects or from the objects that are similar. This paper uses a dataset consisting of 1200 images of which 800 images are equally divided into 8 categories, namely airplane, beach, motorbike, forest, elephants, horses, bus and building, while the other 400 images are randomly picked from the Internet. The best results achieved are from building category.\n    ",
        "submission_date": "2011-04-04T00:00:00",
        "last_modified_date": "2011-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0582",
        "title": "Visual Concept Detection and Real Time Object Detection",
        "authors": [
            "Ran Tao"
        ],
        "abstract": "Bag-of-words model is implemented and tried on 10-class visual concept detection problem. The experimental results show that \"DURF+ERT+SVM\" outperforms \"SIFT+ERT+SVM\" both in detection performance and computation efficiency. Besides, combining DURF and SIFT results in even better detection performance. Real-time object detection using SIFT and RANSAC is also tried on simple objects, e.g. drink can, and good result is achieved.\n    ",
        "submission_date": "2011-04-04T00:00:00",
        "last_modified_date": "2011-04-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1237",
        "title": "A Statistical Nonparametric Approach of Face Recognition: Combination of Eigenface & Modified k-Means Clustering",
        "authors": [
            "Soumen Bag",
            "Soumen Barik",
            "Prithwiraj Sen",
            "Gautam Sanyal"
        ],
        "abstract": "Facial expressions convey non-verbal cues, which play an important role in interpersonal relations. Automatic recognition of human face based on facial expression can be an important component of natural human-machine interface. It may also be used in behavioural science. Although human can recognize the face practically without any effort, but reliable face recognition by machine is a challenge. This paper presents a new approach for recognizing the face of a person considering the expressions of the same human face at different instances of time. This methodology is developed combining Eigenface method for feature extraction and modified k-Means clustering for identification of the human face. This method endowed the face recognition without using the conventional distance measure classifiers. Simulation results show that proposed face recognition using perception of k-Means clustering is useful for face images with different facial expressions.\n    ",
        "submission_date": "2011-04-07T00:00:00",
        "last_modified_date": "2011-04-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1472",
        "title": "Gaussian Affine Feature Detector",
        "authors": [
            "Xiaopeng Xu",
            "Xiaochun Zhang"
        ],
        "abstract": "A new method is proposed to get image features' geometric information. Using Gaussian as an input signal, a theoretical optimal solution to calculate feature's affine shape is proposed. Based on analytic result of a feature model, the method is different from conventional iterative approaches. From the model, feature's parameters such as position, orientation, background luminance, contrast, area and aspect ratio can be extracted. Tested with synthesized and benchmark data, the method achieves or outperforms existing approaches in term of accuracy, speed and stability. The method can detect small, long or thin objects precisely, and works well under general conditions, such as for low contrast, blurred or noisy images.\n    ",
        "submission_date": "2011-04-08T00:00:00",
        "last_modified_date": "2011-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1485",
        "title": "Fuzzy Rules and Evidence Theory for Satellite Image Analysis",
        "authors": [
            "Arijit Laha",
            "J. Das"
        ],
        "abstract": "Design of a fuzzy rule based classifier is proposed. The performance of the classifier for multispectral satellite image classification is improved using Dempster- Shafer theory of evidence that exploits information of the neighboring pixels. The classifiers are tested rigorously with two known images and their performance are found to be better than the results available in the literature. We also demonstrate the improvement of performance while using D-S theory along with fuzzy rule based classifiers over the basic fuzzy rule based classifiers for all the test cases.\n    ",
        "submission_date": "2011-04-08T00:00:00",
        "last_modified_date": "2011-04-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1550",
        "title": "A bio-inspired image coder with temporal scalability",
        "authors": [
            "Khaled Masmoudi",
            "Marc Antonini",
            "Pierre Kornprobst"
        ],
        "abstract": "We present a novel bio-inspired and dynamic coding scheme for static images. Our coder aims at reproducing the main steps of the visual stimulus processing in the mammalian retina taking into account its time behavior. The main novelty of this work is to show how to exploit the time behavior of the retina cells to ensure, in a simple way, scalability and bit allocation. To do so, our main source of inspiration will be the biologically plausible retina model called Virtual Retina. Following a similar structure, our model has two stages. The first stage is an image transform which is performed by the outer layers in the retina. Here it is modelled by filtering the image with a bank of difference of Gaussians with time-delays. The second stage is a time-dependent analog-to-digital conversion which is performed by the inner layers in the retina. Thanks to its conception, our coder enables scalability and bit allocation across time. Also, our decoded images do not show annoying artefacts such as ringing and block effects. As a whole, this article shows how to capture the main properties of a biological system, here the retina, in order to design a new efficient coder.\n    ",
        "submission_date": "2011-04-08T00:00:00",
        "last_modified_date": "2011-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1556",
        "title": "Benchmarking the Quality of Diffusion-Weighted Images",
        "authors": [
            "Jan Klein",
            "Sebastiano Barbieri",
            "Miriam H.A. Bauer",
            "Christopher Nimsky",
            "Horst K. Hahn"
        ],
        "abstract": "We present a novel method that allows for measuring the quality of diffusion-weighted MR images dependent on the image resolution and the image noise. For this purpose, we introduce a new thresholding technique so that noise and the signal can automatically be estimated from a single data set. Thus, no user interaction as well as no double acquisition technique, which requires a time-consuming proper geometrical registration, is needed. As a coarser image resolution or slice thickness leads to a higher signal-to-noise ratio (SNR), our benchmark determines a resolution-independent quality measure so that images with different resolutions can be adequately compared. To evaluate our method, a set of diffusion-weighted images from different vendors is used. It is shown that the quality can efficiently be determined and that the automatically computed SNR is comparable to the SNR which is measured manually in a manually selected region of interest.\n    ",
        "submission_date": "2011-04-08T00:00:00",
        "last_modified_date": "2011-05-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1945",
        "title": "Off-Line Handwritten Signature Retrieval using Curvelet Transforms",
        "authors": [
            "M. S. Shirdhonkar",
            "Manesh B.Kokare"
        ],
        "abstract": "In this paper, a new method for offline handwritten signature retrieval is based on curvelet transform is proposed. Many applications in image processing require similarity retrieval of an image from a large collection of images. In such cases, image indexing becomes important for efficient organization and retrieval of images. This paper addresses this issue in the context of a database of handwritten signature images and describes a system for similarity retrieval. The proposed system uses a curvelet based texture features extraction. The performance of the system has been tested with an image database of 180 signatures. The results obtained indicate that the proposed system is able to identify signatures with great with accuracy even when a part of a signature is missing.\n    ",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2059",
        "title": "Template-based matching using weight maps",
        "authors": [
            "Kwie Min Wong"
        ],
        "abstract": "Template matching is one of the most prevalent pattern recognition methods worldwide. It has found uses in most visual concept detection fields. In this work, we investigate methods for improving template matching by adjusting the weights of different regions of the template. We compare several weight maps and test the methods using the FERET face test set in the context of human eye detection.\n    ",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2069",
        "title": "GEOMIR2K9 - A Similar Scene Finder",
        "authors": [
            "Alwin de Rooij"
        ],
        "abstract": "The main goal of the GEOMIR2K9 project is to create a software program that is able to find similar scenic images clustered by geographical location and sorted by similarity based only on their visual content. The user should be able to input a query image, based on this given query image the program should find relevant visual content and present this to the user in a meaningful way. Technically the goal for the GEOMIR2K9 project is twofold. The first of these two goals is to create a basic low level visual information retrieval system. This includes feature extraction, post processing of the feature data and classification/ clustering based on similarity with a strong focus on scenic images. The second goal of this project is to provide the user with a novel and suitable interface and visualization method so that the user may interact with the retrieved images in a natural and meaningful way.\n    ",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2171",
        "title": "From a Modified Ambrosio-Tortorelli to a Randomized Part Hierarchy Tree",
        "authors": [
            "Sibel Tari",
            "Murat Genctav"
        ],
        "abstract": "We demonstrate the possibility of coding parts, features that are higher level than boundaries, using a modified AT field after augmenting the interaction term of the AT energy with a non-local term and weakening the separation into boundary/not-boundary phases. The iteratively extracted parts using the level curves with double point singularities are organized as a proper binary tree. Inconsistencies due to non-generic configurations for level curves as well as due to visual changes such as occlusion are successfully handled once the tree is endowed with a probabilistic structure. The work is a step in establishing the AT function as a bridge between low and high level visual processing.\n    ",
        "submission_date": "2011-04-12T00:00:00",
        "last_modified_date": "2011-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2175",
        "title": "Extracting Parts of 2D Shapes Using Local and Global Interactions Simultaneously",
        "authors": [
            "Sibel Tari"
        ],
        "abstract": "Perception research provides strong evidence in favor of part based representation of shapes in human visual system. Despite considerable differences among different theories in terms of how part boundaries are found, there is substantial agreement on that the process depends on many local and global geometric factors. This poses an important challenge from the computational point of view. In the first part of the chapter, I present a novel decomposition method by taking both local and global interactions within the shape domain into account. At the top of the partitioning hierarchy, the shape gets split into two parts capturing, respectively, the gross structure and the peripheral structure. The gross structure may be conceived as the least deformable part of the shape which remains stable under visual transformations. The peripheral structure includes limbs, protrusions, and boundary texture. Such a separation is in accord with the behavior of the artists who start with a gross shape and enrich it with details. The method is particularly interesting from the computational point of view as it does not resort to any geometric notions (e.g. curvature, convexity) explicitly. In the second part of the chapter, I relate the new method to PDE based shape representation schemes.\n    ",
        "submission_date": "2011-04-12T00:00:00",
        "last_modified_date": "2011-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2285",
        "title": "Elimination of Specular reflection and Identification of ROI: The First Step in Automated Detection of Cervical Cancer using Digital Colposcopy",
        "authors": [
            "Abhishek Das",
            "Avijit Kar",
            "Debasis Bhattacharyya"
        ],
        "abstract": "Cervical Cancer is one of the most common forms of cancer in women worldwide. Most cases of cervical cancer can be prevented through screening programs aimed at detecting precancerous lesions. During Digital Colposcopy, Specular Reflections (SR) appear as bright spots heavily saturated with white light. These occur due to the presence of moisture on the uneven cervix surface, which act like mirrors reflecting light from the illumination source. Apart from camouflaging the actual features, the SR also affects subsequent segmentation routines and hence must be removed. Our novel technique eliminates the SR and makes the colposcopic images (cervigram) ready for segmentation algorithms. The cervix region occupies about half of the cervigram image. Other parts of the image contain irrelevant information, such as equipment, frames, text and non-cervix tissues. This irrelevant information can confuse automatic identification of the tissues within the cervix. The first step is, therefore, focusing on the cervical borders, so that we have a geometric boundary on the relevant image area. We have proposed a type of modified kmeans clustering algorithm to evaluate the region of interest.\n    ",
        "submission_date": "2011-04-12T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2580",
        "title": "Hypothesize and Bound: A Computational Focus of Attention Mechanism for Simultaneous N-D Segmentation, Pose Estimation and Classification Using Shape Priors",
        "authors": [
            "Diego Rother",
            "Simon Sch\u00fctz",
            "Ren\u00e9 Vidal"
        ],
        "abstract": "Given the ever increasing bandwidth of the visual information available to many intelligent systems, it is becoming essential to endow them with a sense of what is worthwhile their attention and what can be safely disregarded. This article presents a general mathematical framework to efficiently allocate the available computational resources to process the parts of the input that are relevant to solve a given perceptual problem. By this we mean to find the hypothesis H (i.e., the state of the world) that maximizes a function L(H), representing how well each hypothesis \"explains\" the input. Given the large bandwidth of the sensory input, fully evaluating L(H) for each hypothesis H is computationally infeasible (e.g., because it would imply checking a large number of pixels). To address this problem we propose a mathematical framework with two key ingredients. The first one is a Bounding Mechanism (BM) to compute lower and upper bounds of L(H), for a given computational budget. These bounds are much cheaper to compute than L(H) itself, can be refined at any time by increasing the budget allocated to a hypothesis, and are frequently enough to discard a hypothesis. To compute these bounds, we develop a novel theory of shapes and shape priors. The second ingredient is a Focus of Attention Mechanism (FoAM) to select which hypothesis' bounds should be refined next, with the goal of discarding non-optimal hypotheses with the least amount of computation. The proposed framework: 1) is very efficient since most hypotheses are discarded with minimal computation; 2) is parallelizable; 3) is guaranteed to find the globally optimal hypothesis; and 4) its running time depends on the problem at hand, not on the bandwidth of the input. We instantiate the proposed framework for the problem of simultaneously estimating the class, pose, and a noiseless version of a 2D shape in a 2D image.\n    ",
        "submission_date": "2011-04-13T00:00:00",
        "last_modified_date": "2011-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2745",
        "title": "An Axis-Based Representation for Recognition",
        "authors": [
            "Cagri Aslan",
            "Sibel Tari"
        ],
        "abstract": "This paper presents a new axis-based shape representation scheme along with a matching framework to address the problem of generic shape recognition. The main idea is to define the relative spatial arrangement of local symmetry axes and their metric properties in a shape centered coordinate frame. The resulting descriptions are invariant to scale, rotation, small changes in viewpoint and articulations. Symmetry points are extracted from a surface whose level curves roughly mimic the motion by curvature. By increasing the amount of smoothing on the evolving curve, only those symmetry axes that correspond to the most prominent parts of a shape are extracted. The representation does not suffer from the common instability problems of the traditional connected skeletons. It captures the perceptual qualities of shapes well. Therefore finding the similarities and the differences among shapes becomes easier. The matching process gives highly successful results on a diverse database of 2D shapes.\n    ",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2011-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.2751",
        "title": "Disconnected Skeleton: Shape at its Absolute Scale",
        "authors": [
            "C. Aslan",
            "A. Erdem",
            "E. Erdem",
            "S. Tari"
        ],
        "abstract": "We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons, and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global Euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation and articulation, as well as invariant ones.\n    ",
        "submission_date": "2011-04-14T00:00:00",
        "last_modified_date": "2011-04-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3513",
        "title": "An Effect of Spatial Filtering in Visualization of Coronary Arteries Imaging",
        "authors": [
            "B. G. Kodge",
            "P. S. Hiremath"
        ],
        "abstract": "At present, coronary angiography is the well known standard for the diagnosis of coronary artery disease. Conventional coronary angiography is an invasive procedure with a small, yet inherent risk of myocardial infarction, stroke, potential arrhythmias, and death. Other noninvasive diagnostic tools, such as electrocardiography, echocardiography, and nuclear imaging are now widely available but are limited by their inability to directly visualize and quantify coronary artery stenoses and predict the stability of plaques. Coronary magnetic resonance angiography (MRA) is a technique that allows visualization of the coronary arteries by noninvasive means; however, it has not yet reached a stage where it can be used in routine clinical practice. Although coronary MRA is a potentially useful diagnostic tool, it has limitations. Further research should focus on improving the diagnostic resolution and accuracy of coronary MRA. This paper will helps to cardiologists to take the clear look of spatial filtered imaging of coronary arteries.\n    ",
        "submission_date": "2011-03-28T00:00:00",
        "last_modified_date": "2011-03-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3742",
        "title": "Hue Histograms to Spatiotemporal Local Features for Action Recognition",
        "authors": [
            "Fillipe Souza",
            "Eduardo Valle",
            "Guillermo Ch\u00e1vez",
            "Arnaldo Ara\u00fajo"
        ],
        "abstract": "Despite the recent developments in spatiotemporal local features for action recognition in video sequences, local color information has so far been ignored. However, color has been proved an important element to the success of automated recognition of objects and scenes. In this paper we extend the space-time interest point descriptor STIP to take into account the color information on the features' neighborhood. We compare the performance of our color-aware version of STIP (which we have called HueSTIP) with the original one.\n    ",
        "submission_date": "2011-04-19T00:00:00",
        "last_modified_date": "2011-04-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4168",
        "title": "A Meshless Method for Variational Nonrigid 2-D Shape Registration",
        "authors": [
            "Wei Liu",
            "Eraldo Ribeiro"
        ],
        "abstract": "We present a method for nonrigid registration of 2-D geometric shapes. Our contribution is twofold. First, we extend the classic chamfer-matching energy to a variational functional. Secondly, we introduce a meshless deformation model that can handle significant high-curvature deformations. We represent 2-D shapes implicitly using distance transforms, and registration error is defined based on the shape contours' mutual distances. In addition, we model global shape deformation as an approximation blended from local deformation fields using partition-of-unity. The global deformation field is regularized by penalizing inconsistencies between local fields. The representation can be made adaptive to shape's contour, leading to registration that is both flexible and efficient. Finally, registration is achieved by minimizing a variational chamfer-energy functional combined with the consistency regularizer. We demonstrate the effectiveness of our method on a number of experiments.\n    ",
        "submission_date": "2011-04-21T00:00:00",
        "last_modified_date": "2011-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4295",
        "title": "Improving digital signal interpolation: L2-optimal kernels with kernel-invariant interpolation speed",
        "authors": [
            "Oleg S. Pianykh"
        ],
        "abstract": "Interpolation is responsible for digital signal resampling and can significantly degrade the original signal quality if not done properly. For many years, optimal interpolation algorithms were sought within constrained classes of interpolation kernel functions. We derive a new family of unconstrained L2-optimal interpolation kernels, and compare their properties to the previously known. Although digital images are used to illustrate this work, our L2-optimal kernels can be applied to interpolate any digital signals.\n    ",
        "submission_date": "2011-04-21T00:00:00",
        "last_modified_date": "2011-04-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4298",
        "title": "Curved Gabor Filters for Fingerprint Image Enhancement",
        "authors": [
            "Carsten Gottschlich"
        ],
        "abstract": "Gabor filters play an important role in many application areas for the enhancement of various types of images and the extraction of Gabor features. For the purpose of enhancing curved structures in noisy images, we introduce curved Gabor filters which locally adapt their shape to the direction of flow. These curved Gabor filters enable the choice of filter parameters which increase the smoothing power without creating artifacts in the enhanced image. In this paper, curved Gabor filters are applied to the curved ridge and valley structure of low-quality fingerprint images. First, we combine two orientation field estimation methods in order to obtain a more robust estimation for very noisy images. Next, curved regions are constructed by following the respective local orientation and they are used for estimating the local ridge frequency. Lastly, curved Gabor filters are defined based on curved regions and they are applied for the enhancement of low-quality fingerprint images. Experimental results on the FVC2004 databases show improvements of this approach in comparison to state-of-the-art enhancement methods.\n    ",
        "submission_date": "2011-04-21T00:00:00",
        "last_modified_date": "2014-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4385",
        "title": "Convex Approaches to Model Wavelet Sparsity Patterns",
        "authors": [
            "Nikhil S Rao",
            "Robert D. Nowak",
            "Stephen J. Wright",
            "Nick G. Kingsbury"
        ],
        "abstract": "Statistical dependencies among wavelet coefficients are commonly represented by graphical models such as hidden Markov trees(HMTs). However, in linear inverse problems such as deconvolution, tomography, and compressed sensing, the presence of a sensing or observation matrix produces a linear mixing of the simple Markovian dependency structure. This leads to reconstruction problems that are non-convex optimizations. Past work has dealt with this issue by resorting to greedy or suboptimal iterative reconstruction methods. In this paper, we propose new modeling approaches based on group-sparsity penalties that leads to convex optimizations that can be solved exactly and efficiently. We show that the methods we develop perform significantly better in deconvolution and compressed sensing applications, while being as computationally efficient as standard coefficient-wise approaches such as lasso.\n    ",
        "submission_date": "2011-04-22T00:00:00",
        "last_modified_date": "2011-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4704",
        "title": "Positive Semidefinite Metric Learning Using Boosting-like Algorithms",
        "authors": [
            "Chunhua Shen",
            "Junae Kim",
            "Lei Wang",
            "Anton van den Hengel"
        ],
        "abstract": "The success of many machine learning and pattern recognition methods relies heavily upon the identification of an appropriate distance metric on the input data. It is often beneficial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed BoostMetric, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive definite. Semidefinite programming is often used to enforce this constraint, but does not scale well and easy to implement. BoostMetric is instead based on the observation that any positive semidefinite matrix can be decomposed into a linear combination of trace-one rank-one matrices. BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting methods are easy to implement, efficient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semidefinite matrix with trace and rank being one rather than a classifier or regressor. Experiments on various datasets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classification accuracy and running time.\n    ",
        "submission_date": "2011-04-25T00:00:00",
        "last_modified_date": "2012-04-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4723",
        "title": "Bayesian approach for near-duplicate image detection",
        "authors": [
            "Lucas Moutinho Bueno",
            "Eduardo Valle",
            "Ricardo da Silva Torres"
        ],
        "abstract": "In this paper we propose a bayesian approach for near-duplicate image detection, and investigate how different probabilistic models affect the performance obtained. The task of identifying an image whose metadata are missing is often demanded for a myriad of applications: metadata retrieval in cultural institutions, detection of copyright violations, investigation of latent cross-links in archives and libraries, duplicate elimination in storage management, etc. The majority of current solutions are based either on voting algorithms, which are very precise, but expensive; either on the use of visual dictionaries, which are efficient, but less precise. Our approach, uses local descriptors in a novel way, which by a careful application of decision theory, allows a very fine control of the compromise between precision and efficiency. In addition, the method attains a great compromise between those two axes, with more than 99% accuracy with less than 10 database operations.\n    ",
        "submission_date": "2011-04-25T00:00:00",
        "last_modified_date": "2011-04-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4989",
        "title": "Preprocessing: A Step in Automating Early Detection of Cervical Cancer",
        "authors": [
            "Abhishek Das",
            "Avijit Kar",
            "Debasis Bhattacharyya"
        ],
        "abstract": "This paper has been withdrawn\n    ",
        "submission_date": "2011-04-26T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5284",
        "title": "Content-Based Spam Filtering on Video Sharing Social Networks",
        "authors": [
            "Antonio da Luz",
            "Eduardo Valle",
            "Arnaldo Araujo"
        ],
        "abstract": "In this work we are concerned with the detection of spam in video sharing social networks. Specifically, we investigate how much visual content-based analysis can aid in detecting spam in videos. This is a very challenging task, because of the high-level semantic concepts involved; of the assorted nature of social networks, preventing the use of constrained a priori information; and, what is paramount, of the context dependent nature of spam. Content filtering for social networks is an increasingly demanded task: due to their popularity, the number of abuses also tends to increase, annoying the user base and disrupting their services. We systematically evaluate several approaches for processing the visual information: using static and dynamic (motionaware) features, with and without considering the context, and with or without latent semantic analysis (LSA). Our experiments show that LSA is helpful, but taking the context into consideration is paramount. The whole scheme shows good results, showing the feasibility of the concept.\n    ",
        "submission_date": "2011-04-28T00:00:00",
        "last_modified_date": "2011-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.5304",
        "title": "A supervised clustering approach for fMRI-based inference of brain states",
        "authors": [
            "Vincent Michel",
            "Alexandre Gramfort",
            "Ga\u00ebl Varoquaux",
            "Evelyn Eger",
            "Christine Keribin",
            "Bertrand Thirion"
        ],
        "abstract": "We propose a method that combines signals from many brain regions observed in functional Magnetic Resonance Imaging (fMRI) to predict the subject's behavior during a scanning session. Such predictions suffer from the huge number of brain regions sampled on the voxel grid of standard fMRI data sets: the curse of dimensionality. Dimensionality reduction is thus needed, but it is often performed using a univariate feature selection procedure, that handles neither the spatial structure of the images, nor the multivariate nature of the signal. By introducing a hierarchical clustering of the brain volume that incorporates connectivity constraints, we reduce the span of the possible spatial configurations to a single tree of nested regions tailored to the signal. We then prune the tree in a supervised setting, hence the name supervised clustering, in order to extract a parcellation (division of the volume) such that parcel-based signal averages best predict the target information. Dimensionality reduction is thus achieved by feature agglomeration, and the constructed features now provide a multi-scale representation of the signal. Comparisons with reference methods on both simulated and real data show that our approach yields higher prediction accuracy than standard voxel-based approaches. Moreover, the method infers an explicit weighting of the regions involved in the regression or classification task.\n    ",
        "submission_date": "2011-04-28T00:00:00",
        "last_modified_date": "2011-04-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0079",
        "title": "An Automated Size Recognition Technique for Acetabular Implant in Total Hip Replacement",
        "authors": [
            "Azrulhizam Shapi'i",
            "Riza Sulaiman",
            "Mohammad Khatim Hasan",
            "Abdul Yazid Mohd Kassim"
        ],
        "abstract": "Preoperative templating in Total Hip Replacement (THR) is a method to estimate the optimal size and position of the implant. Today, observational (manual) size recognition techniques are still used to find a suitable implant for the patient. Therefore, a digital and automated technique should be developed so that the implant size recognition process can be effectively implemented. For this purpose, we have introduced the new technique for acetabular implant size recognition in THR preoperative planning based on the diameter of acetabulum size. This technique enables the surgeon to recognise a digital acetabular implant size automatically. Ten randomly selected X-rays of unidentified patients were used to test the accuracy and utility of an automated implant size recognition technique. Based on the testing result, the new technique yielded very close results to those obtained by the observational method in nine studies (90%).\n    ",
        "submission_date": "2011-04-30T00:00:00",
        "last_modified_date": "2011-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0821",
        "title": "Considerations and Results in Multimedia and DVB Application Development on Philips Nexperia Platform",
        "authors": [
            "Radu Arsinte",
            "Ciprian Ilioaei"
        ],
        "abstract": "This paper presents some experiments regarding applications development on high performance media processors included in Philips Nexperia Family. The PNX1302 dedicated DVB-T kit used has some limitations. Our work has succeeded to overcome these limitations and to make possible a general-purpose use of this kit. For exemplification two typical applications, important both for multimedia and DVB, are analyzed: MPEG2 video stream decoding and MP3 audio decoding. These original implementations are compared (in speed, memory requirements and costs) with Philips Nexperia Library.\n    ",
        "submission_date": "2011-05-04T00:00:00",
        "last_modified_date": "2011-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2491",
        "title": "A Multiple Component Matching Framework for Person Re-Identification",
        "authors": [
            "Riccardo Satta",
            "Giorgio Fumera",
            "Fabio Roli",
            "Marco Cristani",
            "Vittorio Murino"
        ],
        "abstract": "Person re-identification consists in recognizing an individual that has already been observed over a network of cameras. It is a novel and challenging research topic in computer vision, for which no reference framework exists yet. Despite this, previous works share similar representations of human body based on part decomposition and the implicit concept of multiple instances. Building on these similarities, we propose a Multiple Component Matching (MCM) framework for the person re-identification problem, which is inspired by Multiple Component Learning, a framework recently proposed for object detection. We show that previous techniques for person re-identification can be considered particular implementations of our MCM framework. We then present a novel person re-identification technique as a direct, simple implementation of our framework, focused in particular on robustness to varying lighting conditions, and show that it can attain state of the art performances.\n    ",
        "submission_date": "2011-05-12T00:00:00",
        "last_modified_date": "2011-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2782",
        "title": "$\\ell_0$ Minimization for Wavelet Frame Based Image Restoration",
        "authors": [
            "Yong Zhang",
            "Bin Dong",
            "Zhaosong Lu"
        ],
        "abstract": "The theory of (tight) wavelet frames has been extensively studied in the past twenty years and they are currently widely used for image restoration and other image processing and analysis problems. The success of wavelet frame based models, including balanced approach and analysis based approach, is due to their capability of sparsely approximating piecewise smooth functions like images. Motivated by the balanced approach and analysis based approach, we shall propose a wavelet frame based $\\ell_0$ minimization model, where the $\\ell_0$ \"norm\" of the frame coefficients is penalized. We adapt the penalty decomposition (PD) method to solve the proposed optimization problem. Numerical results showed that the proposed model solved by the PD method can generate images with better quality than those obtained by either analysis based approach or balanced approach in terms of restoring sharp features as well as maintaining smoothness of the recovered images. Some convergence analysis of the PD method will also be provided.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-10-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2795",
        "title": "View subspaces for indexing and retrieval of 3D models",
        "authors": [
            "Helin Dutagaci",
            "Afzal Godil",
            "Bulent Sankur",
            "Y\u00fccel Yemez"
        ],
        "abstract": "View-based indexing schemes for 3D object retrieval are gaining popularity since they provide good retrieval results. These schemes are coherent with the theory that humans recognize objects based on their 2D appearances. The viewbased techniques also allow users to search with various queries such as binary images, range images and even 2D sketches. The previous view-based techniques use classical 2D shape descriptors such as Fourier invariants, Zernike moments, Scale Invariant Feature Transform-based local features and 2D Digital Fourier Transform coefficients. These methods describe each object independent of others. In this work, we explore data driven subspace models, such as Principal Component Analysis, Independent Component Analysis and Nonnegative Matrix Factorization to describe the shape information of the views. We treat the depth images obtained from various points of the view sphere as 2D intensity images and train a subspace to extract the inherent structure of the views within a database. We also show the benefit of categorizing shapes according to their eigenvalue spread. Both the shape categorization and data-driven feature set conjectures are tested on the PSB database and compared with the competitor view-based 3D shape retrieval algorithms\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2796",
        "title": "Salient Local 3D Features for 3D Shape Retrieval",
        "authors": [
            "Afzal Godil",
            "Asim Imdad Wagan"
        ],
        "abstract": "In this paper we describe a new formulation for the 3D salient local features based on the voxel grid inspired by the Scale Invariant Feature Transform (SIFT). We use it to identify the salient keypoints (invariant points) on a 3D voxelized model and calculate invariant 3D local feature descriptors at these keypoints. We then use the bag of words approach on the 3D local features to represent the 3D models for shape retrieval. The advantages of the method are that it can be applied to rigid as well as to articulated and deformable 3D models. Finally, this approach is applied for 3D Shape Retrieval on the McGill articulated shape benchmark and then the retrieval results are presented and compared to other methods.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2797",
        "title": "Face Recognition using 3D Facial Shape and Color Map Information: Comparison and Combination",
        "authors": [
            "Afzal Godil",
            "Sandy Ressler",
            "Patrick Grother"
        ],
        "abstract": "In this paper, we investigate the use of 3D surface geometry for face recognition and compare it to one based on color map information. The 3D surface and color map data are from the CAESAR anthropometric database. We find that the recognition performance is not very different between 3D surface and color map information using a principal component analysis algorithm. We also discuss the different techniques for the combination of the 3D surface and color map information for multi-modal recognition by using different fusion approaches and show that there is significant improvement in results. The effectiveness of various techniques is compared and evaluated on a dataset with 200 subjects in two different positions.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2800",
        "title": "Retrieval and Clustering from a 3D Human Database based on Body and Head Shape",
        "authors": [
            "Afzal Godil",
            "Sandy Ressler"
        ],
        "abstract": "In this paper, we describe a framework for similarity based retrieval and clustering from a 3D human database. Our technique is based on both body and head shape representation and the retrieval is based on similarity of both of them. The 3D human database used in our study is the CAESAR anthropometric database which contains approximately 5000 bodies. We have developed a web-based interface for specifying the queries to interact with the retrieval system. Our approach performs the similarity based retrieval in a reasonable amount of time and is a practical approach.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3270",
        "title": "Optimal Camera Placement to measure Distances Conservativly Regarding Static and Dynamic Obstacles",
        "authors": [
            "Maria H\u00e4nel",
            "Stefan Kuhn",
            "Dominik Henrich",
            "Lars Gr\u00fcne",
            "J\u00fcrgen Pannek"
        ],
        "abstract": "In modern production facilities industrial robots and humans are supposed to interact sharing a common working area. In order to avoid collisions, the distances between objects need to be measured conservatively which can be done by a camera network. To estimate the acquired distance, unmodelled objects, e.g., an interacting human, need to be modelled and distinguished from premodelled objects like workbenches or robots by image processing such as the background subtraction method.\n",
        "submission_date": "2011-05-17T00:00:00",
        "last_modified_date": "2011-05-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3559",
        "title": "Invariant Representative Cocycles of Cohomology Generators using Irregular Graph Pyramids",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Adrian Ion",
            "Mabel Iglesias-Ham",
            "Walter G. Kropatsch"
        ],
        "abstract": "Structural pattern recognition describes and classifies data based on the relationships of features and parts. Topological invariants, like the Euler number, characterize the structure of objects of any dimension. Cohomology can provide more refined algebraic invariants to a topological space than does homology. It assigns `quantities' to the chains used in homology to characterize holes of any dimension. Graph pyramids can be used to describe subdivisions of the same object at multiple levels of detail. This paper presents cohomology in the context of structural pattern recognition and introduces an algorithm to efficiently compute representative cocycles (the basic elements of cohomology) in 2D using a graph pyramid. An extension to obtain scanning and rotation invariant cocycles is given.\n    ",
        "submission_date": "2011-05-18T00:00:00",
        "last_modified_date": "2011-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3617",
        "title": "Face Shape and Reflectance Acquisition using a Multispectral Light Stage",
        "authors": [
            "Abhishek Dutta"
        ],
        "abstract": "In this thesis, we discuss the design and calibration (geometric and radiometric) of a novel shape and reflectance acquisition device called the \"Multispectral Light Stage\". This device can capture highly detailed facial geometry (down to the level of skin pores detail) and Multispectral reflectance map which can be used to estimate biophysical skin parameters such as the distribution of pigmentation and blood beneath the surface of the skin. We extend the analysis of the original spherical gradient photometric stereo method to study the effects of deformed diffuse lobes on the quality of recovered surface normals. Based on our modified radiance equations, we develop a minimal image set method to recover high quality photometric normals using only four, instead of six, spherical gradient images. Using the same radiance equations, we explore a Quadratic Programming (QP) based algorithm for correction of surface normals obtained using spherical gradient photometric stereo. Based on the proposed minimal image sets method, we present a performance capture sequence that significantly reduces the data capture requirement and post-processing computational cost of existing photometric stereo based performance geometry capture methods. Furthermore, we explore the use of images captured in our Light Stage to generate stimuli images for a psychology experiment exploring the neural representation of 3D shape and texture of a human face.\n    ",
        "submission_date": "2011-05-18T00:00:00",
        "last_modified_date": "2011-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3685",
        "title": "Benchmarks, Performance Evaluation and Contests for 3D Shape Retrieval",
        "authors": [
            "Afzal Godil",
            "Zhouhui Lian",
            "Helin Dutagaci",
            "Rui Fang",
            "Vanamali T.P.",
            "Chun Pan Cheung"
        ],
        "abstract": "Benchmarking of 3D Shape retrieval allows developers and researchers to compare the strengths of different algorithms on a standard dataset. Here we describe the procedures involved in developing a benchmark and issues involved. We then discuss some of the current 3D shape retrieval benchmarks efforts of our group and others. We also review the different performance evaluation measures that are developed and used by researchers in the community. After that we give an overview of the 3D shape retrieval contest (SHREC) tracks run under the EuroGraphics Workshop on 3D Object Retrieval and give details of tracks that we organized for SHREC 2010. Finally we demonstrate some of the results based on the different SHREC contest tracks and the NIST shape benchmark.\n    ",
        "submission_date": "2011-05-18T00:00:00",
        "last_modified_date": "2011-05-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3828",
        "title": "An Algorithmic Solution to the Five-Point Pose Problem Based on the Cayley Representation of Rotations",
        "authors": [
            "Evgeniy Martyushev"
        ],
        "abstract": "We give a new algorithmic solution to the well-known five-point relative pose problem. Our approach does not deal with the famous cubic constraint on an essential matrix. Instead, we use the Cayley representation of rotations in order to obtain a polynomial system from epipolar constraints. Solving that system, we directly get relative rotation and translation parameters of the cameras in terms of roots of a 10th degree polynomial.\n    ",
        "submission_date": "2011-05-19T00:00:00",
        "last_modified_date": "2013-02-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3834",
        "title": "A Multiple-Choice Test Recognition System based on the Gamera Framework",
        "authors": [
            "Andrea Spadaccini",
            "Vanni Rizzo"
        ],
        "abstract": "This article describes JECT-OMR, a system that analyzes digital images representing scans of multiple-choice tests compiled by students. The system performs a structural analysis of the document in order to get the chosen answer for each question, and it also contains a bar-code decoder, used for the identification of additional information encoded in the document. JECT-OMR was implemented using the Python programming language, and leverages the power of the Gamera framework in order to accomplish its task. The system exhibits an accuracy of over 99% in the recognition of marked and non-marked squares representing answers, thus making it suitable for real world applications\n    ",
        "submission_date": "2011-05-19T00:00:00",
        "last_modified_date": "2011-05-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4058",
        "title": "Human Identity Verification based on Heart Sounds: Recent Advances and Future Directions",
        "authors": [
            "Francesco Beritelli",
            "Andrea Spadaccini"
        ],
        "abstract": "Identity verification is an increasingly important process in our daily lives, and biometric recognition is a natural solution to the authentication problem.\n",
        "submission_date": "2011-05-20T00:00:00",
        "last_modified_date": "2011-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4183",
        "title": "Cubical Cohomology Ring of 3D Photographs",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Maria Jose Jimenez",
            "Belen Medrano"
        ],
        "abstract": "Cohomology and cohomology ring of three-dimensional (3D) objects are topological invariants that characterize holes and their relations. Cohomology ring has been traditionally computed on simplicial complexes. Nevertheless, cubical complexes deal directly with the voxels in 3D images, no additional triangulation is necessary, facilitating efficient algorithms for the computation of topological invariants in the image context. In this paper, we present formulas to directly compute the cohomology ring of 3D cubical complexes without making use of any additional triangulation. Starting from a cubical complex $Q$ that represents a 3D binary-valued digital picture whose foreground has one connected component, we compute first the cohomological information on the boundary of the object, $\\partial Q$ by an incremental technique; then, using a face reduction algorithm, we compute it on the whole object; finally, applying the mentioned formulas, the cohomology ring is computed from such information.\n    ",
        "submission_date": "2011-05-20T00:00:00",
        "last_modified_date": "2011-05-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4204",
        "title": "Fast O(1) bilateral filtering using trigonometric range kernels",
        "authors": [
            "Kunal Narayan Chaudhury",
            "Daniel Sage",
            "Michael Unser"
        ],
        "abstract": "It is well-known that spatial averaging can be realized (in space or frequency domain) using algorithms whose complexity does not depend on the size or shape of the filter. These fast algorithms are generally referred to as constant-time or O(1) algorithms in the image processing literature. Along with the spatial filter, the edge-preserving bilateral filter [Tomasi1998] involves an additional range kernel. This is used to restrict the averaging to those neighborhood pixels whose intensity are similar or close to that of the pixel of interest. The range kernel operates by acting on the pixel intensities. This makes the averaging process non-linear and computationally intensive, especially when the spatial filter is large. In this paper, we show how the O(1) averaging algorithms can be leveraged for realizing the bilateral filter in constant-time, by using trigonometric range kernels. This is done by generalizing the idea in [Porikli2008] of using polynomial range kernels. The class of trigonometric kernels turns out to be sufficiently rich, allowing for the approximation of the standard Gaussian bilateral filter. The attractive feature of our approach is that, for a fixed number of terms, the quality of approximation achieved using trigonometric kernels is much superior to that obtained in [Porikli2008] using polynomials.\n    ",
        "submission_date": "2011-05-21T00:00:00",
        "last_modified_date": "2011-07-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4354",
        "title": "Preprocessing for Automating Early Detection of Cervical Cancer",
        "authors": [
            "Abhishek Das",
            "Avijit Kar",
            "Debasis Bhattacharyya"
        ],
        "abstract": "Uterine Cervical Cancer is one of the most common forms of cancer in women worldwide. Most cases of cervical cancer can be prevented through screening programs aimed at detecting precancerous lesions. During Digital Colposcopy, colposcopic images or cervigrams are acquired in raw form. They contain specular reflections which appear as bright spots heavily saturated with white light and occur due to the presence of moisture on the uneven cervix surface and. The cervix region occupies about half of the raw cervigram image. Other parts of the image contain irrelevant information, such as equipment, frames, text and non-cervix tissues. This irrelevant information can confuse automatic identification of the tissues within the cervix. Therefore we focus on the cervical borders, so that we have a geometric boundary on the relevant image area. Our novel technique eliminates the SR, identifies the region of interest and makes the cervigram ready for segmentation algorithms.\n    ",
        "submission_date": "2011-05-22T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4477",
        "title": "On the Cohomology of 3D Digital Images",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Pedro Real"
        ],
        "abstract": "We propose a method for computing the cohomology ring of three--dimensional (3D) digital binary-valued pictures. We obtain the cohomology ring of a 3D digital binary--valued picture $I$, via a simplicial complex K(I)topologically representing (up to isomorphisms of pictures) the picture I. The usefulness of a simplicial description of the \"digital\" cohomology ring of 3D digital binary-valued pictures is tested by means of a small program visualizing the different steps of the method. Some examples concerning topological thinning, the visualization of representative (co)cycles of (co)homology generators and the computation of the cup product on the cohomology of simple pictures are showed.\n    ",
        "submission_date": "2011-05-23T00:00:00",
        "last_modified_date": "2011-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4480",
        "title": "A Tool for Integer Homology Computation: Lambda-At Model",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Maria Jose Jimenez",
            "Belen Medrano",
            "Pedro Real"
        ],
        "abstract": "In this paper, we formalize the notion of lambda-AT-model (where $\\lambda$ is a non-null integer) for a given chain complex, which allows the computation of homological information in the integer domain avoiding using the Smith Normal Form of the boundary matrices. We present an algorithm for computing such a model, obtaining Betti numbers, the prime numbers p involved in the invariant factors of the torsion subgroup of homology, the amount of invariant factors that are a power of p and a set of representative cycles of generators of homology mod p, for each p. Moreover, we establish the minimum valid lambda for such a construction, what cuts down the computational costs related to the torsion subgroup. The tools described here are useful to determine topological information of nD structured objects such as simplicial, cubical or simploidal complexes and are applicable to extract such an information from digital pictures.\n    ",
        "submission_date": "2011-05-23T00:00:00",
        "last_modified_date": "2011-05-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.4712",
        "title": "Image Splicing Detection Using Inherent Lens Radial Distortion",
        "authors": [
            "H. R. Chennamma",
            "Lalitha Rangarajan"
        ],
        "abstract": "Image splicing is a common form of image forgery. Such alterations may leave no visual clues of tampering. In recent works camera characteristics consistency across the image has been used to establish the authenticity and integrity of digital images. Such constant camera characteristic properties are inherent from camera manufacturing processes and are unique. The majority of digital cameras are equipped with spherical lens and this introduces radial distortions on images. This aberration is often disturbed and fails to be consistent across the image, when an image is spliced. This paper describes the detection of splicing operation on images by estimating radial distortion from different portions of the image using line-based calibration. For the first time, the detection of image splicing through the verification of consistency of lens radial distortion has been explored in this paper. The conducted experiments demonstrate the efficacy of our proposed approach for the detection of image splicing on both synthetic and real images.\n    ",
        "submission_date": "2011-05-24T00:00:00",
        "last_modified_date": "2011-05-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5307",
        "title": "Efficient Learning of Sparse Invariant Representations",
        "authors": [
            "Karol Gregor",
            "Yann LeCun"
        ],
        "abstract": "We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.\n    ",
        "submission_date": "2011-05-26T00:00:00",
        "last_modified_date": "2011-05-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6014",
        "title": "Neural Networks for Emotion Classification",
        "authors": [
            "Yafei Sun"
        ],
        "abstract": "It is argued that for the computer to be able to interact with humans, it needs to have the communication skills of humans. One of these skills is the ability to understand the emotional state of the person. This thesis describes a neural network-based approach for emotion classification. We learn a classifier that can recognize six basic emotions with an average accuracy of 77% over the Cohn-Kanade database. The novelty of this work is that instead of empirically selecting the parameters of the neural network, i.e. the learning rate, activation function parameter, momentum number, the number of nodes in one layer, etc. we developed a strategy that can automatically select comparatively better combination of these parameters. We also introduce another way to perform back propagation. Instead of using the partial differential of the error function, we use optimal algorithm; namely Powell's direction set to minimize the error function. We were also interested in construction an authentic emotion databases. This is a very important task because nowadays there is no such database available. Finally, we perform several experiments and show that our neural network approach can be successfully used for emotion recognition.\n    ",
        "submission_date": "2011-05-30T00:00:00",
        "last_modified_date": "2011-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6060",
        "title": "Alignment of Microtubule Imagery",
        "authors": [
            "Feiyang Yu",
            "Ard Oerlemans",
            "Erwin M. Bakker"
        ],
        "abstract": "This work discusses preliminary work aimed at simulating and visualizing the growth process of a tiny structure inside the cell---the microtubule. Difficulty of recording the process lies in the fact that the tissue preparation method for electronic microscopes is highly destructive to live cells. Here in this paper, our approach is to take pictures of microtubules at different time slots and then appropriately combine these images into a coherent video. Experimental results are given on real data.\n    ",
        "submission_date": "2011-05-30T00:00:00",
        "last_modified_date": "2011-05-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6277",
        "title": "Incremental Top-k List Comparison Approach to Robust Multi-Structure Model Fitting",
        "authors": [
            "Hoi Sim Wong",
            "Tat-Jun Chin",
            "Jin Yu",
            "David Suter"
        ],
        "abstract": "Random hypothesis sampling lies at the core of many popular robust fitting techniques such as RANSAC. In this paper, we propose a novel hypothesis sampling scheme based on incremental computation of distances between partial rankings (top-$k$ lists) derived from residual sorting information. Our method simultaneously (1) guides the sampling such that hypotheses corresponding to all true structures can be quickly retrieved and (2) filters the hypotheses such that only a small but very promising subset remain. This permits the usage of simple agglomerative clustering on the surviving hypotheses for accurate model selection. The outcome is a highly efficient multi-structure robust estimation technique. Experiments on synthetic and real data show the superior performance of our approach over previous methods.\n    ",
        "submission_date": "2011-05-31T00:00:00",
        "last_modified_date": "2011-05-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0107",
        "title": "Handwritten Character Recognition of South Indian Scripts: A Review",
        "authors": [
            "John Jomy",
            "K. V. Pramod",
            "Balakrishnan Kannan"
        ],
        "abstract": "Handwritten character recognition is always a frontier area of research in the field of pattern recognition and image processing and there is a large demand for OCR on hand written documents. Even though, sufficient studies have performed in foreign scripts like Chinese, Japanese and Arabic characters, only a very few work can be traced for handwritten character recognition of Indian scripts especially for the South Indian scripts. This paper provides an overview of offline handwritten character recognition in South Indian Scripts, namely Malayalam, Tamil, Kannada and Telungu.\n    ",
        "submission_date": "2011-06-01T00:00:00",
        "last_modified_date": "2011-06-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0371",
        "title": "A Novel Image Segmentation Enhancement Technique based on Active Contour and Topological Alignments",
        "authors": [
            "Ashraf A. Aly",
            "Safaai Bin Deris",
            "Nazar Zaki"
        ],
        "abstract": "Topological alignments and snakes are used in image processing, particularly in locating object boundaries. Both of them have their own advantages and limitations. To improve the overall image boundary detection system, we focused on developing a novel algorithm for image processing. The algorithm we propose to develop will based on the active contour method in conjunction with topological alignments method to enhance the image detection approach. The algorithm presents novel technique to incorporate the advantages of both Topological Alignments and snakes. Where the initial segmentation by Topological Alignments is firstly transformed into the input of the snake model and begins its evolvement to the interested object boundary. The results show that the algorithm can deal with low contrast images and shape cells, demonstrate the segmentation accuracy under weak image boundaries, which responsible for lacking accuracy in image detecting techniques. We have achieved better segmentation and boundary detecting for the image, also the ability of the system to improve the low contrast and deal with over and under segmentation.\n    ",
        "submission_date": "2011-06-02T00:00:00",
        "last_modified_date": "2011-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0823",
        "title": "Recovering Epipolar Geometry from Images of Smooth Surfaces",
        "authors": [
            "Oleg Kupervasser"
        ],
        "abstract": "We present four methods for recovering the epipolar geometry from images of smooth surfaces. In the existing methods for recovering epipolar geometry corresponding feature points are used that cannot be found in such images. The first method is based on finding corresponding characteristic points created by illumination (ICPM - illumination characteristic points' method (PM)). The second method is based on correspondent tangency points created by tangents from epipoles to outline of smooth bodies (OTPM - outline tangent PM). These two methods are exact and give correct results for real images, because positions of the corresponding illumination characteristic points and corresponding outline are known with small errors. But the second method is limited either to special type of scenes or to restricted camera motion. We also consider two more methods which are termed CCPM (curve characteristic PM) and CTPM (curve tangent PM), for searching epipolar geometry for images of smooth bodies based on a set of level curves with constant illumination intensity. The CCPM method is based on searching correspondent points on isophoto curves with the help of correlation of curvatures between these lines. The CTPM method is based on property of the tangential to isophoto curve epipolar line to map into the tangential to correspondent isophoto curves epipolar line. The standard method (SM) based on knowledge of pairs of the almost exact correspondent points. The methods have been implemented and tested by SM on pairs of real images. Unfortunately, the last two methods give us only a finite subset of solutions including \"good\" solution. Exception is \"epipoles in infinity\". The main reason is inaccuracy of assumption of constant brightness for smooth bodies. But outline and illumination characteristic points are not influenced by this inaccuracy. So, the first pair of methods gives exact results.\n    ",
        "submission_date": "2011-06-04T00:00:00",
        "last_modified_date": "2012-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0962",
        "title": "An efficient circle detection scheme in digital images using ant system algorithm",
        "authors": [
            "K. Chattopadhyay",
            "J. Basu",
            "A. Konar"
        ],
        "abstract": "Detection of geometric features in digital images is an important exercise in image analysis and computer vision. The Hough Transform techniques for detection of circles require a huge memory space for data processing hence requiring a lot of time in computing the locations of the data space, writing to and searching through the memory space. In this paper we propose a novel and efficient scheme for detecting circles in edge-detected grayscale digital images. We use Ant-system algorithm for this purpose which has not yet found much application in this field. The main feature of this scheme is that it can detect both intersecting as well as non-intersecting circles with a time efficiency that makes it useful in real time applications. We build up an ant system of new type which finds out closed loops in the image and then tests them for circles.\n    ",
        "submission_date": "2011-06-06T00:00:00",
        "last_modified_date": "2011-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.1975",
        "title": "Exact Reconstruction of the Rank Order Coding using Frames Theory",
        "authors": [
            "Khaled Masmoudi",
            "Marc Antonini",
            "Pierre Kornprobst"
        ],
        "abstract": "Our goal is to revisit rank order coding by proposing an original exact decoding procedure for it. Rank order coding was proposed by Simon Thorpe et al. who stated that the retina represents the visual stimulus by the order in which its cells are activated. A classical rank order coder/decoder was then designed on this basis [1]. Though, it appeared that the decoding procedure employed yields reconstruction errors that limit the model Rate/Quality performances when used as an image codec. The attempts made in the literature to overcome this issue are time consuming and alter the coding procedure, or are lacking mathematical support and feasibility for standard size images. Here we solve this problem in an original fashion by using the frames theory, where a frame of a vector space designates an extension for the notion of basis. First, we prove that the analyzing filter bank considered is a frame, and then we define the corresponding dual frame that is necessary for the exact image reconstruction. Second, to deal with the problem of memory overhead, we design a recursive out-of-core blockwise algorithm for the computation of this dual frame. Our work provides a mathematical formalism for the retinal model under study and defines a simple and exact reverse transform for it with up to 270 dB of PSNR gain compared to [1]. Furthermore, the framework presented here can be extended to several models of the visual cortical areas using redundant representations.\n    ",
        "submission_date": "2011-06-10T00:00:00",
        "last_modified_date": "2011-07-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2357",
        "title": "Comparing Haar-Hilbert and Log-Gabor Based Iris Encoders on Bath Iris Image Database",
        "authors": [
            "Nicolaie Popescu-Bodorin",
            "Valentina E. Balas"
        ],
        "abstract": "This papers introduces a new family of iris encoders which use 2-dimensional Haar Wavelet Transform for noise attenuation, and Hilbert Transform to encode the iris texture. In order to prove the usefulness of the newly proposed iris encoding approach, the recognition results obtained by using these new encoders are compared to those obtained using the classical Log- Gabor iris encoder. Twelve tests involving single/multienrollment and conducted on Bath Iris Image Database are presented here. One of these tests achieves an Equal Error Rate comparable to the lowest value reported so far for this database. New Matlab tools for iris image processing are also released together with this paper: a second version of the Circular Fuzzy Iris Segmentator (CFIS2), a fast Log-Gabor encoder and two Haar-Hilbert based encoders.\n    ",
        "submission_date": "2011-06-12T00:00:00",
        "last_modified_date": "2011-06-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2695",
        "title": "Robust Mobile Object Tracking Based on Multiple Feature Similarity and Trajectory Filtering",
        "authors": [
            "Duc Phu Chau",
            "Fran\u00e7ois Bremond",
            "Monique Thonnat",
            "Etienne Corvee"
        ],
        "abstract": "This paper presents a new algorithm to track mobile objects in different scene conditions. The main idea of the proposed tracker includes estimation, multi-features similarity measures and trajectory filtering. A feature set (distance, area, shape ratio, color histogram) is defined for each tracked object to search for the best matching object. Its best matching object and its state estimated by the Kalman filter are combined to update position and size of the tracked object. However, the mobile object trajectories are usually fragmented because of occlusions and misdetections. Therefore, we also propose a trajectory filtering, named global tracker, aims at removing the noisy trajectories and fusing the fragmented trajectories belonging to a same mobile object. The method has been tested with five videos of different scene conditions. Three of them are provided by the ETISEO benchmarking project (",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2011-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3464",
        "title": "Polar Fusion Technique Analysis for Evaluating the Performances of Image Fusion of Thermal and Visual Images for Human Face Recognition",
        "authors": [
            "Mrinal Kanti Bhowmik",
            "Debotosh Bhattacharjee",
            "Dipak Kumar Basu",
            "Mita Nasipuri"
        ],
        "abstract": "This paper presents a comparative study of two different methods, which are based on fusion and polar transformation of visual and thermal images. Here, investigation is done to handle the challenges of face recognition, which include pose variations, changes in facial expression, partial occlusions, variations in illumination, rotation through different angles, change in scale etc. To overcome these obstacles we have implemented and thoroughly examined two different fusion techniques through rigorous experimentation. In the first method log-polar transformation is applied to the fused images obtained after fusion of visual and thermal images whereas in second method fusion is applied on log-polar transformed individual visual and thermal images. After this step, which is thus obtained in one form or another, Principal Component Analysis (PCA) is applied to reduce dimension of the fused images. Log-polar transformed images are capable of handling complicacies introduced by scaling and rotation. The main objective of employing fusion is to produce a fused image that provides more detailed and reliable information, which is capable to overcome the drawbacks present in the individual visual and thermal face images. Finally, those reduced fused images are classified using a multilayer perceptron neural network. The database used for the experiments conducted here is Object Tracking and Classification Beyond Visible Spectrum (OTCBVS) database benchmark thermal and visual face images. The second method has shown better performance, which is 95.71% (maximum) and on an average 93.81% as correct recognition rate.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3466",
        "title": "Next Level of Data Fusion for Human Face Recognition",
        "authors": [
            "Mrinal Kanti Bhowmik",
            "Gautam Majumdar",
            "Debotosh Bhattacharjee",
            "Dipak Kumar Basu",
            "Mita Nasipuri"
        ],
        "abstract": "This paper demonstrates two different fusion techniques at two different levels of a human face recognition process. The first one is called data fusion at lower level and the second one is the decision fusion towards the end of the recognition process. At first a data fusion is applied on visual and corresponding thermal images to generate fused image. Data fusion is implemented in the wavelet domain after decomposing the images through Daubechies wavelet coefficients (db2). During the data fusion maximum of approximate and other three details coefficients are merged together. After that Principle Component Analysis (PCA) is applied over the fused coefficients and finally two different artificial neural networks namely Multilayer Perceptron(MLP) and Radial Basis Function(RBF) networks have been used separately to classify the images. After that, for decision fusion based decisions from both the classifiers are combined together using Bayesian formulation. For experiments, IRIS thermal/visible Face Database has been used. Experimental results show that the performance of multiple classifier system along with decision fusion works well over the single classifier system.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3467",
        "title": "High Performance Human Face Recognition using Independent High Intensity Gabor Wavelet Responses: A Statistical Approach",
        "authors": [
            "Arindam Kar",
            "Debotosh Bhattacharjee",
            "Dipak Kumar Basu",
            "Mita Nasipuri",
            "Mahantapas Kundu"
        ],
        "abstract": "In this paper, we present a technique by which high-intensity feature vectors extracted from the Gabor wavelet transformation of frontal face images, is combined together with Independent Component Analysis (ICA) for enhanced face recognition. Firstly, the high-intensity feature vectors are automatically extracted using the local characteristics of each individual face from the Gabor transformed images. Then ICA is applied on these locally extracted high-intensity feature vectors of the facial images to obtain the independent high intensity feature (IHIF) vectors. These IHIF forms the basis of the work. Finally, the image classification is done using these IHIF vectors, which are considered as representatives of the images. The importance behind implementing ICA along with the high-intensity features of Gabor wavelet transformation is twofold. On the one hand, selecting peaks of the Gabor transformed face images exhibit strong characteristics of spatial locality, scale, and orientation selectivity. Thus these images produce salient local features that are most suitable for face recognition. On the other hand, as the ICA employs locally salient features from the high informative facial parts, it reduces redundancy and represents independent features explicitly. These independent features are most useful for subsequent facial discrimination and associative recall. The efficiency of IHIF method is demonstrated by the experiment on frontal facial images dataset, selected from the FERET, FRAV2D, and the ORL database.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3517",
        "title": "DWT Based Fingerprint Recognition using Non Minutiae Features",
        "authors": [
            "Shashi Kumar D. R.",
            "K. B. Raja",
            "R. K. Chhootaray",
            "Sabyasachi Pattanaik"
        ],
        "abstract": "Forensic applications like criminal investigations, terrorist identification and National security issues require a strong fingerprint data base and efficient identification system. In this paper we propose DWT based Fingerprint Recognition using Non Minutiae (DWTFR) algorithm. Fingerprint image is decomposed into multi resolution sub bands of LL, LH, HL and HH by applying 3 level DWT. The Dominant local orientation angle {\\theta} and Coherence are computed on LL band only. The Centre Area Features and Edge Parameters are determined on each DWT level by considering all four sub bands. The comparison of test fingerprint with database fingerprint is decided based on the Euclidean Distance of all the features. It is observed that the values of FAR, FRR and TSR are improved compared to the existing algorithm.\n    ",
        "submission_date": "2011-06-17T00:00:00",
        "last_modified_date": "2011-06-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.3684",
        "title": "Exploratory simulation of an Intelligent Iris Verifier Distributed System",
        "authors": [
            "Nicolaie Popescu-Bodorin",
            "Valentina E. Balas"
        ],
        "abstract": "This paper discusses some topics related to the latest trends in the field of evolutionary approaches to iris recognition. It presents the results of an exploratory experimental simulation whose goal was to analyze the possibility of establishing an Interchange Protocol for Digital Identities evolved in different geographic locations interconnected through and into an Intelligent Iris Verifier Distributed System (IIVDS) based on multi-enrollment. Finding a logically consistent model for the Interchange Protocol is the key factor in designing the future large-scale iris biometric networks. Therefore, the logical model of such a protocol is also investigated here. All tests are made on Bath Iris Database and prove that outstanding power of discrimination between the intra- and the inter-class comparisons can be achieved by an IIVDS, even when practicing 52.759.182 inter-class and 10.991.943 intra-class comparisons. Still, the test results confirm that inconsistent enrollment can change the logic of recognition from a fuzzified 2-valent consistent logic of biometric certitudes to a fuzzified 3-valent inconsistent possibilistic logic of biometric beliefs justified through experimentally determined probabilities, or to a fuzzified 8-valent logic which is almost consistent as a biometric theory - this quality being counterbalanced by an absolutely reasonable loss in the user comfort level.\n    ",
        "submission_date": "2011-06-18T00:00:00",
        "last_modified_date": "2011-06-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4907",
        "title": "Face Identification from Manipulated Facial Images using SIFT",
        "authors": [
            "H. R. Chennamma",
            "Lalitha Rangarajan",
            "Veerabhadrappa"
        ],
        "abstract": "Editing on digital images is ubiquitous. Identification of deliberately modified facial images is a new challenge for face identification system. In this paper, we address the problem of identification of a face or person from heavily altered facial images. In this face identification problem, the input to the system is a manipulated or transformed face image and the system reports back the determined identity from a database of known individuals. Such a system can be useful in mugshot identification in which mugshot database contains two views (frontal and profile) of each criminal. We considered only frontal view from the available database for face identification and the query image is a manipulated face generated by face transformation software tool available online. We propose SIFT features for efficient face identification in this scenario. Further comparative analysis has been given with well known eigenface approach. Experiments have been conducted with real case images to evaluate the performance of both methods.\n    ",
        "submission_date": "2011-06-24T00:00:00",
        "last_modified_date": "2011-06-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5156",
        "title": "Morphological Reconstruction for Word Level Script Identification",
        "authors": [
            "B.V.Dhandra",
            "Mallikarjun Hangarge"
        ],
        "abstract": "A line of a bilingual document page may contain text words in regional language and numerals in English. For Optical Character Recognition (OCR) of such a document page, it is necessary to identify different script forms before running an individual OCR system. In this paper, we have identified a tool of morphological opening by reconstruction of an image in different directions and regional descriptors for script identification at word level, based on the observation that every text has a distinct visual appearance. The proposed system is developed for three Indian major bilingual documents, Kannada, Telugu and Devnagari containing English numerals. The nearest neighbour and k-nearest neighbour algorithms are applied to classify new word images. The proposed algorithm is tested on 2625 words with various font styles and sizes. The results obtained are quite encouraging\n    ",
        "submission_date": "2011-06-25T00:00:00",
        "last_modified_date": "2011-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5186",
        "title": "Learning Shape and Texture Characteristics of CT Tree-in-Bud Opacities for CAD Systems",
        "authors": [
            "Ulas Bagci",
            "Jianhua Yao",
            "Jesus Caban",
            "Anthony F. Suffredini",
            "Tara N. Palmore",
            "Daniel J. Mollura"
        ],
        "abstract": "Although radiologists can employ CAD systems to characterize malignancies, pulmonary fibrosis and other chronic diseases; the design of imaging techniques to quantify infectious diseases continue to lag behind. There exists a need to create more CAD systems capable of detecting and quantifying characteristic patterns often seen in respiratory tract infections such as influenza, bacterial pneumonia, or tuborculosis. One of such patterns is Tree-in-bud (TIB) which presents \\textit{thickened} bronchial structures surrounding by clusters of \\textit{micro-nodules}. Automatic detection of TIB patterns is a challenging task because of their weak boundary, noisy appearance, and small lesion size. In this paper, we present two novel methods for automatically detecting TIB patterns: (1) a fast localization of candidate patterns using information from local scale of the images, and (2) a M\u00f6bius invariant feature extraction method based on learned local shape and texture properties. A comparative evaluation of the proposed methods is presented with a dataset of 39 laboratory confirmed viral bronchiolitis human parainfluenza (HPIV) CTs and 21 normal lung CTs. Experimental results demonstrate that the proposed CAD system can achieve high detection rate with an overall accuracy of 90.96%.\n    ",
        "submission_date": "2011-06-26T00:00:00",
        "last_modified_date": "2011-06-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5341",
        "title": "Pose Estimation from a Single Depth Image for Arbitrary Kinematic Skeletons",
        "authors": [
            "Daniel L. Ly",
            "Ashutosh Saxena",
            "Hod Lipson"
        ],
        "abstract": "We present a method for estimating pose information from a single depth image given an arbitrary kinematic structure without prior training. For an arbitrary skeleton and depth image, an evolutionary algorithm is used to find the optimal kinematic configuration to explain the observed image. Results show that our approach can correctly estimate poses of 39 and 78 degree-of-freedom models from a single depth image, even in cases of significant self-occlusion.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5460",
        "title": "Automated segmentation of the pulmonary arteries in low-dose CT by vessel tracking",
        "authors": [
            "Jeremiah Wala",
            "Sergei Fotin",
            "Jaesung Lee",
            "Artit Jirapatnakul",
            "Alberto Biancardi",
            "Anthony Reeves"
        ],
        "abstract": "We present a fully automated method for top-down segmentation of the pulmonary arterial tree in low-dose thoracic CT images. The main basal pulmonary arteries are identified near the lung hilum by searching for candidate vessels adjacent to known airways, identified by our previously reported airway segmentation method. Model cylinders are iteratively fit to the vessels to track them into the lungs. Vessel bifurcations are detected by measuring the rate of change of vessel radii, and child vessels are segmented by initiating new trackers at bifurcation points. Validation is accomplished using our novel sparse surface (SS) evaluation metric. The SS metric was designed to quantify the magnitude of the segmentation error per vessel while significantly decreasing the manual marking burden for the human user. A total of 210 arteries and 205 veins were manually marked across seven test cases. 134/210 arteries were correctly segmented, with a specificity for arteries of 90%, and average segmentation error of 0.15 mm. This fully-automated segmentation is a promising method for improving lung nodule detection in low-dose CT screening scans, by separating vessels from surrounding iso-intensity objects.\n    ",
        "submission_date": "2011-06-27T00:00:00",
        "last_modified_date": "2011-06-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5569",
        "title": "Augmented Reality Implementation Methods in Mainstream Applications",
        "authors": [
            "David Prochazka",
            "Tomas Koubek"
        ],
        "abstract": "Augmented reality has became an useful tool in many areas from space exploration to military applications. Although used theoretical principles are well known for almost a decade, the augmented reality is almost exclusively used in high budget solutions with a special hardware. However, in last few years we could see rising popularity of many projects focused on deployment of the augmented reality on different mobile devices. Our article is aimed on developers who consider development of an augmented reality application for the mainstream market. Such developers will be forced to keep the application price, therefore also the development price, at reasonable level. Usage of existing image processing software library could bring a significant cut-down of the development costs. In the theoretical part of the article is presented an overview of the augmented reality application structure. Further, an approach for selection appropriate library as well as the review of the existing software libraries focused in this area is described. The last part of the article outlines our implementation of key parts of the augmented reality application using the OpenCV library.\n    ",
        "submission_date": "2011-06-28T00:00:00",
        "last_modified_date": "2011-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5571",
        "title": "Mobile Augmented Reality Applications",
        "authors": [
            "David Prochazka",
            "Michael Stencl",
            "Ondrej Popelka",
            "Jiri Stastny"
        ],
        "abstract": "Augmented reality have undergone considerable improvement in past years. Many special techniques and hardware devices were developed, but the crucial breakthrough came with the spread of intelligent mobile phones. This enabled mass spread of augmented reality applications. However mobile devices have limited hardware capabilities, which narrows down the methods usable for scene analysis. In this article we propose an augmented reality application which is using cloud computing to enable using of more complex computational methods such as neural networks. Our goal is to create an affordable augmented reality application suitable which will help car designers in by 'virtualizing' car modifications.\n    ",
        "submission_date": "2011-06-28T00:00:00",
        "last_modified_date": "2011-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5737",
        "title": "Fingerprint: DWT, SVD Based Enhancement and Significant Contrast for Ridges and Valleys Using Fuzzy Measures",
        "authors": [
            "D.Bennet",
            "Dr. S. Arumuga Perumal"
        ],
        "abstract": "The performance of the Fingerprint recognition system will be more accurate with respect of enhancement for the fingerprint images. In this paper we develop a novel method for Fingerprint image contrast enhancement technique based on the discrete wavelet transform (DWT) and singular value decomposition (SVD) has been proposed. This technique is compared with conventional image equalization techniques such as standard general histogram equalization and local histogram equalization. An automatic histogram threshold approach based on a fuzziness measure is presented. Then, using an index of fuzziness, a similarity process is started to find the threshold point. A significant contrast between ridges and valleys of the best, medium and poor finger image features to extract from finger images and get maximum recognition rate using fuzzy measures. The experimental results show the recognition of superiority of the proposed method to get maximum performance up gradation to the implementation of this approach.\n    ",
        "submission_date": "2011-06-23T00:00:00",
        "last_modified_date": "2011-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5928",
        "title": "Image denoising assessment using anisotropic stack filtering",
        "authors": [
            "Salvador Gabarda",
            "Gabriel Cristobal"
        ],
        "abstract": "In this paper we propose a measure of anisotropy as a quality parameter to estimate the amount of noise in noisy images. The anisotropy of an image can be determined through a directional measure, using an appropriate statistical distribution of the information contained in the image. This new measure is achieved through a stack filtering paradigm. First, we define a local directional entropy, based on the distribution of 0's and 1's in the neigborhood of every pixel location of each stack level. Then the entropy variation of this directional entropy is used to define an anisotropic measure. The empirical results have shown that this measure can be regarded as an excellent image noise indicator, which is particularly relevant for quality assessment of denoising algorithms. The method has been evaluated with artificial and real-world degraded images.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6341",
        "title": "Vision-Based Navigation III: Pose and Motion from Omnidirectional Optical Flow and a Digital Terrain Map",
        "authors": [
            "Ronen Lerner",
            "Oleg Kupervasser",
            "Ehud Rivlin"
        ],
        "abstract": "An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0399",
        "title": "Vision-Based Navigation I: A navigation filter for fusing DTM/correspondence updates",
        "authors": [
            "Oleg Kupervasser",
            "Vladimir Voronov"
        ],
        "abstract": "An algorithm for pose and motion estimation using corresponding features in images and a digital terrain map is proposed. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. The utilization of data is shown to improve the robustness and accuracy of the inertial navigation algorithm. Extended Kalman filter was used to combine results of inertial navigation algorithm and proposed vision-based navigation algorithm. The feasibility of this algorithms is established through numerical simulations.\n    ",
        "submission_date": "2011-07-02T00:00:00",
        "last_modified_date": "2012-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0550",
        "title": "3D Terrestrial lidar data classification of complex natural scenes using a multi-scale dimensionality criterion: applications in geomorphology",
        "authors": [
            "Nicolas Brodu",
            "Dimitri Lague"
        ],
        "abstract": "3D point clouds of natural environments relevant to problems in geomorphology often require classification of the data into elementary relevant classes. A typical example is the separation of riparian vegetation from ground in fluvial environments, the distinction between fresh surfaces and rockfall in cliff environments, or more generally the classification of surfaces according to their morphology. Natural surfaces are heterogeneous and their distinctive properties are seldom defined at a unique scale, prompting the use of multi-scale criteria to achieve a high degree of classification success. We have thus defined a multi-scale measure of the point cloud dimensionality around each point, which characterizes the local 3D organization. We can thus monitor how the local cloud geometry behaves across scales. We present the technique and illustrate its efficiency in separating riparian vegetation from ground and classifying a mountain stream as vegetation, rock, gravel or water surface. In these two cases, separating the vegetation from ground or other classes achieve accuracy larger than 98 %. Comparison with a single scale approach shows the superiority of the multi-scale analysis in enhancing class separability and spatial resolution. The technique is robust to missing data, shadow zones and changes in point density within the scene. The classification is fast and accurate and can account for some degree of intra-class morphological variability such as different vegetation types. A probabilistic confidence in the classification result is given at each point, allowing the user to remove the points for which the classification is uncertain. The process can be both fully automated, but also fully customized by the user including a graphical definition of the classifiers. Although developed for fully 3D data, the method can be readily applied to 2.5D airborne lidar data.\n    ",
        "submission_date": "2011-07-04T00:00:00",
        "last_modified_date": "2012-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0845",
        "title": "Automatic Road Lighting System (ARLS) Model Based on Image Processing of Moving Object",
        "authors": [
            "Suprijadi",
            "Thomas Muliawan",
            "Sparisoma Viridi"
        ],
        "abstract": "Using a vehicle toy (in next future called vehicle) as a moving object an automatic road lighting system (ARLS) model is constructed. A digital video camera with 25 fps is used to capture the vehicle motion as it moves in the test segment of the road. Captured images are then processed to calculate vehicle speed. This information of the speed together with position of vehicle is then used to control the lighting system along the path that passes by the vehicle. Length of the road test segment is 1 m, the video camera is positioned about 1.1 m above the test segment, and the vehicle toy dimension is 13 cm \\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about 1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s.\n    ",
        "submission_date": "2011-07-05T00:00:00",
        "last_modified_date": "2013-11-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1058",
        "title": "Online Vehicle Detection For Estimating Traffic Status",
        "authors": [
            "Ranch Y.Q. Lai"
        ],
        "abstract": "We propose a traffic congestion estimation system based on unsupervised on-line learning algorithm. The system does not rely on background extraction or motion detection. It extracts local features inside detection regions of variable size which are drawn on lanes in advance. The extracted features are then clustered into two classes using K-means and Gaussian Mixture Models(GMM). A Bayes classifier is used to detect vehicles according to the previous cluster information which keeps updated whenever system is running by on-line EM algorithm. Experimental result shows that our system can be adapted to various traffic scenes for estimating traffic status.\n    ",
        "submission_date": "2011-07-06T00:00:00",
        "last_modified_date": "2011-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1081",
        "title": "Spatial Features for Multi-Font/Multi-Size Kannada Numerals and Vowels Recognition",
        "authors": [
            "B.V. Dhandra",
            "Mallikarjun Hangarge",
            "Gururaj Mukarambi"
        ],
        "abstract": "This paper presents multi-font/multi-size Kannada numerals and vowels recognition based on spatial features. Directional spatial features viz stroke density, stroke length and the number of stokes in an image are employed as potential features to characterize the printed Kannada numerals and vowels. Based on these features 1100 numerals and 1400 vowels are classified with Multi-class Support Vector Machines (SVM). The proposed system achieves the recognition accuracy as 98.45% and 90.64% for numerals and vowels respectively.\n    ",
        "submission_date": "2011-07-06T00:00:00",
        "last_modified_date": "2011-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1470",
        "title": "Vision-Based Navigation II: Error Analysis for a Navigation Algorithm based on Optical-Flow and a Digital Terrain Map",
        "authors": [
            "Oleg Kupervasser",
            "Ronen Lerner",
            "Ehud Rivlin",
            "Hector Rotstein"
        ],
        "abstract": "The paper deals with the error analysis of a navigation algorithm that uses as input a sequence of images acquired by a moving camera and a Digital Terrain Map (DTM) of the region been imaged by the camera during the motion. The main sources of error are more or less straightforward to identify: camera resolution, structure of the observed terrain and DTM accuracy, field of view and camera trajectory. After characterizing and modeling these error sources in the framework of the CDTM algorithm, a closed form expression for their effect on the pose and motion errors of the camera can be found. The analytic expression provides a priori measurements for the accuracy in terms of the parameters mentioned above.\n    ",
        "submission_date": "2011-07-07T00:00:00",
        "last_modified_date": "2011-08-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1561",
        "title": "Analysis and Improvement of Low Rank Representation for Subspace segmentation",
        "authors": [
            "Wei Siming",
            "Lin Zhouchen"
        ],
        "abstract": "We analyze and improve low rank representation (LRR), the state-of-the-art algorithm for subspace segmentation of data. We prove that for the noiseless case, the optimization model of LRR has a unique solution, which is the shape interaction matrix (SIM) of the data matrix. So in essence LRR is equivalent to factorization methods. We also prove that the minimum value of the optimization model of LRR is equal to the rank of the data matrix. For the noisy case, we show that LRR can be approximated as a factorization method that combines noise removal by column sparse robust PCA. We further propose an improved version of LRR, called Robust Shape Interaction (RSI), which uses the corrected data as the dictionary instead of the noisy data. RSI is more robust than LRR when the corruption in data is heavy. Experiments on both synthetic and real data testify to the improved robustness of RSI.\n    ",
        "submission_date": "2011-07-08T00:00:00",
        "last_modified_date": "2011-07-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1644",
        "title": "Prostate biopsy tracking with deformation estimation",
        "authors": [
            "Michael Baumann",
            "Pierre Mozer",
            "Vincent Daanen",
            "Jocelyne Troccaz"
        ],
        "abstract": "Transrectal biopsies under 2D ultrasound (US) control are the current clinical standard for prostate cancer diagnosis. The isoechogenic nature of prostate carcinoma makes it necessary to sample the gland systematically, resulting in a low sensitivity. Also, it is difficult for the clinician to follow the sampling protocol accurately under 2D US control and the exact anatomical location of the biopsy cores is unknown after the intervention. Tracking systems for prostate biopsies make it possible to generate biopsy distribution maps for intra- and post-interventional quality control and 3D visualisation of histological results for diagnosis and treatment planning. They can also guide the clinician toward non-ultrasound targets. In this paper, a volume-swept 3D US based tracking system for fast and accurate estimation of prostate tissue motion is proposed. The entirely image-based system solves the patient motion problem with an a priori model of rectal probe kinematics. Prostate deformations are estimated with elastic registration to maximize accuracy. The system is robust with only 17 registration failures out of 786 (2%) biopsy volumes acquired from 47 patients during biopsy sessions. Accuracy was evaluated to 0.76$\\pm$0.52mm using manually segmented fiducials on 687 registered volumes stemming from 40 patients. A clinical protocol for assisted biopsy acquisition was designed and implemented as a biopsy assistance system, which allows to overcome the draw-backs of the standard biopsy procedure.\n    ",
        "submission_date": "2011-07-07T00:00:00",
        "last_modified_date": "2011-07-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1837",
        "title": "Information-Theoretic Measures for Objective Evaluation of Classifications",
        "authors": [
            "Bao-Gang Hu",
            "Ran He",
            "XiaoTong Yuan"
        ],
        "abstract": "This work presents a systematic study of objective evaluations of abstaining classifications using Information-Theoretic Measures (ITMs). First, we define objective measures for which they do not depend on any free parameter. This definition provides technical simplicity for examining \"objectivity\" or \"subjectivity\" directly to classification evaluations. Second, we propose twenty four normalized ITMs, derived from either mutual information, divergence, or cross-entropy, for investigation. Contrary to conventional performance measures that apply empirical formulas based on users' intuitions or preferences, the ITMs are theoretically more sound for realizing objective evaluations of classifications. We apply them to distinguish \"error types\" and \"reject types\" in binary classifications without the need for input data of cost terms. Third, to better understand and select the ITMs, we suggest three desirable features for classification assessment measures, which appear more crucial and appealing from the viewpoint of classification applications. Using these features as \"meta-measures\", we can reveal the advantages and limitations of ITMs from a higher level of evaluation knowledge. Numerical examples are given to corroborate our claims and compare the differences among the proposed measures. The best measure is selected in terms of the meta-measures, and its specific properties regarding error types and reject types are analytically derived.\n    ",
        "submission_date": "2011-07-10T00:00:00",
        "last_modified_date": "2011-07-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2085",
        "title": "Kunchenko's Polynomials for Template Matching",
        "authors": [
            "Oleg Chertov",
            "Taras Slipets"
        ],
        "abstract": "This paper reviews Kunchenko's polynomials using as template matching method to recognize template in one-dimensional input signal. Kunchenko's polynomials method is compared with classical methods - cross-correlation and sum of squared differences according to numerical statistical example.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2336",
        "title": "A Variation of the Box-Counting Algorithm Applied to Colour Images",
        "authors": [
            "N. S. Nikolaidis",
            "I. N. Nikolaidis",
            "C. C. Tsouros"
        ],
        "abstract": "The box counting method for fractal dimension estimation had not been applied to large or colour images thus far due to the processing time required. In this letter we present a fast, easy to implement and very easily expandable to any number of dimensions variation, the box merging method. It is applied here in RGB images which are considered as sets in 5-D space.\n    ",
        "submission_date": "2011-07-12T00:00:00",
        "last_modified_date": "2011-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2553",
        "title": "Learning Hypergraph Labeling for Feature Matching",
        "authors": [
            "Toufiq Parag",
            "Vladimir Pavlovic",
            "Ahmed Elgammal"
        ],
        "abstract": "This study poses the feature correspondence problem as a hypergraph node labeling problem. Candidate feature matches and their subsets (usually of size larger than two) are considered to be the nodes and hyperedges of a hypergraph. A hypergraph labeling algorithm, which models the subset-wise interaction by an undirected graphical model, is applied to label the nodes (feature correspondences) as correct or incorrect. We describe a method to learn the cost function of this labeling algorithm from labeled examples using a graphical model training algorithm. The proposed feature matching algorithm is different from the most of the existing learning point matching methods in terms of the form of the objective function, the cost function to be learned and the optimization method applied to minimize it. The results on standard datasets demonstrate how learning over a hypergraph improves the matching performance over existing algorithms, notably one that also uses higher order information without learning.\n    ",
        "submission_date": "2011-07-13T00:00:00",
        "last_modified_date": "2011-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2693",
        "title": "A Fuzzy View on k-Means Based Signal Quantization with Application in Iris Segmentation",
        "authors": [
            "Nicolaie Popescu-Bodorin"
        ],
        "abstract": "This paper shows that the k-means quantization of a signal can be interpreted both as a crisp indicator function and as a fuzzy membership assignment describing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy indicator functions are defined here as natural generalizations of the ordinary crisp and fuzzy indicator functions, respectively. An application to iris segmentation is presented together with a demo program.\n    ",
        "submission_date": "2011-07-13T00:00:00",
        "last_modified_date": "2011-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2696",
        "title": "Exploring New Directions in Iris Recognition",
        "authors": [
            "Nicolaie Popescu-Bodorin"
        ],
        "abstract": "A new approach in iris recognition based on Circular Fuzzy Iris Segmentation (CFIS) and Gabor Analytic Iris Texture Binary Encoder (GAITBE) is proposed and tested here. CFIS procedure is designed to guarantee that similar iris segments will be obtained for similar eye images, despite the fact that the degree of occlusion may vary from one image to another. Its result is a circular iris ring (concentric with the pupil) which approximates the actual iris. GAITBE proves better encoding of statistical independence between the iris codes extracted from different irides using Hilbert Transform. Irides from University of Bath Iris Database are binary encoded on two different lengths (768 / 192 bytes) and tested in both single-enrollment and multi-enrollment identification scenarios. All cases illustrate the capacity of the newly proposed methodology to narrow down the distribution of inter-class matching scores, and consequently, to guarantee a steeper descent of the False Accept Rate.\n    ",
        "submission_date": "2011-07-13T00:00:00",
        "last_modified_date": "2011-07-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2723",
        "title": "Topographic Feature Extraction for Bengali and Hindi Character Images",
        "authors": [
            "Soumen Bag",
            "Gaurav Harit"
        ],
        "abstract": "Feature selection and extraction plays an important role in different classification based problems such as face recognition, signature verification, optical character recognition (OCR) etc. The performance of OCR highly depends on the proper selection and extraction of feature set. In this paper, we present novel features based on the topography of a character as visible from different viewing directions on a 2D plane. By topography of a character we mean the structural features of the strokes and their spatial relations. In this work we develop topographic features of strokes visible with respect to views from different directions (e.g. North, South, East, and West). We consider three types of topographic features: closed region, convexity of strokes, and straight line strokes. These features are represented as a shape-based graph which acts as an invariant feature set for discriminating very similar type characters efficiently. We have tested the proposed method on printed and handwritten Bengali and Hindi character images. Initial results demonstrate the efficacy of our approach.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2781",
        "title": "Face Recognition using Curvelet Transform",
        "authors": [
            "Rami Cohen"
        ],
        "abstract": "Face recognition has been studied extensively for more than 20 years now. Since the beginning of 90s the subject has became a major issue. This technology is used in many important real-world applications, such as video surveillance, smart cards, database security, internet and intranet access. This report reviews recent two algorithms for face recognition which take advantage of a relatively new multiscale geometric analysis tool - Curvelet transform, for facial processing and feature extraction. This transform proves to be efficient especially due to its good ability to detect curves and lines, which characterize the human's face. An algorithm which is based on the two algorithms mentioned above is proposed, and its performance is evaluated on three data bases of faces: AT&T (ORL), Essex Grimace and Georgia-Tech. k-nearest neighbour (k-NN) and Support vector machine (SVM) classifiers are used, along with Principal Component Analysis (PCA) for dimensionality reduction. This algorithm shows good results, and it even outperforms other algorithms in some cases.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2782",
        "title": "The Chan-Vese Algorithm",
        "authors": [
            "Rami Cohen"
        ],
        "abstract": "Segmentation is the process of partitioning a digital image into multiple segments (sets of pixels). Such common segmentation tasks including segmenting written text or segmenting tumors from healthy brain tissue in an MRI image, etc. Chan-Vese model for active contours is a powerful and flexible method which is able to segment many types of images, including some that would be quite difficult to segment in means of \"classical\" segmentation - i.e., using thresholding or gradient based methods. This model is based on the Mumford-Shah functional for segmentation, and is used widely in the medical imaging field, especially for the segmentation of the brain, heart and trachea. The model is based on an energy minimization problem, which can be reformulated in the level set formulation, leading to an easier way to solve the problem. In this project, the model will be presented (there is an extension to color (vector-valued) images, but it will not be considered here), and Matlab code that implements it will be introduced.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2807",
        "title": "Modelling Distributed Shape Priors by Gibbs Random Fields of Second Order",
        "authors": [
            "Boris Flach",
            "Dmitrij Schlesinger"
        ],
        "abstract": "We analyse the potential of Gibbs Random Fields for shape prior modelling. We show that the expressive power of second order GRFs is already sufficient to express simple shapes and spatial relations between them simultaneously. This allows to model and recognise complex shapes as spatial compositions of simpler parts.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3194",
        "title": "Fingerprint recognition using standardized fingerprint model",
        "authors": [
            "Le Hoang Thai",
            "Ha Nhat Tam"
        ],
        "abstract": "Fingerprint recognition is one of most popular and accuracy Biometric technologies. Nowadays, it is used in many real applications. However, recognizing fingerprints in poor quality images is still a very complex problem. In recent years, many algorithms, models...are given to improve the accuracy of recognition system. This paper discusses on the standardized fingerprint model which is used to synthesize the template of fingerprints. In this model, after pre-processing step, we find the transformation between templates, adjust parameters, synthesize fingerprint, and reduce noises. Then, we use the final fingerprint to match with others in FVC2004 fingerprint database (DB4) to show the capability of the model.\n    ",
        "submission_date": "2011-07-16T00:00:00",
        "last_modified_date": "2011-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3195",
        "title": "Facial Expression Classification Based on Multi Artificial Neural Network and Two Dimensional Principal Component Analysis",
        "authors": [
            "Thai Le",
            "Phat Tat",
            "Hai Tran"
        ],
        "abstract": "Facial expression classification is a kind of image classification and it has received much attention, in recent years. There are many approaches to solve these problems with aiming to increase efficient classification. One of famous suggestions is described as first step, project image to different spaces; second step, in each of these spaces, images are classified into responsive class and the last step, combine the above classified results into the final result. The advantages of this approach are to reflect fulfill and multiform of image classified. In this paper, we use 2D-PCA and its variants to project the pattern or image into different spaces with different grouping strategies. Then we develop a model which combines many Neural Networks applied for the last step. This model evaluates the reliability of each space and gives the final classification conclusion. Our model links many Neural Networks together, so we call it Multi Artificial Neural Network (MANN). We apply our proposal model for 6 basic facial expressions on JAFFE database consisting 213 images posed by 10 Japanese female models.\n    ",
        "submission_date": "2011-07-16T00:00:00",
        "last_modified_date": "2011-07-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3348",
        "title": "Arithmetic and Frequency Filtering Methods of Pixel-Based Image Fusion Techniques",
        "authors": [
            "Firouz Abdullah Al-Wassai",
            "N.V. Kalyankar",
            "Ali A. Al-Zuky"
        ],
        "abstract": "In remote sensing, image fusion technique is a useful tool used to fuse high spatial resolution panchromatic images (PAN) with lower spatial resolution multispectral images (MS) to create a high spatial resolution multispectral of image fusion (F) while preserving the spectral information in the multispectral image (MS).There are many PAN sharpening techniques or Pixel-Based image fusion techniques that have been developed to try to enhance the spatial resolution and the spectral property preservation of the MS. This paper attempts to undertake the study of image fusion, by using two types of pixel-based image fusion techniques i.e. Arithmetic Combination and Frequency Filtering Methods of Pixel-Based Image Fusion Techniques. The first type includes Brovey Transform (BT), Color Normalized Transformation (CN) and Multiplicative Method (MLT). The second type include High-Pass Filter Additive Method (HPFA), High-Frequency-Addition Method (HFA) High Frequency Modulation Method (HFM) and The Wavelet transform-based fusion method (WT). This paper also devotes to concentrate on the analytical techniques for evaluating the quality of image fusion (F) by using various methods including Standard Deviation (SD), Entropy(En), Correlation Coefficient (CC), Signal-to Noise Ratio (SNR), Normalization Root Mean Square Error (NRMSE) and Deviation Index (DI) to estimate the quality and degree of information improvement of a fused image quantitatively.\n    ",
        "submission_date": "2011-07-18T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4396",
        "title": "The IHS Transformations Based Image Fusion",
        "authors": [
            "Firouz Abdullah Al-Wassai",
            "N.V. Kalyankar",
            "Ali A. Al-Zuky"
        ],
        "abstract": "The IHS sharpening technique is one of the most commonly used techniques for sharpening. Different transformations have been developed to transfer a color image from the RGB space to the IHS space. Through literature, it appears that, various scientists proposed alternative IHS transformations and many papers have reported good results whereas others show bad ones as will as not those obtained which the formula of IHS transformation were used. In addition to that, many papers show different formulas of transformation matrix such as IHS transformation. This leads to confusion what is the exact formula of the IHS transformation?. Therefore, the main purpose of this work is to explore different IHS transformation techniques and experiment it as IHS based image fusion. The image fusion performance was evaluated, in this study, using various methods to estimate the quality and degree of information improvement of a fused image quantitatively.\n    ",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4617",
        "title": "Constant-time filtering using shiftable kernels",
        "authors": [
            "Kunal Narayan Chaudhury"
        ],
        "abstract": "It was recently demonstrated in [5] that the non-linear bilateral filter [14] can be efficiently implemented using a constant-time or O(1) algorithm. At the heart of this algorithm was the idea of approximating the Gaussian range kernel of the bilateral filter using trigonometric functions. In this letter, we explain how the idea in [5] can be extended to few other linear and non-linear filters [14, 17, 2]. While some of these filters have received a lot of attention in recent years, they are known to be computationally intensive. To extend the idea in [5], we identify a central property of trigonometric functions, called shiftability, that allows us to exploit the redundancy inherent in the filtering operations. In particular, using shiftable kernels, we show how certain complex filtering can be reduced to simply that of computing the moving sum of a stack of images. Each image in the stack is obtained through an elementary pointwise transform of the input image. This has a two-fold advantage. First, we can use fast recursive algorithms for computing the moving sum [15, 6], and, secondly, we can use parallel computation to further speed up the computation. We also show how shiftable kernels can also be used to approximate the (non-shiftable) Gaussian kernel that is ubiquitously used in image filtering.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-09-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4637",
        "title": "Efficient variational inference in large-scale Bayesian compressed sensing",
        "authors": [
            "George Papandreou",
            "Alan Yuille"
        ],
        "abstract": "We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4667",
        "title": "Correlation Estimation from Compressed Images",
        "authors": [
            "Vijayaraghavan Thirumalai",
            "Pascal Frossard"
        ],
        "abstract": "This paper addresses the problem of correlation estimation in sets of compressed images. We consider a framework where images are represented under the form of linear measurements due to low complexity sensing or security requirements. We assume that the images are correlated through the displacement of visual objects due to motion or viewpoint change and the correlation is effectively represented by optical flow or motion field models. The correlation is estimated in the compressed domain by jointly processing the linear measurements. We first show that the correlated images can be efficiently related using a linear operator. Using this linear relationship we then describe the dependencies between images in the compressed domain. We further cast a regularized optimization problem where the correlation is estimated in order to satisfy both data consistency and motion smoothness objectives with a Graph Cut algorithm. We analyze in detail the correlation estimation performance and quantify the penalty due to image compression. Extensive experiments in stereo and video imaging applications show that our novel solution stays competitive with methods that implement complex image reconstruction steps prior to correlation estimation. We finally use the estimated correlation in a novel joint image reconstruction scheme that is based on an optimization problem with sparsity priors on the reconstructed images. Additional experiments show that our correlation estimation algorithm leads to an effective reconstruction of pairs of images in distributed image coding schemes that outperform independent reconstruction algorithms by 2 to 4 dB.\n    ",
        "submission_date": "2011-07-23T00:00:00",
        "last_modified_date": "2011-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4763",
        "title": "Diffeomorphic Metric Mapping of High Angular Resolution Diffusion Imaging based on Riemannian Structure of Orientation Distribution Functions",
        "authors": [
            "Jia Du",
            "Alvina Goh",
            "Anqi Qiu"
        ],
        "abstract": "In this paper, we propose a novel large deformation diffeomorphic registration algorithm to align high angular resolution diffusion images (HARDI) characterized by orientation distribution functions (ODFs). Our proposed algorithm seeks an optimal diffeomorphism of large deformation between two ODF fields in a spatial volume domain and at the same time, locally reorients an ODF in a manner such that it remains consistent with the surrounding anatomical structure. To this end, we first review the Riemannian manifold of ODFs. We then define the reorientation of an ODF when an affine transformation is applied and subsequently, define the diffeomorphic group action to be applied on the ODF based on this reorientation. We incorporate the Riemannian metric of ODFs for quantifying the similarity of two HARDI images into a variational problem defined under the large deformation diffeomorphic metric mapping (LDDMM) framework. We finally derive the gradient of the cost function in both Riemannian spaces of diffeomorphisms and the ODFs, and present its numerical implementation. Both synthetic and real brain HARDI data are used to illustrate the performance of our registration algorithm.\n    ",
        "submission_date": "2011-07-24T00:00:00",
        "last_modified_date": "2011-07-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4958",
        "title": "Efficient and Accurate Gaussian Image Filtering Using Running Sums",
        "authors": [
            "Elhanan Elboher",
            "Michael Werman"
        ],
        "abstract": "This paper presents a simple and efficient method to convolve an image with a Gaussian kernel. The computation is performed in a constant number of operations per pixel using running sums along the image rows and columns. We investigate the error function used for kernel approximation and its relation to the properties of the input signal. Based on natural image statistics we propose a quadratic form kernel error function so that the output image l2 error is minimized. We apply the proposed approach to approximate the Gaussian kernel by linear combination of constant functions. This results in very efficient Gaussian filtering method. Our experiments show that the proposed technique is faster than state of the art methods while preserving a similar accuracy.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5000",
        "title": "An iterative feature selection method for GRNs inference by exploring topological properties",
        "authors": [
            "Fabr\u00edcio Martins Lopes",
            "David C. Martins-Jr",
            "Junior Barrera",
            "Roberto M. Cesar-Jr"
        ],
        "abstract": "An important problem in bioinformatics is the inference of gene regulatory networks (GRN) from temporal expression profiles. In general, the main limitations faced by GRN inference methods is the small number of samples with huge dimensionalities and the noisy nature of the expression measurements. In face of these limitations, alternatives are needed to get better accuracy on the GRNs inference problem. This work addresses this problem by presenting an alternative feature selection method that applies prior knowledge on its search strategy, called SFFS-BA. The proposed search strategy is based on the Sequential Floating Forward Selection (SFFS) algorithm, with the inclusion of a scale-free (Barab\u00e1si-Albert) topology information in order to guide the search process to improve inference. The proposed algorithm explores the scale-free property by pruning the search space and using a power law as a weight for reducing it. In this way, the search space traversed by the SFFS-BA method combines a breadth-first search when the number of combinations is small (<k> <= 2) with a depth-first search when the number of combinations becomes explosive (<k> >= 3), being guided by the scale-free prior information. Experimental results show that the SFFS-BA provides a better inference similarities than SFS and SFFS, keeping the robustness of the SFS and SFFS methods, thus presenting very good results.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5186",
        "title": "Fast multi-scale edge-detection in medical ultrasound signals",
        "authors": [
            "Preben Gr\u00e5berg Nes"
        ],
        "abstract": "In this article we suggest a fast multi-scale edge-detection scheme for medical ultrasound signals. The edge-detector is based on well-known properties of the continuous wavelet trans- form. To achieve both good localization of edges and detect only significant edges, we study the maxima-lines of the wavelet transform. One can obtain the maxima-lines between two scales by computing the wavelet transform at several intermediate scales. To reduce computational effort and time we suggest a time-scale filtering procedure which uses only few scales to connect modulus-maxima across time-scale plane. The design of this procedure is based on a study of maxima-lines corresponding to edges typical for medical ultrasound signals. This study allows us to construct an algorithm for medical ultrasound signals which meets the demand for speed, but not on expense of reliability. The edge-detection algorithm has been applied to a large class of medical ultrasound sig- nals including tumour-, liver- and artery-images. Our results show that the proposed algorithm effectively detects major features in such signals, including edges with low contrast.\n    ",
        "submission_date": "2011-07-26T00:00:00",
        "last_modified_date": "2011-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5349",
        "title": "Multi Layer Analysis",
        "authors": [
            "Luca Pinello"
        ],
        "abstract": "This thesis presents a new methodology to analyze one-dimensional signals trough a new approach called Multi Layer Analysis, for short MLA. It also provides some new insights on the relationship between one-dimensional signals processed by MLA and tree kernels, test of randomness and signal processing techniques. The MLA approach has a wide range of application to the fields of pattern discovery and matching, computational biology and many other areas of computer science and signal processing. This thesis includes also some applications of this approach to real problems in biology and seismology.\n    ",
        "submission_date": "2011-07-26T00:00:00",
        "last_modified_date": "2011-07-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.5850",
        "title": "Confidence-Based Dynamic Classifier Combination For Mean-Shift Tracking",
        "authors": [
            "Ibrahim Saygin Topkaya",
            "Hakan Erdogan"
        ],
        "abstract": "We introduce a novel tracking technique which uses dynamic confidence-based fusion of two different information sources for robust and efficient tracking of visual objects. Mean-shift tracking is a popular and well known method used in object tracking problems. Originally, the algorithm uses a similarity measure which is optimized by shifting a search area to the center of a generated weight image to track objects. Recent improvements on the original mean-shift algorithm involves using a classifier that differentiates the object from its surroundings. We adopt this classifier-based approach and propose an application of a classifier fusion technique within this classifier-based context in this work. We use two different classifiers, where one comes from a background modeling method, to generate the weight image and we calculate contributions of the classifiers dynamically using their confidences to generate a final weight image to be used in tracking. The contributions of the classifiers are calculated by using correlations between histograms of their weight images and histogram of a defined ideal weight image in the previous frame. We show with experiments that our dynamic combination scheme selects good contributions for classifiers for different cases and improves tracking accuracy significantly.\n    ",
        "submission_date": "2011-07-29T00:00:00",
        "last_modified_date": "2014-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0007",
        "title": "A Invertible Dimension Reduction of Curves on a Manifold",
        "authors": [
            "Sheng Yi",
            "Hamid Krim",
            "Larry K. Norris"
        ],
        "abstract": "In this paper, we propose a novel lower dimensional representation of a shape sequence. The proposed dimension reduction is invertible and computationally more efficient in comparison to other related works. Theoretically, the differential geometry tools such as moving frame and parallel transportation are successfully adapted into the dimension reduction problem of high dimensional curves. Intuitively, instead of searching for a global flat subspace for curve embedding, we deployed a sequence of local flat subspaces adaptive to the geometry of both of the curve and the manifold it lies on. In practice, the experimental results of the dimension reduction and reconstruction algorithms well illustrate the advantages of the proposed theoretical innovation.\n    ",
        "submission_date": "2011-07-29T00:00:00",
        "last_modified_date": "2011-07-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.0502",
        "title": "An Efficient Real Time Method of Fingertip Detection",
        "authors": [
            "Jagdish Lal Raheja",
            "Karen Das",
            "Ankit Chaudhary"
        ],
        "abstract": "Fingertips detection has been used in many applications, and it is very popular and commonly used in the area of Human Computer Interaction these days. This paper presents a novel time efficient method that will lead to fingertip detection after cropping the irrelevant parts of input image. Binary silhouette of the input image is generated using HSV color space based skin filter and hand cropping done based on histogram of the hand image. The cropped image will be used to figure out the fingertips.\n    ",
        "submission_date": "2011-08-02T00:00:00",
        "last_modified_date": "2011-08-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1122",
        "title": "Leveraging Billions of Faces to Overcome Performance Barriers in Unconstrained Face Recognition",
        "authors": [
            "Yaniv Taigman",
            "Lior Wolf"
        ],
        "abstract": "We employ the face recognition technology developed in house at ",
        "submission_date": "2011-08-04T00:00:00",
        "last_modified_date": "2011-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1169",
        "title": "Learning Representations by Maximizing Compression",
        "authors": [
            "Karol Gregor",
            "Yann LeCun"
        ],
        "abstract": "We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.\n    ",
        "submission_date": "2011-08-04T00:00:00",
        "last_modified_date": "2011-08-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1353",
        "title": "Real time face recognition using adaboost improved fast PCA algorithm",
        "authors": [
            "K.Susheel Kumar",
            "Vijay Bhaskar Semwal",
            "R C Tripathi"
        ],
        "abstract": "This paper presents an automated system for human face recognition in a real time background world for a large homemade dataset of persons face. The task is very difficult as the real time background subtraction in an image is still a challenge. Addition to this there is a huge variation in human face image in terms of size, pose and expression. The system proposed collapses most of this variance. To detect real time human face AdaBoost with Haar cascade is used and a simple fast PCA and LDA is used to recognize the faces detected. The matched face is then used to mark attendance in the laboratory, in our case. This biometric system is a real time attendance system based on the human face recognition with a simple and fast algorithms and gaining a high accuracy rate..\n    ",
        "submission_date": "2011-08-05T00:00:00",
        "last_modified_date": "2011-08-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1636",
        "title": "A new embedding quality assessment method for manifold learning",
        "authors": [
            "Peng Zhang",
            "Yuanyuan Ren",
            "Bo Zhang"
        ],
        "abstract": "Manifold learning is a hot research topic in the field of computer science. A crucial issue with current manifold learning methods is that they lack a natural quantitative measure to assess the quality of learned embeddings, which greatly limits their applications to real-world problems. In this paper, a new embedding quality assessment method for manifold learning, named as Normalization Independent Embedding Quality Assessment (NIEQA), is proposed. Compared with current assessment methods which are limited to isometric embeddings, the NIEQA method has a much larger application range due to two features. First, it is based on a new measure which can effectively evaluate how well local neighborhood geometry is preserved under normalization, hence it can be applied to both isometric and normalized embeddings. Second, it can provide both local and global evaluations to output an overall assessment. Therefore, NIEQA can serve as a natural tool in model selection and evaluation tasks for manifold learning. Experimental results on benchmark data sets validate the effectiveness of the proposed method.\n    ",
        "submission_date": "2011-08-08T00:00:00",
        "last_modified_date": "2011-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2475",
        "title": "Undithering using linear filtering and non-linear diffusion techniques",
        "authors": [
            "V. Asha"
        ],
        "abstract": "Data compression is a method of improving the efficiency of transmission and storage of images. Dithering, as a method of data compression, can be used to convert an 8-bit gray level image into a 1-bit / binary image. Undithering is the process of reconstruction of gray image from binary image obtained from dithering of gray image. In the present paper, I propose a method of undithering using linear filtering followed by anisotropic diffusion which brings the advantage of smoothing and edge enhancement. First-order statistical parameters, second-order statistical parameters, mean-squared error (MSE) between reconstructed image and the original image before dithering, and peak signal to noise ratio (PSNR) are evaluated at each step of diffusion. Results of the experiments show that the reconstructed image is not as sharp as the image before dithering but a large number of gray values are reproduced with reference to those of the original image prior to dithering.\n    ",
        "submission_date": "2011-08-10T00:00:00",
        "last_modified_date": "2011-08-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.2632",
        "title": "Compressive Imaging using Approximate Message Passing and a Markov-Tree Prior",
        "authors": [
            "Subhojit Som",
            "Philip Schniter"
        ],
        "abstract": "We propose a novel algorithm for compressive imaging that exploits both the sparsity and persistence across scales found in the 2D wavelet transform coefficients of natural images. Like other recent works, we model wavelet structure using a hidden Markov tree (HMT) but, unlike other works, ours is based on loopy belief propagation (LBP). For LBP, we adopt a recently proposed \"turbo\" message passing schedule that alternates between exploitation of HMT structure and exploitation of compressive-measurement structure. For the latter, we leverage Donoho, Maleki, and Montanari's recently proposed approximate message passing (AMP) algorithm. Experiments with a large image database suggest that, relative to existing schemes, our turbo LBP approach yields state-of-the-art reconstruction performance with substantial reduction in complexity.\n    ",
        "submission_date": "2011-08-12T00:00:00",
        "last_modified_date": "2011-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3250",
        "title": "The Statistical methods of Pixel-Based Image Fusion Techniques",
        "authors": [
            "Firouz Abdullah Al-Wassai",
            "N.V. Kalyankar",
            "Ali A. Al-Zaky"
        ],
        "abstract": "There are many image fusion methods that can be used to produce high-resolution mutlispectral images from a high-resolution panchromatic (PAN) image and low-resolution multispectral (MS) of remote sensed images. This paper attempts to undertake the study of image fusion techniques with different Statistical techniques for image fusion as Local Mean Matching (LMM), Local Mean and Variance Matching (LMVM), Regression variable substitution (RVS), Local Correlation Modeling (LCM) and they are compared with one another so as to choose the best technique, that can be applied on multi-resolution satellite images. This paper also devotes to concentrate on the analytical techniques for evaluating the quality of image fusion (F) by using various methods including Standard Deviation (SD), Entropy(En), Correlation Coefficient (CC), Signal-to Noise Ratio (SNR), Normalization Root Mean Square Error (NRMSE) and Deviation Index (DI) to estimate the quality and degree of information improvement of a fused image quantitatively.\n    ",
        "submission_date": "2011-08-12T00:00:00",
        "last_modified_date": "2011-08-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3251",
        "title": "Advanced phase retrieval: maximum likelihood technique with sparse regularization of phase and amplitude",
        "authors": [
            "Artem Migukin",
            "Vladimir Katkovnik",
            "Jaakko Astola"
        ],
        "abstract": "Sparse modeling is one of the efficient techniques for imaging that allows recovering lost information. In this paper, we present a novel iterative phase-retrieval algorithm using a sparse representation of the object amplitude and phase. The algorithm is derived in terms of a constrained maximum likelihood, where the wave field reconstruction is performed using a number of noisy intensity-only observations with a zero-mean additive Gaussian noise. The developed algorithm enables the optimal solution for the object wave field reconstruction. Our goal is an improvement of the reconstruction quality with respect to the conventional algorithms. Sparse regularization results in advanced reconstruction accuracy, and numerical simulations demonstrate significant enhancement of imaging.\n    ",
        "submission_date": "2011-08-15T00:00:00",
        "last_modified_date": "2011-08-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3525",
        "title": "Hamiltonian Streamline Guided Feature Extraction with Applications to Face Detection",
        "authors": [
            "Yingjie Miao",
            "Jason J. Corso"
        ],
        "abstract": "We propose a new feature extraction method based on two dynamical systems induced by intensity landscape: the negative gradient system and the Hamiltonian system. We build features based on the Hamiltonian streamlines. These features contain nice global topological information about the intensity landscape, and can be used for object detection. We show that for training images of same size, our feature space is much smaller than that generated by Haar-like features. The training time is extremely short, and detection speed and accuracy is similar to Haar-like feature based classifiers.\n    ",
        "submission_date": "2011-08-17T00:00:00",
        "last_modified_date": "2011-08-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3605",
        "title": "Hierarchical Object Parsing from Structured Noisy Point Clouds",
        "authors": [
            "Adrian Barbu"
        ],
        "abstract": "Object parsing and segmentation from point clouds are challenging tasks because the relevant data is available only as thin structures along object boundaries or other features, and is corrupted by large amounts of noise. To handle this kind of data, flexible shape models are desired that can accurately follow the object boundaries. Popular models such as Active Shape and Active Appearance models lack the necessary flexibility for this task, while recent approaches such as the Recursive Compositional Models make model simplifications in order to obtain computational guarantees. This paper investigates a hierarchical Bayesian model of shape and appearance in a generative setting. The input data is explained by an object parsing layer, which is a deformation of a hidden PCA shape model with Gaussian prior. The paper also introduces a novel efficient inference algorithm that uses informed data-driven proposals to initialize local searches for the hidden variables. Applied to the problem of object parsing from structured point clouds such as edge detection images, the proposed approach obtains state of the art parsing errors on two standard datasets without using any intensity information.\n    ",
        "submission_date": "2011-08-18T00:00:00",
        "last_modified_date": "2012-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4079",
        "title": "Toward Parts-Based Scene Understanding with Pixel-Support Parts-Sparse Pictorial Structures",
        "authors": [
            "Jason J. Corso"
        ],
        "abstract": "Scene understanding remains a significant challenge in the computer vision community. The visual psychophysics literature has demonstrated the importance of interdependence among parts of the scene. Yet, the majority of methods in computer vision remain local. Pictorial structures have arisen as a fundamental parts-based model for some vision problems, such as articulated object detection. However, the form of classical pictorial structures limits their applicability for global problems, such as semantic pixel labeling. In this paper, we propose an extension of the pictorial structures approach, called pixel-support parts-sparse pictorial structures, or PS3, to overcome this limitation. Our model extends the classical form in two ways: first, it defines parts directly based on pixel-support rather than in a parametric form, and second, it specifies a space of plausible parts-based scene models and permits one to be used for inference on any given image. PS3 makes strides toward unifying object-level and pixel-level modeling of scene elements. In this report, we implement the first half of our model and rely upon external knowledge to provide an initial graph structure for a given image. Our experimental results on benchmark datasets demonstrate the capability of this new parts-based view of scene modeling.\n    ",
        "submission_date": "2011-08-20T00:00:00",
        "last_modified_date": "2011-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4098",
        "title": "Multisensor Images Fusion Based on Feature-Level",
        "authors": [
            "Firouz Abdullah Al-Wassai",
            "N.V. Kalyankar",
            "Ali A. Al-Zaky"
        ],
        "abstract": "Until now, of highest relevance for remote sensing data processing and analysis have been techniques for pixel level image fusion. So, This paper attempts to undertake the study of Feature-Level based image fusion. For this purpose, feature based fusion techniques, which are usually based on empirical or heuristic rules, are employed. Hence, in this paper we consider feature extraction (FE) for fusion. It aims at finding a transformation of the original space that would produce such new features, which preserve or improve as much as possible. This study introduces three different types of Image fusion techniques including Principal Component Analysis based Feature Fusion (PCA), Segment Fusion (SF) and Edge fusion (EF). This paper also devotes to concentrate on the analytical techniques for evaluating the quality of image fusion (F) by using various methods including (SD), (En), (CC), (SNR), (NRMSE) and (DI) to estimate the quality and degree of information improvement of a fused image quantitatively.\n    ",
        "submission_date": "2011-08-20T00:00:00",
        "last_modified_date": "2011-08-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4315",
        "title": "Edge detection based on morphological amoebas",
        "authors": [
            "Won Yeol Lee",
            "Young Woo Kim",
            "Se Yun Kim",
            "Jae Young Lim",
            "Dong Hoon Lim"
        ],
        "abstract": "Detecting the edges of objects within images is critical for quality image processing. We present an edge-detecting technique that uses morphological amoebas that adjust their shape based on variation in image contours. We evaluate the method both quantitatively and qualitatively for edge detection of images, and compare it to classic morphological methods. Our amoeba-based edge-detection system performed better than the classic edge detectors.\n    ",
        "submission_date": "2011-08-22T00:00:00",
        "last_modified_date": "2011-08-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5710",
        "title": "Generalized Fast Approximate Energy Minimization via Graph Cuts: Alpha-Expansion Beta-Shrink Moves",
        "authors": [
            "Mark Schmidt",
            "Karteek Alahari"
        ],
        "abstract": "We present alpha-expansion beta-shrink moves, a simple generalization of the widely-used alpha-beta swap and alpha-expansion algorithms for approximate energy minimization. We show that in a certain sense, these moves dominate both alpha-beta-swap and alpha-expansion moves, but unlike previous generalizations the new moves require no additional assumptions and are still solvable in polynomial-time. We show promising experimental results with the new moves, which we believe could be used in any context where alpha-expansions are currently employed.\n    ",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5720",
        "title": "Conjugate Variables as a Resource in Signal and Image Processing",
        "authors": [
            "Michael N\u00f6lle",
            "Martin Suda"
        ],
        "abstract": "In this paper we develop a new technique to model joint distributions of signals. Our technique is based on quantum mechanical conjugate variables. We show that the transition probability of quantum states leads to a distance function on the signals. This distance function obeys the triangle inequality on all quantum states and becomes a metric on pure quantum states. Treating signals as conjugate variables allows us to create a new approach to segment them.\n",
        "submission_date": "2011-08-29T00:00:00",
        "last_modified_date": "2011-08-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6294",
        "title": "Biometric Authorization System using Gait Biometry",
        "authors": [
            "L.R Sudha",
            "Dr.R Bhavani"
        ],
        "abstract": "Human gait, which is a new biometric aimed to recognize individuals by the way they walk have come to play an increasingly important role in visual surveillance applications. In this paper a novel hybrid holistic approach is proposed to show how behavioural walking characteristics can be used to recognize unauthorized and suspicious persons when they enter a surveillance area. Initially background is modelled from the input video captured from cameras deployed for security and the foreground moving object in the individual frames are segmented using the background subtraction algorithm. Then gait representing spatial, temporal and wavelet components are extracted and fused for training and testing multi class support vector machine models (SVM). The proposed system is evaluated using side view videos of NLPR database. The experimental results demonstrate that the proposed system achieves a pleasing recognition rate and also the results indicate that the classification ability of SVM with Radial Basis Function (RBF) is better than with other kernel functions.\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.6304",
        "title": "Anisotropic k-Nearest Neighbor Search Using Covariance Quadtree",
        "authors": [
            "Eraldo Pereira Marinho",
            "Carmen Maria Andreazza"
        ],
        "abstract": "We present a variant of the hyper-quadtree that divides a multidimensional space according to the hyperplanes associated to the principal components of the data in each hyperquadrant. Each of the $2^\\lambda$ hyper-quadrants is a data partition in a $\\lambda$-dimension subspace, whose intrinsic dimensionality $\\lambda\\leq d$ is reduced from the root dimensionality $d$ by the principal components analysis, which discards the irrelevant eigenvalues of the local covariance matrix. In the present method a component is irrelevant if its length is smaller than, or comparable to, the local inter-data spacing. Thus, the covariance hyper-quadtree is fully adaptive to the local dimensionality. The proposed data-structure is used to compute the anisotropic K nearest neighbors (kNN), supported by the Mahalanobis metric. As an application, we used the present k nearest neighbors method to perform density estimation over a noisy data distribution. Such estimation method can be further incorporated to the smoothed particle hydrodynamics, allowing computer simulations of anisotropic fluid flows.\n    ",
        "submission_date": "2011-08-31T00:00:00",
        "last_modified_date": "2011-08-31T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0090",
        "title": "An Efficient Codebook Initialization Approach for LBG Algorithm",
        "authors": [
            "Arup Kumar Pal",
            "Anup Sar"
        ],
        "abstract": "In VQ based image compression technique has three major steps namely (i) Codebook Design, (ii) VQ Encoding Process and (iii) VQ Decoding Process. The performance of VQ based image compression technique depends upon the constructed codebook. A widely used technique for VQ codebook design is the Linde-Buzo-Gray (LBG) algorithm. However the performance of the standard LBG algorithm is highly dependent on the choice of the initial codebook. In this paper, we have proposed a simple and very effective approach for codebook initialization for LBG algorithm. The simulation results show that the proposed scheme is computationally efficient and gives expected performance as compared to the standard LBG algorithm.\n    ",
        "submission_date": "2011-09-01T00:00:00",
        "last_modified_date": "2011-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0138",
        "title": "Automatic Application Level Set Approach in Detection Calcifications in Mammographic Image",
        "authors": [
            "Atef Boujelben",
            "Hedi Tmar",
            "Jameleddine Mnif",
            "Mohamed Abid"
        ],
        "abstract": "Breast cancer is considered as one of a major health problem that constitutes the strongest cause behind mortality among women in the world. So, in this decade, breast cancer is the second most common type of cancer, in term of appearance frequency, and the fifth most common cause of cancer related death. In order to reduce the workload on radiologists, a variety of CAD systems; Computer-Aided Diagnosis (CADi) and Computer-Aided Detection (CADe) have been proposed. In this paper, we interested on CADe tool to help radiologist to detect cancer. The proposed CADe is based on a three-step work flow; namely, detection, analysis and classification. This paper deals with the problem of automatic detection of Region Of Interest (ROI) based on Level Set approach depended on edge and region criteria. This approach gives good visual information from the radiologist. After that, the features extraction using textures characteristics and the vector classification using Multilayer Perception (MLP) and k-Nearest Neighbours (KNN) are adopted to distinguish different ACR (American College of Radiology) classification. Moreover, we use the Digital Database for Screening Mammography (DDSM) for experiments and these results in term of accuracy varied between 60 % and 70% are acceptable and must be ameliorated to aid radiologist.\n    ",
        "submission_date": "2011-09-01T00:00:00",
        "last_modified_date": "2011-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0882",
        "title": "Moving Object Detection by Detecting Contiguous Outliers in the Low-Rank Representation",
        "authors": [
            "Xiaowei Zhou",
            "Can Yang",
            "Weichuan Yu"
        ],
        "abstract": "Object detection is a fundamental step for automated video analysis in many vision applications. Object detection in a video is usually performed by object detectors or background subtraction techniques. Often, an object detector requires manually labeled examples to train a binary classifier, while background subtraction needs a training sequence that contains no objects to build a background model. To automate the analysis, object detection without a separate training phase becomes a critical task. People have tried to tackle this task by using motion information. But existing motion-based methods are usually limited when coping with complex scenarios such as nonrigid motion and dynamic background. In this paper, we show that above challenges can be addressed in a unified framework named DEtecting Contiguous Outliers in the LOw-rank Representation (DECOLOR). This formulation integrates object detection and background learning into a single process of optimization, which can be solved by an alternating algorithm efficiently. We explain the relations between DECOLOR and other sparsity-based methods. Experiments on both simulated data and real sequences demonstrate that DECOLOR outperforms the state-of-the-art approaches and it can work effectively on a wide range of complex scenarios.\n    ",
        "submission_date": "2011-09-05T00:00:00",
        "last_modified_date": "2012-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1057",
        "title": "Toward Designing Intelligent PDEs for Computer Vision: An Optimal Control Approach",
        "authors": [
            "Risheng Liu",
            "Zhouchen Lin",
            "Wei Zhang",
            "Kewei Tang",
            "Zhixun Su"
        ],
        "abstract": "Many computer vision and image processing problems can be posed as solving partial differential equations (PDEs). However, designing PDE system usually requires high mathematical skills and good insight into the problems. In this paper, we consider designing PDEs for various problems arising in computer vision and image processing in a lazy manner: \\emph{learning PDEs from real data via data-based optimal control}. We first propose a general intelligent PDE system which holds the basic translational and rotational invariance rule for most vision problems. By introducing a PDE-constrained optimal control framework, it is possible to use the training data resulting from multiple ways (ground truth, results from other methods, and manual results from humans) to learn PDEs for different computer vision tasks. The proposed optimal control based training framework aims at learning a PDE-based regressor to approximate the unknown (and usually nonlinear) mapping of different vision tasks. The experimental results show that the learnt PDEs can solve different vision problems reasonably well. In particular, we can obtain PDEs not only for problems that traditional PDEs work well but also for problems that PDE-based methods have never been tried before, due to the difficulty in describing those problems in a mathematical way.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1067",
        "title": "Automatic Diagnosis of Abnormal Tumor Region from Brain Computed Tomography Images Using Wavelet Based Statistical Texture Features",
        "authors": [
            "A. Padma",
            "Dr.R. Sukanesh"
        ],
        "abstract": "The research work presented in this paper is to achieve the tissue classification and automatically diagnosis the abnormal tumor region present in Computed Tomography (CT) images using the wavelet based statistical texture analysis method. Comparative studies of texture analysis method are performed for the proposed wavelet based texture analysis method and Spatial Gray Level Dependence Method (SGLDM). Our proposed system consists of four phases i) Discrete Wavelet Decomposition (ii) Feature extraction (iii) Feature selection (iv) Analysis of extracted texture features by classifier. A wavelet based statistical texture feature set is derived from normal and tumor regions. Genetic Algorithm (GA) is used to select the optimal texture features from the set of extracted texture features. We construct the Support Vector Machine (SVM) based classifier and evaluate the performance of classifier by comparing the classification results of the SVM based classifier with the Back Propagation Neural network classifier(BPN). The results of Support Vector Machine (SVM), BPN classifiers for the texture analysis methods are evaluated using Receiver Operating Characteristic (ROC) analysis. Experimental results show that the classification accuracy of SVM is 96% for 10 fold cross validation method. The system has been tested with a number of real Computed Tomography brain images and has achieved satisfactory results.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1068",
        "title": "An Automatic Clustering Technique for Optimal Clusters",
        "authors": [
            "K. Karteeka Pavan",
            "Allam Appa Rao",
            "A. V. Dattatreya Rao"
        ],
        "abstract": "This paper proposes a simple, automatic and efficient clustering algorithm, namely, Automatic Merging for Optimal Clusters (AMOC) which aims to generate nearly optimal clusters for the given datasets automatically. The AMOC is an extension to standard k-means with a two phase iterative procedure combining certain validation techniques in order to find optimal clusters with automation of merging of clusters. Experiments on both synthetic and real data have proved that the proposed algorithm finds nearly optimal clustering structures in terms of number of clusters, compactness and separation.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1133",
        "title": "Color Texture Classification Approach Based on Combination of Primitive Pattern Units and Statistical Features",
        "authors": [
            "Shervan Fekri Ershad"
        ],
        "abstract": "Texture classification became one of the problems which has been paid much attention on by image processing scientists since late 80s. Consequently, since now many different methods have been proposed to solve this problem. In most of these methods the researchers attempted to describe and discriminate textures based on linear and non-linear patterns. The linear and non-linear patterns on any window are based on formation of Grain Components in a particular order. Grain component is a primitive unit of morphology that most meaningful information often appears in the form of occurrence of that. The approach which is proposed in this paper could analyze the texture based on its grain components and then by making grain components histogram and extracting statistical features from that would classify the textures. Finally, to increase the accuracy of classification, proposed approach is expanded to color images to utilize the ability of approach in analyzing each RGB channels, individually. Although, this approach is a general one and it could be used in different applications, the method has been tested on the stone texture and the results can prove the quality of approach.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1175",
        "title": "Estimating 3D Human Shapes from Measurements",
        "authors": [
            "Stefanie Wuhrer",
            "Chang Shu"
        ],
        "abstract": "The recent advances in 3-D imaging technologies give rise to databases of human shapes, from which statistical shape models can be built. These statistical models represent prior knowledge of the human shape and enable us to solve shape reconstruction problems from partial information. Generating human shape from traditional anthropometric measurements is such a problem, since these 1-D measurements encode 3-D shape information. Combined with a statistical shape model, these easy-to-obtain measurements can be leveraged to create 3D human shapes. However, existing methods limit the creation of the shapes to the space spanned by the database and thus require a large amount of training data. In this paper, we introduce a technique that extrapolates the statistically inferred shape to fit the measurement data using nonlinear optimization. This method ensures that the generated shape is both human-like and satisfies the measurement conditions. We demonstrate the effectiveness of the method and compare it to existing approaches through extensive experiments, using both synthetic data and real human measurements.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2012-03-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1247",
        "title": "Devnagari document segmentation using histogram approach",
        "authors": [
            "Vikas J Dongre",
            "Vijay H Mankar"
        ],
        "abstract": "Document segmentation is one of the critical phases in machine recognition of any language. Correct segmentation of individual symbols decides the accuracy of character recognition technique. It is used to decompose image of a sequence of characters into sub images of individual symbols by segmenting lines and words. Devnagari is the most popular script in India. It is used for writing Hindi, Marathi, Sanskrit and Nepali languages. Moreover, Hindi is the third most popular language in the world. Devnagari documents consist of vowels, consonants and various modifiers. Hence proper segmentation of Devnagari word is challenging. A simple histogram based approach to segment Devnagari documents is proposed in this paper. Various challenges in segmentation of Devnagari script are also discussed.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1480",
        "title": "Curvature Prior for MRF-based Segmentation and Shape Inpainting",
        "authors": [
            "Alexander Shekhovtsov",
            "Pushmeet Kohli",
            "Carsten Rother"
        ],
        "abstract": "Most image labeling problems such as segmentation and image reconstruction are fundamentally ill-posed and suffer from ambiguities and noise. Higher order image priors encode high level structural dependencies between pixels and are key to overcoming these problems. However, these priors in general lead to computationally intractable models. This paper addresses the problem of discovering compact representations of higher order priors which allow efficient inference. We propose a framework for solving this problem which uses a recently proposed representation of higher order functions where they are encoded as lower envelopes of linear functions. Maximum a Posterior inference on our learned models reduces to minimizing a pairwise function of discrete variables, which can be done approximately using standard methods. Although this is a primarily theoretical paper, we also demonstrate the practical effectiveness of our framework on the problem of learning a shape prior for image segmentation and reconstruction. We show that our framework can learn a compact representation that approximates a prior that encourages low curvature shapes. We evaluate the approximation accuracy, discuss properties of the trained model, and show various results for shape inpainting and image segmentation.\n    ",
        "submission_date": "2011-09-07T00:00:00",
        "last_modified_date": "2011-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1865",
        "title": "Progressive versus Random Projections for Compressive Capture of Images, Lightfields and Higher Dimensional Visual Signals",
        "authors": [
            "Rohit Pandharkar",
            "Ashok Veeraraghavan",
            "Ramesh Raskar"
        ],
        "abstract": "Computational photography involves sophisticated capture methods. A new trend is to capture projection of higher dimensional visual signals such as videos, multi-spectral data and lightfields on lower dimensional sensors. Carefully designed capture methods exploit the sparsity of the underlying signal in a transformed domain to reduce the number of measurements and use an appropriate reconstruction method. Traditional progressive methods may capture successively more detail using a sequence of simple projection basis, such as DCT or wavelets and employ straightforward backprojection for reconstruction. Randomized projection methods do not use any specific sequence and use L0 minimization for reconstruction. In this paper, we analyze the statistical properties of natural images, videos, multi-spectral data and light-fields and compare the effectiveness of progressive and random projections. We define effectiveness by plotting reconstruction SNR against compression factor. The key idea is a procedure to measure best-case effectiveness that is fast, independent of specific hardware and independent of the reconstruction procedure. We believe this is the first empirical study to compare different lossy capture strategies without the complication of hardware or reconstruction ambiguity. The scope is limited to linear non-adaptive sensing. The results show that random projections produce significant advantages over other projections only for higher dimensional signals, and suggest more research to nascent adaptive and non-linear projection methods.\n    ",
        "submission_date": "2011-09-09T00:00:00",
        "last_modified_date": "2011-09-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2389",
        "title": "A Probabilistic Framework for Discriminative Dictionary Learning",
        "authors": [
            "Bernard Ghanem",
            "Narendra Ahuja"
        ],
        "abstract": "In this paper, we address the problem of discriminative dictionary learning (DDL), where sparse linear representation and classification are combined in a probabilistic framework. As such, a single discriminative dictionary and linear binary classifiers are learned jointly. By encoding sparse representation and discriminative classification models in a MAP setting, we propose a general optimization framework that allows for a data-driven tradeoff between faithful representation and accurate classification. As opposed to previous work, our learning methodology is capable of incorporating a diverse family of classification cost functions (including those used in popular boosting methods), while avoiding the need for involved optimization techniques. We show that DDL can be solved by a sequence of updates that make use of well-known and well-studied sparse coding and dictionary learning algorithms from the literature. To validate our DDL framework, we apply it to digit classification and face recognition and test it on standard benchmarks.\n    ",
        "submission_date": "2011-09-12T00:00:00",
        "last_modified_date": "2011-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2449",
        "title": "Multi-Hypothesis CRF-Segmentation of Neural Tissue in Anisotropic EM Volumes",
        "authors": [
            "Jan Funke",
            "Bj\u00f6rn Andres",
            "Fred Hamprecht",
            "Albert Cardona",
            "Matthew Cook"
        ],
        "abstract": "We present an approach for the joint segmentation and grouping of similar components in anisotropic 3D image data and use it to segment neural tissue in serial sections electron microscopy (EM) images.\n",
        "submission_date": "2011-09-12T00:00:00",
        "last_modified_date": "2011-09-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3126",
        "title": "A Non-Iterative Solution to the Four-Point Three-Views Pose Problem in Case of Collinear Cameras",
        "authors": [
            "Evgeniy Martyushev"
        ],
        "abstract": "We give a non-iterative solution to a particular case of the four-point three-views pose problem when three camera centers are collinear. Using the well-known Cayley representation of orthogonal matrices, we derive from the epipolar constraints a system of three polynomial equations in three variables. The eliminant of that system is a multiple of a 36th degree univariate polynomial. The true (unique) solution to the problem can be expressed in terms of one of real roots of that polynomial. Experiments on synthetic data confirm that our method is robust enough even in case of planar configurations.\n    ",
        "submission_date": "2011-09-14T00:00:00",
        "last_modified_date": "2011-09-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3317",
        "title": "Design of an Optical Character Recognition System for Camera-based Handheld Devices",
        "authors": [
            "Ayatullah Faruk Mollah",
            "Nabamita Majumder",
            "Subhadip Basu",
            "Mita Nasipuri"
        ],
        "abstract": "This paper presents a complete Optical Character Recognition (OCR) system for camera captured image/graphics embedded textual documents for handheld devices. At first, text regions are extracted and skew corrected. Then, these regions are binarized and segmented into lines and characters. Characters are passed into the recognition module. Experimenting with a set of 100 business card images, captured by cell phone camera, we have achieved a maximum recognition accuracy of 92.74%. Compared to Tesseract, an open source desktop-based powerful OCR engine, present recognition accuracy is worth contributing. Moreover, the developed technique is computationally efficient and consumes low memory so as to be applicable on handheld devices.\n    ",
        "submission_date": "2011-09-15T00:00:00",
        "last_modified_date": "2011-09-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3637",
        "title": "Connectivity-Enforcing Hough Transform for the Robust Extraction of Line Segments",
        "authors": [
            "Rui F. C. Guerreiro",
            "Pedro M. Q. Aguiar"
        ],
        "abstract": "Global voting schemes based on the Hough transform (HT) have been widely used to robustly detect lines in images. However, since the votes do not take line connectivity into account, these methods do not deal well with cluttered images. In opposition, the so-called local methods enforce connectivity but lack robustness to deal with challenging situations that occur in many realistic scenarios, e.g., when line segments cross or when long segments are corrupted. In this paper, we address the critical limitations of the HT as a line segment extractor by incorporating connectivity in the voting process. This is done by only accounting for the contributions of edge points lying in increasingly larger neighborhoods and whose position and directional content agree with potential line segments. As a result, our method, which we call STRAIGHT (Segment exTRAction by connectivity-enforcInG HT), extracts the longest connected segments in each location of the image, thus also integrating into the HT voting process the usually separate step of individual segment extraction. The usage of the Hough space mapping and a corresponding hierarchical implementation make our approach computationally feasible. We present experiments that illustrate, with synthetic and real images, how STRAIGHT succeeds in extracting complete segments in several situations where current methods fail.\n    ",
        "submission_date": "2011-09-16T00:00:00",
        "last_modified_date": "2011-09-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3767",
        "title": "Generalised Object Detection and Semantic Analysis: Casino Example using Matlab",
        "authors": [
            "Othman Ahmad"
        ],
        "abstract": "Matlab version 7.1 had been used to detect playing cards on a Casino table and the suits and ranks of these cards had been identified. The process gives an example of an application of computer vision to a problem where rectangular objects are to be detected and the information content of the objects are extracted out. In the case of playing cards, it is the suit and rank of each card. The image processing system is done in two passes. Pass 1 detects rectangular shapes and template matched with a template of the left and right edges of the cards. Pass 2 extracts the suit and rank of the cards by matching the top left portion of the card that contains both rank and suit information, with stored templates of ranks and suits of the playing cards using a series of if-then statements.\n    ",
        "submission_date": "2011-09-17T00:00:00",
        "last_modified_date": "2011-09-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3850",
        "title": "Digital (co)homology modules and digital Pontryagin algebras",
        "authors": [
            "Dae-Woong Lee"
        ],
        "abstract": "In the current study, we explore digital homology and cohomology modules, and investigate their fundamental properties on pointed digital images. We also examine pointed digital Hopf spaces and base point preserving digital Hopf functions between the pointed digital Hopf spaces with suitable digital multiplications, and explore the digital primitive homology and cohomology classes, the digital Pontryagin algebras and coalgebras on the digital Hopf spaces as digital images.\n    ",
        "submission_date": "2011-09-18T00:00:00",
        "last_modified_date": "2020-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4683",
        "title": "Detachable Object Detection: Segmentation and Depth Ordering From Short-Baseline Video",
        "authors": [
            "Alper Ayvaci",
            "Stefano Soatto"
        ],
        "abstract": "We describe an approach for segmenting an image into regions that correspond to surfaces in the scene that are partially surrounded by the medium. It integrates both appearance and motion statistics into a cost functional, that is seeded with occluded regions and minimized efficiently by solving a linear programming problem. Where a short observation time is insufficient to determine whether the object is detachable, the results of the minimization can be used to seed a more costly optimization based on a longer sequence of video data. The result is an entirely unsupervised scheme to detect and segment an arbitrary and unknown number of objects. We test our scheme to highlight the potential, as well as limitations, of our approach.\n    ",
        "submission_date": "2011-09-22T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4744",
        "title": "Probabilistic prototype models for attributed graphs",
        "authors": [
            "S. Deepak Srinivasan",
            "Klaus Obermayer"
        ],
        "abstract": "This contribution proposes a new approach towards developing a class of probabilistic methods for classifying attributed graphs. The key concept is random attributed graph, which is defined as an attributed graph whose nodes and edges are annotated by random variables. Every node/edge has two random processes associated with it- occurence probability and the probability distribution over the attribute values. These are estimated within the maximum likelihood framework. The likelihood of a random attributed graph to generate an outcome graph is used as a feature for classification. The proposed approach is fast and robust to noise.\n    ",
        "submission_date": "2011-09-22T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4909",
        "title": "Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D Rigid-Body Motion Registration",
        "authors": [
            "Chris Slaughter",
            "Allen Y. Yang",
            "Justin Bagwell",
            "Costa Checkles",
            "Luis Sentis",
            "Sriram Vishwanath"
        ],
        "abstract": "Motivated by an emerging theory of robust low-rank matrix representation, in this paper, we introduce a novel solution for online rigid-body motion registration. The goal is to develop algorithmic techniques that enable a robust, real-time motion registration solution suitable for low-cost, portable 3-D camera devices. Assuming 3-D image features are tracked via a standard tracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape representation of the rigid body. Robust PCA finds the global optimal solution of the initialization, while its complexity is comparable to singular value decomposition. In the online update stage, we propose a more efficient algorithm for sparse subspace projection to sequentially project new feature observations onto the shape subspace. The lightweight update stage guarantees the real-time performance of the solution while maintaining good registration even when the image sequence is contaminated by noise, gross data corruption, outlying features, and missing data. The state-of-the-art accuracy of the solution is validated through extensive simulation and a real-world experiment, while the system enjoys one to two orders of magnitude speed-up compared to well-established RANSAC solutions. The new algorithm will be released online to aid peer evaluation.\n    ",
        "submission_date": "2011-09-22T00:00:00",
        "last_modified_date": "2011-09-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.4920",
        "title": "Beyond pixels and regions: A non local patch means (NLPM) method for content-level restoration, enhancement, and reconstruction of degraded document images",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "abstract": "A patch-based non-local restoration and reconstruction method for preprocessing degraded document images is introduced. The method collects relative data from the whole input image, while the image data are first represented by a content-level descriptor based on patches. This patch-equivalent representation of the input image is then corrected based on similar patches identified using a modified genetic algorithm (GA) resulting in a low computational load. The corrected patch-equivalent is then converted to the output restored image. The fact that the method uses the patches at the content level allows it to incorporate high-level restoration in an objective and self-sufficient way. The method has been applied to several degraded document images, including the DIBCO'09 contest dataset with promising results.\n    ",
        "submission_date": "2011-09-22T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5114",
        "title": "Improvements on \"Fast space-variant elliptical filtering using box splines\"",
        "authors": [
            "Kunal N. Chaudhury",
            "Sebanti Sanyal"
        ],
        "abstract": "It is well-known that box filters can be efficiently computed using pre-integrations and local finite-differences [Crow1984,Heckbert1986,Viola2001]. By generalizing this idea and by combining it with a non-standard variant of the Central Limit Theorem, a constant-time or O(1) algorithm was proposed in [Chaudhury2010] that allowed one to perform space-variant filtering using Gaussian-like kernels. The algorithm was based on the observation that both isotropic and anisotropic Gaussians could be approximated using certain bivariate splines called box splines. The attractive feature of the algorithm was that it allowed one to continuously control the shape and size (covariance) of the filter, and that it had a fixed computational cost per pixel, irrespective of the size of the filter. The algorithm, however, offered a limited control on the covariance and accuracy of the Gaussian approximation. In this work, we propose some improvements by appropriately modifying the algorithm in [Chaudhury2010].\n    ",
        "submission_date": "2011-09-23T00:00:00",
        "last_modified_date": "2012-02-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5370",
        "title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images",
        "authors": [
            "Jia Zeng",
            "Wei Feng",
            "William K. Cheung",
            "Chun-Hung Li"
        ],
        "abstract": "This paper studies the topic modeling problem of tagged documents and images. Higher-order relations among tagged documents and images are major and ubiquitous characteristics, and play positive roles in extracting reliable and interpretable topics. In this paper, we propose the tag-topic models (TTM) to depict such higher-order topic structural dependencies within the Markov random field (MRF) framework. First, we use the novel factor graph representation of latent Dirichlet allocation (LDA)-based topic models from the MRF perspective, and present an efficient loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Second, we propose the factor hypergraph representation of TTM, and focus on both pairwise and higher-order relation modeling among tagged documents and images. Efficient loopy BP algorithm is developed to learn TTM, which encourages the topic labeling smoothness among tagged documents and images. Extensive experimental results confirm the incorporation of higher-order relations to be effective in enhancing the overall topic modeling performance, when compared with current state-of-the-art topic models, in many text and image mining tasks of broad interests such as word and link prediction, document classification, and tag recommendation.\n    ",
        "submission_date": "2011-09-25T00:00:00",
        "last_modified_date": "2011-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5453",
        "title": "Posterior Mean Super-resolution with a Causal Gaussian Markov Random Field Prior",
        "authors": [
            "Takayuki Katsuki",
            "Akira Torii",
            "Masato Inoue"
        ],
        "abstract": "We propose a Bayesian image super-resolution (SR) method with a causal Gaussian Markov random field (MRF) prior. SR is a technique to estimate a spatially high-resolution image from given multiple low-resolution images. An MRF model with the line process supplies a preferable prior for natural images with edges. We improve the existing image transformation model, the compound MRF model, and its hyperparameter prior model. We also derive the optimal estimator -- not the joint maximum a posteriori (MAP) or marginalized maximum likelihood (ML), but the posterior mean (PM) -- from the objective function of the L2-norm (mean square error) -based peak signal-to-noise ratio (PSNR). Point estimates such as MAP and ML are generally not stable in ill-posed high-dimensional problems because of overfitting, while PM is a stable estimator because all the parameters in the model are evaluated as distributions. The estimator is numerically determined by using variational Bayes. Variational Bayes is a widely used method that approximately determines a complicated posterior distribution, but it is generally hard to use because it needs the conjugate prior. We solve this problem with simple Taylor approximations. Experimental results have shown that the proposed method is more accurate or comparable to existing methods.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2012-04-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5730",
        "title": "Hypothesize and Bound: A Computational Focus of Attention Mechanism for Simultaneous 3D Shape Reconstruction, Pose Estimation and Classification from a Single 2D Image",
        "authors": [
            "Diego Rother",
            "Siddharth Mahendran",
            "Ren\u00e9 Vidal"
        ],
        "abstract": "This article presents a mathematical framework to simultaneously tackle the problems of 3D reconstruction, pose estimation and object classification, from a single 2D image. In sharp contrast with state of the art methods that rely primarily on 2D information and solve each of these three problems separately or iteratively, we propose a mathematical framework that incorporates prior \"knowledge\" about the 3D shapes of different object classes and solves these problems jointly and simultaneously, using a hypothesize-and-bound (H&B) algorithm. In the proposed H&B algorithm one hypothesis is defined for each possible pair [object class, object pose], and the algorithm selects the hypothesis H that maximizes a function L(H) encoding how well each hypothesis \"explains\" the input image. To find this maximum efficiently, the function L(H) is not evaluated exactly for each hypothesis H, but rather upper and lower bounds for it are computed at a much lower cost. In order to obtain bounds for L(H) that are tight yet inexpensive to compute, we extend the theory of shapes described in [14] to handle projections of shapes. This extension allows us to define a probabilistic relationship between the prior knowledge given in 3D and the 2D input image. This relationship is derived from first principles and is proven to be the only relationship having the properties that we intuitively expect from a \"projection.\" In addition to the efficiency and optimality characteristics of H&B algorithms, the proposed framework has the desirable property of integrating information in the 2D image with information in the 3D prior to estimate the optimal reconstruction. While this article focuses primarily on the problem mentioned above, we believe that the theory presented herein has multiple other potential applications.\n    ",
        "submission_date": "2011-09-26T00:00:00",
        "last_modified_date": "2011-09-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6442",
        "title": "ABHIVYAKTI: A Vision Based Intelligent System for Elder and Sick Persons",
        "authors": [
            "Ankit Chaudhary",
            "Jagdish L. Raheja"
        ],
        "abstract": "This paper describes an intelligent system ABHIVYAKTI, which would be pervasive in nature and based on the Computer Vision. It would be very easy in use and deployment. Elder and sick people who are not able to talk or walk, they are dependent on other human beings and need continuous monitoring, while our system provides flexibility to the sick or elder person to announce his or her need to their caretaker by just showing a particular gesture with the developed system, if the caretaker is not nearby. This system will use fingertip detection techniques for acquiring gesture and Artificial Neural Networks (ANNs) will be used for gesture recognition.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-10-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6638",
        "title": "The Statistical Inefficiency of Sparse Coding for Images (or, One Gabor to Rule them All)",
        "authors": [
            "James Bergstra",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "abstract": "Sparse coding is a proven principle for learning compact representations of images. However, sparse coding by itself often leads to very redundant dictionaries. With images, this often takes the form of similar edge detectors which are replicated many times at various positions, scales and orientations. An immediate consequence of this observation is that the estimation of the dictionary components is not statistically efficient. We propose a factored model in which factors of variation (e.g. position, scale and orientation) are untangled from the underlying Gabor-like filters. There is so much redundancy in sparse codes for natural images that our model requires only a single dictionary element (a Gabor-like edge detector) to outperform standard sparse coding. Our model scales naturally to arbitrary-sized images while achieving much greater statistical efficiency during learning. We validate this claim with a number of experiments showing, in part, superior compression of out-of-sample data using a sparse coding dictionary learned with only a single image.\n    ",
        "submission_date": "2011-09-29T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.6840",
        "title": "A Novel comprehensive method for real time Video Motion Detection Surveillance",
        "authors": [
            "Sumita Mishra"
        ],
        "abstract": "This article describes a comprehensive system for surveillance and monitoring applications. The development of an efficient real time video motion detection system is motivated by their potential for deployment in the areas where security is the main concern. The paper presents a platform for real time video motion detection and subsequent generation of an alarm condition as set by the parameters of the control system. The prototype consists of a mobile platform mounted with RF camera which provides continuous feedback of the environment. The received visual information is then analyzed by user for appropriate control action, thus enabling the user to operate the system from a remote location. The system is also equipped with the ability to process the image of an object and generate control signals which are automatically transmitted to the mobile platform to track the object.\n    ",
        "submission_date": "2011-09-30T00:00:00",
        "last_modified_date": "2011-09-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0107",
        "title": "Learning to relate images: Mapping units, complex cells and simultaneous eigenspaces",
        "authors": [
            "Roland Memisevic"
        ],
        "abstract": "A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. We present an analysis of the role that multiplicative interactions play in learning such correspondences, and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices. We review a variety of recent multiplicative sparse coding methods in light of this observation. We also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions. This suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances.\n    ",
        "submission_date": "2011-10-01T00:00:00",
        "last_modified_date": "2012-04-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0264",
        "title": "Face Recognition using Optimal Representation Ensemble",
        "authors": [
            "Hanxi Li",
            "Chunhua Shen",
            "Yongsheng Gao"
        ],
        "abstract": "Recently, the face recognizers based on linear representations have been shown to deliver state-of-the-art performance. In real-world applications, however, face images usually suffer from expressions, disguises and random occlusions. The problematic facial parts undermine the validity of the linear-subspace assumption and thus the recognition performance deteriorates significantly. In this work, we address the problem in a learning-inference-mixed fashion. By observing that the linear-subspace assumption is more reliable on certain face patches rather than on the holistic face, some Bayesian Patch Representations (BPRs) are randomly generated and interpreted according to the Bayes' theory. We then train an ensemble model over the patch-representations by minimizing the empirical risk w.r.t the \"leave-one-out margins\". The obtained model is termed Optimal Representation Ensemble (ORE), since it guarantees the optimality from the perspective of Empirical Risk Minimization. To handle the unknown patterns in test faces, a robust version of BPR is proposed by taking the non-face category into consideration. Equipped with the Robust-BPRs, the inference ability of ORE is increased dramatically and several record-breaking accuracies (99.9% on Yale-B and 99.5% on AR) and desirable efficiencies (below 20 ms per face in Matlab) are achieved. It also overwhelms other modular heuristics on the faces with random occlusions, extreme expressions and disguises. Furthermore, to accommodate immense BPRs sets, a boosting-like algorithm is also derived. The boosted model, a.k.a Boosted-ORE, obtains similar performance to its prototype. Besides the empirical superiorities, two desirable features of the proposed methods, namely, the training-determined model-selection and the data-weight-free boosting procedure, are also theoretically verified.\n    ",
        "submission_date": "2011-10-03T00:00:00",
        "last_modified_date": "2011-10-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0585",
        "title": "Discriminately Decreasing Discriminability with Learned Image Filters",
        "authors": [
            "Jacob Whitehill",
            "Javier Movellan"
        ],
        "abstract": "In machine learning and computer vision, input images are often filtered to increase data discriminability. In some situations, however, one may wish to purposely decrease discriminability of one classification task (a \"distractor\" task), while simultaneously preserving information relevant to another (the task-of-interest): For example, it may be important to mask the identity of persons contained in face images before submitting them to a crowdsourcing site (e.g., Mechanical Turk) when labeling them for certain facial attributes. Another example is inter-dataset generalization: when training on a dataset with a particular covariance structure among multiple attributes, it may be useful to suppress one attribute while preserving another so that a trained classifier does not learn spurious correlations between attributes. In this paper we present an algorithm that finds optimal filters to give high discriminability to one task while simultaneously giving low discriminability to a distractor task. We present results showing the effectiveness of the proposed technique on both simulated data and natural face images.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0872",
        "title": "Non-Gaussian Scale Space Filtering with 2 by 2 Matrix of Linear Filters",
        "authors": [
            "Toshiro Kubota"
        ],
        "abstract": "Construction of a scale space with a convolution filter has been studied extensively in the past. It has been proven that the only convolution kernel that satisfies the scale space requirements is a Gaussian type. In this paper, we consider a matrix of convolution filters introduced in [1] as a building kernel for a scale space, and shows that we can construct a non-Gaussian scale space with a $2\\times 2$ matrix of filters. The paper derives sufficient conditions for the matrix of filters for being a scale space kernel, and present some numerical demonstrations.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0879",
        "title": "Linearized Additive Classifiers",
        "authors": [
            "Subhransu Maji"
        ],
        "abstract": "We revisit the additive model learning literature and adapt a penalized spline formulation due to Eilers and Marx, to train additive classifiers efficiently. We also propose two new embeddings based two classes of orthogonal basis with orthogonal derivatives, which can also be used to efficiently learn additive classifiers. This paper follows the popular theme in the current literature where kernel SVMs are learned much more efficiently using a approximate embedding and linear machine. In this paper we show that spline basis are especially well suited for learning additive models because of their sparsity structure and the ease of computing the embedding which enables one to train these models in an online manner, without incurring the memory overhead of precomputing the storing the embeddings. We show interesting connections between B-Spline basis and histogram intersection kernel and show that for a particular choice of regularization and degree of the B-Splines, our proposed learning algorithm closely approximates the histogram intersection kernel SVM. This enables one to learn additive models with almost no memory overhead compared to fast a linear solver, such as LIBLINEAR, while being only 5-6X slower on average. On two large scale image classification datasets, MNIST and Daimler Chrysler pedestrians, the proposed additive classifiers are as accurate as the kernel SVM, while being two orders of magnitude faster to train.\n    ",
        "submission_date": "2011-10-05T00:00:00",
        "last_modified_date": "2011-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1208",
        "title": "Rotation, Scaling and Translation Analysis of Biometric Signature Templates",
        "authors": [
            "Aman Chadha",
            "Divya Jyoti",
            "M. Mani Roja"
        ],
        "abstract": "Biometric authentication systems that make use of signature verification methods often render optimum performance only under limited and restricted conditions. Such methods utilize several training samples so as to achieve high accuracy. Moreover, several constraints are imposed on the end-user so that the system may work optimally, and as expected. For example, the user is made to sign within a small box, in order to limit their signature to a predefined set of dimensions, thus eliminating scaling. Moreover, the angular rotation with respect to the referenced signature that will be inadvertently introduced as human error, hampers performance of biometric signature verification systems. To eliminate this, traditionally, a user is asked to sign exactly on top of a reference line. In this paper, we propose a robust system that optimizes the signature obtained from the user for a large range of variation in Rotation-Scaling-Translation (RST) and resolves these error parameters in the user signature according to the reference signature stored in the database.\n    ",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2011-10-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1485",
        "title": "A Face Recognition Scheme using Wavelet Based Dominant Features",
        "authors": [
            "Hafiz Imtiaz",
            "Shaikh Anowarul Fattah"
        ],
        "abstract": "In this paper, a multi-resolution feature extraction algorithm for face recognition is proposed based on two-dimensional discrete wavelet transform (2D-DWT), which efficiently exploits the local spatial variations in a face image. For the purpose of feature extraction, instead of considering the entire face image, an entropy-based local band selection criterion is developed, which selects high-informative horizontal segments from the face image. In order to capture the local spatial variations within these highinformative horizontal bands precisely, the horizontal band is segmented into several small spatial modules. Dominant wavelet coefficients corresponding to each local region residing inside those horizontal bands are selected as features. In the selection of the dominant coefficients, a threshold criterion is proposed, which not only drastically reduces the feature dimension but also provides high within-class compactness and high between-class separability. A principal component analysis is performed to further reduce the dimensionality of the feature space. Extensive experimentation is carried out upon standard face databases and a very high degree of recognition accuracy is achieved by the proposed method in comparison to those obtained by some of the existing methods.\n    ",
        "submission_date": "2011-10-07T00:00:00",
        "last_modified_date": "2011-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1509",
        "title": "A Comparative Experiment of Several Shape Methods in Recognizing Plants",
        "authors": [
            "A. Kadir",
            "L.E. Nugroho",
            "A. Susanto",
            "P.I. Santosa"
        ],
        "abstract": "Shape is an important aspects in recognizing plants. Several approaches have been introduced to identify objects, including plants. Combination of geometric features such as aspect ratio, compactness, and dispersion, or moments such as moment invariants were usually used toidentify plants. In this research, a comparative experiment of 4 methods to identify plants using shape features was accomplished. Two approaches have never been used in plants identification yet, Zernike moments and Polar Fourier Transform (PFT), were incorporated. The experimental comparison was done on 52 kinds of plants with various shapes. The result, PFT gave best performance with 64% in accuracy and outperformed the other methods.\n    ",
        "submission_date": "2011-10-07T00:00:00",
        "last_modified_date": "2011-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1513",
        "title": "Foliage Plant Retrieval using Polar Fourier Transform, Color Moments and Vein Features",
        "authors": [
            "Abdul Kadir",
            "Lukito Edi Nugroho",
            "Adhi Susanto",
            "Paulus Insap Santosa"
        ],
        "abstract": "This paper proposed a method that combines Polar Fourier Transform, color moments, and vein features to retrieve leaf images based on a leaf image. The method is very useful to help people in recognizing foliage plants. Foliage plants are plants that have various colors and unique patterns in the leaf. Therefore, the colors and its patterns are information that should be counted on in the processing of plant identification. To compare the performance of retrieving system to other result, the experiments used Flavia dataset, which is very popular in recognizing plants. The result shows that the method gave better performance than PNN, SVM, and Fourier Transform. The method was also tested using foliage plants with various colors. The accuracy was 90.80% for 50 kinds of plants.\n    ",
        "submission_date": "2011-10-07T00:00:00",
        "last_modified_date": "2011-10-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1804",
        "title": "The proximal point method for a hybrid model in image restoration",
        "authors": [
            "Zhi-Feng Pang",
            "Li-Lian Wang",
            "Yu-Fei Yang"
        ],
        "abstract": "Models including two $L^1$ -norm terms have been widely used in image restoration. In this paper we first propose the alternating direction method of multipliers (ADMM) to solve this class of models. Based on ADMM, we then propose the proximal point method (PPM), which is more efficient than ADMM. Following the operator theory, we also give the convergence analysis of the proposed methods. Furthermore, we use the proposed methods to solve a class of hybrid models combining the ROF model with the LLT model. Some numerical results demonstrate the viability and efficiency of the proposed methods.\n    ",
        "submission_date": "2011-10-09T00:00:00",
        "last_modified_date": "2012-08-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2053",
        "title": "Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control",
        "authors": [
            "Stefano Soatto"
        ],
        "abstract": "This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of \"complete information\" that relates to the minimal sufficient statistics of a complete representation. It is shown that the \"actionable information gap\" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called \"signal-to-symbol barrier\" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video.\n",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2017-12-27T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2210",
        "title": "Closed-Loop Learning of Visual Control Policies",
        "authors": [
            "S. R. Jodogne",
            "J. H. Piater"
        ],
        "abstract": "In this paper we present a general, flexible framework for learning mappings from images to actions by interacting with the environment. The basic idea is to introduce a feature-based image classifier in front of a reinforcement learning algorithm. The classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing. We also address the problem of fighting overfitting in such a greedy algorithm. Finally, we show how high-level visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states. This is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features. We demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical Car on the Hill control problem.\n    ",
        "submission_date": "2011-10-10T00:00:00",
        "last_modified_date": "2011-10-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3109",
        "title": "Robust Image Analysis by L1-Norm Semi-supervised Learning",
        "authors": [
            "Zhiwu Lu",
            "Yuxin Peng"
        ],
        "abstract": "This paper presents a novel L1-norm semi-supervised learning algorithm for robust image analysis by giving new L1-norm formulation of Laplacian regularization which is the key step of graph-based semi-supervised learning. Since our L1-norm Laplacian regularization is defined directly over the eigenvectors of the normalized Laplacian matrix, we successfully formulate semi-supervised learning as an L1-norm linear reconstruction problem which can be effectively solved with sparse coding. By working with only a small subset of eigenvectors, we further develop a fast sparse coding algorithm for our L1-norm semi-supervised learning. Due to the sparsity induced by sparse coding, the proposed algorithm can deal with the noise in the data to some extent and thus has important applications to robust image analysis, such as noise-robust image classification and noise reduction for visual and textual bag-of-words (BOW) models. In particular, this paper is the first attempt to obtain robust image representation by sparse co-refinement of visual and textual BOW models. The experimental results have shown the promising performance of the proposed algorithm.\n    ",
        "submission_date": "2011-10-14T00:00:00",
        "last_modified_date": "2013-01-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3194",
        "title": "Controlled Total Variation regularization for inverse problems",
        "authors": [
            "Qiyu Jin",
            "Ion Grama",
            "Quansheng Liu"
        ],
        "abstract": "This paper provides a new algorithm for solving inverse problems, based on the minimization of the $L^2$ norm and on the control of the Total Variation. It consists in relaxing the role of the Total Variation in the classical Total Variation minimization approach, which permits us to get better approximation to the inverse problems. The numerical results on the deconvolution problem show that our method outperforms some previous ones.\n    ",
        "submission_date": "2011-10-14T00:00:00",
        "last_modified_date": "2011-10-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3767",
        "title": "Anti-sparse coding for approximate nearest neighbor search",
        "authors": [
            "Herv\u00e9 J\u00e9gou",
            "Teddy Furon",
            "Jean-Jacques Fuchs"
        ],
        "abstract": "This paper proposes a binarization scheme for vectors of high dimension based on the recent concept of anti-sparse coding, and shows its excellent performance for approximate nearest neighbor search. Unlike other binarization schemes, this framework allows, up to a scaling factor, the explicit reconstruction from the binary representation of the original vector. The paper also shows that random projections which are used in Locality Sensitive Hashing algorithms, are significantly outperformed by regular frames for both synthetic and real data if the number of bits exceeds the vector dimensionality, i.e., when high precision is required.\n    ",
        "submission_date": "2011-10-17T00:00:00",
        "last_modified_date": "2011-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.4970",
        "title": "Studying Satellite Image Quality Based on the Fusion Techniques",
        "authors": [
            "Firouz Abdullah Al-Wassai",
            "N.V. Kalyankar",
            "Ali A. Al-Zaky"
        ],
        "abstract": "Various and different methods can be used to produce high-resolution multispectral images from high-resolution panchromatic image (PAN) and low-resolution multispectral images (MS), mostly on the pixel level. However, the jury is still out on the benefits of a fused image compared to its original images. There is also a lack of measures for assessing the objective quality of the spatial resolution for the fusion methods. Therefore, an objective quality of the spatial resolution assessment for fusion images is required. So, this study attempts to develop a new qualitative assessment to evaluate the spatial quality of the pan sharpened images by many spatial quality metrics. Also, this paper deals with a comparison of various image fusion techniques based on pixel and feature fusion techniques.\n    ",
        "submission_date": "2011-10-22T00:00:00",
        "last_modified_date": "2011-10-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5015",
        "title": "Spectral descriptors for deformable shapes",
        "authors": [
            "Alexander M. Bronstein"
        ],
        "abstract": "Informative and discriminative feature descriptors play a fundamental role in deformable shape analysis. For example, they have been successfully employed in correspondence, registration, and retrieval tasks. In the recent years, significant attention has been devoted to descriptors obtained from the spectral decomposition of the Laplace-Beltrami operator associated with the shape. Notable examples in this family are the heat kernel signature (HKS) and the wave kernel signature (WKS). Laplacian-based descriptors achieve state-of-the-art performance in numerous shape analysis tasks; they are computationally efficient, isometry-invariant by construction, and can gracefully cope with a variety of transformations. In this paper, we formulate a generic family of parametric spectral descriptors. We argue that in order to be optimal for a specific task, the descriptor should take into account the statistics of the corpus of shapes to which it is applied (the \"signal\") and those of the class of transformations to which it is made insensitive (the \"noise\"). While such statistics are hard to model axiomatically, they can be learned from examples. Following the spirit of the Wiener filter in signal processing, we show a learning scheme for the construction of optimal spectral descriptors and relate it to Mahalanobis metric learning. The superiority of the proposed approach is demonstrated on the SHREC'10 benchmark.\n    ",
        "submission_date": "2011-10-23T00:00:00",
        "last_modified_date": "2011-10-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5102",
        "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models",
        "authors": [
            "Congcong Li",
            "Adarsh Kowdle",
            "Ashutosh Saxena",
            "Tsuhan Chen"
        ],
        "abstract": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier.\n",
        "submission_date": "2011-10-24T00:00:00",
        "last_modified_date": "2011-10-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5404",
        "title": "Face Recognition Based on SVM and 2DPCA",
        "authors": [
            "Thai Hoang Le",
            "Len Bui"
        ],
        "abstract": "The paper will present a novel approach for solving face recognition problem. Our method combines 2D Principal Component Analysis (2DPCA), one of the prominent methods for extracting feature vectors, and Support Vector Machine (SVM), the most powerful discriminative method for classification. Experiments based on proposed method have been conducted on two public data sets FERET and AT&T; the results show that the proposed method could improve the classification rates.\n    ",
        "submission_date": "2011-10-25T00:00:00",
        "last_modified_date": "2011-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5450",
        "title": "Hand Tracking based on Hierarchical Clustering of Range Data",
        "authors": [
            "Roberto Cespi",
            "Andreas Kolb",
            "Marvin Lindner"
        ],
        "abstract": "Fast and robust hand segmentation and tracking is an essential basis for gesture recognition and thus an important component for contact-less human-computer interaction (HCI). Hand gesture recognition based on 2D video data has been intensively investigated. However, in practical scenarios purely intensity based approaches suffer from uncontrollable environmental conditions like cluttered background colors. In this paper we present a real-time hand segmentation and tracking algorithm using Time-of-Flight (ToF) range cameras and intensity data. The intensity and range information is fused into one pixel value, representing its combined intensity-depth homogeneity. The scene is hierarchically clustered using a GPU based parallel merging algorithm, allowing a robust identification of both hands even for inhomogeneous backgrounds. After the detection, both hands are tracked on the CPU. Our tracking algorithm can cope with the situation that one hand is temporarily covered by the other hand.\n    ",
        "submission_date": "2011-10-25T00:00:00",
        "last_modified_date": "2011-10-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5945",
        "title": "A New Similarity Measure for Non-Local Means Filtering of MRI Images",
        "authors": [
            "Sudipto Dolui",
            "Alan Kuurstra",
            "Iv\u00e1n C. Salgado Patarroyo",
            "Oleg V. Michailovich"
        ],
        "abstract": "The acquisition of MRI images offers a trade-off in terms of acquisition time, spatial/temporal resolution and signal-to-noise ratio (SNR). Thus, for instance, increasing the time efficiency of MRI often comes at the expense of reduced SNR. This, in turn, necessitates the use of post-processing tools for noise rejection, which makes image de-noising an indispensable component of computer assistance diagnosis. In the field of MRI, a multitude of image de-noising methods have been proposed hitherto. In this paper, the application of a particular class of de-noising algorithms - known as non-local mean (NLM) filters - is investigated. Such filters have been recently applied for MRI data enhancement and they have been shown to provide more accurate results as compared to many alternative de-noising algorithms. Unfortunately, virtually all existing methods for NLM filtering have been derived under the assumption of additive white Gaussian (AWG) noise contamination. Since this assumption is known to fail at low values of SNR, an alternative formulation of NLM filtering is required, which would take into consideration the correct Rician statistics of MRI noise. Accordingly, the contribution of the present paper is two-fold. First, it points out some principal disadvantages of the earlier methods of NLM filtering of MRI images and suggests means to rectify them. Second, the paper introduces a new similarity measure for NLM filtering of MRI Images, which is derived under bona fide statistical assumptions and results in more accurate reconstruction of MR scans as compared to alternative NLM approaches. Finally, the utility and viability of the proposed method is demonstrated through a series of numerical experiments using both in silico and in vivo MRI data.\n    ",
        "submission_date": "2011-10-26T00:00:00",
        "last_modified_date": "2011-10-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0466",
        "title": "Kernel diff-hash",
        "authors": [
            "Michael M Bronstein"
        ],
        "abstract": "This paper presents a kernel formulation of the recently introduced diff-hash algorithm for the construction of similarity-sensitive hash functions. Our kernel diff-hash algorithm that shows superior performance on the problem of image feature descriptor matching.\n    ",
        "submission_date": "2011-11-02T00:00:00",
        "last_modified_date": "2011-11-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0885",
        "title": "Graph Regularized Nonnegative Matrix Factorization for Hyperspectral Data Unmixing",
        "authors": [
            "Roozbeh Rajabi",
            "Mahdi Khodadadzadeh",
            "Hassan Ghassemian"
        ],
        "abstract": "Spectral unmixing is an important tool in hyperspectral data analysis for estimating endmembers and abundance fractions in a mixed pixel. This paper examines the applicability of a recently developed algorithm called graph regularized nonnegative matrix factorization (GNMF) for this aim. The proposed approach exploits the intrinsic geometrical structure of the data besides considering positivity and full additivity constraints. Simulated data based on the measured spectral signatures, is used for evaluating the proposed algorithm. Results in terms of abundance angle distance (AAD) and spectral angle distance (SAD) show that this method can effectively unmix hyperspectral data.\n    ",
        "submission_date": "2011-11-03T00:00:00",
        "last_modified_date": "2011-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1014",
        "title": "Sparsity and Robustness in Face Recognition",
        "authors": [
            "John Wright",
            "Arvind Ganesh",
            "Allen Yang",
            "Zihan Zhou",
            "Yi Ma"
        ],
        "abstract": "This report concerns the use of techniques for sparse signal representation and sparse error correction for automatic face recognition. Much of the recent interest in these techniques comes from the paper \"Robust Face Recognition via Sparse Representation\" by Wright et al. (2009), which showed how, under certain technical conditions, one could cast the face recognition problem as one of seeking a sparse representation of a given input face image in terms of a \"dictionary\" of training images and images of individual pixels. In this report, we have attempted to clarify some frequently encountered questions about this work and particularly, on the validity of using sparse representation techniques for face recognition.\n    ",
        "submission_date": "2011-11-03T00:00:00",
        "last_modified_date": "2011-11-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1090",
        "title": "A robust, low-cost approach to Face Detection and Face Recognition",
        "authors": [
            "Divya Jyoti",
            "Aman Chadha",
            "Pallavi Vaidya",
            "M. Mani Roja"
        ],
        "abstract": "In the domain of Biometrics, recognition systems based on iris, fingerprint or palm print scans etc. are often considered more dependable due to extremely low variance in the properties of these entities with respect to time. However, over the last decade data processing capability of computers has increased manifold, which has made real-time video content analysis possible. This shows that the need of the hour is a robust and highly automated Face Detection and Recognition algorithm with credible accuracy rate. The proposed Face Detection and Recognition system using Discrete Wavelet Transform (DWT) accepts face frames as input from a database containing images from low cost devices such as VGA cameras, webcams or even CCTV's, where image quality is inferior. Face region is then detected using properties of L*a*b* color space and only Frontal Face is extracted such that all additional background is eliminated. Further, this extracted image is converted to grayscale and its dimensions are resized to 128 x 128 pixels. DWT is then applied to entire image to obtain the coefficients. Recognition is carried out by comparison of the DWT coefficients belonging to the test image with those of the registered reference image. On comparison, Euclidean distance classifier is deployed to validate the test image from the database. Accuracy for various levels of DWT Decomposition is obtained and hence, compared.\n    ",
        "submission_date": "2011-11-04T00:00:00",
        "last_modified_date": "2011-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1093",
        "title": "Securing Biometric Images using Reversible Watermarking",
        "authors": [
            "Sabu M. Thampi",
            "Ann Jisma Jacob"
        ],
        "abstract": "Biometric security is a fast growing area. Protecting biometric data is very important since it can be misused by attackers. In order to increase security of biometric data there are different methods in which watermarking is widely accepted. A more acceptable, new important development in this area is reversible watermarking in which the original image can be completely restored and the watermark can be retrieved. But reversible watermarking in biometrics is an understudied area. Reversible watermarking maintains high quality of biometric data. This paper proposes Rotational Replacement of LSB as a reversible watermarking scheme for biometric images. PSNR is the regular method used for quality measurement of biometric data. In this paper we also show that SSIM Index is a better alternate for effective quality assessment for reversible watermarked biometric data by comparing with the well known reversible watermarking scheme using Difference Expansion.\n    ",
        "submission_date": "2011-11-04T00:00:00",
        "last_modified_date": "2011-11-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1311",
        "title": "Covariant fractional extension of the modified Laplace-operator used in 3D-shape recovery",
        "authors": [
            "Richard Herrmann"
        ],
        "abstract": "Extending the Liouville-Caputo definition of a fractional derivative to a nonlocal covariant generalization of arbitrary bound operators acting on multidimensional Riemannian spaces an appropriate approach for the 3D shape recovery of aperture afflicted 2D slide sequences is proposed. We demonstrate, that the step from a local to a nonlocal algorithm yields an order of magnitude in accuracy and by using the specific fractional approach an additional factor 2 in accuracy of the derived results.\n    ",
        "submission_date": "2011-11-05T00:00:00",
        "last_modified_date": "2011-11-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1423",
        "title": "Face Recognition Using Discrete Cosine Transform for Global and Local Features",
        "authors": [
            "Aman R. Chadha",
            "Pallavi P. Vaidya",
            "M. Mani Roja"
        ],
        "abstract": "Face Recognition using Discrete Cosine Transform (DCT) for Local and Global Features involves recognizing the corresponding face image from the database. The face image obtained from the user is cropped such that only the frontal face image is extracted, eliminating the background. The image is restricted to a size of 128 x 128 pixels. All images in the database are gray level images. DCT is applied to the entire image. This gives DCT coefficients, which are global features. Local features such as eyes, nose and mouth are also extracted and DCT is applied to these features. Depending upon the recognition rate obtained for each feature, they are given weightage and then combined. Both local and global features are used for comparison. By comparing the ranks for global and local features, the false acceptance rate for DCT can be minimized.\n    ",
        "submission_date": "2011-11-06T00:00:00",
        "last_modified_date": "2011-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1461",
        "title": "Multimodal diff-hash",
        "authors": [
            "Michael M. Bronstein"
        ],
        "abstract": "Many applications require comparing multimodal data with different structure and dimensionality that cannot be compared directly. Recently, there has been increasing interest in methods for learning and efficiently representing such multimodal similarity. In this paper, we present a simple algorithm for multimodal similarity-preserving hashing, trying to map multimodal data into the Hamming space while preserving the intra- and inter-modal similarities. We show that our method significantly outperforms the state-of-the-art method in the field.\n    ",
        "submission_date": "2011-11-07T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1562",
        "title": "Iris Recognition Based on LBP and Combined LVQ Classifier",
        "authors": [
            "M. Y. Shams",
            "M. Z. Rashad",
            "O. Nomir",
            "R. M. El-Awady"
        ],
        "abstract": "Iris recognition is considered as one of the best biometric methods used for human identification and verification, this is because of its unique features that differ from one person to another, and its importance in the security field. This paper proposes an algorithm for iris recognition and classification using a system based on Local Binary Pattern and histogram properties as a statistical approaches for feature extraction, and Combined Learning Vector Quantization Classifier as Neural Network approach for classification, in order to build a hybrid model depends on both features. The localization and segmentation techniques are presented using both Canny edge detection and Hough Circular Transform in order to isolate an iris from the whole eye image and for noise detection .Feature vectors results from LBP is applied to a Combined LVQ classifier with different classes to determine the minimum acceptable performance, and the result is based on majority voting among several LVQ classifier. Different iris datasets CASIA, MMU1, MMU2, and LEI with different extensions and size are presented. Since LBP is working on a grayscale level so colored iris images should be transformed into a grayscale level. The proposed system gives a high recognition rate 99.87 % on different iris datasets compared with other methods.\n    ",
        "submission_date": "2011-11-07T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1599",
        "title": "Efficient Hierarchical Markov Random Fields for Object Detection on a Mobile Robot",
        "authors": [
            "Colin S. Lea",
            "Jason J. Corso"
        ],
        "abstract": "Object detection and classification using video is necessary for intelligent planning and navigation on a mobile robot. However, current methods can be too slow or not sufficient for distinguishing multiple classes. Techniques that rely on binary (foreground/background) labels incorrectly identify areas with multiple overlapping objects as single segment. We propose two Hierarchical Markov Random Field models in efforts to distinguish connected objects using tiered, binary label sets. Near-realtime performance has been achieved using efficient optimization methods which runs up to 11 frames per second on a dual core 2.2 Ghz processor. Evaluation of both models is done using footage taken from a robot obstacle course at the 2010 Intelligent Ground Vehicle Competition.\n    ",
        "submission_date": "2011-11-07T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1752",
        "title": "New Method for 3D Shape Retrieval",
        "authors": [
            "Abdelghni Lakehal",
            "Omar El Beqqali"
        ],
        "abstract": "The recent technological progress in acquisition, modeling and processing of 3D data leads to the proliferation of a large number of 3D objects databases. Consequently, the techniques used for content based 3D retrieval has become necessary. In this paper, we introduce a new method for 3D objects recognition and retrieval by using a set of binary images CLI (Characteristic level images). We propose a 3D indexing and search approach based on the similarity between characteristic level images using Hu moments for it indexing. To measure the similarity between 3D objects we compute the Hausdorff distance between a vectors descriptor. The performance of this new approach is evaluated at set of 3D object of well known database, is NTU (National Taiwan University) database.\n    ",
        "submission_date": "2011-11-07T00:00:00",
        "last_modified_date": "2011-11-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1947",
        "title": "Discriminative Local Sparse Representations for Robust Face Recognition",
        "authors": [
            "Yi Chen",
            "Umamahesh Srinivas",
            "Thong T. Do",
            "Vishal Monga",
            "Trac D. Tran"
        ],
        "abstract": "A key recent advance in face recognition models a test face image as a sparse linear combination of a set of training face images. The resulting sparse representations have been shown to possess robustness against a variety of distortions like random pixel corruption, occlusion and disguise. This approach however makes the restrictive (in many scenarios) assumption that test faces must be perfectly aligned (or registered) to the training data prior to classification. In this paper, we propose a simple yet robust local block-based sparsity model, using adaptively-constructed dictionaries from local features in the training data, to overcome this misalignment problem. Our approach is inspired by human perception: we analyze a series of local discriminative features and combine them to arrive at the final classification decision. We propose a probabilistic graphical model framework to explicitly mine the conditional dependencies between these distinct sparse local features. In particular, we learn discriminative graphs on sparse representations obtained from distinct local slices of a face. Conditional correlations between these sparse features are first discovered (in the training phase), and subsequently exploited to bring about significant improvements in recognition rates. Experimental results obtained on benchmark face databases demonstrate the effectiveness of the proposed algorithms in the presence of multiple registration errors (such as translation, rotation, and scaling) as well as under variations of pose and illumination.\n    ",
        "submission_date": "2011-11-08T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.2391",
        "title": "A Novel Approach to Texture classification using statistical feature",
        "authors": [
            "B. Vijayalakshmi",
            "V. Subbiah Bharathi"
        ],
        "abstract": "Texture is an important spatial feature which plays a vital role in content based image retrieval. The enormous growth of the internet and the wide use of digital data have increased the need for both efficient image database creation and retrieval procedure. This paper describes a new approach for texture classification by combining statistical texture features of Local Binary Pattern and Texture spectrum. Since most significant information of a texture often appears in the high frequency channels, the features are extracted by the computation of LBP and Texture Spectrum and Legendre Moments. Euclidean distance is used for similarity measurement. The experimental result shows that 97.77% classification accuracy is obtained by the proposed method.\n    ",
        "submission_date": "2011-11-10T00:00:00",
        "last_modified_date": "2011-11-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3000",
        "title": "Digital Manifolds and the Theorem of Jordan-Brouwer",
        "authors": [
            "Martin H\u00fcnniger"
        ],
        "abstract": "We give an answer to the question given by ",
        "submission_date": "2011-11-13T00:00:00",
        "last_modified_date": "2011-11-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3281",
        "title": "A prototype system for handwritten sub-word recognition: Toward Arabic-manuscript transliteration",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Mohamed Cheriet",
            "Thomas Milo",
            "Robert Wisnovsky"
        ],
        "abstract": "A prototype system for the transliteration of diacritics-less Arabic manuscripts at the sub-word or part of Arabic word (PAW) level is developed. The system is able to read sub-words of the input manuscript using a set of skeleton-based features. A variation of the system is also developed which reads archigraphemic Arabic manuscripts, which are dot-less, into archigraphemes transliteration. In order to reduce the complexity of the original highly multiclass problem of sub-word recognition, it is redefined into a set of binary descriptor classifiers. The outputs of trained binary classifiers are combined to generate the sequence of sub-word letters. SVMs are used to learn the binary classifiers. Two specific Arabic databases have been developed to train and test the system. One of them is a database of the Naskh style. The initial results are promising. The systems could be trained on other scripts found in Arabic manuscripts.\n    ",
        "submission_date": "2011-11-14T00:00:00",
        "last_modified_date": "2011-11-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3818",
        "title": "Good Pairs of Adjacency Relations in Arbitrary Dimensions",
        "authors": [
            "Martin H\u00fcnniger"
        ],
        "abstract": "In this text we show, that the notion of a \"good pair\" that was introduced in the paper \"Digital Manifolds and the Theorem of Jordan-Brouwer\" has actually known models. We will show, how to choose cubical adjacencies, the generalizations of the well known 4- and 8-neighborhood to arbitrary dimensions, in order to find good pairs. Furthermore, we give another proof for the well known fact that the Khalimsky-topology implies good pairs. The outcome is consistent with the known theory as presented by T.Y. Kong, A. Rosenfeld, G.T. Herman and M. Khachan ",
        "submission_date": "2011-11-16T00:00:00",
        "last_modified_date": "2011-11-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.3969",
        "title": "The Object Projection Feature Estimation Problem in Unsupervised Markerless 3D Motion Tracking",
        "authors": [
            "Luis Quesada",
            "Alejandro J. Le\u00f3n"
        ],
        "abstract": "3D motion tracking is a critical task in many computer vision applications. Existing 3D motion tracking techniques require either a great amount of knowledge on the target object or specific hardware. These requirements discourage the wide spread of commercial applications based on 3D motion tracking. 3D motion tracking systems that require no knowledge on the target object and run on a single low-budget camera require estimations of the object projection features (namely, area and position). In this paper, we define the object projection feature estimation problem and we present a novel 3D motion tracking system that needs no knowledge on the target object and that only requires a single low-budget camera, as installed in most computers and smartphones. Our system estimates, in real time, the three-dimensional position of a non-modeled unmarked object that may be non-rigid, non-convex, partially occluded, self occluded, or motion blurred, given that it is opaque, evenly colored, and enough contrasting with the background in each frame. Our system is also able to determine the most relevant object to track in the screen. Our 3D motion tracking system does not impose hard constraints, therefore it allows a market-wide implementation of applications that use 3D motion tracking.\n    ",
        "submission_date": "2011-11-16T00:00:00",
        "last_modified_date": "2011-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4052",
        "title": "A Facial Expression Classification System Integrating Canny, Principal Component Analysis and Artificial Neural Network",
        "authors": [
            "Le Hoang Thai",
            "Nguyen Do Thai Nguyen",
            "Tran Son Hai"
        ],
        "abstract": "Facial Expression Classification is an interesting research problem in recent years. There are a lot of methods to solve this problem. In this research, we propose a novel approach using Canny, Principal Component Analysis (PCA) and Artificial Neural Network. Firstly, in preprocessing phase, we use Canny for local region detection of facial images. Then each of local region's features will be presented based on Principal Component Analysis (PCA). Finally, using Artificial Neural Network (ANN)applies for Facial Expression Classification. We apply our proposal method (Canny_PCA_ANN) for recognition of six basic facial expressions on JAFFE database consisting 213 images posed by 10 Japanese female models. The experimental result shows the feasibility of our proposal method.\n    ",
        "submission_date": "2011-11-17T00:00:00",
        "last_modified_date": "2011-11-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4290",
        "title": "A Single Euler Number Feature for Multi-font Multi-size Kannada Numeral Recognition",
        "authors": [
            "B.V.Dhandra",
            "R.G.Benne",
            "Mallikarjun Hangarge"
        ],
        "abstract": "In this paper a novel approach is proposed based on single Euler number feature which is free from thinning and size normalization for multi-font and multi-size Kannada numeral recognition system. A nearest neighbor classification is used for classification of Kannada numerals by considering the Euclidian distance. A total 1500 numeral images with different font sizes between (10..84) are tested for algorithm efficiency and the overall the classification accuracy is found to be 99.00% .The said method is thinning free, fast, and showed encouraging results on varying font styles and sizes of Kannada numerals.\n    ",
        "submission_date": "2011-11-18T00:00:00",
        "last_modified_date": "2011-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4291",
        "title": "Multi-font Multi-size Kannada Numeral Recognition Based on Structural Features",
        "authors": [
            "B.V.Dhandra",
            "R.G.Benne",
            "Mallikarjun Hangarge"
        ],
        "abstract": "In this paper a fast and novel method is proposed for multi-font multi-size Kannada numeral recognition which is thinning free and without size normalization approach. The different structural feature are used for numeral recognition namely, directional density of pixels in four directions, water reservoirs, maximum profile distances, and fill hole density are used for the recognition of Kannada numerals. A Euclidian minimum distance criterion is used to find minimum distances and K-nearest neighbor classifier is used to classify the Kannada numerals by varying the size of numeral image from 16 to 50 font sizes for the 20 different font styles from NUDI and BARAHA popular word processing Kannada software. The total 1150 numeral images are tested and the overall accuracy of classification is found to be 100%. The average time taken by this method is 0.1476 seconds.\n    ",
        "submission_date": "2011-11-18T00:00:00",
        "last_modified_date": "2011-11-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4619",
        "title": "Redundant Wavelets on Graphs and High Dimensional Data Clouds",
        "authors": [
            "Idan Ram",
            "Michael Elad",
            "Israel Cohen"
        ],
        "abstract": "In this paper, we propose a new redundant wavelet transform applicable to scalar functions defined on high dimensional coordinates, weighted graphs and networks. The proposed transform utilizes the distances between the given data points. We modify the filter-bank decomposition scheme of the redundant wavelet transform by adding in each decomposition level linear operators that reorder the approximation coefficients. These reordering operators are derived by organizing the tree-node features so as to shorten the path that passes through these points. We explore the use of the proposed transform to image denoising, and show that it achieves denoising results that are close to those obtained with the BM3D algorithm.\n    ",
        "submission_date": "2011-11-20T00:00:00",
        "last_modified_date": "2011-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4654",
        "title": "A self-portrait of young Leonardo",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "One of the most famous drawings by Leonardo da Vinci is a self-portrait in red chalk, where he looks quite old. In fact, there is a sketch in one of his notebooks, partially covered by written notes, that can be a self-portrait of the artist when he was young. The use of image processing, to remove the handwritten text and improve the image, allows a comparison of the two portraits.\n    ",
        "submission_date": "2011-11-20T00:00:00",
        "last_modified_date": "2011-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4676",
        "title": "Facial Asymmetry and Emotional Expression",
        "authors": [
            "Andrew Pickin"
        ],
        "abstract": "This report is about facial asymmetry, its connection to emotional expression, and methods of measuring facial asymmetry in videos of faces. The research was motivated by two factors: firstly, there was a real opportunity to develop a novel measure of asymmetry that required minimal human involvement and that improved on earlier measures in the literature; and secondly, the study of the relationship between facial asymmetry and emotional expression is both interesting in its own right, and important because it can inform neuropsychological theory and answer open questions concerning emotional processing in the brain. The two aims of the research were: first, to develop an automatic frame-by-frame measure of facial asymmetry in videos of faces that improved on previous measures; and second, to use the measure to analyse the relationship between facial asymmetry and emotional expression, and connect our findings with previous research of the relationship.\n    ",
        "submission_date": "2011-11-20T00:00:00",
        "last_modified_date": "2011-11-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4800",
        "title": "Enhancement of Image Resolution by Binarization",
        "authors": [
            "Aroop Mukherjee",
            "Soumen Kanrar"
        ],
        "abstract": "Image segmentation is one of the principal approaches of image processing. The choice of the most appropriate Binarization algorithm for each case proved to be a very interesting procedure itself. In this paper, we have done the comparison study between the various algorithms based on Binarization algorithms and propose a methodologies for the validation of Binarization algorithms. In this work we have developed two novel algorithms to determine threshold values for the pixels value of the gray scale image. The performance estimation of the algorithm utilizes test images with, the evaluation metrics for Binarization of textual and synthetic images. We have achieved better resolution of the image by using the Binarization method of optimum thresholding techniques.\n    ",
        "submission_date": "2011-11-21T00:00:00",
        "last_modified_date": "2011-11-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.4840",
        "title": "Distributed Multi-view Matching in Networks with Limited Communications",
        "authors": [
            "Eduardo Montijano",
            "Rosario Aragues",
            "Carlos Sagues"
        ],
        "abstract": "We address the problem of distributed matching of features in networks with vision systems. Every camera in the network has limited communication capabilities and can only exchange local matches with its neighbors. We propose a distributed algorithm that takes these local matches and computes global correspondences by a proper propagation in the network. When the algorithm finishes, each camera knows the global correspondences between its features and the features of all the cameras in the network. The presence of spurious introduced by the local matcher may produce inconsistent global correspondences, which are association paths between features from the same camera. The contributions of this work are the propagation of the local matches and the detection and resolution of these inconsistencies by deleting local matches. Our resolution algorithm considers the quality of each local match, when this information is provided by the local matcher. We formally prove that after executing the algorithm, the network finishes with a global data association free of inconsistencies. We provide a fully decentralized solution to the problem which does not rely on any particular communication topology. Simulations and experimental results with real images show the performance of the method considering different features, matching functions and scenarios.\n    ",
        "submission_date": "2011-11-21T00:00:00",
        "last_modified_date": "2012-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5108",
        "title": "A Theory for Optical flow-based Transport on Image Manifolds",
        "authors": [
            "Sriram Nagaraj",
            "Aswin C. Sankaranarayanan",
            "Richard G. Baraniuk"
        ],
        "abstract": "An image articulation manifold (IAM) is the collection of images formed when an object is articulated in front of a camera. IAMs arise in a variety of image processing and computer vision applications, where they provide a natural low-dimensional embedding of the collection of high-dimensional images. To date IAMs have been studied as embedded submanifolds of Euclidean spaces. Unfortunately, their promise has not been realized in practice, because real world imagery typically contains sharp edges that render an IAM non-differentiable and hence non-isometric to the low-dimensional parameter space under the Euclidean metric. As a result, the standard tools from differential geometry, in particular using linear tangent spaces to transport along the IAM, have limited utility. In this paper, we explore a nonlinear transport operator for IAMs based on the optical flow between images and develop new analytical tools reminiscent of those from differential geometry using the idea of optical flow manifolds (OFMs). We define a new metric for IAMs that satisfies certain local isometry conditions, and we show how to use this metric to develop a new tools such as flow fields on IAMs, parallel flow fields, parallel transport, as well as a intuitive notion of curvature. The space of optical flow fields along a path of constant curvature has a natural multi-scale structure via a monoid structure on the space of all flow fields along a path. We also develop lower bounds on approximation errors while approximating non-parallel flow fields by parallel flow fields.\n    ",
        "submission_date": "2011-11-22T00:00:00",
        "last_modified_date": "2011-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5135",
        "title": "A New IRIS Normalization Process For Recognition System With Cryptographic Techniques",
        "authors": [
            "S. Nithyanandam",
            "K. S. Gayathri",
            "P. L. K. Priyadarshini"
        ],
        "abstract": "Biometric technologies are the foundation of personal identification systems. It provides an identification based on a unique feature possessed by the individual. This paper provides a walkthrough for image acquisition, segmentation, normalization, feature extraction and matching based on the Human Iris imaging. A Canny Edge Detection scheme and a Circular Hough Transform, is used to detect the iris boundaries in the eye's digital image. The extracted IRIS region was normalized by using Image Registration technique. A phase correlation base method is used for this iris image registration purpose. The features of the iris region is encoded by convolving the normalized iris region with 2D Gabor filter. Hamming distance measurement is used to compare the quantized vectors and authenticate the users. To improve the security, Reed-Solomon technique is employed directly to encrypt and decrypt the data. Experimental results show that our system is quite effective and provides encouraging performance. Keywords: Biometric, Iris Recognition, Phase correlation, cryptography, Reed-Solomon\n    ",
        "submission_date": "2011-11-22T00:00:00",
        "last_modified_date": "2011-11-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5612",
        "title": "Distributed Representation of Geometrically Correlated Images with Compressed Linear Measurements",
        "authors": [
            "Vijayaraghavan Thirumalai",
            "Pascal Frossard"
        ],
        "abstract": "This paper addresses the problem of distributed coding of images whose correlation is driven by the motion of objects or positioning of the vision sensors. It concentrates on the problem where images are encoded with compressed linear measurements. We propose a geometry-based correlation model in order to describe the common information in pairs of images. We assume that the constitutive components of natural images can be captured by visual features that undergo local transformations (e.g., translation) in different images. We first identify prominent visual features by computing a sparse approximation of a reference image with a dictionary of geometric basis functions. We then pose a regularized optimization problem to estimate the corresponding features in correlated images given by quantized linear measurements. The estimated features have to comply with the compressed information and to represent consistent transformation between images. The correlation model is given by the relative geometric transformations between corresponding features. We then propose an efficient joint decoding algorithm that estimates the compressed images such that they stay consistent with both the quantized measurements and the correlation model. Experimental results show that the proposed algorithm effectively estimates the correlation between images in multi-view datasets. In addition, the proposed algorithm provides effective decoding performance that compares advantageously to independent coding solutions as well as state-of-the-art distributed coding schemes based on disparity learning.\n    ",
        "submission_date": "2011-11-23T00:00:00",
        "last_modified_date": "2011-11-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6030",
        "title": "An image processing of a Raphael's portrait of Leonardo",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "abstract": "In one of his paintings, the School of Athens, Raphael is depicting Leonardo da Vinci as the philosopher Plato. Some image processing tools can help us in comparing this portrait with two Leonardo's portraits, considered as self-portraits.\n    ",
        "submission_date": "2011-11-25T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6276",
        "title": "Compressed sensing of astronomical images:orthogonal wavelets domains",
        "authors": [
            "Vasil Kolev"
        ],
        "abstract": "A simple approach for orthogonal wavelets in compressed sensing (CS) applications is presented. We compare efficient algorithm for different orthogonal wavelet measurement matrices in CS for image processing from scanned photographic plates (SPP). Some important characteristics were obtained for astronomical image processing of SPP. The best orthogonal wavelet choice for measurement matrix construction in CS for image compression of images of SPP is given. The image quality measure for linear and nonlinear image compression method is defined.\n    ",
        "submission_date": "2011-11-27T00:00:00",
        "last_modified_date": "2021-08-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.7271",
        "title": "Invariant texture analysis through Local Binary Patterns",
        "authors": [
            "Rodrigo Nava",
            "Gabriel Crist\u00f3bal",
            "Boris Escalante-Ram\u00edrez"
        ],
        "abstract": "In many image processing applications, such as segmentation and classification, the selection of robust features descriptors is crucial to improve the discrimination capabilities in real world scenarios. In particular, it is well known that image textures constitute power visual cues for feature extraction and classification. In the past few years the local binary pattern (LBP) approach, a texture descriptor method proposed by Ojala et al., has gained increased acceptance due to its computational simplicity and more importantly for encoding a powerful signature for describing textures. However, the original algorithm presents some limitations such as noise sensitivity and its lack of rotational invariance which have led to many proposals or extensions in order to overcome such limitations. In this paper we performed a quantitative study of the Ojala's original LBP proposal together with other recently proposed LBP extensions in the presence of rotational, illumination and noisy changes. In the experiments we have considered two different databases: Brodatz and CUReT for different sizes of LBP masks. Experimental results demonstrated the effectiveness and robustness of the described texture descriptors for images that are subjected to geometric or radiometric changes.\n    ",
        "submission_date": "2011-11-30T00:00:00",
        "last_modified_date": "2011-11-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0059",
        "title": "Local Naive Bayes Nearest Neighbor for Image Classification",
        "authors": [
            "Sancho McCann",
            "David G. Lowe"
        ],
        "abstract": "We present Local Naive Bayes Nearest Neighbor, an improvement to the NBNN image classification algorithm that increases classification accuracy and improves its ability to scale to large numbers of object classes. The key observation is that only the classes represented in the local neighborhood of a descriptor contribute significantly and reliably to their posterior probability estimates. Instead of maintaining a separate search structure for each class, we merge all of the reference data together into one search structure, allowing quick identification of a descriptor's local neighborhood. We show an increase in classification accuracy when we ignore adjustments to the more distant classes and show that the run time grows with the log of the number of classes rather than linearly in the number of classes as did the original. This gives a 100 times speed-up over the original method on the Caltech 256 dataset. We also provide the first head-to-head comparison of NBNN against spatial pyramid methods using a common set of input features. We show that local NBNN outperforms all previous NBNN based methods and the original spatial pyramid model. However, we find that local NBNN, while competitive with, does not beat state-of-the-art spatial pyramid methods that use local soft assignment and max-pooling.\n    ",
        "submission_date": "2011-12-01T00:00:00",
        "last_modified_date": "2011-12-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0655",
        "title": "A Biomimetic Model of the Outer Plexiform Layer by Incorporating Memristive Devices",
        "authors": [
            "Andras Gelencser",
            "Themistoklis Prodromakis",
            "Christofer Toumazou",
            "Tamas Roska"
        ],
        "abstract": "In this paper we present a biorealistic model for the first part of the early vision processing by incorporating memristive nanodevices. The architecture of the proposed network is based on the organisation and functioning of the outer plexiform layer (OPL) in the vertebrate retina. We demonstrate that memristive devices are indeed a valuable building block for neuromorphic architectures, as their highly non-linear and adaptive response could be exploited for establishing ultra-dense networks with similar dynamics to their biological counterparts. We particularly show that hexagonal memristive grids can be employed for faithfully emulating the smoothing-effect occurring at the OPL for enhancing the dynamic range of the system. In addition, we employ a memristor-based thresholding scheme for detecting the edges of grayscale images, while the proposed system is also evaluated for its adaptation and fault tolerance capacity against different light or noise conditions as well as distinct device yields.\n    ",
        "submission_date": "2011-12-03T00:00:00",
        "last_modified_date": "2011-12-03T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.0974",
        "title": "Optimality Bounds for a Variational Relaxation of the Image Partitioning Problem",
        "authors": [
            "Jan Lellmann",
            "Frank Lenzen",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "We consider a variational convex relaxation of a class of optimal partitioning and multiclass labeling problems, which has recently proven quite successful and can be seen as a continuous analogue of Linear Programming (LP) relaxation methods for finite-dimensional problems. While for the latter case several optimality bounds are known, to our knowledge no such bounds exist in the continuous setting. We provide such a bound by analyzing a probabilistic rounding method, showing that it is possible to obtain an integral solution of the original partitioning problem from a solution of the relaxed problem with an a priori upper bound on the objective, ensuring the quality of the result from the viewpoint of optimization. The approach has a natural interpretation as an approximate, multiclass variant of the celebrated coarea formula.\n    ",
        "submission_date": "2011-12-05T00:00:00",
        "last_modified_date": "2011-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1120",
        "title": "Classification with Invariant Scattering Representations",
        "authors": [
            "Joan Bruna",
            "St\u00e9phane Mallat"
        ],
        "abstract": "A scattering transform defines a signal representation which is invariant to translations and Lipschitz continuous relatively to deformations. It is implemented with a non-linear convolution network that iterates over wavelet and modulus operators. Lipschitz continuity locally linearizes deformations. Complex classes of signals and textures can be modeled with low-dimensional affine spaces, computed with a PCA in the scattering domain. Classification is performed with a penalized model selection. State of the art results are obtained for handwritten digit recognition over small training sets, and for texture classification.\n    ",
        "submission_date": "2011-12-05T00:00:00",
        "last_modified_date": "2011-12-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1187",
        "title": "Meaningful Matches in Stereovision",
        "authors": [
            "Neus Sabater",
            "Andr\u00e9s Almansa",
            "Jean-Michel Morel"
        ],
        "abstract": "This paper introduces a statistical method to decide whether two blocks in a pair of of images match reliably. The method ensures that the selected block matches are unlikely to have occurred \"just by chance.\" The new approach is based on the definition of a simple but faithful statistical \"background model\" for image blocks learned from the image itself. A theorem guarantees that under this model not more than a fixed number of wrong matches occurs (on average) for the whole image. This fixed number (the number of false alarms) is the only method parameter. Furthermore, the number of false alarms associated with each match measures its reliability. This \"a contrario\" block-matching method, however, cannot rule out false matches due to the presence of periodic objects in the images. But it is successfully complemented by a parameterless \"self-similarity threshold.\" Experimental evidence shows that the proposed method also detects occlusions and incoherent motions due to vehicles and pedestrians in non simultaneous stereo.\n    ",
        "submission_date": "2011-12-06T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1200",
        "title": "A multi-feature tracking algorithm enabling adaptation to context variations",
        "authors": [
            "Duc Phu Chau",
            "Fran\u00e7ois Bremond",
            "Monique Thonnat"
        ],
        "abstract": "We propose in this paper a tracking algorithm which is able to adapt itself to different scene contexts. A feature pool is used to compute the matching score between two detected objects. This feature pool includes 2D, 3D displacement distances, 2D sizes, color histogram, histogram of oriented gradient (HOG), color covariance and dominant color. An offline learning process is proposed to search for useful features and to estimate their weights for each context. In the online tracking process, a temporal window is defined to establish the links between the detected objects. This enables to find the object trajectories even if the objects are misdetected in some frames. A trajectory filter is proposed to remove noisy trajectories. Experimentation on different contexts is shown. The proposed tracker has been tested in videos belonging to three public datasets and to the Caretaker European project. The experimental results prove the effect of the proposed feature weight learning, and the robustness of the proposed tracker compared to some methods in the state of the art. The contributions of our approach over the state of the art trackers are: (i) a robust tracking algorithm based on a feature pool, (ii) a supervised learning scheme to learn feature weights for each context, (iii) a new method to quantify the reliability of HOG descriptor, (iv) a combination of color covariance and dominant color features with spatial pyramid distance to manage the case of object occlusion.\n    ",
        "submission_date": "2011-12-06T00:00:00",
        "last_modified_date": "2011-12-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1484",
        "title": "POCS Based Super-Resolution Image Reconstruction Using an Adaptive Regularization Parameter",
        "authors": [
            "S.S. Panda",
            "M.S.R.S Prasad",
            "G. Jena"
        ],
        "abstract": "Crucial information barely visible to the human eye is often embedded in a series of low-resolution images taken of the same scene. Super-resolution enables the extraction of this information by reconstructing a single image, at a high resolution than is present in any of the individual images. This is particularly useful in forensic imaging, where the extraction of minute details in an image can help to solve a crime. Super-resolution image restoration has been one of the most important research areas in recent years which goals to obtain a high resolution (HR) image from several low resolutions (LR) blurred, noisy, under sampled and displaced images. Relation of the HR image and LR images can be modeled by a linear system using a transformation matrix and additive noise. However, a unique solution may not be available because of the singularity of transformation matrix. To overcome this problem, POCS method has been used. However, their performance is not good because the effect of noise energy has been ignored. In this paper, we propose an adaptive regularization approach based on the fact that the regularization parameter should be a linear function of noise variance. The performance of the proposed approach has been tested on several images and the obtained results demonstrate the superiority of our approach compared with existing methods.\n    ",
        "submission_date": "2011-12-07T00:00:00",
        "last_modified_date": "2011-12-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.1496",
        "title": "Re-initialization Free Level Set Evolution via Reaction Diffusion",
        "authors": [
            "Kaihua Zhang",
            "Lei Zhang",
            "Huihui Song",
            "David Zhang"
        ],
        "abstract": "This paper presents a novel reaction-diffusion (RD) method for implicit active contours, which is completely free of the costly re-initialization procedure in level set evolution (LSE). A diffusion term is introduced into LSE, resulting in a RD-LSE equation, to which a piecewise constant solution can be derived. In order to have a stable numerical solution of the RD based LSE, we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSE equation: first iterating the LSE equation, and then solving the diffusion equation. The second step regularizes the level set function obtained in the first step to ensure stability, and thus the complex and costly re-initialization procedure is completely eliminated from LSE. By successfully applying diffusion to LSE, the RD-LSE model is stable by means of the simple finite difference method, which is very easy to implement. The proposed RD method can be generalized to solve the LSE for both variational level set method and PDE-based level set method. The RD-LSE method shows very good performance on boundary anti-leakage, and it can be readily extended to high dimensional level set method. The extensive and promising experimental results on synthetic and real images validate the effectiveness of the proposed RD-LSE approach.\n    ",
        "submission_date": "2011-12-07T00:00:00",
        "last_modified_date": "2012-08-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2386",
        "title": "Improvement of BM3D Algorithm and Employment to Satellite and CFA Images Denoising",
        "authors": [
            "'Omid Pakdelazar'",
            "'Gholamali Rezai-rad'"
        ],
        "abstract": "This paper proposes a new procedure in order to improve the performance of block matching and 3-D filtering (BM3D) image denoising algorithm. It is demonstrated that it is possible to achieve a better performance than that of BM3D algorithm in a variety of noise levels. This method changes BM3D algorithm parameter values according to noise level, removes prefiltering, which is used in high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual quality get improved, and BM3D complexities and processing time are reduced. This improved BM3D algorithm is extended and used to denoise satellite and color filter array (CFA) images. Output results show that the performance has upgraded in comparison with current methods of denoising satellite and CFA images. In this regard this algorithm is compared with Adaptive PCA algorithm, that has led to superior performance for denoising CFA images, on the subject of PSNR and visual quality. Also the processing time has decreased significantly.\n    ",
        "submission_date": "2011-12-11T00:00:00",
        "last_modified_date": "2011-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2903",
        "title": "Large Scale Correlation Clustering Optimization",
        "authors": [
            "Shai Bagon",
            "Meirav Galun"
        ],
        "abstract": "Clustering is a fundamental task in unsupervised learning. The focus of this paper is the Correlation Clustering functional which combines positive and negative affinities between the data points. The contribution of this paper is two fold: (i) Provide a theoretic analysis of the functional. (ii) New optimization algorithms which can cope with large scale problems (>100K variables) that are infeasible using existing methods. Our theoretic analysis provides a probabilistic generative interpretation for the functional, and justifies its intrinsic \"model-selection\" capability. Furthermore, we draw an analogy between optimizing this functional and the well known Potts energy minimization. This analogy allows us to suggest several new optimization algorithms, which exploit the intrinsic \"model-selection\" capability of the functional to automatically recover the underlying number of clusters. We compare our algorithms to existing methods on both synthetic and real data. In addition we suggest two new applications that are made possible by our algorithms: unsupervised face identification and interactive multi-object segmentation by rough boundary delineation.\n    ",
        "submission_date": "2011-12-13T00:00:00",
        "last_modified_date": "2011-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2988",
        "title": "Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns",
        "authors": [
            "Tsvi Achler"
        ],
        "abstract": "Matching animal-like flexibility in recognition and the ability to quickly incorporate new information remains difficult. Limits are yet to be adequately addressed in neural models and recognition algorithms. This work proposes a configuration for recognition that maintains the same function of conventional algorithms but avoids combinatorial problems. Feedforward recognition algorithms such as classical artificial neural networks and machine learning algorithms are known to be subject to catastrophic interference and forgetting. Modifying or learning new information (associations between patterns and labels) causes loss of previously learned information. I demonstrate using mathematical analysis how supervised generative models, with feedforward and feedback connections, can emulate feedforward algorithms yet avoid catastrophic interference and forgetting. Learned information in generative models is stored in a more intuitive form that represents the fixed points or solutions of the network and moreover displays similar difficulties as cognitive phenomena. Brain-like capabilities and limits associated with generative models suggest the brain may perform recognition and store information using a similar approach. Because of the central role of recognition, progress understanding the underlying principles may reveal significant insight on how to better study and integrate with the brain.\n    ",
        "submission_date": "2011-12-13T00:00:00",
        "last_modified_date": "2012-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3010",
        "title": "A new variational principle for the Euclidean distance function: Linear approach to the non-linear eikonal problem",
        "authors": [
            "Karthik S. Gurumoorthy",
            "Anand Rangarajan"
        ],
        "abstract": "We present a fast convolution-based technique for computing an approximate, signed Euclidean distance function $S$ on a set of 2D and 3D grid locations. Instead of solving the non-linear, static Hamilton-Jacobi equation ($\\|\\nabla S\\|=1$), our solution stems from first solving for a scalar field $\\phi$ in a linear differential equation and then deriving the solution for $S$ by taking the negative logarithm. In other words, when $S$ and $\\phi$ are related by $\\phi = \\exp \\left(-\\frac{S}{\\tau} \\right)$ and $\\phi$ satisfies a specific linear differential equation corresponding to the extremum of a variational problem, we obtain the approximate Euclidean distance function $S = -\\tau \\log(\\phi)$ which converges to the true solution in the limit as $\\tau \\rightarrow 0$. This is in sharp contrast to techniques like the fast marching and fast sweeping methods which directly solve the Hamilton-Jacobi equation by the Godunov upwind discretization scheme. Our linear formulation results in a closed-form solution to the approximate Euclidean distance function expressible as a discrete convolution, and hence efficiently computable using the fast Fourier transform (FFT). Our solution also circumvents the need for spatial discretization of the derivative operator. As $\\tau\\rightarrow0$ we show the convergence of our results to the true solution and also bound the error for a given value of $\\tau$. The differentiability of our solution allows us to compute---using a set of convolutions---the first and second derivatives of the approximate distance function. In order to determine the sign of the distance function (defined to be positive inside a closed region and negative outside), we compute the winding number in 2D and the topological degree in 3D, whose computations can also be performed via fast convolutions. We demonstrate the efficacy of our method through a set of experimental results.\n    ",
        "submission_date": "2011-12-13T00:00:00",
        "last_modified_date": "2015-02-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3166",
        "title": "Higher-Order Momentum Distributions and Locally Affine LDDMM Registration",
        "authors": [
            "Stefan Sommer",
            "Mads Nielsen",
            "Sune Darkner",
            "Xavier Pennec"
        ],
        "abstract": "To achieve sparse parametrizations that allows intuitive analysis, we aim to represent deformation with a basis containing interpretable elements, and we wish to use elements that have the description capacity to represent the deformation compactly. To accomplish this, we introduce in this paper higher-order momentum distributions in the LDDMM registration framework. While the zeroth order moments previously used in LDDMM only describe local displacement, the first-order momenta that are proposed here represent a basis that allows local description of affine transformations and subsequent compact description of non-translational movement in a globally non-rigid deformation. The resulting representation contains directly interpretable information from both mathematical and modeling perspectives. We develop the mathematical construction of the registration framework with higher-order momenta, we show the implications for sparse image registration and deformation description, and we provide examples of how the parametrization enables registration with a very low number of parameters. The capacity and interpretability of the parametrization using higher-order momenta lead to natural modeling of articulated movement, and the method promises to be useful for quantifying ventricle expansion and progressing atrophy during Alzheimer's disease.\n    ",
        "submission_date": "2011-12-14T00:00:00",
        "last_modified_date": "2012-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3173",
        "title": "Automatic post-picking improves particle image detection from Cryo-EM micrographs",
        "authors": [
            "Ramin Norousi",
            "Stephan Wickles",
            "Thomas Becker",
            "Roland Beckmann",
            "Volker J. Schmid",
            "Achim Tresch"
        ],
        "abstract": "Cryo-electron microscopy (cryo-EM) studies using single particle reconstruction is extensively used to reveal structural information of macromolecular complexes. Aiming at the highest achievable resolution, state of the art electron microscopes acquire thousands of high-quality images. Having collected these data, each single particle must be detected and windowed out. Several fully- or semi-automated approaches have been developed for the selection of particle images from digitized micrographs. However they still require laborious manual post processing, which will become the major bottleneck for next generation of electron microscopes. Instead of focusing on improvements in automated particle selection from micrographs, we propose a post-picking step for classifying small windowed images, which are output by common picking software. A supervised strategy for the classification of windowed micrograph images into particles and non-particles reduces the manual workload by orders of magnitude. The method builds on new powerful image features, and the proper training of an ensemble classifier. A few hundred training samples are enough to achieve a human-like classification performance.\n    ",
        "submission_date": "2011-12-14T00:00:00",
        "last_modified_date": "2012-01-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3697",
        "title": "Insights from Classifying Visual Concepts with Multiple Kernel Learning",
        "authors": [
            "Alexander Binder",
            "Shinichi Nakajima",
            "Marius Kloft",
            "Christina M\u00fcller",
            "Wojciech Samek",
            "Ulf Brefeld",
            "Klaus-Robert M\u00fcller",
            "Motoaki Kawanabe"
        ],
        "abstract": "Combining information from various image features has become a standard technique in concept recognition tasks. However, the optimal way of fusing the resulting kernel functions is usually unknown in practical applications. Multiple kernel learning (MKL) techniques allow to determine an optimal linear combination of such similarity matrices. Classical approaches to MKL promote sparse mixtures. Unfortunately, so-called 1-norm MKL variants are often observed to be outperformed by an unweighted sum kernel. The contribution of this paper is twofold: We apply a recently developed non-sparse MKL variant to state-of-the-art concept recognition tasks within computer vision. We provide insights on benefits and limits of non-sparse MKL and compare it against its direct competitors, the sum kernel SVM and the sparse MKL. We report empirical results for the PASCAL VOC 2009 Classification and ImageCLEF2010 Photo Annotation challenge data sets. About to be submitted to PLoS ONE.\n    ",
        "submission_date": "2011-12-16T00:00:00",
        "last_modified_date": "2011-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4060",
        "title": "A real time vehicles detection algorithm for vision based sensors",
        "authors": [
            "Bart\u0142omiej P\u0142aczek"
        ],
        "abstract": "A vehicle detection plays an important role in the traffic control at signalised intersections. This paper introduces a vision-based algorithm for vehicles presence recognition in detection zones. The algorithm uses linguistic variables to evaluate local attributes of an input image. The image attributes are categorised as vehicle, background or unknown features. Experimental results on complex traffic scenes show that the proposed algorithm is effective for a real-time vehicles detection.\n    ",
        "submission_date": "2011-12-17T00:00:00",
        "last_modified_date": "2011-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4064",
        "title": "Vehicles Recognition Using Fuzzy Descriptors of Image Segments",
        "authors": [
            "Bart\u0142omiej P\u0142aczek"
        ],
        "abstract": "In this paper a vision-based vehicles recognition method is presented. Proposed method uses fuzzy description of image segments for automatic recognition of vehicles recorded in image data. The description takes into account selected geometrical properties and shape coefficients determined for segments of reference image (vehicle model). The proposed method was implemented using reasoning system with fuzzy rules. A vehicles recognition algorithm was developed based on the fuzzy rules describing shape and arrangement of the image segments that correspond to visible parts of a vehicle. An extension of the algorithm with set of fuzzy rules defined for different reference images (and various vehicle shapes) enables vehicles classification in traffic scenes. The devised method is suitable for application in video sensors for road traffic control and surveillance systems.\n    ",
        "submission_date": "2011-12-17T00:00:00",
        "last_modified_date": "2011-12-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4135",
        "title": "A Reduced Reference Image Quality Measure Using Bessel K Forms Model for Tetrolet Coefficients",
        "authors": [
            "Abdelkaher Ait Abdelouahad",
            "Mohammed El Hassouni",
            "Hocine Cherifi",
            "Driss Aboutajdine"
        ],
        "abstract": "In this paper, we introduce a Reduced Reference Image Quality Assessment (RRIQA) measure based on the natural image statistic approach. A new adaptive transform called \"Tetrolet\" is applied to both reference and distorted images. To model the marginal distribution of tetrolet coefficients Bessel K Forms (BKF) density is proposed. Estimating the parameters of this distribution allows to summarize the reference image with a small amount of side information. Five distortion measures based on the BKF parameters of the original and processed image are used to predict quality scores. A comparison between these measures is presented showing a good consistency with human judgment.\n    ",
        "submission_date": "2011-12-18T00:00:00",
        "last_modified_date": "2011-12-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4164",
        "title": "A Geometric Approach For Fully Automatic Chromosome Segmentation",
        "authors": [
            "Shervin Minaee",
            "Mehran Fotouhi",
            "Babak Hossein Khalaj"
        ],
        "abstract": "A fundamental task in human chromosome analysis is chromosome segmentation. Segmentation plays an important role in chromosome karyotyping. The first step in segmentation is to remove intrusive objects such as stain debris and other noises. The next step is detection of touching and overlapping chromosomes, and the final step is separation of such chromosomes. Common methods for separation between touching chromosomes are interactive and require human intervention for correct separation between touching and overlapping chromosomes. In this paper, a geometric-based method is used for automatic detection of touching and overlapping chromosomes and separating them. The proposed scheme performs segmentation in two phases. In the first phase, chromosome clusters are detected using three geometric criteria, and in the second phase, chromosome clusters are separated using a cut-line. Most of earlier methods did not work properly in case of chromosome clusters that contained more than two chromosomes. Our method, on the other hand, is quite efficient in separation of such chromosome clusters. At each step, one separation will be performed and this algorithm is repeated until all individual chromosomes are separated. Another important point about the proposed method is that it uses the geometric features of chromosomes which are independent of the type of images and it can easily be applied to any type of images such as binary images and does not require multispectral images as well. We have applied our method to a database containing 62 touching and partially overlapping chromosomes and a success rate of 91.9% is achieved.\n    ",
        "submission_date": "2011-12-18T00:00:00",
        "last_modified_date": "2014-09-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5298",
        "title": "Zero-Temperature Limit of a Convergent Algorithm to Minimize the Bethe Free Energy",
        "authors": [
            "Tomas Werner"
        ],
        "abstract": "After the discovery that fixed points of loopy belief propagation coincide with stationary points of the Bethe free energy, several researchers proposed provably convergent algorithms to directly minimize the Bethe free energy. These algorithms were formulated only for non-zero temperature (thus finding fixed points of the sum-product algorithm) and their possible extension to zero temperature is not obvious. We present the zero-temperature limit of the double-loop algorithm by Heskes, which converges a max-product fixed point. The inner loop of this algorithm is max-sum diffusion. Under certain conditions, the algorithm combines the complementary advantages of the max-product belief propagation and max-sum diffusion (LP relaxation): it yields good approximation of both ground states and max-marginals.\n    ",
        "submission_date": "2011-12-22T00:00:00",
        "last_modified_date": "2011-12-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5638",
        "title": "Discretization of Parametrizable Signal Manifolds",
        "authors": [
            "Elif Vural",
            "Pascal Frossard"
        ],
        "abstract": "Transformation-invariant analysis of signals often requires the computation of the distance from a test pattern to a transformation manifold. In particular, the estimation of the distances between a transformed query signal and several transformation manifolds representing different classes provides essential information for the classification of the signal. In many applications the computation of the exact distance to the manifold is costly, whereas an efficient practical solution is the approximation of the manifold distance with the aid of a manifold grid. In this paper, we consider a setting with transformation manifolds of known parameterization. We first present an algorithm for the selection of samples from a single manifold that permits to minimize the average error in the manifold distance estimation. Then we propose a method for the joint discretization of multiple manifolds that represent different signal classes, where we optimize the transformation-invariant classification accuracy yielded by the discrete manifold representation. Experimental results show that sampling each manifold individually by minimizing the manifold distance estimation error outperforms baseline sampling solutions with respect to registration and classification accuracy. Performing an additional joint optimization on all samples improves the classification performance further. Moreover, given a fixed total number of samples to be selected from all manifolds, an asymmetric distribution of samples to different manifolds depending on their geometric structures may also increase the classification accuracy in comparison with the equal distribution of samples.\n    ",
        "submission_date": "2011-12-23T00:00:00",
        "last_modified_date": "2011-12-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5640",
        "title": "Learning Smooth Pattern Transformation Manifolds",
        "authors": [
            "Elif Vural",
            "Pascal Frossard"
        ],
        "abstract": "Manifold models provide low-dimensional representations that are useful for processing and analyzing data in a transformation-invariant way. In this paper, we study the problem of learning smooth pattern transformation manifolds from image sets that represent observations of geometrically transformed signals. In order to construct a manifold, we build a representative pattern whose transformations accurately fit various input images. We examine two objectives of the manifold building problem, namely, approximation and classification. For the approximation problem, we propose a greedy method that constructs a representative pattern by selecting analytic atoms from a continuous dictionary manifold. We present a DC (Difference-of-Convex) optimization scheme that is applicable to a wide range of transformation and dictionary models, and demonstrate its application to transformation manifolds generated by rotation, translation and anisotropic scaling of a reference pattern. Then, we generalize this approach to a setting with multiple transformation manifolds, where each manifold represents a different class of signals. We present an iterative multiple manifold building algorithm such that the classification accuracy is promoted in the learning of the representative patterns. Experimental results suggest that the proposed methods yield high accuracy in the approximation and classification of data compared to some reference methods, while the invariance to geometric transformations is achieved due to the transformation manifold model.\n    ",
        "submission_date": "2011-12-23T00:00:00",
        "last_modified_date": "2012-10-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5895",
        "title": "Online Adaptive Statistical Compressed Sensing of Gaussian Mixture Models",
        "authors": [
            "Julio Duarte-Carvajalino",
            "Guillermo Sapiro",
            "Guoshen Yu",
            "Lawrence Carin"
        ],
        "abstract": "A framework of online adaptive statistical compressed sensing is introduced for signals following a mixture model. The scheme first uses non-adaptive measurements, from which an online decoding scheme estimates the model selection. As soon as a candidate model has been selected, an optimal sensing scheme for the selected model continues to apply. The final signal reconstruction is calculated from the ensemble of both the non-adaptive and the adaptive measurements. For signals generated from a Gaussian mixture model, the online adaptive sensing algorithm is given and its performance is analyzed. On both synthetic and real image data, the proposed adaptive scheme considerably reduces the average reconstruction error with respect to standard statistical compressed sensing that uses fully random measurements, at a marginally increased computational complexity.\n    ",
        "submission_date": "2011-12-26T00:00:00",
        "last_modified_date": "2011-12-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5997",
        "title": "Multispectral Palmprint Recognition Using a Hybrid Feature",
        "authors": [
            "Sina Akbari Mistani",
            "Shervin Minaee",
            "Emad Fatemizadeh"
        ],
        "abstract": "Personal identification problem has been a major field of research in recent years. Biometrics-based technologies that exploit fingerprints, iris, face, voice and palmprints, have been in the center of attention to solve this problem. Palmprints can be used instead of fingerprints that have been of the earliest of these biometrics technologies. A palm is covered with the same skin as the fingertips but has a larger surface, giving us more information than the fingertips. The major features of the palm are palm-lines, including principal lines, wrinkles and ridges. Using these lines is one of the most popular approaches towards solving the palmprint recognition problem. Another robust feature is the wavelet energy of palms. In this paper we used a hybrid feature which combines both of these features. %Moreover, multispectral analysis is applied to improve the performance of the system. At the end, minimum distance classifier is used to match test images with one of the training samples. The proposed algorithm has been tested on a well-known multispectral palmprint dataset and achieved an average accuracy of 98.8\\%.\n    ",
        "submission_date": "2011-12-27T00:00:00",
        "last_modified_date": "2015-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6269",
        "title": "Automated PolyU Palmprint sample Registration and Coarse Classification",
        "authors": [
            "Dhananjay D. M.",
            "C.V. Guru Rao",
            "I.V.Muralikrishna"
        ],
        "abstract": "Biometric based authentication for secured access to resources has gained importance, due to their reliable, invariant and discriminating features. Palmprint is one such biometric entity. Prior to classification and identification registering a sample palmprint is an important activity. In this paper we propose a computationally effective method for automated registration of samples from PlolyU palmprint database. In our approach we preprocess the sample and trace the border to find the nearest point from center of sample. Angle between vector representing the nearest point and vector passing through the center is used for automated palm sample registration. The angle of inclination between start and end point of heart line and life line is used for basic classification of palmprint samples in left class and right class.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6291",
        "title": "Descriptor learning for omnidirectional image matching",
        "authors": [
            "Jonathan Masci",
            "Davide Migliore",
            "Michael M. Bronstein",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Feature matching in omnidirectional vision systems is a challenging problem, mainly because complicated optical systems make the theoretical modelling of invariance and construction of invariant feature descriptors hard or even impossible. In this paper, we propose learning invariant descriptors using a training set of similar and dissimilar descriptor pairs. We use the similarity-preserving hashing framework, in which we are trying to map the descriptor data to the Hamming space preserving the descriptor similarity on the training set. A neural network is used to solve the underlying optimization problem. Our approach outperforms not only straightforward descriptor matching, but also state-of-the-art similarity-preserving hashing methods.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.0139",
        "title": "A Fast Statistical Method for Multilevel Thresholding in Wavelet Domain",
        "authors": [
            "Madhur Srivastava",
            "Prateek Katiyar",
            "Yashwant Yashu",
            "Satish K. Singha",
            "Prasanta K. Panigrahi"
        ],
        "abstract": "An algorithm is proposed for the segmentation of image into multiple levels using mean and standard deviation in the wavelet domain. The procedure provides for variable size segmentation with bigger block size around the mean, and having smaller blocks at the ends of histogram plot of each horizontal, vertical and diagonal components, while for the approximation component it provides for finer block size around the mean, and larger blocks at the ends of histogram plot coefficients. It is found that the proposed algorithm has significantly less time complexity, achieves superior PSNR and Structural Similarity Measurement Index as compared to similar space domain algorithms[1]. In the process it highlights finer image structures not perceptible in the original image. It is worth emphasizing that after the segmentation only 16 (at threshold level 3) wavelet coefficients captures the significant variation of image.\n    ",
        "submission_date": "2010-12-30T00:00:00",
        "last_modified_date": "2010-12-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.1266",
        "title": "Extending Bron Kerbosch for Solving the Maximum Weight Clique Problem",
        "authors": [
            "Brijnesh Jain",
            "Klaus Obermayer"
        ],
        "abstract": "This contribution extends the Bron Kerbosch algorithm for solving the maximum weight clique problem, where continuous-valued weights are assigned to both, vertices and edges. We applied the proposed algorithm to graph matching problems.\n    ",
        "submission_date": "2011-01-06T00:00:00",
        "last_modified_date": "2011-01-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.2286",
        "title": "Group Invariant Scattering",
        "authors": [
            "St\u00e9phane Mallat"
        ],
        "abstract": "This paper constructs translation invariant operators on L2(R^d), which are Lipschitz continuous to the action of diffeomorphisms. A scattering propagator is a path ordered product of non-linear and non-commuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz continuous to the action of diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform which is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2 (G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on L2(R^d) and on Ld (SO(d)) defines a translation and rotation invariant scattering on L2(R^d).\n    ",
        "submission_date": "2011-01-12T00:00:00",
        "last_modified_date": "2012-04-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3124",
        "title": "SafeVchat: Detecting Obscene Content and Misbehaving Users in Online Video Chat Services",
        "authors": [
            "Xinyu Xing",
            "Yu-Li Liang",
            "Hanqiang Cheng",
            "Jianxun Dang",
            "Sui Huang",
            "Richard Han",
            "Xue Liu",
            "Qin Lv",
            "Shivakant Mishra"
        ],
        "abstract": "Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This paper presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory. The paper introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real world data and image traces obtained from ",
        "submission_date": "2011-01-17T00:00:00",
        "last_modified_date": "2011-01-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.3381",
        "title": "Efficient Independence-Based MAP Approach for Robust Markov Networks Structure Discovery",
        "authors": [
            "Facundo Bromberg",
            "Federico Schl\u00fcter"
        ],
        "abstract": "This work introduces the IB-score, a family of independence-based score functions for robust learning of Markov networks independence structures. Markov networks are a widely used graphical representation of probability distributions, with many applications in several fields of science. The main advantage of the IB-score is the possibility of computing it without the need of estimation of the numerical parameters, an NP-hard problem, usually solved through an approximate, data-intensive, iterative optimization. We derive a formal expression for the IB-score from first principles, mainly maximum a posteriori and conditional independence properties, and exemplify several instantiations of it, resulting in two novel algorithms for structure learning: IBMAP-HC and IBMAP-TS. Experimental results over both artificial and real world data show these algorithms achieve important error reductions in the learnt structures when compared with the state-of-the-art independence-based structure learning algorithm GSMN, achieving increments of more than 50% in the amount of independencies they encode correctly, and in some cases, learning correctly over 90% of the edges that GSMN learnt incorrectly. Theoretical analysis shows IBMAP-HC proceeds efficiently, achieving these improvements in a time polynomial to the number of random variables in the domain.\n    ",
        "submission_date": "2011-01-18T00:00:00",
        "last_modified_date": "2011-01-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4373",
        "title": "Statistical Multiresolution Dantzig Estimation in Imaging: Fundamental Concepts and Algorithmic Framework",
        "authors": [
            "Klaus Frick",
            "Philipp Marnitz",
            "Axel Munk"
        ],
        "abstract": "In this paper we are concerned with fully automatic and locally adaptive estimation of functions in a \"signal + noise\"-model where the regression function may additionally be blurred by a linear operator, e.g. by a convolution. To this end, we introduce a general class of statistical multiresolution estimators and develop an algorithmic framework for computing those. By this we mean estimators that are defined as solutions of convex optimization problems with supremum-type constraints. We employ a combination of the alternating direction method of multipliers with Dykstra's algorithm for computing orthogonal projections onto intersections of convex sets and prove numerical convergence. The capability of the proposed method is illustrated by various examples from imaging and signal detection.\n    ",
        "submission_date": "2011-01-23T00:00:00",
        "last_modified_date": "2012-02-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4918",
        "title": "Using Feature Weights to Improve Performance of Neural Networks",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Different features have different relevance to a particular learning problem. Some features are less relevant; while some very important. Instead of selecting the most relevant features using feature selection, an algorithm can be given this knowledge of feature importance based on expert opinion or prior learning. Learning can be faster and more accurate if learners take feature importance into account. Correlation aided Neural Networks (CANN) is presented which is such an algorithm. CANN treats feature importance as the correlation coefficient between the target attribute and the features. CANN modifies normal feed-forward Neural Network to fit both correlation values and training data. Empirical evaluation shows that CANN is faster and more accurate than applying the two step approach of feature selection and then using normal learning algorithms.\n    ",
        "submission_date": "2011-01-25T00:00:00",
        "last_modified_date": "2011-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1101.4924",
        "title": "A Generalized Method for Integrating Rule-based Knowledge into Inductive Methods Through Virtual Sample Creation",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for classification. Methods that use domain knowledge have been shown to perform better than inductive learners. However, there is no general method to include domain knowledge into all inductive learning algorithms as all hybrid methods are highly specialized for a particular algorithm. We present an algorithm that will take domain knowledge in the form of propositional rules, generate artificial examples from the rules and also remove instances likely to be flawed. This enriched dataset then can be used by any learning algorithm. Experimental results of different scenarios are shown that demonstrate this method to be more effective than simple inductive learning.\n    ",
        "submission_date": "2011-01-25T00:00:00",
        "last_modified_date": "2011-01-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0048",
        "title": "Smart depth of field optimization applied to a robotised view camera",
        "authors": [
            "St\u00e9phane Mottelet",
            "Luc de Saint Germain",
            "Olivier Mondin"
        ],
        "abstract": "The great flexibility of a view camera allows to take high quality photographs that would not be possible any other way. But making a given object into focus is a long and tedious task, although the underlying laws are well known. This paper presents the result of a project which has lead to the design of a computer controlled view camera and to its companion software. Thanks to the high precision machining of its components, and to the known optical parameters of lenses and sensor, we have been able to consider a reliable mathematical model of the view camera, allowing the acquisition of 3D coordinates to build a geometrical model of the object. Then many problems can be solved, e.g. minimizing the f-number while maintaining the object within the depth of field, which takes the form of a constrained optimization problem. All optimization algorithms have been validated on a virtual view camera before implementation on the prototype\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2011-04-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0059",
        "title": "Statistical methods for tissue array images - algorithmic scoring and co-training",
        "authors": [
            "Donghui Yan",
            "Pei Wang",
            "Michael Linden",
            "Beatrice Knudsen",
            "Timothy Randolph"
        ],
        "abstract": "Recent advances in tissue microarray technology have allowed immunohistochemistry to become a powerful medium-to-high throughput analysis tool, particularly for the validation of diagnostic and prognostic biomarkers. However, as study size grows, the manual evaluation of these assays becomes a prohibitive limitation; it vastly reduces throughput and greatly increases variability and expense. We propose an algorithm - Tissue Array Co-Occurrence Matrix Analysis (TACOMA) - for quantifying cellular phenotypes based on textural regularity summarized by local inter-pixel relationships. The algorithm can be easily trained for any staining pattern, is absent of sensitive tuning parameters and has the ability to report salient pixels in an image that contribute to its score. Pathologists' input via informative training patches is an important aspect of the algorithm that allows the training for any specific marker or cell type. With co-training, the error rate of TACOMA can be reduced substantially for a very small training sample (e.g., with size 30). We give theoretical insights into the success of co-training via thinning of the feature set in a high-dimensional setting when there is \"sufficient\" redundancy among the features. TACOMA is flexible, transparent and provides a scoring process that can be evaluated with clarity and confidence. In a study based on an estrogen receptor (ER) marker, we show that TACOMA is comparable to, or outperforms, pathologists' performance in terms of accuracy and repeatability.\n    ",
        "submission_date": "2011-02-01T00:00:00",
        "last_modified_date": "2012-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0817",
        "title": "Natural images from the birthplace of the human eye",
        "authors": [
            "Ga\u0161per Tka\u010dik",
            "Patrick Garrigan",
            "Charles Ratliff",
            "Grega Mil\u010dinski",
            "Jennifer M Klein",
            "Lucia H Seyfarth",
            "Peter Sterling",
            "David Brainard",
            "Vijay Balasubramanian"
        ],
        "abstract": "Here we introduce a database of calibrated natural images publicly available through an easy-to-use web interface. Using a Nikon D70 digital SLR camera, we acquired about 5000 six-megapixel images of Okavango Delta of Botswana, a tropical savanna habitat similar to where the human eye is thought to have evolved. Some sequences of images were captured unsystematically while following a baboon troop, while others were designed to vary a single parameter such as aperture, object distance, time of day or position on the horizon. Images are available in the raw RGB format and in grayscale. Images are also available in units relevant to the physiology of human cone photoreceptors, where pixel values represent the expected number of photoisomerizations per second for cones sensitive to long (L), medium (M) and short (S) wavelengths. This database is distributed under a Creative Commons Attribution-Noncommercial Unported license to facilitate research in computer vision, psychophysics of perception, and visual neuroscience.\n    ",
        "submission_date": "2011-02-04T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.0899",
        "title": "Evidence Feed Forward Hidden Markov Model: A New Type of Hidden Markov Model",
        "authors": [
            "Michael DelRose",
            "Christian Wagner",
            "Philip Frederick"
        ],
        "abstract": "The ability to predict the intentions of people based solely on their visual actions is a skill only performed by humans and animals. The intelligence of current computer algorithms has not reached this level of complexity, but there are several research efforts that are working towards it. With the number of classification algorithms available, it is hard to determine which algorithm works best for a particular situation. In classification of visual human intent data, Hidden Markov Models (HMM), and their variants, are leading candidates.\n",
        "submission_date": "2011-02-04T00:00:00",
        "last_modified_date": "2011-02-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2615",
        "title": "Guaranteeing Convergence of Iterative Skewed Voting Algorithms for Image Segmentation",
        "authors": [
            "Doru C. Balcan",
            "Gowri Srinivasa",
            "Matthew Fickus",
            "Jelena Kovacevic"
        ],
        "abstract": "In this paper we provide rigorous proof for the convergence of an iterative voting-based image segmentation algorithm called Active Masks. Active Masks (AM) was proposed to solve the challenging task of delineating punctate patterns of cells from fluorescence microscope images. Each iteration of AM consists of a linear convolution composed with a nonlinear thresholding; what makes this process special in our case is the presence of additive terms whose role is to \"skew\" the voting when prior information is available. In real-world implementation, the AM algorithm always converges to a fixed point. We study the behavior of AM rigorously and present a proof of this convergence. The key idea is to formulate AM as a generalized (parallel) majority cellular automaton, adapting proof techniques from discrete dynamical systems.\n    ",
        "submission_date": "2011-02-13T00:00:00",
        "last_modified_date": "2011-02-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.2684",
        "title": "Chernoff information of exponential families",
        "authors": [
            "Frank Nielsen"
        ],
        "abstract": "Chernoff information upper bounds the probability of error of the optimal Bayesian decision rule for $2$-class classification problems. However, it turns out that in practice the Chernoff bound is hard to calculate or even approximate. In statistics, many usual distributions, such as Gaussians, Poissons or frequency histograms called multinomials, can be handled in the unified framework of exponential families. In this note, we prove that the Chernoff information for members of the same exponential family can be either derived analytically in closed form, or efficiently approximated using a simple geodesic bisection optimization technique based on an exact geometric characterization of the \"Chernoff point\" on the underlying statistical manifold.\n    ",
        "submission_date": "2011-02-14T00:00:00",
        "last_modified_date": "2011-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3328",
        "title": "An Efficient and Integrated Algorithm for Video Enhancement in Challenging Lighting Conditions",
        "authors": [
            "Xuan Dong",
            "Jiangtao",
            "Weixin Li",
            "Pang",
            "Guan Wang",
            "Yao Lu",
            "Wei Meng"
        ],
        "abstract": "We describe a novel integrated algorithm for real-time enhancement of video acquired under challenging lighting conditions. Such conditions include low lighting, haze, and high dynamic range situations. The algorithm automatically detects the dominate source of impairment, then depending on whether it is low lighting, haze or others, a corresponding pre-processing is applied to the input video, followed by the core enhancement algorithm. Temporal and spatial redundancies in the video input are utilized to facilitate real-time processing and to improve temporal and spatial consistency of the output. The proposed algorithm can be used as an independent module, or be integrated in either a video encoder or a video decoder for further optimizations.\n    ",
        "submission_date": "2011-02-16T00:00:00",
        "last_modified_date": "2011-02-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.3828",
        "title": "Searching in one billion vectors: re-rank with source coding",
        "authors": [
            "Herv\u00e9 J\u00e9gou",
            "Romain Tavenard",
            "Matthijs Douze",
            "Laurent Amsaleg"
        ],
        "abstract": "Recent indexing techniques inspired by source coding have been shown successful to index billions of high-dimensional vectors in memory. In this paper, we propose an approach that re-ranks the neighbor hypotheses obtained by these compressed-domain indexing methods. In contrast to the usual post-verification scheme, which performs exact distance calculation on the short-list of hypotheses, the estimated distances are refined based on short quantization codes, to avoid reading the full vectors from disk. We have released a new public dataset of one billion 128-dimensional vectors and proposed an experimental setup to evaluate high dimensional indexing algorithms on a realistic scale. Experiments show that our method accurately and efficiently re-ranks the neighbor hypotheses using little memory compared to the full vectors representation.\n    ",
        "submission_date": "2011-02-18T00:00:00",
        "last_modified_date": "2011-02-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4803",
        "title": "Detection of objects in noisy images and site percolation on square lattices",
        "authors": [
            "Mikhail A. Langovoy",
            "Olaf Wittich"
        ],
        "abstract": "We propose a novel probabilistic method for detection of objects in noisy images. The method uses results from percolation and random graph theories. We present an algorithm that allows to detect objects of unknown shapes in the presence of random noise. Our procedure substantially differs from wavelets-based algorithms. The algorithm has linear complexity and exponential accuracy and is appropriate for real-time systems. We prove results on consistency and algorithmic complexity of our procedure.\n    ",
        "submission_date": "2011-02-23T00:00:00",
        "last_modified_date": "2011-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4816",
        "title": "Computationally efficient algorithms for statistical image processing. Implementation in R",
        "authors": [
            "Mikhail A. Langovoy",
            "Olaf Wittich"
        ],
        "abstract": "In the series of our earlier papers on the subject, we proposed a novel statistical hypothesis testing method for detection of objects in noisy images. The method uses results from percolation theory and random graph theory. We developed algorithms that allowed to detect objects of unknown shapes in the presence of nonparametric noise of unknown level and of unknown distribution. No boundary shape constraints were imposed on the objects, only a weak bulk condition for the object's interior was required. Our algorithms have linear complexity and exponential accuracy. In the present paper, we describe an implementation of our nonparametric hypothesis testing method. We provide a program that can be used for statistical experiments in image processing. This program is written in the statistical programming language R.\n    ",
        "submission_date": "2011-02-23T00:00:00",
        "last_modified_date": "2011-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1102.4873",
        "title": "Weighted Radial Variation for Node Feature Classification",
        "authors": [
            "C. Andris"
        ],
        "abstract": "Connections created from a node-edge matrix have been traditionally difficult to visualize and analyze because of the number of flows to be rendered in a limited feature or cartographic space. Because analyzing connectivity patterns is useful for understanding the complex dynamics of human and information flow that connect non-adjacent space, techniques that allow for visual data mining or static representations of system dynamics are a growing field of research. Here, we create a Weighted Radial Variation (WRV) technique to classify a set of nodes based on the configuration of their radially-emanating vector flows. Each entity's vector is syncopated in terms of cardinality, direction, length, and flow magnitude. The WRV process unravels each star-like entity's individual flow vectors on a 0-360\u00b0 spectrum, to form a unique signal whose distribution depends on the flow presence at each step around the entity, and is further characterized by flow distance and magnitude. The signals are processed with an unsupervised classification method that clusters entities with similar signatures in order to provide a typology for each node in the system of spatial flows. We use a case study of U.S. county-to-county human incoming and outgoing migration data to test our method.\n    ",
        "submission_date": "2011-02-23T00:00:00",
        "last_modified_date": "2011-02-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2356",
        "title": "Adaptive mosaic image representation for image processing",
        "authors": [
            "Evgenia Gelman"
        ],
        "abstract": "Method for a mosaic image representation (MIR) is proposed for a selective treatment of image fragments of different transition frequency. MIR method is based on piecewise-constant image approximation on a non-uniform orthogonal grid constructed by the following recurrent multigrid algorithm. A sequence of nested uniform grids is built, such that each cell of a current grid is subdivided into four smaller cells for the next grid designing. In each grid the cells are selected, where the color intensity function can be approximated by its average value with a given precision (thereafter 'good' cells). After replacing colors of good cells by their approximating constants the reconstructed image looks like a mosaic composed of one-colored cells. Multigrid algorithm results in the stratification of the image space into regions of different transition frequency. Sizes of these regions depend on the few tuning precision parameters that characterizes adaptability of the method to the image fragments of different non-homogeneity degree. The method is found efficient for prominent contour (skeleton) extraction, edge detection as well as for the Lossy Compression of single images and video sequence of images.\n    ",
        "submission_date": "2011-03-11T00:00:00",
        "last_modified_date": "2011-03-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2539",
        "title": "SO(3)-invariant asymptotic observers for dense depth field estimation based on visual data and known camera motion",
        "authors": [
            "Nadege Zarrouati",
            "Emanuel Aldea",
            "Pierre Rouchon"
        ],
        "abstract": "In this paper, we use known camera motion associated to a video sequence of a static scene in order to estimate and incrementally refine the surrounding depth field. We exploit the SO(3)-invariance of brightness and depth fields dynamics to customize standard image processing techniques. Inspired by the Horn-Schunck method, we propose a SO(3)-invariant cost to estimate the depth field. At each time step, this provides a diffusion equation on the unit Riemannian sphere that is numerically solved to obtain a real time depth field estimation of the entire field of view. Two asymptotic observers are derived from the governing equations of dynamics, respectively based on optical flow and depth estimations: implemented on noisy sequences of synthetic images as well as on real data, they perform a more robust and accurate depth estimation. This approach is complementary to most methods employing state observers for range estimation, which uniquely concern single or isolated feature points.\n    ",
        "submission_date": "2011-03-13T00:00:00",
        "last_modified_date": "2011-09-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.2756",
        "title": "Sparse Transfer Learning for Interactive Video Search Reranking",
        "authors": [
            "Xinmei Tian",
            "Dacheng Tao",
            "Yong Rui"
        ],
        "abstract": "Visual reranking is effective to improve the performance of the text-based video search. However, existing reranking algorithms can only achieve limited improvement because of the well-known semantic gap between low level visual features and high level semantic concepts. In this paper, we adopt interactive video search reranking to bridge the semantic gap by introducing user's labeling effort. We propose a novel dimension reduction tool, termed sparse transfer learning (STL), to effectively and efficiently encode user's labeling information. STL is particularly designed for interactive video search reranking. Technically, it a) considers the pair-wise discriminative information to maximally separate labeled query relevant samples from labeled query irrelevant ones, b) achieves a sparse representation for the subspace to encodes user's intention by applying the elastic net penalty, and c) propagates user's labeling information from labeled samples to unlabeled samples by using the data distribution knowledge. We conducted extensive experiments on the TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular dimension reduction algorithms. We report superior performance by using the proposed STL based interactive video search reranking.\n    ",
        "submission_date": "2011-03-14T00:00:00",
        "last_modified_date": "2011-12-21T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3430",
        "title": "Identification of arabic word from bilingual text using character features",
        "authors": [
            "Sofiene Haboubi",
            "Samia Maddouri",
            "Hamid Amiri"
        ],
        "abstract": "The identification of the language of the script is an important stage in the process of recognition of the writing. There are several works in this research area, which treat various languages. Most of the used methods are global or statistical. In this present paper, we study the possibility of using the features of scripts to identify the language. The identification of the language of the script by characteristics returns the identification in the case of multilingual documents less difficult. We present by this work, a study on the possibility of using the structural features to identify the Arabic language from an Arabic / Latin text.\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.3532",
        "title": "4D Wavelet-Based Regularization for Parallel MRI Reconstruction: Impact on Subject and Group-Levels Statistical Sensitivity in fMRI",
        "authors": [
            "Lotfi Chaari",
            "S\u00e9bastien M\u00e9riaux",
            "Solveig Badillo",
            "Jean-Christophe Pesquet",
            "Philippe Ciuciu"
        ],
        "abstract": "Parallel MRI is a fast imaging technique that enables the acquisition of highly resolved images in space. It relies on $k$-space undersampling and multiple receiver coils with complementary sensitivity profiles in order to reconstruct a full Field-Of-View (FOV) image. The performance of parallel imaging mainly depends on the reconstruction algorithm, which can proceed either in the original $k$-space (GRAPPA, SMASH) or in the image domain (SENSE-like methods). To improve the performance of the widely used SENSE algorithm, 2D- or slice-specific regularization in the wavelet domain has been efficiently investigated. In this paper, we extend this approach using 3D-wavelet representations in order to handle all slices together and address reconstruction artifacts which propagate across adjacent slices. The extension also accounts for temporal correlations that exist between successive scans in functional MRI (fMRI). The proposed 4D reconstruction scheme is fully \\emph{unsupervised} in the sense that all regularization parameters are estimated in the maximum likelihood sense on a reference scan. The gain induced by such extensions is first illustrated on EPI image reconstruction but also measured in terms of statistical sensitivity during a fast event-related fMRI protocol. The proposed 4D-UWR-SENSE algorithm outperforms the SENSE reconstruction at the subject and group-levels (15 subjects) for different contrasts of interest and using different parallel acceleration factors on $2\\times2\\times3$mm$^3$ EPI images.\n    ",
        "submission_date": "2011-03-17T00:00:00",
        "last_modified_date": "2011-03-17T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4487",
        "title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs",
        "authors": [
            "Dan C. Cire\u015fan",
            "Ueli Meier",
            "Luca M. Gambardella",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.\n    ",
        "submission_date": "2011-03-23T00:00:00",
        "last_modified_date": "2011-03-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1103.4767",
        "title": "A comparison of Gap statistic definitions with and without logarithm function",
        "authors": [
            "Mojgan Mohajer",
            "Karl-Hans Englmeier",
            "Volker J. Schmid"
        ],
        "abstract": "The Gap statistic is a standard method for determining the number of clusters in a set of data. The Gap statistic standardizes the graph of $\\log(W_{k})$, where $W_{k}$ is the within-cluster dispersion, by comparing it to its expectation under an appropriate null reference distribution of the data. We suggest to use $W_{k}$ instead of $\\log(W_{k})$, and to compare it to the expectation of $W_{k}$ under a null reference distribution. In fact, whenever a number fulfills the original Gap statistic inequality, this number also fulfills the inequality of a Gap statistic using $W_{k}$, but not \\textit{vice versa}. The two definitions of the Gap function are evaluated on several simulated data sets and on a real data of DCE-MR images.\n    ",
        "submission_date": "2011-03-24T00:00:00",
        "last_modified_date": "2011-03-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.0654",
        "title": "Block-Sparse Recovery via Convex Optimization",
        "authors": [
            "Ehsan Elhamifar",
            "Rene Vidal"
        ],
        "abstract": "Given a dictionary that consists of multiple blocks and a signal that lives in the range space of only a few blocks, we study the problem of finding a block-sparse representation of the signal, i.e., a representation that uses the minimum number of blocks. Motivated by signal/image processing and computer vision applications, such as face recognition, we consider the block-sparse recovery problem in the case where the number of atoms in each block is arbitrary, possibly much larger than the dimension of the underlying subspace. To find a block-sparse representation of a signal, we propose two classes of non-convex optimization programs, which aim to minimize the number of nonzero coefficient blocks and the number of nonzero reconstructed vectors from the blocks, respectively. Since both classes of problems are NP-hard, we propose convex relaxations and derive conditions under which each class of the convex programs is equivalent to the original non-convex formulation. Our conditions depend on the notions of mutual and cumulative subspace coherence of a dictionary, which are natural generalizations of existing notions of mutual and cumulative coherence. We evaluate the performance of the proposed convex programs through simulations as well as real experiments on face recognition. We show that treating the face recognition problem as a block-sparse recovery problem improves the state-of-the-art results by 10% with only 25% of the training data.\n    ",
        "submission_date": "2011-04-04T00:00:00",
        "last_modified_date": "2012-04-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.1892",
        "title": "\"Improved FCM algorithm for Clustering on Web Usage Mining\"",
        "authors": [
            "K. Suresh"
        ],
        "abstract": "In this paper we present clustering method is very sensitive to the initial center values, requirements on the data set too high, and cannot handle noisy data the proposal method is using information entropy to initialize the cluster centers and introduce weighting parameters to adjust the location of cluster centers and noise ",
        "submission_date": "2011-04-11T00:00:00",
        "last_modified_date": "2011-04-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.3571",
        "title": "Visualization techniques for data mining of Latur district satellite imagery",
        "authors": [
            "B. G. Kodge",
            "P. S. Hiremath"
        ],
        "abstract": "This study presents a new visualization tool for classification of satellite imagery. Visualization of feature space allows exploration of patterns in the image data and insight into the classification process and related uncertainty. Visual Data Mining provides added value to image classifications as the user can be involved in the classification process providing increased confidence in and understanding of the results. In this study, we present a prototype visualization tool for visual data mining (VDM) of satellite imagery. The visualization tool is showcased in a classification study of highresolution imageries of Latur district in Maharashtra state of India.\n    ",
        "submission_date": "2011-03-28T00:00:00",
        "last_modified_date": "2012-05-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1104.4376",
        "title": "Intent Inference and Syntactic Tracking with GMTI Measurements",
        "authors": [
            "Alex Wang",
            "Vikram Krishnamurthy",
            "Bhashyam Balaji"
        ],
        "abstract": "In conventional target tracking systems, human operators use the estimated target tracks to make higher level inference of the target behaviour/intent. This paper develops syntactic filtering algorithms that assist human operators by extracting spatial patterns from target tracks to identify suspicious/anomalous spatial trajectories. The targets' spatial trajectories are modeled by a stochastic context free grammar (SCFG) and a switched mode state space model. Bayesian filtering algorithms for stochastic context free grammars are presented for extracting the syntactic structure and illustrated for a ground moving target indicator (GMTI) radar example. The performance of the algorithms is tested with the experimental data collected using DRDC Ottawa's X-band Wideband Experimental Airborne Radar (XWEAR).\n    ",
        "submission_date": "2011-04-22T00:00:00",
        "last_modified_date": "2011-04-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0121",
        "title": "Methods of Hierarchical Clustering",
        "authors": [
            "Fionn Murtagh",
            "Pedro Contreras"
        ],
        "abstract": "We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.\n    ",
        "submission_date": "2011-04-30T00:00:00",
        "last_modified_date": "2011-04-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.0826",
        "title": "Streaming Multimedia Information Using the Features of the DVB-S Card",
        "authors": [
            "Radu Arsinte",
            "Eugen Lupu"
        ],
        "abstract": "This paper presents a study of audio-video streaming using the additional possibilities of a DVB-S card. The board used for experiments (Technisat SkyStar 2) is one of the most frequently used cards for this purpose. Using the main blocks of the board's software support it is possible the implement a really useful and full functional system for audio-video streaming. The streaming is possible to be implemented either for decoded MPEG stream or for transport stream. In this last case it is possible to view not only a program, but any program from the same multiplex. This allows us to implement\n    ",
        "submission_date": "2011-05-04T00:00:00",
        "last_modified_date": "2011-05-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.1302",
        "title": "A Modified Cross Correlation Algorithm for Reference-free Image Alignment of Non-Circular Projections in Single-Particle Electron Microscopy",
        "authors": [
            "Wooram Park",
            "Gregory S. Chirikjian"
        ],
        "abstract": "In this paper we propose a modified cross correlation method to align images from the same class in single-particle electron microscopy of highly non-spherical structures. In this new method, First we coarsely align projection images, and then re-align the resulting images using the cross correlation (CC) method. The coarse alignment is obtained by matching the centers of mass and the principal axes of the images. The distribution of misalignment in this coarse alignment can be quantified based on the statistical properties of the additive background noise. As a consequence, the search space for re-alignment in the cross correlation method can be reduced to achieve better alignment. In order to overcome problems associated with false peaks in the cross correlations function, we use artificially blurred images for the early stage of the iterative cross correlation method and segment the intermediate class average from every iteration step. These two additional manipulations combined with the reduced search space size in the cross correlation method yield better alignments for low signal-to-noise ratio images than both classical cross correlation and maximum likelihood(ML) methods.\n    ",
        "submission_date": "2011-05-06T00:00:00",
        "last_modified_date": "2011-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.2831",
        "title": "Planar Pixelations and Image Recognition",
        "authors": [
            "Brandon Rowekamp"
        ],
        "abstract": "Any subset of the plane can be approximated by a set of square pixels. This transition from a shape to its pixelation is rather brutal since it destroys geometric and topological information about the shape. Using a technique inspired by Morse Theory, we algorithmically produce a PL approximation of the original shape using only information from its pixelation. This approximation converges to the original shape in a very strong sense: as the size of the pixels goes to zero we can recover important geometric and topological invariants of the original shape such as Betti numbers, area, perimeter and curvature measures.\n    ",
        "submission_date": "2011-05-13T00:00:00",
        "last_modified_date": "2011-05-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.3829",
        "title": "Hierarchical Recursive Running Median",
        "authors": [
            "Alexander Alekseychuk"
        ],
        "abstract": "To date, the histogram-based running median filter of Perreault and H\u00e9bert is considered the fastest for 8-bit images, being roughly O(1) in average case. We present here another approximately constant time algorithm which further improves the aforementioned one and exhibits lower associated constant, being at the time of writing the lowest theoretical complexity algorithm for calculation of 2D and higher dimensional median filters. The algorithm scales naturally to higher precision (e.g. 16-bit) integer data without any modifications. Its adaptive version offers additional speed-up for images showing compact modes in gray-value distribution. The experimental comparison to the previous constant-time algorithm defines the application domain of this new development, besides theoretical interest, as high bit depth data and/or hardware without SIMD extensions. The C/C++ implementation of the algorithm is available under GPL for research purposes.\n    ",
        "submission_date": "2011-05-19T00:00:00",
        "last_modified_date": "2012-01-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.5675",
        "title": "Scale-Invariant Local Descriptor for Event Recognition in 1D Sensor Signals",
        "authors": [
            "Jierui Xie",
            "Mandis S. Beigi"
        ],
        "abstract": "In this paper, we introduce a shape-based, time-scale invariant feature descriptor for 1-D sensor signals. The time-scale invariance of the feature allows us to use feature from one training event to describe events of the same semantic class which may take place over varying time scales such as walking slow and walking fast. Therefore it requires less training set. The descriptor takes advantage of the invariant location detection in the scale space theory and employs a high level shape encoding scheme to capture invariant local features of events. Based on this descriptor, a scale-invariant classifier with \"R\" metric (SIC-R) is designed to recognize multi-scale events of human activities. The R metric combines the number of matches of keypoint in scale space with the Dynamic Time Warping score. SICR is tested on various types of 1-D sensors data from passive infrared, accelerometer and seismic sensors with more than 90% classification accuracy.\n    ",
        "submission_date": "2011-05-28T00:00:00",
        "last_modified_date": "2011-05-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1105.6084",
        "title": "RASID: A Robust WLAN Device-free Passive Motion Detection System",
        "authors": [
            "Ahmed E. Kosba",
            "Ahmed Saeed",
            "Moustafa Youssef"
        ],
        "abstract": "WLAN Device-free passive DfP indoor localization is an emerging technology enabling the localization of entities that do not carry any devices nor participate actively in the localization process using the already installed wireless infrastructure. This technology is useful for a variety of applications such as intrusion detection, smart homes and border protection. We present the design, implementation and evaluation of RASID, a DfP system for human motion detection. RASID combines different modules for statistical anomaly detection while adapting to changes in the environment to provide accurate, robust, and low-overhead detection of human activities using standard WiFi hardware. Evaluation of the system in two different testbeds shows that it can achieve an accurate detection capability in both environments with an F-measure of at least 0.93. In addition, the high accuracy and low overhead performance are robust to changes in the environment as compared to the current state of the art DfP detection systems. We also relay the lessons learned during building our system and discuss future research directions.\n    ",
        "submission_date": "2011-05-30T00:00:00",
        "last_modified_date": "2012-02-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0357",
        "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction",
        "authors": [
            "Mohamad Tarifi",
            "Meera Sitharam",
            "Jeffery Ho"
        ],
        "abstract": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.\n    ",
        "submission_date": "2011-06-02T00:00:00",
        "last_modified_date": "2011-06-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.0987",
        "title": "Nearest Prime Simplicial Complex for Object Recognition",
        "authors": [
            "Junping Zhang",
            "Ziyu Xie",
            "Stan Z. Li"
        ],
        "abstract": "The structure representation of data distribution plays an important role in understanding the underlying mechanism of generating data. In this paper, we propose nearest prime simplicial complex approaches (NSC) by utilizing persistent homology to capture such structures. Assuming that each class is represented with a prime simplicial complex, we classify unlabeled samples based on the nearest projection distances from the samples to the simplicial complexes. We also extend the extrapolation ability of these complexes with a projection constraint term. Experiments in simulated and practical datasets indicate that compared with several published algorithms, the proposed NSC approaches achieve promising performance without losing the structure representation.\n    ",
        "submission_date": "2011-06-06T00:00:00",
        "last_modified_date": "2011-06-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2124",
        "title": "Omni-tomography/Multi-tomography -- Integrating Multiple Modalities for Simultaneous Imaging",
        "authors": [
            "Ge Wang",
            "Jie Zhang",
            "Hao Gao",
            "Victor Weir",
            "Hengyong Yu",
            "Wenxiang Cong",
            "Xiaochen Xu",
            "Haiou Shen",
            "James Bennett",
            "Yue Wang",
            "Michael Vannier"
        ],
        "abstract": "Current tomographic imaging systems need major improvements, especially when multi-dimensional, multi-scale, multi-temporal and multi-parametric phenomena are under investigation. Both preclinical and clinical imaging now depend on in vivo tomography, often requiring separate evaluations by different imaging modalities to define morphologic details, delineate interval changes due to disease or interventions, and study physiological functions that have interconnected aspects. Over the past decade, fusion of multimodality images has emerged with two different approaches: post-hoc image registration and combined acquisition on PET-CT, PET-MRI and other hybrid scanners. There are intrinsic limitations for both the post-hoc image analysis and dual/triple modality approaches defined by registration errors and physical constraints in the acquisition chain. We envision that tomography will evolve beyond current modality fusion and towards grand fusion, a large scale fusion of all or many imaging modalities, which may be referred to as omni-tomography or multi-tomography. Unlike modality fusion, grand fusion is here proposed for truly simultaneous but often localized reconstruction in terms of all or many relevant imaging mechanisms such as CT, MRI, PET, SPECT, US, optical, and possibly more. In this paper, the technical basis for omni-tomography is introduced and illustrated with a top-level design of a next generation scanner, interior tomographic reconstructions of representative modalities, and anticipated applications of omni-tomography.\n    ",
        "submission_date": "2011-06-10T00:00:00",
        "last_modified_date": "2011-06-10T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2233",
        "title": "Clustering with Multi-Layer Graphs: A Spectral Perspective",
        "authors": [
            "Xiaowen Dong",
            "Pascal Frossard",
            "Pierre Vandergheynst",
            "Nikolai Nefedov"
        ],
        "abstract": "Observational data usually comes with a multimodal nature, which means that it can be naturally represented by a multi-layer graph whose layers share the same set of vertices (users) with different edges (pairwise relationships). In this paper, we address the problem of combining different layers of the multi-layer graph for improved clustering of the vertices compared to using layers independently. We propose two novel methods, which are based on joint matrix factorization and graph regularization framework respectively, to efficiently combine the spectrum of the multiple graph layers, namely the eigenvectors of the graph Laplacian matrices. In each case, the resulting combination, which we call a \"joint spectrum\" of multiple graphs, is used for clustering the vertices. We evaluate our approaches by simulations with several real world social network datasets. Results demonstrate the superior or competitive performance of the proposed methods over state-of-the-art technique and common baseline methods, such as co-regularization and summation of information from individual graphs.\n    ",
        "submission_date": "2011-06-11T00:00:00",
        "last_modified_date": "2011-06-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2696",
        "title": "Who clicks there!: Anonymizing the photographer in a camera saturated society",
        "authors": [
            "Peter Schaffer",
            "Djamila Aouada",
            "Shishir Nagaraja"
        ],
        "abstract": "In recent years, social media has played an increasingly important role in reporting world events. The publication of crowd-sourced photographs and videos in near real-time is one of the reasons behind the high impact. However, the use of a camera can draw the photographer into a situation of conflict. Examples include the use of cameras by regulators collecting evidence of Mafia operations; citizens collecting evidence of corruption at a public service outlet; and political dissidents protesting at public rallies. In all these cases, the published images contain fairly unambiguous clues about the location of the photographer (scene viewpoint information). In the presence of adversary operated cameras, it can be easy to identify the photographer by also combining leaked information from the photographs themselves. We call this the camera location detection attack. We propose and review defense techniques against such attacks. Defenses such as image obfuscation techniques do not protect camera-location information; current anonymous publication technologies do not help either. However, the use of view synthesis algorithms could be a promising step in the direction of providing probabilistic privacy guarantees.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2011-06-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.2729",
        "title": "Nested Graph Words for Object Recognition",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "R\u00e9mi M\u00e9gret"
        ],
        "abstract": "In this paper, we propose a new, scalable approach for the task of object based image search or object recognition. Despite the very large literature existing on the scalability issues in CBIR in the sense of retrieval approaches, the scalability of media and scalability of features remain an issue. In our work we tackle the problem of scalability and structural organization of features. The proposed features are nested local graphs built upon sets of SURF feature points with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth to a Bag-of-Graph-Words representation. The nested nature of the descriptors consists in scaling from trivial Delaunay graphs - isolated feature points - by increasing the number of nodes layer by layer up to graphs with maximal number of nodes. For each layer of graphs its proper visual dictionary is built. The experiments conducted on the SIVAL data set reveal that the graph features at different layers exhibit complementary performances on the same content. The nested approach, the combination of all existing layers, yields significant improvement of the object recognition performance compared to single level approaches.\n    ",
        "submission_date": "2011-06-14T00:00:00",
        "last_modified_date": "2014-04-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.4632",
        "title": "Inferring 3D Articulated Models for Box Packaging Robot",
        "authors": [
            "Heran Yang",
            "Tiffany Low",
            "Matthew Cong",
            "Ashutosh Saxena"
        ],
        "abstract": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.\n    ",
        "submission_date": "2011-06-23T00:00:00",
        "last_modified_date": "2011-06-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5793",
        "title": "A Replica Inference Approach to Unsupervised Multi-Scale Image Segmentation",
        "authors": [
            "Dandan Hu",
            "Peter Ronhovde",
            "Zohar Nussinov"
        ],
        "abstract": "We apply a replica inference based Potts model method to unsupervised image segmentation on multiple scales. This approach was inspired by the statistical mechanics problem of \"community detection\" and its phase diagram. Specifically, the problem is cast as identifying tightly bound clusters (\"communities\" or \"solutes\") against a background or \"solvent\". Within our multiresolution approach, we compute information theory based correlations among multiple solutions (\"replicas\") of the same graph over a range of resolutions. Significant multiresolution structures are identified by replica correlations as manifest in information theory overlaps. With the aid of these correlations as well as thermodynamic measures, the phase diagram of the corresponding Potts model is analyzed both at zero and finite temperatures. Optimal parameters corresponding to a sensible unsupervised segmentation correspond to the \"easy phase\" of the Potts model. Our algorithm is fast and shown to be at least as accurate as the best algorithms to date and to be especially suited to the detection of camouflaged images.\n    ",
        "submission_date": "2011-06-28T00:00:00",
        "last_modified_date": "2011-06-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.5829",
        "title": "Active Classification: Theory and Application to Underwater Inspection",
        "authors": [
            "Geoffrey A. Hollinger",
            "Urbashi Mitra",
            "Gaurav S. Sukhatme"
        ],
        "abstract": "We discuss the problem in which an autonomous vehicle must classify an object based on multiple views. We focus on the active classification setting, where the vehicle controls which views to select to best perform the classification. The problem is formulated as an extension to Bayesian active learning, and we show connections to recent theoretical guarantees in this area. We formally analyze the benefit of acting adaptively as new information becomes available. The analysis leads to a probabilistic algorithm for determining the best views to observe based on information theoretic costs. We validate our approach in two ways, both related to underwater inspection: 3D polyhedra recognition in synthetic depth maps and ship hull inspection with imaging sonar. These tasks encompass both the planning and recognition aspects of the active classification problem. The results demonstrate that actively planning for informative views can reduce the number of necessary views by up to 80% when compared to passive methods.\n    ",
        "submission_date": "2011-06-29T00:00:00",
        "last_modified_date": "2011-06-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1106.6242",
        "title": "Visual Secret Sharing Scheme using Grayscale Images",
        "authors": [
            "Sandeep Katta"
        ],
        "abstract": "Pixel expansion and the quality of the reconstructed secret image has been a major issue of visual secret sharing (VSS) schemes. A number of probabilistic VSS schemes with minimum pixel expansion have been proposed for black and white (binary) secret images. This paper presents a probabilistic (2, 3)-VSS scheme for gray scale images. Its pixel expansion is larger in size but the quality of the image is perfect when it's reconstructed. The construction of the shadow images (transparent shares) is based on the binary OR operation.\n    ",
        "submission_date": "2011-06-30T00:00:00",
        "last_modified_date": "2011-06-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.0169",
        "title": "Unstructured Human Activity Detection from RGBD Images",
        "authors": [
            "Jaeyong Sung",
            "Colin Ponce",
            "Bart Selman",
            "Ashutosh Saxena"
        ],
        "abstract": "Being able to detect and recognize human activities is essential for several applications, including personal assistive robotics. In this paper, we perform detection and recognition of unstructured human activity in unstructured environments. We use a RGBD sensor (Microsoft Kinect) as the input sensor, and compute a set of features based on human pose and motion, as well as based on image and pointcloud information. Our algorithm is based on a hierarchical maximum entropy Markov model (MEMM), which considers a person's activity as composed of a set of sub-activities. We infer the two-layered graph structure using a dynamic programming approach. We test our algorithm on detecting and recognizing twelve different activities performed by four people in different environments, such as a kitchen, a living room, an office, etc., and achieve good performance even when the person was not seen before in the training set.\n    ",
        "submission_date": "2011-07-01T00:00:00",
        "last_modified_date": "2012-02-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1119",
        "title": "Integrating Generic Sensor Fusion Algorithms with Sound State Representations through Encapsulation of Manifolds",
        "authors": [
            "Christoph Hertzberg",
            "Ren\u00e9 Wagner",
            "Udo Frese",
            "Lutz Schr\u00f6der"
        ],
        "abstract": "Common estimation algorithms, such as least squares estimation or the Kalman filter, operate on a state in a state space S that is represented as a real-valued vector. However, for many quantities, most notably orientations in 3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a vector space locally but has a more complex global topological structure. For integrating these quantities, several ad-hoc approaches have been proposed.\n",
        "submission_date": "2011-07-06T00:00:00",
        "last_modified_date": "2011-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1257",
        "title": "Evidence-Based Filters for Signal Detection: Application to Evoked Brain Responses",
        "authors": [
            "M. Asim Mubeen",
            "Kevin H. Knuth"
        ],
        "abstract": "Template-based signal detection most often relies on computing a correlation, or a dot product, between an incoming data stream and a signal template. Such a correlation results in an ongoing estimate of the magnitude of the signal in the data stream. However, it does not directly indicate the presence or absence of the signal. The problem is really one of model-testing, and the relevant quantity is the Bayesian evidence (marginal likelihood) of the signal model. Given a signal template and an ongoing data stream, we have developed an evidence-based filter that computes the Bayesian evidence that a signal is present in the data. We demonstrate this algorithm by applying it to brain-machine interface (BMI) data obtained by recording human brain electrical activity, or electroencephalography (EEG). A very popular and effective paradigm in EEG-based BMI is based on the detection of the P300 evoked brain response which is generated in response to particular sensory stimuli. The goal is to detect the presence of a P300 signal in ongoing EEG activity as accurately and as fast as possible. Our algorithm uses a subject-specific P300 template to compute the Bayesian evidence that a applying window of EEG data contains the signal. The efficacy of this algorithm is demonstrated by comparing receiver operating characteristic (ROC) curves of the evidence-based filter to the usual correlation method. Our results show a significant improvement in single-trial P300 detection. The evidence-based filter promises to improve the accuracy and speed of the detection of evoked brain responses in BMI applications as well the detection of template signals in more general signal processing applications\n    ",
        "submission_date": "2011-07-06T00:00:00",
        "last_modified_date": "2011-07-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.1987",
        "title": "Median Algorithm for Sector Spectra Calculation from Images Registered by the Spectral Airglow Temperature Imager",
        "authors": [
            "Atanas Marinov Atanassov"
        ],
        "abstract": "The Spectral Airglow Temperature Imager is an instrument, specially designed for investigation of the wave processes in the Mesosphere-Lower Thermosphere. In order to determine the kinematic parameters of a wave, the values of a physical quantity in different space points and their changes in the time should be known. As a result of the possibilities of the SATI instrument for space scanning, different parts of the images (sectors of spectrograms) correspond to the respective mesopause areas (where the radiation is generated). An approach is proposed for sector spectra determination from SATI images based on ordered statistics instead of meaning. Comparative results are shown.\n    ",
        "submission_date": "2011-07-11T00:00:00",
        "last_modified_date": "2011-07-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2347",
        "title": "BSVM: A Banded Suport Vector Machine",
        "authors": [
            "Gautam V. Pendse"
        ],
        "abstract": "We describe a novel binary classification technique called Banded SVM (B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the decision rule is encouraged to lie in the interval [1, \\infty]. The new B-SVM objective function contains a penalty term that encourages the decision rule to lie in a user specified range [\\rho_1, \\rho_2]. In addition to the standard set of support vectors (SVs) near the class boundaries, B-SVM results in a second set of SVs in the interior of each class.\n    ",
        "submission_date": "2011-07-12T00:00:00",
        "last_modified_date": "2011-07-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2859",
        "title": "Label-Specific Training Set Construction from Web Resource for Image Annotation",
        "authors": [
            "Jinhui Tang",
            "Shuicheng Yan",
            "Tat-Seng Chua",
            "Ramesh Jain"
        ],
        "abstract": "Recently many research efforts have been devoted to image annotation by leveraging on the associated tags/keywords of web images as training labels. A key issue to resolve is the relatively low accuracy of the tags. In this paper, we propose a novel semi-automatic framework to construct a more accurate and effective training set from these web media resources for each label that we want to learn. Experiments conducted on a real-world dataset demonstrate that the constructed training set can result in higher accuracy for image annotation.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.2875",
        "title": "A Hilbert Scheme in Computer Vision",
        "authors": [
            "Chris Aholt",
            "Bernd Sturmfels",
            "Rekha Thomas"
        ],
        "abstract": "Multiview geometry is the study of two-dimensional images of three-dimensional scenes, a foundational subject in computer vision. We determine a universal Groebner basis for the multiview ideal of n generic cameras. As the cameras move, the multiview varieties vary in a family of dimension 11n-15. This family is the distinguished component of a multigraded Hilbert scheme with a unique Borel-fixed point. We present a combinatorial study of ideals lying on that Hilbert scheme.\n    ",
        "submission_date": "2011-07-14T00:00:00",
        "last_modified_date": "2011-07-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3499",
        "title": "Applying Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) spectral indices for geological mapping and mineral identification on the Tibetan Plateau",
        "authors": [
            "Robert Corrie",
            "Yoshiki Ninomiya",
            "Jonathan Aitchison"
        ],
        "abstract": "The Tibetan Plateau holds clues to understanding the dynamics and mechanisms associated with continental growth. Part of the region is characterized by zones of ophiolitic melange believed to represent the remnants of ancient oceanic crust and underlying upper mantle emplaced during oceanic closures. However, due to the remoteness of the region and the inhospitable terrain many areas have not received detailed investigation. Increased spatial and spectral resolution of satellite sensors have made it possible to map in greater detail the mineralogy and lithology than in the past. Recent work by Yoshiki Ninomiya of the Geological Survey of Japan has pioneered the use of several spectral indices for the mapping of quartzose, carbonate, and silicate rocks using Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) thermal infrared (TIR) data. In this study, ASTER TIR indices have been applied to a region in western-central Tibet for the purposes of assessing their effectiveness for differentiating ophiolites and other lithologies. The results agree well with existing geological maps and other published data. The study area was chosen due to its diverse range of rock types, including an ophiolitic melange, associated with the Bangong-Nujiang suture (BNS) that crops out on the northern shores of Lagkor Tso and Dong Tso (\"Tso\" is Tibetan for lake). The techniques highlighted in this paper could be applied to other geographical regions where similar geological questions need to be resolved. The results of this study aim to show the utility of ASTER TIR imagery for geological mapping in semi-arid and sparsely vegetated areas on the Tibetan Plateau.\n    ",
        "submission_date": "2011-07-18T00:00:00",
        "last_modified_date": "2011-07-18T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3680",
        "title": "3-Phase Recognition Approach to Pseudo 3D Building Generation from 2D Floor Plan",
        "authors": [
            "Raj Kishen Moloo",
            "Muhammad Ajmal Sheik Dawood",
            "Abu Salmaan Auleear"
        ],
        "abstract": "Nowadays three dimension (3D) architectural visualisation has become a powerful tool in the conceptualisation, design and presentation of architectural products in the construction industry, providing realistic interaction and walkthrough on engineering products. Traditional ways of implementing 3D models involves the use of specialised 3D authoring tools along with skilled 3D designers with blueprints of the model and this is a slow and laborious process. The aim of this paper is to automate this process by simply analyzing the blueprint document and generating the 3D scene automatically. For this purpose we have devised a 3-Phase recognition approach to pseudo 3D building generation from 2D floor plan and developed a software accordingly. Our 3-phased 3D building system has been implemented using C, C++ and OpenCV library [24] for the Image Processing module; The Save Module generated an XML file for storing the processed floor plan objects attributes; while the Irrlitch [14] game engine was used to implement the Interactive 3D module. Though still at its infancy, our proposed system gave commendable results. We tested our system on 6 floor plans with complexities ranging from low to high and the results seems to be very promising with an average processing time of around 3s and a 3D generation in 4s. In addition the system provides an interactive walk-though and allows users to modify components.\n    ",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.3823",
        "title": "Weakly Supervised Learning of Foreground-Background Segmentation using Masked RBMs",
        "authors": [
            "Nicolas Heess",
            "Nicolas Le Roux",
            "John Winn"
        ],
        "abstract": "We propose an extension of the Restricted Boltzmann Machine (RBM) that allows the joint shape and appearance of foreground objects in cluttered images to be modeled independently of the background. We present a learning scheme that learns this representation directly from cluttered images with only very weak supervision. The model generates plausible samples and performs foreground-background segmentation. We demonstrate that representing foreground objects independently of the background can be beneficial in recognition tasks.\n    ",
        "submission_date": "2011-07-19T00:00:00",
        "last_modified_date": "2011-07-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4619",
        "title": "On the Hilbert transform of wavelets",
        "authors": [
            "Kunal Narayan Chaudhury",
            "Michael Unser"
        ],
        "abstract": "A wavelet is a localized function having a prescribed number of vanishing moments. In this correspondence, we provide precise arguments as to why the Hilbert transform of a wavelet is again a wavelet. In particular, we provide sharp estimates of the localization, vanishing moments, and smoothness of the transformed wavelet. We work in the general setting of non-compactly supported wavelets. Our main result is that, in the presence of some minimal smoothness and decay, the Hilbert transform of a wavelet is again as smooth and oscillating as the original wavelet, whereas its localization is controlled by the number of vanishing moments of the original wavelet. We motivate our results using concrete examples.\n    ",
        "submission_date": "2011-07-22T00:00:00",
        "last_modified_date": "2011-07-22T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1107.4985",
        "title": "Variational Gaussian Process Dynamical Systems",
        "authors": [
            "Andreas C. Damianou",
            "Michalis K. Titsias",
            "Neil D. Lawrence"
        ],
        "abstract": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.\n    ",
        "submission_date": "2011-07-25T00:00:00",
        "last_modified_date": "2011-07-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.1500",
        "title": "Gender Recognition Based on Sift Features",
        "authors": [
            "Sahar Yousefi",
            "Morteza Zahedi"
        ],
        "abstract": "This paper proposes a robust approach for face detection and gender classification in color images. Previous researches about gender recognition suppose an expensive computational and time-consuming pre-processing step in order to alignment in which face images are aligned so that facial landmarks like eyes, nose, lips, chin are placed in uniform locations in image. In this paper, a novel technique based on mathematical analysis is represented in three stages that eliminates alignment step. First, a new color based face detection method is represented with a better result and more robustness in complex backgrounds. Next, the features which are invariant to affine transformations are extracted from each face using scale invariant feature transform (SIFT) method. To evaluate the performance of the proposed algorithm, experiments have been conducted by employing a SVM classifier on a database of face images which contains 500 images from distinct people with equal ratio of male and female.\n    ",
        "submission_date": "2011-08-06T00:00:00",
        "last_modified_date": "2011-08-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.3298",
        "title": "A Machine Learning Perspective on Predictive Coding with PAQ",
        "authors": [
            "Byron Knoll",
            "Nando de Freitas"
        ],
        "abstract": "PAQ8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. This report presents a detailed description of PAQ8 from a statistical machine learning perspective. It shows that it is possible to understand some of the modules of PAQ8 and use this understanding to improve the method. However, intuitive statistical explanations of the behavior of other modules remain elusive. We hope the description in this report will be a starting point for discussions that will increase our understanding, lead to improvements to PAQ8, and facilitate a transfer of knowledge from PAQ8 to other machine learning methods, such a recurrent neural networks and stochastic memoizers. Finally, the report presents a broad range of new applications of PAQ to machine learning tasks including language modeling and adaptive text prediction, adaptive game playing, classification, and compression using features from the field of deep learning.\n    ",
        "submission_date": "2011-08-16T00:00:00",
        "last_modified_date": "2011-08-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.4973",
        "title": "Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields",
        "authors": [
            "Alexandre L. M. Levada"
        ],
        "abstract": "Markov Random Field models are powerful tools for the study of complex systems. However, little is known about how the interactions between the elements of such systems are encoded, especially from an information-theoretic perspective. In this paper, our goal is to enlight the connection between Fisher information, Shannon entropy, information geometry and the behavior of complex systems modeled by isotropic pairwise Gaussian Markov random fields. We propose analytical expressions to compute local and global versions of these measures using Besag's pseudo-likelihood function, characterizing the system's behavior through its \\emph{Fisher curve}, a parametric trajectory accross the information space that provides a geometric representation for the study of complex systems. Computational experiments show how the proposed tools can be useful in extrating relevant information from complex patterns. The obtained results quantify and support our main conclusion, which is: in terms of information, moving towards higher entropy states (A --> B) is different from moving towards lower entropy states (B --> A), since the \\emph{Fisher curves} are not the same given a natural orientation (the direction of time).\n    ",
        "submission_date": "2011-08-25T00:00:00",
        "last_modified_date": "2013-10-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5359",
        "title": "Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering",
        "authors": [
            "Risheng Liu",
            "Zhouchen Lin",
            "Siming Wei",
            "Zhixun Su"
        ],
        "abstract": "In the past decades, exactly recovering the intrinsic data structure from corrupted observations, which is known as robust principal component analysis (RPCA), has attracted tremendous interests and found many applications in computer vision. Recently, this problem has been formulated as recovering a low-rank component and a sparse component from the observed data matrix. It is proved that under some suitable conditions, this problem can be exactly solved by principal component pursuit (PCP), i.e., minimizing a combination of nuclear norm and $l_1$ norm. Most of the existing methods for solving PCP require singular value decompositions (SVD) of the data matrix, resulting in a high computational complexity, hence preventing the applications of RPCA to very large scale computer vision problems. In this paper, we propose a novel algorithm, called $l_1$ filtering, for \\emph{exactly} solving PCP with an $O(r^2(m+n))$ complexity, where $m\\times n$ is the size of data matrix and $r$ is the rank of the matrix to recover, which is supposed to be much smaller than $m$ and $n$. Moreover, $l_1$ filtering is \\emph{highly parallelizable}. It is the first algorithm that can \\emph{exactly} solve a nuclear norm minimization problem in \\emph{linear time} (with respect to the data size). Experiments on both synthetic data and real applications testify to the great advantage of $l_1$ filtering in speed over state-of-the-art algorithms.\n    ",
        "submission_date": "2011-08-26T00:00:00",
        "last_modified_date": "2012-05-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1108.5395",
        "title": "Noise Covariance Properties in Dual-Tree Wavelet Decompositions",
        "authors": [
            "Caroline Chaux",
            "Jean-Christophe Pesquet",
            "Laurent Duval"
        ],
        "abstract": "Dual-tree wavelet decompositions have recently gained much popularity, mainly due to their ability to provide an accurate directional analysis of images combined with a reduced redundancy. When the decomposition of a random process is performed -- which occurs in particular when an additive noise is corrupting the signal to be analyzed -- it is useful to characterize the statistical properties of the dual-tree wavelet coefficients of this process. As dual-tree decompositions constitute overcomplete frame expansions, correlation structures are introduced among the coefficients, even when a white noise is analyzed. In this paper, we show that it is possible to provide an accurate description of the covariance properties of the dual-tree coefficients of a wide-sense stationary process. The expressions of the (cross-)covariance sequences of the coefficients are derived in the one and two-dimensional cases. Asymptotic results are also provided, allowing to predict the behaviour of the second-order moments for large lag values or at coarse resolution. In addition, the cross-correlations between the primal and dual wavelets, which play a primary role in our theoretical analysis, are calculated for a number of classical wavelet families. Simulation results are finally provided to validate these results.\n    ",
        "submission_date": "2011-08-26T00:00:00",
        "last_modified_date": "2011-08-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0217",
        "title": "Vessel Segmentation in Medical Imaging Using a Tight-Frame Based Algorithm",
        "authors": [
            "Xiaohao Cai",
            "Raymond Chan",
            "Serena Morigi",
            "Fiorella Sgallari"
        ],
        "abstract": "Tight-frame, a generalization of orthogonal wavelets, has been used successfully in various problems in image processing, including inpainting, impulse noise removal, super-resolution image restoration, etc. Segmentation is the process of identifying object outlines within images. There are quite a few efficient algorithms for segmentation that depend on the variational approach and the partial differential equation (PDE) modeling.\n",
        "submission_date": "2011-08-13T00:00:00",
        "last_modified_date": "2011-08-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.0820",
        "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing",
        "authors": [
            "Shai Shalev-Shwartz",
            "Yonatan Wexler",
            "Amnon Shashua"
        ],
        "abstract": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.\n    ",
        "submission_date": "2011-09-05T00:00:00",
        "last_modified_date": "2011-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1149",
        "title": "On Partial Opimality by Auxiliary Submodular Problems",
        "authors": [
            "Alexander Shekhovtsov",
            "Vaclav Hlavac"
        ],
        "abstract": "In this work, we prove several relations between three different energy minimization techniques. A recently proposed methods for determining a provably optimal partial assignment of variables by Ivan Kovtun (IK), the linear programming relaxation approach (LP) and the popular expansion move algorithm by Yuri Boykov. We propose a novel sufficient condition of optimal partial assignment, which is based on LP relaxation and called LP-autarky. We show that methods of Kovtun, which build auxiliary submodular problems, fulfill this sufficient condition. The following link is thus established: LP relaxation cannot be tightened by IK. For non-submodular problems this is a non-trivial result. In the case of two labels, LP relaxation provides optimal partial assignment, known as persistency, which, as we show, dominates IK. Relating IK with expansion move, we show that the set of fixed points of expansion move with any \"truncation\" rule for the initial problem and the problem restricted by one-vs-all method of IK would coincide -- i.e. expansion move cannot be improved by this method. In the case of two labels, expansion move with a particular truncation rule coincide with one-vs-all method.\n    ",
        "submission_date": "2011-09-06T00:00:00",
        "last_modified_date": "2011-09-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.1646",
        "title": "Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation",
        "authors": [
            "Guangcan Liu",
            "Huan Xu",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, we address the following matrix recovery problem: suppose we are given a set of data points containing two parts, one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers, or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces, efficiently and exactly? We utilize a so-called {\\em Low-Rank Representation} (LRR) method to solve this problem, and prove that under mild technical conditions, any solution to LRR exactly recovers the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space, this further implies that LRR can perform exact subspace segmentation and outlier detection, in an efficient way.\n    ",
        "submission_date": "2011-09-08T00:00:00",
        "last_modified_date": "2014-03-30T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2227",
        "title": "A radial version of the Central Limit Theorem",
        "authors": [
            "Kunal Narayan Chaudhury"
        ],
        "abstract": "In this note, we give a probabilistic interpretation of the Central Limit Theorem used for approximating isotropic Gaussians in [1].\n    ",
        "submission_date": "2011-09-10T00:00:00",
        "last_modified_date": "2011-09-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2304",
        "title": "Efficient Minimization of Higher Order Submodular Functions using Monotonic Boolean Functions",
        "authors": [
            "Srikumar Ramalingam",
            "Chris Russell",
            "Lubor Ladicky",
            "Philip H.S. Torr"
        ],
        "abstract": "Submodular function minimization is a key problem in a wide variety of applications in machine learning, economics, game theory, computer vision, and many others. The general solver has a complexity of $O(n^3 \\log^2 n . E +n^4 {\\log}^{O(1)} n)$ where $E$ is the time required to evaluate the function and $n$ is the number of variables \\cite{Lee2015}. On the other hand, many computer vision and machine learning problems are defined over special subclasses of submodular functions that can be written as the sum of many submodular cost functions defined over cliques containing few variables. In such functions, the pseudo-Boolean (or polynomial) representation \\cite{BorosH02} of these subclasses are of degree (or order, or clique size) $k$ where $k \\ll n$. In this work, we develop efficient algorithms for the minimization of this useful subclass of submodular functions. To do this, we define novel mapping that transform submodular functions of order $k$ into quadratic ones. The underlying idea is to use auxiliary variables to model the higher order terms and the transformation is found using a carefully constructed linear program. In particular, we model the auxiliary variables as monotonic Boolean functions, allowing us to obtain a compact transformation using as few auxiliary variables as possible.\n    ",
        "submission_date": "2011-09-11T00:00:00",
        "last_modified_date": "2017-01-23T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.2388",
        "title": "MIS-Boost: Multiple Instance Selection Boosting",
        "authors": [
            "Emre Akbas",
            "Bernard Ghanem",
            "Narendra Ahuja"
        ],
        "abstract": "In this paper, we present a new multiple instance learning (MIL) method, called MIS-Boost, which learns discriminative instance prototypes by explicit instance selection in a boosting framework. Unlike previous instance selection based MIL methods, we do not restrict the prototypes to a discrete set of training instances but allow them to take arbitrary values in the instance feature space. We also do not restrict the total number of prototypes and the number of selected-instances per bag; these quantities are completely data-driven. We show that MIS-Boost outperforms state-of-the-art MIL methods on a number of benchmark datasets. We also apply MIS-Boost to large-scale image classification, where we show that the automatically selected prototypes map to visually meaningful image regions.\n    ",
        "submission_date": "2011-09-12T00:00:00",
        "last_modified_date": "2011-09-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.3827",
        "title": "Online Robust Subspace Tracking from Partial Information",
        "authors": [
            "Jun He",
            "Laura Balzano",
            "John C.S. Lui"
        ],
        "abstract": "This paper presents GRASTA (Grassmannian Robust Adaptive Subspace Tracking Algorithm), an efficient and robust online algorithm for tracking subspaces from highly incomplete information. The algorithm uses a robust $l^1$-norm cost function in order to estimate and track non-stationary subspaces when the streaming data vectors are corrupted with outliers. We apply GRASTA to the problems of robust matrix completion and real-time separation of background from foreground in video. In this second application, we show that GRASTA performs high-quality separation of moving objects from background at exceptional speeds: In one popular benchmark video example, GRASTA achieves a rate of 57 frames per second, even when run in MATLAB on a personal laptop.\n    ",
        "submission_date": "2011-09-18T00:00:00",
        "last_modified_date": "2011-09-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1109.5323",
        "title": "Squiggle - A Glyph Recognizer for Gesture Input",
        "authors": [
            "Jeremy Lee"
        ],
        "abstract": "Squiggle is a template-based glyph recognizer in the lineage of `$1 Recognizer' and `Protractor'. It seeks a good fit linear affine mapping between the input and template glyphs which are represented as a list of milestone points along the glyph path. The algorithm can recognize input glyphs invariant of rotation, scaling, skew, and reflection symmetries. In practice the algorithm is fast and robust enough to recognize user-generated glyphs as they are being drawn in real time, and to project `shadows' of the matching templates as feedback.\n    ",
        "submission_date": "2011-09-25T00:00:00",
        "last_modified_date": "2011-09-25T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0061",
        "title": "Learning image transformations without training examples",
        "authors": [
            "Sergey Pankov"
        ],
        "abstract": "The use of image transformations is essential for efficient modeling and learning of visual data. But the class of relevant transformations is large: affine transformations, projective transformations, elastic deformations, ... the list goes on. Therefore, learning these transformations, rather than hand coding them, is of great conceptual interest. To the best of our knowledge, all the related work so far has been concerned with either supervised or weakly supervised learning (from correlated sequences, video streams, or image-transform pairs). In this paper, on the contrary, we present a simple method for learning affine and elastic transformations when no examples of these transformations are explicitly given, and no prior knowledge of space (such as ordering of pixels) is included either. The system has only access to a moderately large database of natural images arranged in no particular order.\n    ",
        "submission_date": "2011-10-01T00:00:00",
        "last_modified_date": "2011-10-01T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0169",
        "title": "Robust artificial neural networks and outlier detection. Technical report",
        "authors": [
            "Gleb Beliakov",
            "Andrei Kelarev",
            "John Yearwood"
        ],
        "abstract": "Large outliers break down linear and nonlinear regression models. Robust regression methods allow one to filter out the outliers when building a model. By replacing the traditional least squares criterion with the least trimmed squares criterion, in which half of data is treated as potential outliers, one can fit accurate regression models to strongly contaminated data. High-breakdown methods have become very well established in linear regression, but have started being applied for non-linear regression only recently. In this work, we examine the problem of fitting artificial neural networks to contaminated data using least trimmed squares criterion. We introduce a penalized least trimmed squares criterion which prevents unnecessary removal of valid data. Training of ANNs leads to a challenging non-smooth global optimization problem. We compare the efficiency of several derivative-free optimization methods in solving it, and show that our approach identifies the outliers correctly when ANNs are used for nonlinear regression.\n    ",
        "submission_date": "2011-10-02T00:00:00",
        "last_modified_date": "2011-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0214",
        "title": "Eclectic Extraction of Propositional Rules from Neural Networks",
        "authors": [
            "Ridwan Al Iqbal"
        ],
        "abstract": "Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a \"Black Box\" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.\n    ",
        "submission_date": "2011-10-02T00:00:00",
        "last_modified_date": "2011-10-02T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0641",
        "title": "Identifying relationships between drugs and medical conditions: winning experience in the Challenge 2 of the OMOP 2010 Cup",
        "authors": [
            "Vladimir Nikulin"
        ],
        "abstract": "There is a growing interest in using a longitudinal observational databases to detect drug safety signal. In this paper we present a novel method, which we used online during the OMOP Cup. We consider homogeneous ensembling, which is based on random re-sampling (known, also, as bagging) as a main innovation compared to the previous publications in the related field. This study is based on a very large simulated database of the 10 million patients records, which was created by the Observational Medical Outcomes Partnership (OMOP). Compared to the traditional classification problem, the given data are unlabelled. The objective of this study is to discover hidden associations between drugs and conditions. The main idea of the approach, which we used during the OMOP Cup is to compare the numbers of observed and expected patterns. This comparison may be organised in several different ways, and the outcomes (base learners) may be quite different as well. It is proposed to construct the final decision function as an ensemble of the base learners. Our method was recognised formally by the Organisers of the OMOP Cup as a top performing method for the Challenge N2.\n    ",
        "submission_date": "2011-10-04T00:00:00",
        "last_modified_date": "2011-10-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.0957",
        "title": "Dictionary Learning for Deblurring and Digital Zoom",
        "authors": [
            "Florent Couzinie-Devy",
            "Julien Mairal",
            "Francis Bach",
            "Jean Ponce"
        ],
        "abstract": "This paper proposes a novel approach to image deblurring and digital zooming using sparse local models of image appearance. These models, where small image patches are represented as linear combinations of a few elements drawn from some large set (dictionary) of candidates, have proven well adapted to several image restoration tasks. A key to their success has been to learn dictionaries adapted to the reconstruction of small image patches. In contrast, recent works have proposed instead to learn dictionaries which are not only adapted to data reconstruction, but also tuned for a specific task. We introduce here such an approach to deblurring and digital zoom, using pairs of blurry/sharp (or low-/high-resolution) images for training, as well as an effective stochastic gradient algorithm for solving the corresponding optimization task. Although this learning problem is not convex, once the dictionaries have been learned, the sharp/high-resolution image can be recovered via convex optimization at test time. Experiments with synthetic and real data demonstrate the effectiveness of the proposed approach, leading to state-of-the-art performance for non-blind image deblurring and digital zoom.\n    ",
        "submission_date": "2011-10-05T00:00:00",
        "last_modified_date": "2011-10-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.1358",
        "title": "Runtime Guarantees for Regression Problems",
        "authors": [
            "Hui Han Chin",
            "Aleksander Madry",
            "Gary Miller",
            "Richard Peng"
        ],
        "abstract": "We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems. these problems are motivated by the lasso framework and have applications in machine learning and computer vision.\n",
        "submission_date": "2011-10-06T00:00:00",
        "last_modified_date": "2012-09-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2306",
        "title": "Ground Metric Learning",
        "authors": [
            "Marco Cuturi",
            "David Avis"
        ],
        "abstract": "Transportation distances have been used for more than a decade now in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, transportation distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of transportation distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two polyhedral convex functions over a convex set of distance matrices. We follow the presentation of our algorithms with promising experimental results on binary classification tasks using GIST descriptors of images taken in the Caltech-256 set.\n    ",
        "submission_date": "2011-10-11T00:00:00",
        "last_modified_date": "2011-10-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.2855",
        "title": "Sparse Image Representation with Epitomes",
        "authors": [
            "Louise Beno\u00eet",
            "Julien Mairal",
            "Francis Bach",
            "Jean Ponce"
        ],
        "abstract": "Sparse coding, which is the decomposition of a vector using only a few basis elements, is widely used in machine learning and image processing. The basis set, also called dictionary, is learned to adapt to specific data. This approach has proven to be very effective in many image processing tasks. Traditionally, the dictionary is an unstructured \"flat\" set of atoms. In this paper, we study structured dictionaries which are obtained from an epitome, or a set of epitomes. The epitome is itself a small image, and the atoms are all the patches of a chosen size inside this image. This considerably reduces the number of parameters to learn and provides sparse image decompositions with shiftinvariance properties. We propose a new formulation and an algorithm for learning the structured dictionaries associated with epitomes, and illustrate their use in image denoising tasks.\n    ",
        "submission_date": "2011-10-13T00:00:00",
        "last_modified_date": "2011-10-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3649",
        "title": "Algorithms to automatically quantify the geometric similarity of anatomical surfaces",
        "authors": [
            "D. Boyer",
            "Y. Lipman",
            "E. St. Clair",
            "J. Puente",
            "T. Funkhouser",
            "B. Patel",
            "J. Jernvall",
            "I. Daubechies"
        ],
        "abstract": "We describe new approaches for distances between pairs of 2-dimensional surfaces (embedded in 3-dimensional space) that use local structures and global information contained in inter-structure geometric relationships. We present algorithms to automatically determine these distances as well as geometric correspondences. This is motivated by the aspiration of students of natural science to understand the continuity of form that unites the diversity of life. At present, scientists using physical traits to study evolutionary relationships among living and extinct animals analyze data extracted from carefully defined anatomical correspondence points (landmarks). Identifying and recording these landmarks is time consuming and can be done accurately only by trained morphologists. This renders these studies inaccessible to non-morphologists, and causes phenomics to lag behind genomics in elucidating evolutionary patterns. Unlike other algorithms presented for morphological correspondences our approach does not require any preliminary marking of special features or landmarks by the user. It also differs from other seminal work in computational geometry in that our algorithms are polynomial in nature and thus faster, making pairwise comparisons feasible for significantly larger numbers of digitized surfaces. We illustrate our approach using three datasets representing teeth and different bones of primates and humans, and show that it leads to highly accurate results.\n    ",
        "submission_date": "2011-10-17T00:00:00",
        "last_modified_date": "2012-03-15T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3741",
        "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis",
        "authors": [
            "Ko-Jen Hsiao",
            "Kevin S. Xu",
            "Jeff Calder",
            "Alfred O. Hero III"
        ],
        "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.\n    ",
        "submission_date": "2011-10-17T00:00:00",
        "last_modified_date": "2013-01-07T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.3907",
        "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem",
        "authors": [
            "Peng Sun",
            "Mark D. Reid",
            "Jie Zhou"
        ],
        "abstract": "This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter's more careful treatment of the above two factors.\n",
        "submission_date": "2011-10-18T00:00:00",
        "last_modified_date": "2012-07-04T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.5097",
        "title": "Absolute Uniqueness of Phase Retrieval with Random Illumination",
        "authors": [
            "Albert Fannjiang"
        ],
        "abstract": "Random illumination is proposed to enforce absolute uniqueness and resolve all types of ambiguity, trivial or nontrivial, from phase retrieval. Almost sure irreducibility is proved for any complex-valued object of a full rank support. While the new irreducibility result can be viewed as a probabilistic version of the classical result by Bruck, Sodin and Hayes, it provides a novel perspective and an effective method for phase retrieval.\n",
        "submission_date": "2011-10-23T00:00:00",
        "last_modified_date": "2012-07-20T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1110.6483",
        "title": "Iris Codes Classification Using Discriminant and Witness Directions",
        "authors": [
            "N. Popescu-Bodorin",
            "V. E. Balas",
            "I. M. Motoc"
        ],
        "abstract": "The main topic discussed in this paper is how to use intelligence for biometric decision defuzzification. A neural training model is proposed and tested here as a possible solution for dealing with natural fuzzification that appears between the intra- and inter-class distribution of scores computed during iris recognition tests. It is shown here that the use of proposed neural network support leads to an improvement in the artificial perception of the separation between the intra- and inter-class score distributions by moving them away from each other.\n    ",
        "submission_date": "2011-10-28T00:00:00",
        "last_modified_date": "2011-11-08T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0268",
        "title": "Topology on locally finite metric spaces",
        "authors": [
            "Valerio Capraro"
        ],
        "abstract": "The necessity of a theory of General Topology and, most of all, of Algebraic Topology on locally finite metric spaces comes from many areas of research in both Applied and Pure Mathematics: Molecular Biology, Mathematical Chemistry, Computer Science, Topological Graph Theory and Metric Geometry. In this paper we propose the basic notions of such a theory and some applications: we replace the classical notions of continuous function, homeomorphism and homotopic equivalence with the notions of NPP-function, NPP-local-isomorphism and NPP-homotopy (NPP stands for Nearest Point Preserving); we also introduce the notion of NPP-isomorphism. We construct three invariants under NPP-isomorphisms and, in particular, we define the fundamental group of a locally finite metric space. As first applications, we propose the following: motivated by the longstanding question whether there is a purely metric condition which extends the notion of amenability of a group to any metric space, we propose the property SN (Small Neighborhood); motivated by some applicative problems in Computer Science, we prove the analog of the Jordan curve theorem in $\\mathbb Z^2$; motivated by a question asked during a lecture at Lausanne, we extend to any locally finite metric space a recent inequality of ",
        "submission_date": "2011-11-01T00:00:00",
        "last_modified_date": "2011-11-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.0654",
        "title": "Distributed Lossy Source Coding Using Real-Number Codes",
        "authors": [
            "Mojtaba Vaezi",
            "Fabrice Labeau"
        ],
        "abstract": "We show how real-number codes can be used to compress correlated sources, and establish a new framework for lossy distributed source coding, in which we quantize compressed sources instead of compressing quantized sources. This change in the order of binning and quantization blocks makes it possible to model correlation between continuous-valued sources more realistically and correct quantization error when the sources are completely correlated. The encoding and decoding procedures are described in detail, for discrete Fourier transform (DFT) codes. Reconstructed signal, in the mean squared error sense, is seen to be better than that in the conventional approach.\n    ",
        "submission_date": "2011-11-02T00:00:00",
        "last_modified_date": "2012-06-19T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.1373",
        "title": "Speculative Parallel Evaluation Of Classification Trees On GPGPU Compute Engines",
        "authors": [
            "Jason Spencer"
        ],
        "abstract": "We examine the problem of optimizing classification tree evaluation for on-line and real-time applications by using GPUs. Looking at trees with continuous attributes often used in image segmentation, we first put the existing algorithms for serial and data-parallel evaluation on solid footings. We then introduce a speculative parallel algorithm designed for single instruction, multiple data (SIMD) architectures commonly found in GPUs. A theoretical analysis shows how the run times of data and speculative decompositions compare assuming independent processors. To compare the algorithms in the SIMD environment, we implement both on a CUDA 2.0 architecture machine and compare timings to a serial CPU implementation. Various optimizations and their effects are discussed, and results are given for all algorithms. Our specific tests show a speculative algorithm improves run time by 25% compared to a data decomposition.\n    ",
        "submission_date": "2011-11-06T00:00:00",
        "last_modified_date": "2011-11-06T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5358",
        "title": "Contextually Guided Semantic Labeling and Search for 3D Point Clouds",
        "authors": [
            "Abhishek Anand",
            "Hema Swetha Koppula",
            "Thorsten Joachims",
            "Ashutosh Saxena"
        ],
        "abstract": "RGB-D cameras, which give an RGB image to- gether with depths, are becoming increasingly popular for robotic perception. In this paper, we address the task of detecting commonly found objects in the 3D point cloud of indoor scenes obtained from such cameras. Our method uses a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model's parsimony becomes important and we address that by using multiple types of edge potentials. We train the model using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views), we get a performance of 84.06% and 73.38% in labeling office and home scenes respectively for 17 object classes each. We also present a method for a robot to search for an object using the learned model and the contextual information available from the current labelings of the scene. We applied this algorithm successfully on a mobile robot for the task of finding 12 object classes in 10 different offices and achieved a precision of 97.56% with 78.43% recall.\n    ",
        "submission_date": "2011-11-22T00:00:00",
        "last_modified_date": "2012-09-05T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.5867",
        "title": "Suboptimality of Nonlocal Means for Images with Sharp Edges",
        "authors": [
            "Arian Maleki",
            "Manjari Narayan",
            "Richard G. Baraniuk"
        ],
        "abstract": "We conduct an asymptotic risk analysis of the nonlocal means image denoising algorithm for the Horizon class of images that are piecewise constant with a sharp edge discontinuity. We prove that the mean square risk of an optimally tuned nonlocal means algorithm decays according to $n^{-1}\\log^{1/2+\\epsilon} n$, for an $n$-pixel image with $\\epsilon>0$. This decay rate is an improvement over some of the predecessors of this algorithm, including the linear convolution filter, median filter, and the SUSAN filter, each of which provides a rate of only $n^{-2/3}$. It is also within a logarithmic factor from optimally tuned wavelet thresholding. However, it is still substantially lower than the the optimal minimax rate of $n^{-4/3}$.\n    ",
        "submission_date": "2011-11-24T00:00:00",
        "last_modified_date": "2011-11-24T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6285",
        "title": "Ward's Hierarchical Clustering Method: Clustering Criterion and Agglomerative Algorithm",
        "authors": [
            "Fionn Murtagh",
            "Pierre Legendre"
        ],
        "abstract": "The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. However there are different interpretations in the literature and there are different implementations of the Ward agglomerative algorithm in commonly used software systems, including differing expressions of the agglomerative criterion. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward's hierarchical clustering method.\n    ",
        "submission_date": "2011-11-27T00:00:00",
        "last_modified_date": "2011-12-11T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6387",
        "title": "3D Model Retrieval Based on Semantic and Shape Indexes",
        "authors": [
            "My Abdellah Kassimi",
            "Omar El beqqali"
        ],
        "abstract": "The size of 3D models used on the web or stored in databases is becoming increasingly high. Then, an efficient method that allows users to find similar 3D objects for a given 3D model query has become necessary. Keywords and the geometry of a 3D model cannot meet the needs of users' retrieval because they do not include the semantic information. In this paper, a new method has been proposed to 3D models retrieval using semantic concepts combined with shape indexes. To obtain these concepts, we use the machine learning methods to label 3D models by k-means algorithm in measures and shape indexes space. Moreover, semantic concepts have been organized and represented by ontology language OWL and spatial relationships are used to disambiguate among models of similar appearance. The SPARQL query language has been used to question the information displayed in this language and to compute the similarity between two 3D models. We interpret our results using the Princeton Shape Benchmark Database and the results show the performance of the proposed new approach to retrieval 3D models. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic, ontology\n    ",
        "submission_date": "2011-11-28T00:00:00",
        "last_modified_date": "2011-11-28T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.6923",
        "title": "Efficient Adaptive Compressive Sensing Using Sparse Hierarchical Learned Dictionaries",
        "authors": [
            "Akshay Soni",
            "Jarvis Haupt"
        ],
        "abstract": "Recent breakthrough results in compressed sensing (CS) have established that many high dimensional objects can be accurately recovered from a relatively small number of non- adaptive linear projection observations, provided that the objects possess a sparse representation in some basis. Subsequent efforts have shown that the performance of CS can be improved by exploiting the structure in the location of the non-zero signal coefficients (structured sparsity) or using some form of online measurement focusing (adaptivity) in the sensing process. In this paper we examine a powerful hybrid of these two techniques. First, we describe a simple adaptive sensing procedure and show that it is a provably effective method for acquiring sparse signals that exhibit structured sparsity characterized by tree-based coefficient dependencies. Next, employing techniques from sparse hierarchical dictionary learning, we show that representations exhibiting the appropriate form of structured sparsity can be learned from collections of training data. The combination of these techniques results in an effective and efficient adaptive compressive acquisition procedure.\n    ",
        "submission_date": "2011-11-29T00:00:00",
        "last_modified_date": "2011-11-29T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1111.7100",
        "title": "Determining a rotation of a tetrahedron from a projection",
        "authors": [
            "Richard J. Gardner",
            "Paolo Gronchi",
            "Thorsten Theobald"
        ],
        "abstract": "The following problem, arising from medical imaging, is addressed: Suppose that $T$ is a known tetrahedron in $\\R^3$ with centroid at the origin. Also known is the orthogonal projection $U$ of the vertices of the image $\\phi T$ of $T$ under an unknown rotation $\\phi$ about the origin. Under what circumstances can $\\phi$ be determined from $T$ and $U$?\n    ",
        "submission_date": "2011-11-30T00:00:00",
        "last_modified_date": "2012-07-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.2095",
        "title": "Real-time face swapping as a tool for understanding infant self-recognition",
        "authors": [
            "Sao Mai Nguyen",
            "Masaki Ogino",
            "Minoru Asada"
        ],
        "abstract": "To study the preference of infants for contingency of movements and familiarity of faces during self-recognition task, we built, as an accurate and instantaneous imitator, a real-time face- swapper for videos. We present a non-constraint face-swapper based on 3D visual tracking that achieves real-time performance through parallel computing. Our imitator system is par- ticularly suited for experiments involving children with Autistic Spectrum Disorder who are often strongly disturbed by the constraints of other methods.\n    ",
        "submission_date": "2011-12-09T00:00:00",
        "last_modified_date": "2011-12-09T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3059",
        "title": "Data Processing For Atomic Resolution EELS",
        "authors": [
            "Paul Cueva",
            "Robert Hovden",
            "Julia A. Mundy",
            "Huolin L. Xin",
            "David A. Muller"
        ],
        "abstract": "The high beam current and sub-angstrom resolution of aberration-corrected scanning transmission electron microscopes has enabled electron energy loss spectroscopic (EELS) mapping with atomic resolution. These spectral maps are often dose-limited and spatially oversampled, leading to low counts/channel and are thus highly sensitive to errors in background estimation. However, by taking advantage of redundancy in the dataset map one can improve background estimation and increase chemical sensitivity. We consider two such approaches- linear combination of power laws and local background averaging-that reduce background error and improve signal extraction. Principal components analysis (PCA) can also be used to analyze spectrum images, but the poor peak-to-background ratio in EELS can lead to serious artifacts if raw EELS data is PCA filtered. We identify common artifacts and discuss alternative approaches. These algorithms are implemented within the Cornell Spectrum Imager, an open source software package for spectroscopic analysis.\n    ",
        "submission_date": "2011-12-13T00:00:00",
        "last_modified_date": "2011-12-13T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3110",
        "title": "GPU-based Image Analysis on Mobile Devices",
        "authors": [
            "Andrew Ensor",
            "Seth Hall"
        ],
        "abstract": "With the rapid advances in mobile technology many mobile devices are capable of capturing high quality images and video with their embedded camera. This paper investigates techniques for real-time processing of the resulting images, particularly on-device utilizing a graphical processing unit. Issues and limitations of image processing on mobile devices are discussed, and the performance of graphical processing units on a range of devices measured through a programmable shader implementation of Canny edge detection.\n    ",
        "submission_date": "2011-12-14T00:00:00",
        "last_modified_date": "2011-12-14T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.3972",
        "title": "Developing Autonomic Properties for Distributed Pattern-Recognition Systems with ASSL: A Distributed MARF Case Study",
        "authors": [
            "Emil Vassev",
            "Serguei A. Mokhov"
        ],
        "abstract": "In this paper, we discuss our research towards developing special properties that introduce autonomic behavior in pattern-recognition systems. In our approach we use ASSL (Autonomic System Specification Language) to formally develop such properties for DMARF (Distributed Modular Audio Recognition Framework). These properties enhance DMARF with an autonomic middleware that manages the four stages of the framework's pattern-recognition pipeline. DMARF is a biologically inspired system employing pattern recognition, signal processing, and natural language processing helping us process audio, textual, or imagery data needed by a variety of scientific applications, e.g., biometric applications. In that context, the notion go autonomic DMARF (ADMARF) can be employed by autonomous and robotic systems that theoretically require less-to-none human intervention other than data collection for pattern analysis and observing the results. In this article, we explain the ASSL specification models for the autonomic properties of DMARF.\n    ",
        "submission_date": "2011-12-16T00:00:00",
        "last_modified_date": "2011-12-16T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.4434",
        "title": "Oracle inequalities and minimax rates for non-local means and related adaptive kernel-based methods",
        "authors": [
            "Ery Arias-Castro",
            "Joseph Salmon",
            "Rebecca Willett"
        ],
        "abstract": "This paper describes a novel theoretical characterization of the performance of non-local means (NLM) for noise removal. NLM has proven effective in a variety of empirical studies, but little is understood fundamentally about how it performs relative to classical methods based on wavelets or how various parameters (e.g., patch size) should be chosen. For cartoon images and images which may contain thin features and regular textures, the error decay rates of NLM are derived and compared with those of linear filtering, oracle estimators, variable-bandwidth kernel methods, Yaroslavsky's filter and wavelet thresholding estimators. The trade-off between global and local search for matching patches is examined, and the bias reduction associated with the local polynomial regression version of NLM is analyzed. The theoretical results are validated via simulations for 2D images corrupted by additive white Gaussian noise.\n    ",
        "submission_date": "2011-12-19T00:00:00",
        "last_modified_date": "2012-04-26T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.5771",
        "title": "On B-spline framelets derived from the unitary extension principle",
        "authors": [
            "Zuowei Shen",
            "Zhiqiang Xu"
        ],
        "abstract": "Spline wavelet tight frames of Ron-Shen have been used widely in frame based image analysis and restorations. However, except for the tight frame property and the approximation order of the truncated series, there are few other properties of this family of spline wavelet tight frames to be known. This paper is to present a few new properties of this family that will provide further understanding of it and, hopefully, give some indications why it is efficient in image analysis and restorations. In particular, we present a recurrence formula of computing generators of higher order spline wavelet tight frames from the lower order ones. We also represent each generator of spline wavelet tight frames as certain order of derivative of some univariate box spline. With this, we further show that each generator of sufficiently high order spline wavelet tight frames is close to a right order of derivative of a properly scaled Gaussian function. This leads to the result that the wavelet system generated by a finitely many consecutive derivatives of a properly scaled Gaussian function forms a frame whose frame bounds can be almost tight.\n    ",
        "submission_date": "2011-12-25T00:00:00",
        "last_modified_date": "2012-01-12T00:00:00"
    },
    {
        "url": "https://arxiv.org/abs/1112.6371",
        "title": "Multi-q Analysis of Image Patterns",
        "authors": [
            "Ricardo Fabbri",
            "Wesley N. Gon\u00e7alves",
            "Francisco J. P. Lopes",
            "Odemir M. Bruno"
        ],
        "abstract": "This paper studies the use of the Tsallis Entropy versus the classic Boltzmann-Gibbs-Shannon entropy for classifying image patterns. Given a database of 40 pattern classes, the goal is to determine the class of a given image sample. Our experiments show that the Tsallis entropy encoded in a feature vector for different $q$ indices has great advantage over the Boltzmann-Gibbs-Shannon entropy for pattern classification, boosting recognition rates by a factor of 3. We discuss the reasons behind this success, shedding light on the usefulness of the Tsallis entropy.\n    ",
        "submission_date": "2011-12-29T00:00:00",
        "last_modified_date": "2011-12-29T00:00:00"
    }
]